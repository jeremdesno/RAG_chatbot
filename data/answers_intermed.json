{"1fc3948b-1d17-41b0-928a-43bcd384050d": "Base permissions in the context of an organization on GitHub refer to the default access level that all members of an organization will have when accessing any of the organization's repositories, unless they are granted a higher level of access for a specific repository. These permissions can be set by an administrator in the organization's settings and can be changed at any time, affecting both new and existing members. By default, members of an organization will have read permissions, but this can be adjusted to a different level of access, such as write or admin. Base permissions do not apply to outside collaborators.", "dee873bd-3e8b-4405-9210-d668a91111c6": "A user can report abuse or spam on GitHub by following these steps:  1. To report a user, navigate to their profile page and click the \"Block or report\" option. Then, complete the contact form to report the user's behavior and click \"Send request.\"  2. To report an issue, pull request, or comment, click the \"Report content\" option in the upper-right corner of the item. Then, select an email address and a category, and click \"Send request.\"  3. To report a discussion, click the \"Report content\" option in the upper-right corner of the discussion. Then, select an email address and a category, and click \"Send request.\"  4. To report an app in GitHub Marketplace, browse the marketplace and report the app by following the instructions provided by the app's developer.  Types of content that can be reported include users who violate GitHub's Community Guidelines or Terms of Service, issues, pull requests, comments, and discussions that contain abusive or spammy content. Apps in GitHub Marketplace can also be reported if they violate GitHub's policies or if they contain malicious or unwanted content.", "8e58dab0-9ef9-4306-966a-cc2a123c2e07": "The {% data variables.product.prodname_actions %} platform enables creators to verify their identity and maintain the trust of their users by providing a verified creator badge. This badge indicates that the creator has gone through a verification process to confirm their identity and ensure the authenticity and security of their actions. This feature helps users to easily identify trustworthy actions and reduces the risk of using actions from unverified creators, thereby enhancing the overall trust and reliability of the {% data variables.product.prodname_actions %} platform.", "416870f3-8dfa-41a5-9921-b4d95ba81697": "The URL that enterprise members should use for Git operations over SSH if their organization requires SSH certificates is provided by the organization that requires SSH certificates. This URL should be used instead of the default Git URL for accessing the organization's repositories using SSH. The URL can be found in the organization's settings or by contacting the organization's administrator.", "2b4737da-0804-4dc5-b0d6-0e0dbbce4566": "In the context provided, a Commit URL is a direct link to a specific commit in a repository, while a SHA reference is a unique identifier assigned to a specific commit. While a Commit URL can be used to navigate directly to the commit, a SHA reference can be used to reference the commit in other contexts, such as in a Git command or in a GitHub issue or pull request. Essentially, a Commit URL is a human-readable way to access a commit, while a SHA reference is a machine-readable way to reference a commit.", "5c363f70-f4bd-4dec-8407-da5ad89d0170": "To create a migration inventory for your GitHub repository using the gh-repo-stats tool, follow these steps:  1. Install the gh-repo-stats tool by running the following command in your terminal:     ```    npm install -g gh-repo-stats    ```  2. Navigate to the directory where you want to save the CSV file.  3. Run the following command to generate the migration inventory:     ```    gh-repo-stats --json --repo <your-github-repo> --token <your-github-token> > migration-inventory.json    ```     Replace `<your-github-repo>` with the name of your GitHub repository, and `<your-github-token>` with your personal access token.  4. The tool will generate a JSON file named `migration-inventory.json` in the current directory.  5. To convert the JSON file to a CSV file, run the following command:     ```    jq -r 'tojson(.) | tostring' migration-inventory.json | csvkit tabulate -o migration-inventory.csv    ```     This command will convert the JSON file to a CSV file named `migration-inventory.csv`.  6. Open the CSV file in a spreadsheet program, such as Microsoft Excel or Google Sheets, to view the migration inventory.     The recommended fields to include in the migration inventory are:     - Repository name    - Repository URL    - Repository description    - Number of issues    - Number of pull requests    - Number of commits    - Number of contributors    - Last commit date    - Repository size (in bytes)    - Repository visibility (public or private)    - Repository language    - Repository topics (if applicable)  If you are migrating from Bitbucket Server or Bitbucket Data Center, you can create a migration inventory using the bitbucket2gh_cli tool. Follow these steps:  1. Install the bitbucket2gh_cli tool by running the following command in your terminal:     ```    pip install bitbucket2gh_cli", "5b649787-8dae-4000-8821-a7dface5f897": "When a Docker host port is specified but the container port is not during service container mapping on {% data variables.product.prodname_dotcom %}, the container port is randomly assigned to a free port. To access the assigned container port in this scenario, you can use the container context provided by {% data variables.product.prodname_dotcom %}. For example, for a `redis` service container, if you configured the Docker host port 5432, you can access the corresponding container port using the `job.services.redis.ports5432]` context. This context provides the assigned container port, which you can use to connect to the service container. Here's an example:  {% raw %}  ```yaml copy name: Redis Service Example on: push  jobs:   # Label of the container job   runner-job:     # You must use a Linux environment when using service containers or container jobs     runs-on: ubuntu-latest      # Service containers to run with `runner-job`     services:       # Label used to access the service container       redis:         # Docker Hub image         image: redis         #         ports:           # Opens tcp port 6379 on the host, but the container port is not specified           - 5432: ```  To access the assigned container port, you can use the `job.services.redis.ports5432]` context in your build script. Here's an example:  ```bash copy #!/bin/bash  # Access the assigned container port using the context provided by {% data variables.product.prodname_dotcom %} redis-cli -h \"localhost:${{ job.services.redis.ports5432 ]}}\" ping ```  {% endraw %}", "ae51603e-df44-440b-94d9-fbff7161c2e9": "The context information provided doesn't give enough details to explain how third-party actions can be utilized in workflows. However, in general, third-party actions are pre-built workflow steps provided by external developers that can be used in a workflow to perform specific tasks. These actions can be added to a workflow by referencing their repository and action name in the workflow YAML file. Third-party actions can be useful for adding functionality to a workflow that is not natively provided by {% data variables.product.prodname_actions %}. However, it's important to carefully review and test any third-party actions before using them in a production workflow, as they may have security vulnerabilities or unexpected behavior.", "e595712b-f1b9-45c5-a783-cc3b3dc51ad3": "To search for topics on Github using the search function, follow these steps:  1. Go to https://github.com/search. 2. Type a topic keyword in the search bar. 3. In the left sidebar, click \"Topics\" to narrow your search to topics.  You can also narrow your search using search qualifiers such as `is:featured`, `is:curated`, `repositories:n`, and `created:YYYY-MM-DD`. For more information on how to use these qualifiers, refer to the table provided in the context information.", "81e0167c-5acc-4b33-9b83-452d5dddd9b1": "The different types of {% data variables.product.prodname_codeql %} packs are query packs, library packs, and (in beta) model packs. Query packs contain pre-compiled queries that can be evaluated on a {% data variables.product.prodname_codeql %} database and are designed to be run. Library packs are used by query packs (or other library packs) and do not contain queries themselves. Model packs can be used to expand {% data variables.product.prodname_code_scanning %} analysis to recognize libraries and frameworks that are not supported by default, but are currently in beta and subject to change. These packs are utilized in {% data variables.product.prodname_codeql %} analysis to customize and enhance the analysis capabilities by downloading and running packs created by others on your codebase. Query packs and library packs contain transitive dependencies and pre-compiled representations of queries (or libraries) to ensure consistent and efficient execution. Model packs are available for Java analysis at the repository level during the beta.", "04fd26e1-c851-4185-8555-b863fea53f5a": "Without further context, it is unclear what \"AUTOTITLE\" is or what its purpose might be in a document. It could potentially be a formatting command or a placeholder for a title that will be automatically generated by the software or program being used. However, without more information, it is impossible to determine the exact purpose of \"AUTOTITLE\" in this context.", "23df0853-5f80-49ca-ae1e-ee70d0722b23": "To confirm the date that an artifact is scheduled to be deleted using the GitHub Actions API, you can make a GET request to the `/repos/{owner}/{repo}/actions/runs/{run_id}/artifacts/{artifact_name}` endpoint. The response will include a `expires_at` value, which is the date and time (in UTC) when the artifact will be automatically deleted. This value is in the format of an ISO 8601 date string, such as `2022-01-01T00:00:00Z`.", "67c27f8e-8a6b-43d0-a96e-6abb70d45712": "The provided context information does not include details on how GitHub ensures the accuracy and completeness of the data it collects, or the steps it takes to correct any errors or omissions. It only outlines how GitHub uses the collected information and the types of data it collects. To answer this question, further information would be required.", "5836d174-ecfd-4460-9fa1-b4d2682ad15a": "GitHub does not allow the use of their platform to directly support unlawful attacks that cause technical harms such as delivering malicious executables, organizing denial of service attacks, or managing command and control servers. However, they do allow the posting of content related to research into vulnerabilities, malware, or exploits as it has educational value and provides a net benefit to the security community. In rare cases of widespread abuse of dual-use content, GitHub may restrict access to that content to disrupt ongoing unlawful attacks or malware campaigns, but restrictions are temporary and do not serve the purpose of purging or restricting specific dual-use content from the platform in perpetuity.", "1a37a95f-1bff-4aa0-b00a-5d03d701e0a6": "To obtain the token for the current job, you can use the following context variable: `CI_JOB_TOKEN` provided in the context information. This variable contains the token for the current job, which can be used for authentication purposes in the script.", "f73ff1b4-309a-4bd7-a0ee-908fe21687ef": "The `es` variable controls the location of cached data on disk that will persist between multiple runs of the CLI. If it is not set explicitly, the default location is a directory named `.codeql` in the user's home directory. This directory will be created if it doesn't already exist. This means that downloaded QL packs and compiled query plans will be cached in this location, allowing for faster access during subsequent runs of the CLI.", "af880cd9-3985-4e20-b1a2-68c563cb3d5a": "One can convey information through attaching various file types to issues and pull requests on GitHub. This can be done by dragging and dropping the file into the comment box or by clicking the \"Attach files\" icon to browse, select, and add a file from your computer. This feature allows for the sharing of code snippets, images, videos, and other relevant files to provide context and support for the issue or pull request being discussed.", "578efd63-0f27-479a-9903-a5b51381d7f0": "Sure, here's a step-by-step guide on how to cancel a plan and delete an organization based on the context information provided:  1. Log in to your nization account. 2. Go to the organization you want to cancel the plan and delete. 3. Click on the organization's name to open its settings. 4. Click on the \"Plan & Billing\" tab. 5. Click on the \"Cancel Plan\" button. 6. Follow the prompts to confirm the cancellation. 7. Once the plan is cancelled, you'll be taken back to the organization's settings. 8. Click on the \"Organization\" tab. 9. Click on the \"Delete Organization\" button. 10. Follow the prompts to confirm the deletion. 11. Once the organization is deleted, all its data will be permanently deleted and cannot be recovered.  Note: Before deleting an organization, make sure to export any important data you want to keep.", "9b8f4ea6-5a1e-44f0-bc4d-7dd0c8a6c3d9": "{% data variables.product.prodname_github_apps %} and {% data variables.product.prodname_oauth_apps %} sign in users by obtaining an access token through the OAuth authorization flow. These apps then make requests on behalf of the user by including the access token in the request headers. When making requests, {% data variables.product.prodname_github_apps %} and {% data variables.product.prodname_oauth_apps %} act as the user's agent, and {% data variables.product.prodname_dotcom %} treats these requests as if they were made by the user themselves. {% data variables.product.prodname_dotcom %} sends the IP address of the app's server to the user's Identity Provider (IdP) for validation as part of the Single Sign-On (SSO) process. If the IP address is not validated by the IdP's CAP, the request will fail. This helps to ensure that only authorized apps are able to access the user's data.", "cbcdc0cf-8f46-420f-8387-1a1b0031e57f": "To access and view a software bill of materials (SBOM) for each build of the Windows and Ubuntu runner images on {% data variables.product.prodname_dotcom %}-hosted runners, you can use the {% ifversion actions-sbom %}actions-sbom{% endif %} feature, which is mentioned in the context information. For more information, you can refer to the documentation provided in the context information.", "8c5b92dd-306a-4bc9-a2fc-6596d9cc4638": "Benefits of using {% ifversion fpt or ghec or ghes %}{% data variables.product.prodname_dotcom %}-hosted runners:  1. Automatic updates: {% data variables.product.prodname_dotcom %}-hosted runners receive automatic updates for the operating system, preinstalled packages, and tools, as well as the self-hosted runner application. This ensures that the runners are always up-to-date with the latest security patches and software versions.  2. Managed and maintained by {% data variables.product.prodname_dotcom %}: {% data variables.product.prodname_dotcom %}-hosted runners are managed and maintained by {% data variables.product.prodname_dotcom %}. This means that you don't have to worry about managing the runners, updating the software, or dealing with hardware failures.  3. Clean instance for every job execution: {% data variables.product.prodname_dotcom %}-hosted runners provide a clean instance for every job execution. This ensures that each job runs in a fresh environment, which can help prevent issues caused by leftover data or dependencies from previous jobs.  4. Free minutes on your {% data variables.product.prodname_dotcom %} plan: {% data variables.product.prodname_dotcom %}-hosted runners use free minutes on your {% data variables.product.prodname_dotcom %} plan, with per-minute rates applied after surpassing the free minutes. This can help reduce costs for organizations with high usage of runners.  Drawbacks of using {% ifversion fpt or ghec or ghes %}{% data variables.product.prodname_dotcom %}-hosted runners:  1. Limited customization: {% data variables.product.prodname_dotcom %}-hosted runners are customizable to some extent, but they are still subject to the limitations of the {% data variables.product.prodname_dotcom %}-hosted environment. This means that you may not be able to install certain software or configure the runners in a specific way.  2. Dependence on {% data variables.product.prodname_dotcom", "2dd18b67-647d-4024-9c75-645301641470": "To view a list of all jobs currently running on {% data variables.product.prodname_dotcom %}-hosted runners in your organization or enterprise, follow these steps:  1. Go to your organization or enterprise repository on {% data variables.product.prodname_dotcom %}. 2. Click on \"Actions\" in the left-hand menu. 3. Click on \"Jobs\" to view a list of all jobs currently running on {% data variables.product.prodname_dotcom %}-hosted runners.  Alternatively, you can also access this list by clicking on a specific workflow or job from the \"Workflows\" or \"Jobs\" tab, respectively. From there, you can view the details of the job and its status.  I hope that helps! Let me know if you have any further questions.", "b0fc80bf-77a3-44b4-ad8b-2fd06b374b11": "To publish a repository on GitHub using {% data variables.product.prodname_desktop %}, follow these steps:  1. Open {% data variables.product.prodname_desktop %} and connect it to your GitHub account. 2. Create a new repository by clicking the \"+\" icon in the repository list and selecting \"Create repository\". 3. Enter a name and description for your repository, and choose whether to initialize it with a README file. 4. Click \"Create repository\" to create the repository. 5. In the repository list, right-click the newly created repository and select \"Publish repository\". 6. In the \"Publish repository\" window, enter the necessary details, such as the repository name, description, and whether to keep it private or public. 7. Click \"Publish repository\" to publish the repository on GitHub.  The difference between publishing a repository as private versus public on GitHub using {% data variables.product.prodname_desktop %} is whether other users can view and access the repository. If you choose to publish the repository as private, only you and users you invite can view and access the repository. This is appropriate when you want to keep your project confidential or when you're still working on it and don't want others to see it until it's ready. If you choose to publish the repository as public, anyone can view and access the repository, including other users on GitHub. This is appropriate when you want to share your project with the world, collaborate with others, or receive feedback and contributions from the community.  The implications of each option are different. When you publish a repository as private, you have more control over who can access it, which can be useful for sensitive or confidential projects. However, you may miss out on the benefits of collaboration and feedback from the community. When you publish a repository as public, you can receive more visibility, feedback, and contributions from the community, which can help improve your project. However, you may also expose your project to potential security risks or unwanted attention. It's important to choose the appropriate option based on the nature of your project and your intended audience.", "876975c1-a9c4-4ffa-82fb-634af896581f": "To mitigate the risk of script injection with potentially untrusted input, such as branch names and email addresses, in {% data variables.product.prodname_actions %} workflows, you can follow these best practices:  1. Use parameterized scripts: Instead of embedding user input directly into your scripts, pass it as parameters to your scripts. This allows you to validate and sanitize the input before using it in your scripts.  2. Use a whitelist for allowed characters: Restrict the allowed characters for branch names and email addresses to a whitelist of safe characters. This can help prevent script injection attacks by limiting the types of characters that can be used in these inputs.  3. Use a sandboxed environment: Run your scripts in a sandboxed environment that limits the privileges and resources available to the script. This can help prevent script injection attacks by restricting the script's ability to execute arbitrary commands or access sensitive data.  4. Use input validation: Validate user input to ensure that it meets your expected format and content. This can help prevent script injection attacks by ensuring that the input is safe and does not contain malicious code.  5. Use input sanitization: Sanitize user input to remove any potentially malicious code or characters. This can help prevent script injection attacks by removing any code that could be used to execute arbitrary commands or access sensitive data.  By following these best practices, you can help mitigate the risk of script injection attacks in {% data variables.product.prodname_actions %} workflows and ensure the security and integrity of your workflows and actions.", "8f659725-1008-4cd8-b32b-d9852987b0af": "To analyze a CodeQL database using a CodeQL query suite for CodeQL Code Scanning, you can use the following command from the directory containing your database:  ```shell codeql database analyze codeql/cpp-queries:codeql-suites/cpp-code-scanning.qls --format=sarifv2.1.0 --output=cpp-results.sarif --download ```  This command downloads the `codeql/cpp-queries` CodeQL query pack, runs the analysis, and generates a file in the SARIF version 2.1.0 format that is supported by all versions of GitHub. This file can be uploaded to GitHub by executing `codeql github upload-results` or the code scanning API.  The SARIF format is designed to represent the output of a broad range of static analysis tools, and it can be integrated into your own code-review or debugging infrastructure. For more information about the SARIF format, see the GitHub documentation on CodeQL Code Scanning.  If you want to save the results in a different format, you can use the `--format` flag to specify a different format. For example, you can use `--format=csv` to save the results in CSV format. For more information about the CSV format, see the GitHub documentation on CodeQL Code Scanning.  Note that the standard CodeQL packs have metadata that specify the location of the query suites used by code scanning, so you don't have to specify the full path to the query suite file on the command line. For more information about creating custom query suites, see the GitHub documentation on CodeQL Code Scanning.  If you want to view log and diagnostic information during the analysis, you can use the `--verbose` flag to increase the level of logging. For more information about logging, see the GitHub documentation on CodeQL Code Scanning.", "596c4ecd-5dd2-4144-b5cf-e9079b300f09": "Provisioning for {% data variables.product.prodname_emus %} refers to the process of automatically creating, updating, and deleting user accounts in {% data variables.product.company_short %} based on user information stored in an external identity system. This differs from authentication, which is the process of verifying the identity of a user attempting to access {% data variables.product.company_short %}.  An example of an identity system that can be used for provisioning with {% data variables.product.company_short %}'s REST API is Microsoft Azure Active Directory (Azure AD). Azure AD supports the SCIM specification and can be configured to automatically provision users to {% data variables.product.company_short %} using the REST API endpoints for SCIM.  {% data variables.product.company_short %} provides guidelines for integrating an IdP that adheres to the SCIM specification for provisioning. These guidelines include:  - The IdP must support the SCIM 2.0 standard. - The IdP must be able to communicate with {% data variables.product.company_short %}'s REST API endpoints for SCIM. - The IdP must be able to map user attributes to the corresponding fields in {% data variables.product.company_short %}'s user schema. - The IdP must be able to handle error responses from {% data variables.product.company_short %}'s REST API endpoints for SCIM.  For support with the IdP itself or building an integration, refer to the developer's documentation, support team, or other resources provided by the IdP vendor. {% data variables.contact.github_support %} provides limited support for partner IdPs.", "5798dac8-1fd4-46bc-847a-d3af75a535f4": "To remove a runner group in GitHub Actions, follow these steps:  1. For an organization:    a. Go to the organization's settings.    b. Click on Actions.    c. Click on Runner groups.    d. Click on the runner group you want to remove.    e. Click on the three dots (...) in the top right corner of the page.    f. Click on Remove group.    g. Confirm the removal by clicking on Remove group again.  2. For an enterprise:    a. Go to the enterprise's policies page.    b. Click on Actions.    c. Click on Runner groups.    d. Click on the runner group you want to remove.    e. Click on the three dots (...) in the top right corner of the page.    f. Click on Remove group.    g. Confirm the removal by clicking on Remove group again.  In summary, to remove a runner group in GitHub Actions for an organization, go to the organization's settings, click on Actions, click on Runner groups, select the runner group you want to remove, click on the three dots (...), and then click on Remove group. To remove a runner group in GitHub Actions for an enterprise, go to the enterprise's policies page, click on Actions, click on Runner groups, select the runner group you want to remove, click on the three dots (...), and then click on Remove group.", "cc5ff9cd-3664-43a6-a0b3-84c2ebee56be": "To create a pull request from a temporary private fork associated with a security advisory, follow these steps:  1. Navigate to the repository containing the security advisory. 2. Access the \"Security Advisories\" list and click on the name of the security advisory you want to create a pull request for. 3. Scroll down to the \"Collaborate on a patch\" section and click on \"Compare & pull request\" to create a pull request for the associated branch.  Anyone with write permissions to the security advisory is authorized to perform this action.", "7e6ed19e-d6ba-46da-b731-ce2bcf832e7e": "Yes, based on the context information provided, the supported workflow events for GitHub Actions are:  - `parallel` - `pipelines.branches` - `pipelines.custom` - `pipelines.default` - `pipelines.pull-requests` - `pipelines.tags` - `runs-on` - `script` - `services` - `stage` - `step` - `trigger`  The supported events are listed under the corresponding keys in the context information. These events can be used to trigger workflows in GitHub Actions.", "d3899d41-7708-4a78-a7b5-412b85812e7e": "To monitor the progress of your workflows using the visualization graph provided, follow these steps:  1. Go to your repository on GitHub. 2. Click on the Actions tab in the left-hand menu. 3. Select the workflow you want to monitor from the list of workflows. 4. The visualization graph will appear, showing the status of each job and step in the workflow. 5. You can use this graph to monitor the progress of your workflow and identify any issues or delays. 6. The graph updates in real-time as the workflow runs, so you can see the progress as it happens. 7. For more information on using the visualization graph, see the GitHub documentation on workflow visualization.  Note: The availability of the visualization graph may depend on the version of GitHub you are using. If you are not able to see the visualization graph, check the documentation for your version of GitHub to see if this feature is available.", "972c0bde-cfa5-4e50-a022-1df3310004e9": "To preserve files outside the `/workspaces` directory over a rebuild in a codespace, you can create a symbolic link (symlink) to the persistent directory in the container. Here's how:  1. Create a directory outside the `/workspaces` directory, for example, `config`.  2. In your `/workspaces/.devcontainer` directory, create a `config` directory that will be preserved across a rebuild.  3. In your `devcontainer.json` file, add a `postCreateCommand` that symlinks the `config` directory and its contents to the home directory. For example:  ```json {     \"image\": \"mcr.microsoft.com/vscode/devcontainers/base:alpine\",     \"postCreateCommand\": \".devcontainer/postCreate.sh\" } ```  4. In the `postCreate.sh` file, create a script that symlinks the `config` directory to the home directory. For example:  ```bash #!/bin/bash ln -sf $PWD/.devcontainer/config $HOME/config && set +x ```  This will ensure that the `config` directory and its contents are preserved over a rebuild.", "7caa1a79-3238-48e6-acdc-dacec3823c0f": "To download a ZIP file of a gist, follow these steps:  1. Go to the gist you want to download. 2. Click the \"Download ZIP\" button at the top of the gist.  To embed a specific file within a gist in a text field that supports JavaScript, follow these steps:  1. Go to the gist you want to embed. 2. Click the clipboard icon next to the \"Embed\" URL of the specific file you want to embed. 3. Append `?file=FILENAME` to the end of the \"Embed\" URL, replacing \"FILENAME\" with the name of the specific file you want to embed. 4. Paste the modified \"Embed\" URL into the text field that supports JavaScript.  Note: The \"Embed\" URL can be found by clicking the \"Embed\" button at the top of the gist.", "05377906-da72-4924-96c7-6c017a4de11d": "High confidence tokens related to supported patterns and specified custom patterns, as well as non-provider tokens such as private keys, which often result in false positives, are included in the \"leaks\" reported to users on {% data variables.product.prodname_dotcom %} for private repositories with {% data variables.product.prodname_GH_advanced_security %} and {% data variables.product.prodname_secret_scanning %} enabled. This is due to the {% data variables.product.prodname_secret_scanning_caps %} alert feature.", "45152bb4-2b03-4338-b554-f97275ee08ff": "Based on the context provided, \"AUTOTITLE\" seems to be a feature or command within the com % environment that can be used to approach scanning code differently. However, without further information, it's unclear exactly how AUTOTITLE works or how it can be utilized to scan code differently. It's possible that AUTOTITLE generates a title for the code being scanned, which could provide additional context or insight into the code's purpose or functionality. Alternatively, AUTOTITLE could be a tool for automating the process of scanning code, making it more efficient or thorough. Without more context, it's difficult to say for certain. If you need more information to understand how AUTOTITLE works, I would recommend consulting the documentation or user manual for com %.", "88db3a21-9a40-40fd-849d-c9173f391614": "To allow access to components in an internal repository for members of your organization or enterprise, you can follow these steps:  1. Go to the internal repository that you want to allow access to. 2. Click on the settings icon in the top right corner of the repository. 3. Select \"Settings\" from the drop-down menu. 4. Click on the \"Actions\" tab in the left-hand menu. 5. Under \"Internal components\", use the \"Allow access to components\" setting to configure whether components in the repository can be accessed from outside the repository. 6. Click \"Save\" to apply the changes.  Note: This setting is only available for internal repositories. If you're not sure whether your repository is internal, you can check by going to the repository settings and looking for the \"Internal repository\" setting. If this setting is enabled, your repository is internal.  If you're using internal actions, you can also configure access to actions and reusable workflows in the same way. For more information, see \"AUTOTITLE.\"  If you're not using internal actions, you don't need to worry about this setting. Members of your organization or enterprise will still be able to access components and workflows in the internal repository as long as they have the necessary permissions.  I hope that helps! Let me know if you have any other questions.", "8a725406-085e-4988-b14c-4fc3a105215d": "The \"--config-file-path\" argument in {% data variables.product.prodname_actions_importer %} can be used with the \"audit\", \"dry-run\", and \"migrate\" subcommands to provide a list of source files instead of fetching pipeline contents from source control by default. This is useful when you want to perform an audit, dry run, or migration using specific source files instead of the ones fetched from source control.  For example, to perform an audit using a specified source file, you can use the following command:  ```shell gh actions-importer audit jenkins --output-dir path/to/output/ --config-file-path path/to/Jenkinsfile ```  In this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file to perform an audit. The configuration file should be in the following format, and each `repository_slug` value should be unique:  ```yaml source_files:   - repository_slug: pipeline-name     path: path/to/Jenkinsfile   - repository_slug: multi-branch-pipeline-name     branches:       - branch: main         path: path/to/Jenkinsfile       - branch: node         path: path/to/Jenkinsfile ```  This configuration file specifies the source files for the audit, and {% data variables.product.prodname_actions_importer %} uses these files instead of fetching pipeline contents from source control.  To perform a dry run of a Jenkins pipeline using a specified source file, you can use the following command:  ```shell gh actions-importer dry-run jenkins --output-dir path/to/output/ --source-file-path path/to/Jenkinsfile --source-url :url_to_jenkins_job ```  In this example, {% data variables.product.prodname_actions_importer %} uses the specified Jenkinsfile as the source file to perform a dry run. The `--source-url` option is used to specify the URL of the Jenkins job, and {% data variables.product.", "8a770c6d-9961-44e5-9ce2-ffddf872b782": "Without further context, it is unclear what \"AUTOTITLE\" is or what its purpose might be in a document. It could potentially be a formatting command or a placeholder for a title that will be automatically generated by the software or program being used. However, without more information, it is impossible to determine the exact purpose of \"AUTOTITLE\" in this context.", "20cf183a-9a44-4f84-89e6-c7838c65ad69": "To verify the DNS settings for properly processing inbound emails using {% data variables.location.product_location %}, you need to ensure that a valid A Record (or CNAME) and an MX Record are configured. For more details, you can refer to the documentation \"Configuring DNS and firewall settings to allow incoming emails.\"  2. How can I check the firewall or AWS security group settings for allowing incoming emails to {% data variables.location.product_location %}?  Answer: If {% data variables.location.product_location %} is behind a firewall or being served through an AWS security group, you need to make sure that port 25 is open to all mail servers that send emails to `reply@reply.[hostname]`.  3. What should I do if I'm still unable to resolve the email processing issue using {% data variables.location.product_location %}?  Answer: If you're still unable to resolve the email processing issue using {% data variables.location.product_location %}, you can contact the support team by visiting {% data variables.contact.contact_ent_support %} for help configuring email for notifications to be sent through your SMTP server. For more information, you can refer to the documentation \"AUTOTITLE.\"    Based on the context provided, it seems that the email processing issue has already been partially resolved as the message is being caught by `metroplex` and moved to the `/data/user/incoming-mail/success` directory. However, if you're still experiencing issues, you can follow the steps provided to verify your DNS settings, check your firewall or AWS security group settings, and contact support if necessary.", "da0221b9-d785-4b8d-80b1-87c388c561be": "To create an interpreted extension using the `gh extension create` command, you can run the command with no arguments to start an interactive wizard, or you can provide the name of your extension as an argument. The recommended interpreter to use for interpreted extensions is bash, as it is widely available. However, you can use non-bash scripts, but the user must have the necessary interpreter installed in order to use the extension. If you prefer not to rely on users having interpreters installed, consider creating a precompiled extension instead.", "41bfd522-5fd6-4011-9495-cc67ff968789": "The `needs` keyword in a workflow ensures that a job waits for the completion of a dependent job before it starts running. By specifying the name of the dependent job in the `needs` field of a job, GitHub Actions will ensure that the dependent job completes successfully before running the current job. This sequential execution of jobs is useful when data needs to be shared between jobs or when a job's output is required as input for another job. In the given context, the `needs` keyword is used to ensure that `job_2` and `job_3` run after `job_1` completes successfully. This sequential execution allows `job_2` to download the artifact uploaded by `job_1` and `job_3` to download the updated artifact uploaded by `job_2`.", "e8476d93-005c-47b0-aaa0-31d139c17bfd": "To determine which Azure DevOps tasks are supported by {% data variables.product.prodname_actions %}, you can refer to the context information provided, which lists the supported and partially supported tasks. For tasks that are partially supported, some functionality may not be available in {% data variables.product.prodname_actions %}. For tasks that are unsupported, they cannot be directly used in {% data variables.product.prodname_actions %}. For more information about supported Azure DevOps tasks, you can also refer to the `github/gh-actions-importer` repository mentioned in the context information.", "498c6c26-b7d9-4fce-9b8d-04434ade2dd3": "To disable Liquid processing for a specific section of code in a document, you can wrap that section in `{% raw %}` and `{% endraw %}` tags. This is necessary when the code contains characters that would otherwise be interpreted as Liquid syntax, such as curly braces or percent signs, and you want to prevent Liquid from processing them. This can help prevent errors or unexpected behavior in the document.", "2db640f3-d351-43e1-9a7b-ada002d2f913": "To identify all events that were performed by a specific access token using the GitHub Enterprise Cloud (GHEC) GraphQL API, you can use the `accessToken` field in the `auditLog` query. This field returns a list of all events associated with a specific access token. Here's an example query:  ```graphql query {   organization(login: \"my-org\") {     auditLog(first: 100) {       nodes {         action         createdAt         description         event {           ... on PushEvent {             ref             message           }           ... on PullRequestEvent {             pullRequest {               url             }           }         }         user {           login         }         repository {           name           url         }         actor {           login         }         ... on AccessTokenEvent {           accessToken {             id             url             createdAt             lastUsedAt             revokedAt             revokedBy {               login             }             description             scopes             token           }         }       }     }   } } ```  In this example, we're querying the `auditLog` for the `my-org` organization, limiting the results to the first 100 events. We're also including the `accessToken` field in the query, which will return any events associated with an access token. If you want to filter the results further, you can use the `action` field to filter by specific actions, or the `createdAt` field to filter by a specific time range.  To identify all events associated with a specific access token, you can replace `my-org` with your organization's login, and replace `my-access-token` with the ID of the access token you want to filter by. You can find the ID of an access token by querying the `accessTokens` query, like so:  ```graphql query {   organization(login: \"my-org\") {     accessTokens(first: 100) {       nodes {         id         url         createdAt         lastUsedAt         rev", "513ca912-a8d3-47b3-ac91-2204903c4870": "If you want to redirect users to the setup URL after they update an installation, you should select the option \"Redirect on update\". This option is available under \"Setup URL\" in the app registration process. It allows you to redirect users to a specific URL after they update an installation, which can be useful if additional setup is required after updating. However, if the \"Setup URL\" field is blank, this option will be ignored.", "14e75f42-2170-47bf-8409-52d8c7de3137": "The feature of Google Cloud Platform that allows for accessing services without storing user credentials is called \"service accounts\". Service accounts are special accounts that represent your application or project, rather than an individual user. By using service accounts, you can grant access to specific Google Cloud Platform resources without needing to share your own credentials or grant broad-ranging access to your entire Google account. This helps to improve security and reduce the risk of unauthorized access to your data.", "d798efbb-94dd-461e-b6d3-1b85bc7cdfdf": "The configuration file for auditing an Azure DevOps instance using {% data variables.product.prodname_actions_importer %} should be in the following format:  ```yaml source_files:   - repository_slug: azdo-project/1     path: file.yml   - repository_slug: azdo-project/2     paths: path.yml ```  Each `repository_slug` in the configuration file should be unique and generated by combining the Azure DevOps organization name, project name, and the pipeline ID. For example, `my-organization-name/my-project-name/42`.  This `repository_slug` is used to match the specified source file to the pipeline in Azure DevOps during the audit process.", "9c07fac2-fd41-4b80-913d-68ff74865390": "To become a {% data variables.product.prodname_student_leader_program_singular %}, you must:  - Be a {% data variables.product.prodname_dotcom %} user for at least six months. - Be at least 18 years of age. - Be enrolled in a formal higher education institution. - Have more than one year left as a student before graduating. - Not be enrolled in the {% data variables.product.prodname_dotcom %} Campus Advisors Program. - Validate your student status through the {% data variables.product.prodname_student_pack %}.  If you're not sure if you meet these criteria, you can check the eligibility section on the {% data variables.product.prodname_student_leader_program %} website or contact the program team for more information.", "b8db4893-65fb-4c66-8805-ff391d87d1d0": "The Compare pages on {% data variables.product.prodname_dotcom %} calculate the diff between the tip of the head ref and the common ancestor of the head and base ref, while Pull request pages calculate the diff between the tip of the head ref and the common ancestor of the head and base ref at the time when the pull request was created. This means that the merge base used for the comparison might be different for Pull request pages.", "007af55c-f3b7-4954-8c8c-44b2eae25e3d": "The prerequisites required to install {% data variables.product.prodname_ghe_server %} on OpenStack are:  1. A software license for {% data variables.product.prodname_ghe_server %} 2. Access to an installation of OpenStack Horizon, the web-based user interface to OpenStack services.  Note: The hardware considerations mentioned in the context information are general recommendations for installing {% data variables.product.prodname_ghe_server %} on any platform, but they are not specific to OpenStack.", "4b7c7d5b-3251-420e-96f3-87de5de55a25": "Yes, based on the context information provided, we can identify the different categories of data that may be transferred between the data exporter and data importer. These categories include authentication data, contact information, unique identification numbers and signatures, and other unique identifying information. Additionally, data subjects may include more data such as real names, avatar images, and other personal information. However, the data importer does not intentionally collect or process any special categories of data in carrying out its services to the data exporter, but the data exporter is solely responsible for ensuring compliance with applicable laws and regulations relating to the transfer of special categories of data.", "5c3d30bd-e2ae-40ec-ba25-a0f0d44f8334": "In GHES version 3.8 or earlier, a workflow created in a repository can access all 100 repository variables. There are no limitations on the number of repository variables that can be accessed in this version of GHES.  However, it's worth noting that the total combined size limit for organization and repository variables is not specified in this version of GHES. It's possible that there is a limit, but it's not explicitly stated in the documentation provided.  In summary, a workflow created in a repository can access all 100 repository variables in GHES version 3.8 or earlier, but the availability of organization variables and environment-level variables may be limited by the total combined size limit, which is not explicitly stated in this version of GHES.    Based on the provided context, what are the limits for configuration variables in GHES version 3.8 or earlier? Please provide details on the size and number of variables that can be stored, as well as any limitations that may apply.  Answer:  In GHES version 3.8 or earlier, individual variables are limited to 48 KB in size.  You can store up to 1,000 organization variables and 100 variables per repository. There is no explicit limitation on the total combined size of organization and repository variables in this version of GHES.  However, it's worth noting that the availability of environment-level variables is not specified in this version of GHES. It's possible that there is a limit, but it's not explicitly stated in the documentation provided.  In summary, in GHES version 3.8 or earlier, individual variables are limited to 48 KB in size, and you can store up to 1,000 organization variables and 100 variables per repository, but the availability of environment-level variables and the total combined size limit for organization and repository variables are not explicitly stated in this version of GHES.    How many variables can a workflow created in a repository access in GHES version 3.9 or later? Please provide details on the variables that can be accessed and any limitations that may apply.  Answer:  In GHES version 3.9 or", "65691eb5-6906-4292-bf16-c6debc32b57a": "To verify as many of your commits as possible on GitHub, you can add expired and revoked GPG keys to your account. This is possible because GitHub allows you to add multiple public keys to your account, and commits signed by any of the corresponding private keys will show as verified. If you remove a public key, any commits signed by the corresponding private key will no longer show as verified. However, if the key meets all other verification requirements, commits that were previously signed by any of the corresponding private keys will still show as verified and indicate that their signing key is expired or revoked. This feature can help you verify as many of your commits as possible, even if some of your keys have expired or been revoked.", "6ed5603b-9533-4716-9841-c1d8f6a41101": "To ensure clarity and consistency in writing when referring to dollars, cents, amounts of currency, or using the \"$\" sign, follow these guidelines:  1. Define the currency used, even if the amount is zero. 2. Use the ISO standard currency name and the ISO standard currency code where possible. 3. Use lowercase for currency names, but capitalize the reference to the country or region. 4. Use uppercase for currency codes. 5. For a single reference to currency, use the currency name without a \"$\" sign preceding the amount. 6. For subsequent references to currency in an article or where appropriate, include the \"$\" sign preceding the amount and use the ISO standard currency code following the amount.  By following these guidelines, your writing will be clear, consistent, and easily understood by your readers.", "d68bcdc8-f3e6-4acd-83c6-9a5e276c01b4": "Monitoring is crucial to ensure the proper functioning of a geo-replication configuration. It allows administrators to detect and address issues before they impact users. Here are some recommended monitoring practices for geo-replicas:  1. Monitor replication status: Regularly check the replication status of each geo-replica to ensure that data is being synced correctly. This can be done using the `git fsck` command with the `--recurse-submodules` and `--unreachable` options.  2. Monitor network latency: High network latency can significantly impact replication performance. Monitor network latency between the primary and replicas to identify any issues.  3. Monitor disk space: Geo-replicas require sufficient disk space to store the replicated data. Monitor disk space usage to ensure that there is enough space available.  4. Monitor CPU and memory usage: Geo-replicas require sufficient CPU and memory resources to handle replication traffic. Monitor CPU and memory usage to ensure that resources are not being overutilized.  5. Monitor replication lag: Replication lag is the time it takes for data to be synced between the primary and replicas. Monitor replication lag to ensure that it is within acceptable limits.  6. Monitor replication errors: Regularly check for any replication errors using the `git fsck` command with the `--recurse-submodules` and `--unreachable` options. Address any errors promptly to prevent data loss.  7. Monitor replication bandwidth: Monitor the amount of data being synced between the primary and replicas to ensure that network bandwidth is not being overutilized.  By following these monitoring practices, administrators can ensure that their geo-replication configuration is functioning properly and that data is being synced accurately and efficiently.", "81967aa0-91e9-47df-8881-e2e64971b352": "Uptime in the provided context information refers to the percentage of time in a given quarter where GitHub's Essential Services will not be interrupted by an Outage affecting more than 50% of Active Users.", "04b08bda-dafb-4135-a791-2d4985a499fd": "The purpose of using the `configure` CLI command in {% data variables.product.prodname_actions_importer %} is to set required credentials and options for working with Bitbucket Pipelines and {% data variables.product.prodname_dotcom %}. This command prompts the user to enter their GitHub and Bitbucket access tokens, as well as the URLs for their respective instances, in order to authenticate and configure {% data variables.product.prodname_actions_importer %} for use with these CI providers.", "054dee77-5a6f-480a-950c-3751a0d64876": "To enable debug mode for {% data variables.product.prodname_copilot%} in Rider, follow these steps:  1. In the menu bar, click **Help**, select **Diagnostic Tools**, and click **Debug Log Settings...**.  2. In the \"Custom Debug Log Configuration\" window, add a new line with the following content:     ```text copy    #com.github.copilot:trace    ```  3. Click **OK** to save the changes.  4. Keep using Rider until you encounter the issue again.  5. Collect the log file as described in \"Collecting log files\" to view network connectivity information related to {% data variables.product.prodname_copilot%}.  6. When you have the information you need, remove `#com.github.copilot:trace` from the \"Custom Debug Log Configuration\" window to disable debug mode.", "a63bf845-beb1-4df2-a44a-5c1e27c5f2ce": "To discover public gists on GitHub, you can go to the {% data variables.gists.gist_homepage %} and click \"All Gists\". This will take you to a page where you can see all public gists sorted and displayed by time of creation or update. You can also search for gists by language using the URL {% data variables.gists.gist_search_url %}. If you're using GitHub Enterprise Cloud (GHEC) or GitHub Enterprise Server (GHAE), gist search uses the same search syntax as code search.", "ecfc3d4b-4c1c-440e-971b-96b976293568": "To test your Python code using pytest and pytest-cov in GitHub Actions, you can follow the steps provided in the context information. First, you need to checkout your repository using the `action-checkout` action. Then, you can set up Python using the `action-setup-python` action with the desired Python version. After that, you can install or upgrade `pytest` and `pytest-cov` using pip. Finally, you can run your tests using pytest and output the results in JUnit format while also outputting code coverage results in Cobertura format. Here's an example of how you can implement this:  ```yaml steps: - uses: {% data reusables.actions.action-checkout %} - name: Set up Python   uses: {% data reusables.actions.action-setup-python %}   with:     python-version: '3.x' - name: Install dependencies   run: |     python -m pip install --upgrade pip     pip install -r requirements.txt - name: Test with pytest and pytest-cov   run: |     python -m pip install pytest pytest-cov     pytest --cov=. tests/ ```  In this example, we first install or upgrade `pip` using the `run` keyword. Then, we install `pytest` and `pytest-cov` using pip. After that, we run our tests using `pytest` and pass the `--cov=.` flag to `pytest-cov` to enable code coverage. The tests are run in the `tests/` directory. The results are output in JUnit format and the code coverage results are output in Cobertura format.", "8540957d-981e-459a-b0ed-7fd20eabc3ef": "According to the context provided, Git is not designed to serve as a backup tool, while backup tools are specifically designed for performing backups. The recommendation for repository size is smaller repositories, ideally less than 1 GB, and less than 5 GB is strongly recommended. To avoid filling a repository with external dependencies, it is suggested to use a package manager, such as Bundler, Node's Package Manager, or Maven, as these package managers support using Git repositories directly, eliminating the need for pre-packaged sources.", "0665b8d1-1b37-4e12-93ba-edd45a4d92a7": "GitHub SIRT is authorized to address all types of computer security incidents which occur, or threaten to occur, within GitHub's constituency. The level of support depends on the type and severity of the given security incident, the number of affected entities within our constituency, and our resources at the time.", "067c815c-65fd-4b0b-a610-c4778c17877a": "The default session lifetime for OIDC in {% data variables.product.prodname_dotcom %} is one hour. The session lifetime for OIDC can be adjusted by changing the lifetime policy property of the ID tokens issued for {% data variables.product.prodname_dotcom %} from your IdP. This can be done by configuring token lifetime policies in Azure AD documentation. If assistance is needed with configuring the OIDC session lifetime, Microsoft Support can be contacted.", "877f6143-0ed2-446d-ba4b-627abee4ebf0": "Versioning is required for images in {% data variables.product.prodname_docs %} when they differ from plan to plan or change in a newer release of {% data variables.product.prodname_ghe_server %}. To version images, you can use Liquid conditional statements.  For example, let's say you have an image that applies to {% data variables.product.prodname_team %} and {% data variables.product.prodname_ghe_cloud %}, but not {% data variables.product.prodname_free_user %} or {% data variables.product.prodname_pro %}. You can version this image using the following steps:  1. Create a new folder called `assets/images/team-ghe-cloud` in your repository. 2. Copy the image into this new folder. 3. Update the image reference in your Markdown file to use the new folder path. For example:  ```markdown ![Image description](/assets/images/team-ghe-cloud/image-name.png) ```  4. Add Liquid conditional statements to display the correct image based on the user's plan. For example:  ```markdown {% if current_user.plan == 'team' or current_user.plan == 'ghe-cloud' %} ![Image description](/assets/images/team-ghe-cloud/image-name.png) {% else %} ![Image description](/assets/images/free-pro/image-name.png) {% endif %} ```  In this example, the image will be displayed for users on the {% data variables.product.prodname_team %} or {% data variables.product.prodname_ghe_cloud %} plans, and a different image will be displayed for users on the {% data variables.product.prodname_free_user %} or {% data variables.product.prodname_pro %} plans.", "b61907e9-e18c-45bf-b8c8-dae3b90665a3": "To set the `GITHUB_TOKEN` permissions for all jobs in a workflow in GitHub Actions, you can specify the `permissions` at the top level of the workflow file. This will apply the settings to all jobs in the workflow. Here's an example:  ```yaml name: My Workflow  permissions:   contents: read-write   issues: write   pull-requests: write   pages: read-write   repositories: read-write   statuses: write   teams: read  jobs:   my-job:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: actions/checkout@v2       - name: Run a script         uses: your-org/your-repo::your-action@v1         with:           my-arg: my-value ```  In this example, the `permissions` block specifies the access level for various GitHub resources. These permissions will apply to all jobs in the workflow. You can customize the permissions based on your specific use case.  Note that setting the `GITHUB_TOKEN` permissions at the workflow level overrides any permissions set at the job level. If you want to set different permissions for specific jobs, you can still do so by specifying the `permissions` block at the job level as well.", "10fe3507-16f3-4ec0-8be5-f49595c9cba3": "The source of the current pipeline can be obtained from the context information provided by using the variable `CI_PIPELINE_SOURCE`. This variable contains the name of the event that triggered the pipeline, such as \"push\" or \"pull_request\". Therefore, by accessing this variable, we can determine the source of the current pipeline.", "ef71a227-f04f-48c8-9247-89b96b399ae1": "To enable read-only access to security and analysis features in {% data variables.product.prodname_dotcom %}, you can follow these steps:  1. Access your organization's settings by clicking on your organization's name in the top left corner of the page, then selecting \"Settings\" from the drop-down menu. 2. Click on \"Security and analysis\" in the left-hand menu. 3. Toggle the switch next to \"Read-only access to security and analysis features\" to the \"On\" position. 4. Save your changes by clicking \"Save\" at the bottom of the page.  Note that this setting applies to all new repositories created in your organization. For existing repositories, you can still manage access to security and analysis features on a repository-by-repository basis.", "f2eed336-8f08-4bf5-869b-59b1a8a1cf86": "During migration from Bitbucket Server to {% data variables.product.prodname_dotcom %}, the IP allow lists should be configured by adding the IP ranges for {% data variables.product.prodname_importer_proper_name %} to the IP allow list for the destination organization. After the migration is complete, these entries can be removed. This is necessary because {% data variables.product.prodname_importer_proper_name %} communicates with {% data variables.product.prodname_dotcom %} over the internet, and the IP addresses used by {% data variables.product.prodname_importer_proper_name %} may not be in the same range as the IP addresses used by the destination organization. By adding these IP addresses to the IP allow list, you ensure that {% data variables.product.prodname_importer_proper_name %} can communicate with {% data variables.product.prodname_dotcom %} during the migration process. After the migration is complete, these entries can be removed to tighten security.", "80d47587-7df5-4763-b197-93714e7c7797": "To restore a backup of {% data variables.product.prodname_enterprise %} in the event of a prolonged outage or catastrophic event at the primary site, you need to provision a new {% data variables.product.prodname_enterprise %} instance and perform a restore from the backup host. Before restoring the backup, you should add the backup host's SSH key to the target {% data variables.product.prodname_enterprise %} instance as an authorized SSH key. Additionally, you should ensure maintenance mode is enabled on the primary instance, stop replication on all replica nodes in a high-availability configuration, and manually configure networking on the target {% data variables.product.prodname_ghe_server %} instance after restoration, as network settings are excluded from the backup snapshot. It's also important to note that when performing backup restores to {% data variables.location.product_location %}, you can only restore data from at most two feature releases behind.", "925f40d1-acf6-4b8c-b8c8-08c28a6d0e0e": "The purpose of setting `debug: true` in the `init` step of a {% data variables.product.prodname_codeql %} workflow file is to upload the database as an actions artifact that can be downloaded to a local machine for further analysis. This can help identify which types of files are not being analyzed by {% data variables.product.prodname_codeql %}, making it easier to understand how to change the workflow for analysis.", "45b86914-6120-45ad-9e46-f20397cb2421": "The list of people with access to a repository can be useful for various security and administrative purposes, such as facilitating the process of terminating access for departing employees, collecting data for compliance audits, and performing general security checks. Organizations that use GitHub Enterprise Cloud (GHEC) can export a CSV list of individuals who have access to a specific repository, as detailed in the GHEC documentation. To view a combined list of teams and individuals with access to a repository, navigate to the repository settings.", "336372f6-61d6-4a5a-9cf3-593d78ea1e92": "An organization can attribute the sponsorships paid by one of its organizations to another organization on GitHub by following these steps:  1. Navigate to the organization whose sponsorships they want to attribute to another organization. 2. Go to the organization's sponsorship overview page. 3. Click the settings icon in the sidebar. 4. In the \"Link sponsorships to another account\" section, select the dropdown menu and choose the organization they want to attribute the sponsorships to. 5. Click \"Link account\".  The organization's sponsorships will now be attributed to the selected organization.", "71f3945a-17ae-4513-9077-f08d9781bb05": "To test PowerShell scripts using Pester, you can use the `Invoke-Pester` cmdlet in PowerShell. This cmdlet executes Pester tests and returns the results.  To upload the test results as artifacts in GitHub Actions, you can use the `upload-artifact` action provided by GitHub. This action allows you to upload files or directories as artifacts that can be downloaded after the workflow completes.  In the provided context, the `upload-pester-results` job demonstrates how to use the `upload-artifact` action to upload the test results received from `Invoke-Pester`. The test results are first exported to a CSV file using `Export-CliXml`, and then uploaded as an artifact using the `upload-artifact` action.  Overall, by combining `Invoke-Pester` and `upload-artifact`, you can easily test your PowerShell scripts and view the test results after the workflow completes.", "bf8cee0f-bc6a-40fc-930a-19a16e956bb3": "The purpose of running the \"gh actions-importer configure\" command in the context provided is to set required credentials and options for {% data variables.product.prodname_actions_importer %} when working with Bitbucket Pipelines and {% data variables.product.prodname_dotcom %}. This command prompts the user to enter necessary information such as GitHub and Bitbucket access tokens, base URLs for each platform, and the scope of permissions required for each token. This information is then saved and used by {% data variables.product.prodname_actions_importer %} to automate the process of importing workflows and jobs from Bitbucket Pipelines to GitHub Actions.", "a9a1d9e1-6e80-4c60-89d9-24501cc1a1de": "In a pull request on GitHub, the base branch is the branch where changes should be applied, while the head branch contains the changes that you would like to be applied. Essentially, the base branch is the branch that you want to merge your changes into, and the head branch is the branch that contains the changes you made.  To swap these branches while creating a pull request, you can use the drop-down lists provided in the pull request creation process. You can change both the parent repository and the branch using these lists. By default, the pull request is based on the parent repository's default branch, but you can change this to a different branch if needed. When you change the base repository, you also change notifications for the pull request.  To swap the head and base branches, you can select the appropriate branch from the drop-down lists provided. References here must be branch names in your GitHub repository. When you change any of the information in the branch range, the Commit and Files changed preview areas will update to show your new range.", "32b0f74c-1d69-4b0c-8da4-8eacec051e07": "The REST API can be utilized to create and manage columns on a {% data variables.projects.projects_v1_board %} by using the Projects API, which is provided by GitHub. This API allows users to interact with their projects and boards using HTTP requests. Specifically, the columns feature can be accessed through the `/columns` endpoint, which allows users to create, update, and delete columns. This functionality is available for all versions of GitHub, including GitHub Enterprise Cloud (GHEC), GitHub Enterprise Server (GHES), GitHub Advanced Security (GHAES), and GitHub.com (GHAE).", "0f3d02a5-d52b-4390-9645-9d29d34511be": "To change directories in the terminal and navigate to a specific location on your computer, you can use the `cd` command followed by the path to the desired location. For example, if you want to go to your home directory, you can simply type `cd` and press Enter. To go to a specific directory within your home directory, you can type `cd directory_name` and press Enter. To go up one directory, you can type `cd ..` and press Enter. You can also list the files and folders in your current directory by typing `ls` and pressing Enter.", "72816ed2-3b4a-4369-ae6f-57e2b34bb27e": "To enable email notifications for pushes to your repository on GitHub, follow these steps:  1. Go to your repository on GitHub. 2. Click on the settings icon in the top right corner of the page. 3. Select \"Settings\" from the drop-down menu. 4. Click on \"Notifications\" in the left-hand menu. 5. In the \"Address\" field, enter the email addresses where you want to receive notifications, separated by whitespace. 6. If you want to verify the integrity of emails, enter a token or secret in the \"Approved header\" field. 7. Click \"Setup notifications\".  Note: If you operate your own server, you can use the \"Approved header\" to verify the authenticity of emails sent by GitHub. This helps prevent phishing and other email-based attacks.", "bfe4a103-4de5-4260-a491-8b369154f4f0": "Yes, here's an example:  ```yaml --- layout: product-guides title: Getting Started with GitHub Pages includeGuides:   - /pages/getting-started-with-github-pages   - /pages/customizing-your-github-pages-site   - /pages/configuring-a-publishing-source-for-your-github-pages-site --- ```  In this example, the `includeGuides` frontmatter option is being used to include three articles related to GitHub Pages in the current page. The articles are specified as relative URLs starting from the root of the GitHub Pages site. The `layout` frontmatter option is also being used to set the layout of the current page to `product-guides`, which is a specific layout provided by GitHub Pages for documentation sites. The `title` frontmatter option is being used to set the title of the current page.", "a898a99c-d2fd-46a1-9ebe-744f6d88846a": "Enabling a security and analysis feature for all existing repositories means that the feature will be applied to all the repositories that already exist in the organization. This will scan the existing repositories for any issues related to the enabled feature. On the other hand, enabling a feature for all new repositories means that the feature will be automatically enabled for all the new repositories that are created in the organization.  Disallowing {% data variables.product.prodname_GH_advanced_security %} for an organization means that the organization will not be able to use any of the {% data variables.product.prodname_GH_advanced_security %} features, including the ones that are enabled for all existing repositories or for all new repositories. This is because disallowing {% data variables.product.prodname_GH_advanced_security %} overrides any settings related to {% data variables.product.prodname_GH_advanced_security %} for that organization. As a result, any existing or new repositories in that organization will not be scanned for issues related to the disabled features, and the organization will not be able to use any of the {% data variables.product.prodname_GH_advanced_security %} features until {% data variables.product.prodname_GH_advanced_security %} is allowed for that organization.", "eac1d1ea-b15b-4f8f-9ca7-15bc37c7a2ef": "In a high-availability configuration for {% data variables.product.prodname_ghe_server %}, both high availability and clustering can provide redundancy for the following failure scenarios:  - Primary node failure: If the primary node fails, the replica node takes over as the primary node. - Network failure: If there is a network failure between the primary node and the replica node, the replica node can still take over as the primary node. - Disk failure: If there is a disk failure on the primary node, the replica node can still take over as the primary node. - Power failure: If there is a power failure, the replica node can still take over as the primary node as long as there is a backup power source.  In a high-availability configuration, the replica node acts as a standby server, waiting for the primary node to fail before taking over. In contrast, in a clustering configuration, multiple nodes share the workload and can take over if the primary node fails. However, both configurations provide redundancy and ensure that the {% data variables.product.prodname_ghe_server %} instance remains available in the event of a failure.", "413d6297-6f93-4d98-9e70-b0453f04b695": "The variable `BITBUCKET_PROJECT_KEY` in this context is dynamically generated using a GitHub action called `github.repository_owner`. This action retrieves the owner of the GitHub repository where the workflow is running, and the value of this variable is assigned to the `BITBUCKET_PROJECT_KEY` variable in the Bitbucket Pipelines configuration file. This allows for the integration of GitHub and Bitbucket Pipelines, as the same repository can be used in both platforms.  2. The variable `BITBUCKET_STEP_TRIGGERER_UUID` in this context is `{% raw %}${{ github.actor_id }}`{% endraw %  Answer:  The variable `BITBUCKET_STEP_TRIGGERER_UUID` in this context is also dynamically generated using a GitHub action called `github.actor_id`. This action retrieves the ID of the user or application that triggered the GitHub workflow, and the value of this variable is assigned to the `BITBUCKET_STEP_TRIGGERER_UUID` variable in the Bitbucket Pipelines configuration file. This allows for the tracking of the user or application that initiated the pipeline, which can be useful for debugging and auditing purposes.    Based on the context information provided, can you explain the purpose of the variables `BITBUCKET_PROJECT_KEY` and `BITBUCKET_STEP_TRIGGERER_UUID` in this context?", "b7f5eb4c-7109-46b9-9b76-822eec2ac360": "{% data reusables.gpg.supported-gpg-key-algorithms %}  The context information provided lists the supported GPG key algorithms for GitHub. The answer is: RSA, RSA-PSS, ECDSA, and ECDSA-PSS.  % endif %}  {% ifversion upload-expired-or-revoked-gpg-key %}  Given the context information and not prior knowledge, answer the following question.   Can expired and revoked GPG keys be added to my GitHub account for verification purposes?  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  Yes, according to the context information provided, expired and revoked GPG keys can be added to your GitHub account for verification purposes. This is mentioned in the \"About addition of GPG keys to your account\" section.  % endif %}  {% ifversion upload-expired-or-revoked-gpg-key %}  Given the context information and not prior knowledge, answer the following question.   What happens when a commit is signed by a key that has expired or been revoked?  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  According to the context information provided, when a commit is signed by a key that has expired or been revoked, it will still be considered verified on GitHub. However, a label will be displayed indicating that the signing key is expired or revoked. This is mentioned in the \"About addition of GPG keys to your account\" section.  % endif %}  {% ifversion upload-expired-or-revoked-gpg-key %}  Given the context information and not prior knowledge, answer the following question.   How can I verify as many of my commits as possible on GitHub?  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.", "e9810533-f1a0-4876-bfd0-e5b5a4b58e96": "To promote a high availability replica for {% data variables.product.prodname_actions %} external storage, no additional configuration is required for {% data variables.product.prodname_actions %} during this process. When enabling a high availability configuration, any replicas are automatically configured to use the {% data variables.product.prodname_actions %} external storage configuration. To initiate a failover and promote a replica, you can follow the steps provided by your storage provider's documentation. No extra configuration changes are required for {% data variables.product.prodname_actions %} during this process.", "262540bb-00cc-4354-8323-6db27f869428": "OpenID Connect (OIDC) allows your {% data variables.product.prodname_actions %} workflows to access resources in your cloud provider without having to store any credentials as long-lived {% data variables.product.prodname_dotcom %} secrets by configuring your cloud provider to trust {% data variables.product.prodname_dotcom %}'s OIDC as a federated identity and updating your workflows to authenticate using tokens. This can be done using official actions provided by your cloud provider or by manually requesting the JSON Web Token (JWT) from {% data variables.product.prodname_dotcom %}'s OIDC provider using environment variables provided by {% data variables.product.prodname_dotcom %}.", "fe71f669-8d2d-4aeb-a60b-1835f701d950": "The environment variable that should be set before configuring and running the self-hosted runner application to disable TLS certificate verification is `GITHUB_ACTIONS_RUNNER_TLS_NO_VERIFY` and it should be set to `1`.", "dd9522b8-756d-4156-8f4c-76b6f4a1852b": "The URL to clone the repository being worked on as specified by the variable `CI_REPOSITORY_URL` is {% raw %}`${{ github.event.repository.clone_url }}`{% endraw %}.", "351dc383-c7d4-4f52-ae57-7900b7fc8144": "If you enable the setting to allow people with admin access to choose any visibility for an existing repository, even if the organization does not allow that type of repository to be created, it means that people with admin access will still be able to change the visibility of existing repositories, regardless of the organization's restrictions on repository visibility during creation. This could potentially lead to sensitive information being exposed if the new visibility setting is not in line with the organization's data protection policies. It's important to carefully consider the implications of enabling this setting and ensure that it aligns with your organization's security and compliance requirements.", "b6ba8b77-fba1-447c-b447-9a6cf3ede8fc": "The {% data variables.product.pat_generic %} for running a migration from Azure DevOps requires the scopes `work item (read)`, `code (read)`, and `identity (read)`.", "7555c8e0-495f-4d8e-af51-f72db9c9e44a": "To write a commit message and push changes in GitHub Desktop, follow these steps:  1. Stage the changes you want to include in your commit. 2. Click the \"Commit changes\" button in the top right corner of the window. 3. Write a commit message in the text field provided. 4. If you've collaborated on a commit, you can attribute it to multiple authors by clicking the \"Add author\" button and entering their name and email address. 5. Click the \"Commit changes\" button to commit your changes. 6. Push your changes to the remote repository by clicking the \"Push\" button in the top left corner of the window.  Note that if you've enabled the \"Tags\" and \"Push with commits\" settings, you'll be prompted to create a tag and push it along with your commit.  If you're not sure how to write a good commit message, refer to the GitHub Help documentation on commit messages for guidance.", "37d75131-aebe-421d-876f-e03d86f59c21": "The different ways to locate queries when creating a query suite in {% data variables.product.prodname_codeql %} are:  1. Using the `query` instruction to specify one or more `.ql` files relative to the {% data variables.product.prodname_codeql %} pack containing the suite definition. 2. Using the `queries` instruction to recursively scan a directory.  Note: When using custom queries with the {% data variables.product.prodname_codeql_cli %}, they must be in a {% data variables.product.prodname_codeql %} pack and contain the correct query metadata.", "2b7b9651-dbdf-4faa-9578-640d320ea497": "According to the context information provided, if you need a different operating system or require a specific hardware configuration for your runners, you can host your own runners for {% data variables.product.product_name %}. The context doesn't specify which operating systems are available for self-hosted runners, but it does mention that for {% data variables.product.prodname_dotcom %}, Ubuntu Linux, Microsoft Windows, and macOS runners are provided by {% data variables.product.company_short %}. If you're using {% data variables.product.prodname_ghe_cloud %} or {% data variables.product.prodname_ghe_server %}, you must host your own runners. For more information about self-hosted runners, you can refer to the documentation provided by {% data variables.product.company_short %}.", "5856d978-d471-41a5-bd12-b6b0010cbe31": "To manage instance settings for {% data variables.location.product_location %}, you can use the {% data variables.enterprise.management_console %}. This tool will guide you through the initial setup process when first launching {% data variables.location.product_location %}, and you can also use it to manage settings such as the license, domain, authentication, and TLS. For more information, refer to the documentation provided by {% data variables.product.prodname_dotcom %}.", "6defef02-1cad-4aa9-84db-19809984ebee": "The difference between granting read and write access to a machine user as a collaborator on a personal repository, and granting read, write, or admin access to it as an outside collaborator on an organization repository is that in the former case, the machine user has access only to the specific repository it was added to, while in the latter case, it has access to all the repositories in the organization, with varying levels of permissions depending on whether read, write, or admin access is granted. This means that granting read and write access to a machine user as a collaborator on a personal repository is more restrictive, while granting read, write, or admin access to it as an outside collaborator on an organization repository provides broader access to multiple repositories.", "447f098f-3bce-4aa0-9136-97da9c1a5dd0": "The purpose of the command \"bundle exec appraisal install\" in the provided context is to install appraisal, which is a Ruby gem used for managing multiple Ruby versions and their associated gems. This command is executed using the bundle executable, which is a Ruby tool for managing dependencies, and the appraisal gem, which is being installed. The installation of appraisal is necessary for running the appraisal command, which is executed in the next step using the command \"bundle exec appraisal rake\". This step is likely being used to ensure that the application can be run on different Ruby versions, which is a common practice in Ruby on Rails development.", "b2eb3a6b-c34e-4325-b2de-86ed87b59479": "Without further context, it is unclear what \"AUTOTITLE\" refers to or what its meaning is. The provided context does not offer any clues or explanations to help determine its significance.", "401bc528-30ac-48a2-bb23-c90c586b538f": "To view the logs of a failed workflow run in GitHub Actions to diagnose failures and troubleshoot issues, follow these steps:  1. Go to your GitHub repository. 2. Click on the \"Actions\" tab in the left-hand menu. 3. Find the failed workflow run and click on it to open it. 4. Click on the job that failed to view its logs. 5. Scroll down to the step that failed and click on it to view its logs. 6. To view the logs for a specific line, click on the line number in the log output to copy a permalink, which you can share with your team.  To search for specific steps or text within the logs of a workflow run in GitHub Actions, follow these steps:  1. Go to your GitHub repository. 2. Click on the \"Actions\" tab in the left-hand menu. 3. Find the workflow run you want to search and click on it to open it. 4. Click on the job that contains the step you want to search. 5. In the upper-right corner of the log output, type your search query into the \"Search logs\" search box. 6. Only expanded steps are included in the search results. 7. Use keywords, phrases, or regular expressions to narrow down your search. 8. Use the \"Next\" and \"Previous\" buttons to navigate through the search results. 9. Use the \"Clear\" button to clear the search query and view the full log output.  Note: The search functionality is case-sensitive, so be sure to use the correct capitalization in your search query.", "62a1457c-52fa-4bcc-8dac-b4e38020c5de": "If the token stored in the local file becomes invalid, you should delete the `.token` file and then run the `login` command in your terminal from the directory where `app_cli.rb` is stored to generate a new user access token.", "cd1a6dd8-26d4-4916-b4ed-dfc304cd7614": "To permanently remove a project from GitHub, follow these steps:  1. Go to the project you want to delete. 2. Click on the project settings icon (the gear icon) in the top right corner of the page. 3. In the left-hand menu, click on \"Danger zone\". 4. Click on \"Delete this project\" next to the warning message. 5. Read the warnings, then type the name of your project into the text box. 6. Click \"I understand the consequences, delete this project\".  To close a project on GitHub without permanently deleting it, follow these steps:  1. Go to the project you want to close. 2. Click on the project settings icon (the gear icon) in the top right corner of the page. 3. In the left-hand menu, click on \"Danger zone\". 4. Click on \"Close this project\" next to the warning message.  The consequences of closing a project are that it will be removed from the list of projects, but the repository and its contents will still be accessible. You can reopen the project at any time by following these steps:  1. Go to your organization or user profile. 2. Click on \"Projects\" in the left-hand menu. 3. Click on the three dots (...) next to the closed project. 4. Click on \"Reopen project\".  If you don't have enough context to answer a question, you should say that you don't have enough information to provide an answer.", "c6f12f7d-f8ef-416d-8279-d724487d4790": "The `--format` option specifies the format for the results file generated during analysis, while the `--output` option specifies the location where the SARIF results file should be saved, including the desired filename with the `.sarif` extension. In other words, `--format` determines the format of the results, while `--output` determines where the results should be saved.", "e70fa3cb-670f-4e2e-bfb2-4602c5917619": "Input properties can be passed to an action or a reusable workflow either through the `with` keyword in the `jobs` section of an external workflow that calls the action or reusable workflow, or through the `workflow_call` event configuration for reusable workflows, or through the `workflow_dispatch` event configuration for manually triggered workflows. The input names and types are defined in these configurations. The input properties are then accessible in the `inputs` context of the action or reusable workflow. This context is only available in a reusable workflow or in a workflow triggered by the `workflow_dispatch` event. The properties in the `inputs` context are defined in the workflow file. They are defined as string or boolean values.", "8435503c-fcca-4ff3-9d0e-dec64af1f08f": "The purpose of using the \"--extractor-options-file\" command-line option in CodeQL database init or \"codeql database begin-tracing\" is to specify extractor option bundle files that set extractor options. These files are read in the order they are specified, and if different extractor option bundle files specify the same extractor option, the behavior depends on the type that the extractor option expects. String options will use the last value provided, and array options will use all the values provided, in order. This option is processed before extractor options given via \"--extractor-option\". When passed to CodeQL database init or \"codeql database begin-tracing\", the options will only be applied to the indirect tracing environment. If your workflow also makes calls to \"codeql database trace-command\", then the options also need to be passed there if desired. This option is useful for setting extractor options in a more organized and structured way, especially when dealing with multiple extractors and options.", "f2370e56-7286-413d-9f10-82c56697b95a": "Yes, according to the context information provided, to remove an external collaborator from a repository, you can access the repository settings and view the access overview. From there, you can follow the steps to remove the collaborator's access to the repository. The specific process may vary depending on the platform or version control system being used, but the general idea is to locate the collaborator's access level and revoke it. If you're not familiar with the platform or process, it's always a good idea to consult the documentation or support resources for more detailed instructions.", "ff5e9722-09aa-4760-8b80-04cd50144b00": "Based on the context provided, the main difference between the `issues` and `ping` events in the context of the webhook delivery is the type of event being delivered. The `issues` event is related to issues in a GitHub repository, specifically, the opening or closing of an issue. The `ping` event, on the other hand, is a test event sent by GitHub to ensure that the webhook is working properly. This event does not contain any specific data, as indicated by the message \"GitHub sent the ping event\" in the code snippet.", "6d53abe8-8ef6-47e4-b4f9-87e3254e94b0": "The app should authenticate as itself to generate an installation access token, as stated in the context information provided.", "35549cd3-ff87-48c5-8d6b-10819f21c84b": "To edit an existing page on a wiki on GitHub, follow these steps:  1. Go to the repository that contains the wiki. 2. In the left-hand menu, click on \"Wiki\" to open the wiki. 3. Navigate to the page you want to edit using the sidebar on the right-hand side. 4. Click on the \"Edit\" button in the upper-right corner of the page. 5. Make your changes to the page using the text editor. 6. Type a commit message describing the changes you made in the \"Edit message\" field. 7. Click \"Save Page\" to commit your changes and publish the updated page.  Alternatively, you can also clone the wiki to your local machine using Git, make your changes, and then push your changes back to the repository. This allows you to work offline and collaborate with others using a Git workflow.", "30ee4cd2-971e-4a93-b003-86f0e26a334e": "In the context of the GraphQL schema provided, the term \"implementation\" signifies that object `Y` conforms to the requirements specified by interface `X`. This means that object `Y` must have all the fields and arguments defined in interface `X`, as well as any additional fields specific to object `Y`. The concept of implementation in this context is similar to the concept of inheritance in object-oriented programming, where a subclass inherits the properties and methods of a superclass. However, in GraphQL, the relationship between objects and interfaces is more flexible, as objects can implement multiple interfaces and interfaces can be nested within each other.", "4b151b49-ebd5-4025-8636-26461f69e78b": "Based on the context information provided, the purpose of the \"AUTOTITLE\" setting is unclear as it is mentioned three times without any further explanation. Without additional context, it is not possible to determine the specific meaning or function of this setting. It is recommended to consult the resources provided, such as \"AUTOTITLE,\" \"AUTOTITLE,\" and \"AUTOTITLE,\" for further clarification.", "139498e1-53a5-4072-897c-583f5dbfca37": "Context information can be utilized in creating questions for an examination or quiz by providing real-life scenarios or situations that the students may encounter in their respective fields. This can help them apply their knowledge and skills in a practical setting. Additionally, context information can be used to make the questions more relevant and relatable to the students, making the learning experience more engaging and effective. However, if the context doesn't provide enough information to answer a question, it's essential to communicate that to the students and explain why more context is needed to provide an accurate answer.", "8fde813f-ba36-4fe1-9107-e71f59889f7b": "A self-hosted runner is a system that you deploy and manage to execute jobs from {% data variables.product.prodname_actions %} on {% ifversion ghae or ghec %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %}. It differs from a {% ifversion ghae %}{% data variables.product.prodname_dotcom %}-hosted runner in that a self-hosted runner is a highly configurable way to run workflows in your own custom environment, while a {% data variables.product.prodname_dotcom %}-hosted runner offers a quicker, simpler way to run workflows. Self-hosted runners can be added at various levels in the management hierarchy, including repository-level, organization-level, and enterprise-level, while {% data variables.product.prodname_dotcom %}-hosted runners are provided by {% data variables.product.prodname_dotcom %}. Additionally, self-hosted runners can be updated automatically when a new version is released, while {% data variables.product.prodname_dotcom %}-hosted runners are managed by {% data variables.product.prodname_dotcom %}.", "3d6c647d-dd73-441f-8252-250bd37f61b4": "SendGrid is mentioned as an SMS notification provider for 2 Factor Authentication in the context information. This suggests that SendGrid plays a role in sending SMS messages as a secondary form of authentication, in addition to a user's password or username, to verify their identity. This helps to provide an extra layer of security to the login process, making it more difficult for unauthorized users to gain access to sensitive information or accounts.", "eb4f1bc1-9c2e-4594-a879-f4cced6d049b": "Based on the context provided, required cookies are essential for the proper functioning of GitHub's websites and are necessary for the websites to work. Analytics cookies are used to understand how users interact with the websites, which helps GitHub improve their functionality. Social media cookies are used to show users personalized ads and content based on their social media profiles and activity on GitHub's websites. Advertising cookies are used to show users new ads based on the ads they have already seen. Overall, these cookies contribute to the overall functionality of GitHub's websites by providing a better user experience, improving performance, and enabling personalized advertising.", "85f8ffd6-33bb-474a-a5be-c57a89430f21": "The red {% octicon \"diff-removed\" aria-label=\"The diff removed icon color-red\" %} icon indicates removed files, the yellow {% octicon \"diff-modified\" aria-label=\"The diff modified icon color-yellow\" %} icon indicates modified files, and the green {% octicon \"diff-added\" aria-label=\"The diff added icon color-green\" %} icon indicates added files in the \"Changes\" tab of {% data variables.product.prodname_desktop %}.", "352a3a97-0224-4f19-bc23-c7b6dbe422ef": "Unfortunately, the provided context doesn't include information about your organization's eligibility for the free tier of the {% data variables.product.prodname_container_registry %}. You may want to check the documentation or contact {% data variables.product.company_short %} support for more information about the free tier and eligibility requirements.", "92845972-48a7-4d75-aa20-fb5918450bb2": "To notify a specific person or team on GitHub using mentions, you can type `@` followed by their username or team name in the comment or issue you're creating. This will render as bold, clickable text. When you mention a parent team, members of its child teams also receive notifications, simplifying communication with multiple groups of people. However, a person will only be notified about a mention if they have read access to the repository and, if the repository is owned by an organization, they are a member of the organization. This ensures that only relevant people receive the notification and helps to prevent unnecessary clutter in their notifications list.", "7e88def8-4a72-4c05-8fb5-096527825f21": "The context information provided doesn't give enough details to define specific outputs for jobs in GitHub Actions. However, in general, context information can be utilized to define outputs for jobs by providing relevant data or variables that can be used by subsequent jobs or steps in the workflow. This can include things like build artifacts, test results, or environment variables. By defining outputs for jobs, you can make it easier to reuse work across multiple jobs and workflows, and reduce the amount of duplicated effort required.", "dd540c0a-fc47-49ea-969c-82415f31e01a": "Students on the {% data variables.product.prodname_global_campus %} portal can also gain in-depth understanding of features, tools, or topics with curated experiences, explore and claim offers from the Student Developer Pack, view upcoming in-person and virtual events, see assignments from GitHub Classroom, rewatch recent Campus TV episodes, and discover student-created repositories from {% data variables.product.prodname_community_exchange %}.", "eff1ec44-a47a-4244-a984-719bdc46b2b5": "The `--evaluator-log` option in the context information provided is used to output structured logs about evaluator performance to a specified file. This option allows for the monitoring of evaluator performance and provides a stream of JSON objects that can be summarized using the `codeql generate log-summary` command. The format of the log file is subject to change without notice, but it will be overwritten if it already exists. This option differs from the `--warnings` option, which is used to control how warnings from the QL compiler are handled. The `--warnings` option can be set to `hide`, `show`, or `error`, depending on the desired behavior.", "09c00393-d3d6-4305-a9ba-d44198696774": "Job summaries are ordered by job completion time when multiple jobs generate summaries. To add a blank line between Markdown content in a summary, you can append an empty string (`\"\"`) to the `GITHUB_STEP_SUMMARY` environment file using either Bash or PowerShell. This automatically adds a newline character after the blank line.", "22d06123-7842-4725-9bf9-6f574eb81baa": "According to the context information provided, the hardware considerations for launching {% data variables.product.prodname_ghe_server %} on Azure include ensuring that the machine type selected meets the minimum resource requirements for {% data variables.product.prodname_ghe_server %} and considering memory optimization for improved performance. It's also recommended to review the minimum resource requirements for {% data variables.product.prodname_ghe_server %} before launching. Additionally, the context mentions that most actions needed to launch the VM can be performed using the Azure Portal, but it's recommended to install the Azure command line interface (CLI) for initial setup.", "6577d4fd-35c1-4e12-ad43-7de4d23b3381": "The current scope limitation of {% data variables.product.prodname_copilot_for_prs %} in pull requests is that larger pull requests with more than 30 referenced files may require more time to be processed thoroughly due to capacity constraints. The first 30 files are accounted for, and any additional files are omitted from the summarization. This limitation may impact the summarization of larger pull requests by potentially omitting important details or context from the summary, which may require the user to make modifications before saving and publishing their pull request with this description.", "0b955d31-0b8c-4271-945d-96c22950bd60": "To give options to the JVM running the CLI tool provided, you can use the `-J` option followed by the desired JVM options. This option is advanced and should be used with caution as options containing spaces may not be handled correctly. Here's an example:  ``` codeql scan -J-Xms1G -J-Xmx2G myrepo ```  In this example, we're passing the `-Xms1G` and `-Xmx2G` options to the JVM to set the initial and maximum heap sizes, respectively. These options will be passed to the JVM when the CLI tool is executed.", "9323342c-8cbf-4bdd-857e-e80b82b37a0a": "To cancel a workflow run in {% data variables.product.prodname_dotcom %}, you can follow these steps:  1. Go to your repository in {% data variables.product.prodname_dotcom %}. 2. Click on the \"Actions\" tab. 3. Select the workflow that you want to cancel. 4. Click on the name of the workflow run that you want to cancel, which should be in the \"Queued\" or \"In Progress\" state. 5. Click on the \"Cancel workflow\" button in the upper-right corner of the workflow.  When {% data variables.product.prodname_dotcom %} cancels a workflow run, it first re-evaluates the `if` conditions for all currently running jobs. If the condition evaluates to `true`, the job will not get canceled. For jobs that need to be canceled, {% data variables.product.prodname_dotcom %} sends a cancellation message to all the runner machines with jobs that need to be canceled. For steps that need to be canceled, {% data variables.product.prodname_dotcom %} sends `SIGINT/Ctrl-C` to the step's entry process, and if the process doesn't exit within 7500 ms, {% data variables.product.prodname_dotcom %} sends `SIGTERM/Ctrl-Break` to the process, then waits for 2500 ms for the process to exit. If the process is still running, {% data variables.product.prodname_dotcom %} kills the process tree.", "82f156ef-9e27-4ddf-a393-b26398c088d5": "According to the provided context information, the terms and conditions that apply to the use of Previews are as follows:  1. Limited right to use a non-production instance of the Preview. 2. Previews are provided \"AS-IS\", \"WITH ALL FAULTS\" and \"AS AVAILABLE\". 3. GitHub may change or discontinue Previews at any time without notice. 4. Any information we give you about a private Preview will be considered GitHub\u2019s confidential information. 5. If you choose to provide comments or suggestions about a Preview, we may use that feedback for any purpose without obligation of any kind. 6. GitHub\u2019s maximum liability is limited to direct damages up to US $500. 7. GitHub has no obligation to defend, indemnify, or hold you harmless for claims brought by third parties arising from your use of Previews.  These terms and conditions apply to Previews provided for preview, evaluation, demonstration or trial purposes, or pre-release versions of those, such as alpha, beta, or early access. If your Agreement does not include terms and conditions that address Previews, then these terms apply.", "174e85fe-1fab-4525-bee8-2e375a5db245": "To open a repository in your preferred editor tool using {% data variables.product.prodname_desktop %}, you can use the keyboard shortcut Shift + Command + A. This will open the repository in your preferred editor tool, which you can set up in your preferences. If you haven't set up your preferred editor tool yet, you can do so by going to Preferences (Command + ,) and selecting the \"Code Editing\" tab. From there, you can choose your preferred editor tool from the \"Code Editing\" section. Once you have set up your preferred editor tool, you can use the Shift + Command + A shortcut to open the repository in that tool.", "0c445958-10ae-48d0-89fd-7b9e8f8c8bda": "The value of the environment variable `BITBUCKET_CLONE_DIR` represents the local directory where the Git repository is cloned during the build process. This variable can be used to access the cloned repository and perform various operations such as running tests, building artifacts, or executing scripts. In this context, it can be used to navigate to the cloned repository and execute commands or scripts within it. For example, to run tests in the cloned repository, you can use the following command:  ```bash cd $BITBUCKET_CLONE_DIR && python -m unittest discover tests/ ```  This command changes the current working directory to the cloned repository and then runs the unittest discovery tool to execute all the tests in the tests directory.", "99ff899a-daa3-42b4-9ae3-7f6413db7f3d": "According to the context information provided, it is the responsibility of the user to ensure that the hostnames for replica nodes in GitLab Enterprise Edition are unique and differ from the corresponding active node's hostname. The warning in step 7 highlights the importance of this requirement. However, the context doesn't provide any specific guidance on how to achieve this. It's up to the user to come up with a pattern for the replica nodes' hostnames that meets this requirement.", "b0ba60b6-e93a-4e6f-8825-7268d0433503": "To allow access to components in a private repository, you can follow these steps:  1. Go to the repository that contains the components you want to share. 2. Click on the settings icon in the top right corner of the repository page. 3. Select \"Settings\" from the drop-down menu. 4. In the left-hand menu, select \"Actions\". 5. Click on the component you want to share. 6. In the component's settings, scroll down to the \"Access\" section. 7. Choose one of the access settings:     - \"Not accessible\" - Components in this repository cannot be accessed by other repositories.    - \"Accessible from repositories in the 'ORGANIZATION NAME' organization\" - Components in this repository can be accessed by other repositories that are part of the 'ORGANIZATION NAME' organization.    - \"Accessible from repositories in the 'ENTERPRISE NAME' enterprise\" - Components in this repository can be accessed by other repositories that are part of the 'ENTERPRISE NAME' enterprise.  8. Click \"Save\" to apply the settings.  Note that the access settings for components are separate from the repository-level access settings. You can configure both settings independently to control access to components and the repository as a whole.  If you're using GitHub Enterprise Cloud (GHEC), the access settings for components are the same as for repositories. However, the \"Accessible from repositories in the 'ORGANIZATION NAME' organization\" setting is replaced with \"Accessible from repositories in the 'ENTERPRISE NAME' enterprise\" to reflect the fact that components can only be accessed by other repositories within the same enterprise.  If you're using GitHub Enterprise Server (GHES) or GitHub Advanced Security (GHAS), the access settings for components are the same as for repositories, but they're configured at the repository level instead of at the component level. To configure repository-level access settings, follow the steps in the previous answer.  If you're using GitHub.com, the access settings for components are configured at the component level, as described above.  In summary, to allow access to components", "4d9610b1-08b4-40a1-8179-22c5885fd850": "To check which access policies are being applied to a secret in your organization using the GitHub CLI, you can use the `gh secret view` subcommand with the `--org` or `-o` flag followed by the organization name and the name of the secret. This will display the details of the secret, including its access policies.  ```shell gh secret view --org ORG_NAME SECRET_NAME ```  Alternatively, you can list all secrets for your organization and their access policies using the `gh secret list` subcommand with the `--org` or `-o` flag followed by the organization name.  ```shell gh secret list --org ORG_NAME ```  This will display a list of all secrets in your organization, along with their names, values, and access policies.  Note that the `gh secret view` and `gh secret list` subcommands require the `admin:org` scope to access organization secrets. If you haven't already authorized this scope, you'll need to run `gh auth login --scopes \"admin:org\"` to do so.", "11a01e27-66a7-43d9-a5f0-7bad6366b923": "Upon Customer\u2019s reasonable request, unless prohibited by law, GitHub will return, destroy, or deidentify all Customer Personal Data and related data at all locations where it is stored after it is no longer needed for the Permitted Purposes within thirty days of request. GitHub may retain Customer Personal Data and related data to the extent required by the Applicable Data Protection Laws, and only to the extent and for such period as required by the Applicable Data Protection Laws, provided that GitHub will ensure that Customer Personal Data is processed only as necessary for the purpose specified in the Applicable Data Protection Laws and no other purpose, and Customer Personal Data remains protected by the Applicable Data Protection Laws. This process for deleting Customer Personal Data and related data after it is no longer needed for the Permitted Purposes is outlined in Section 3.3 of the DPA.", "d00e18f6-57db-440d-afed-6d4bf0652806": "As an organization owner, you can assign a custom organization role to a user or team in a GitHub organization by following these steps:  1. Go to the organization settings by clicking on your organization's name in the top left corner of GitHub, then clicking \"Settings\" in the drop-down menu. 2. In the left-hand menu, click \"Access\" and then click \"Organization roles\" under \"Collaborators and teams\". 3. Click \"New role assignment\" at the top right of the page. 4. Search for the user or team you want to assign the role to, then select it. 5. Choose the custom organization role you want to assign from the drop-down menu. 6. Click \"Add new assignment\" to complete the process.  Note that as an organization owner, you can assign multiple custom roles to the same user or team, but you must repeat these steps for each role you want to assign.", "8bfa311c-1b18-4bef-a4d5-98de1f4c2421": "Students enrolled in {% data variables.product.prodname_global_campus %} receive the following benefits from {% data variables.product.prodname_dotcom %}:  1. Access to the {% data variables.product.prodname_global_campus %} portal, where they can connect with a local Campus Expert, gain in-depth understanding of features and tools, explore offers from the Student Developer Pack, view upcoming events, assignments, and Campus TV episodes, and discover student-created repositories.  2. A free subscription for {% data variables.product.prodname_copilot %}, which is automatically notified to verified students when they visit the subscription page in their account settings.  3. {% data variables.product.prodname_github_codespaces %} benefits, which include a free monthly usage limit and discounted hourly rates for additional usage.  Note: The context doesn't provide details on the exact amount of the discounted hourly rates for additional usage.", "ec386f1f-4911-4851-badc-08d025c78d4d": "The label that should be applied to an issue that has been inactive for a certain period of time using the `actions/stale` action in the workflow is specified by the `stale-issue-label` value in the context information provided.", "22e8c4f9-d856-45f6-a7bb-1cbaa4dfb5c3": "To copy the new token to your clipboard in [PRODUCT NAME], you can click the {% octicon \"copy\" aria-label=\"Copy token\" %} icon next to the blurred-out token on the \"{% data variables.product.pat_generic_caps_plural %}\" page.", "7d06aed6-a9c4-4aeb-8240-d161ec80bc7e": "GitHub shares personal data with service providers who process the information on their behalf to provide or improve their Service. These service providers may perform various services such as payment processing, customer support ticketing, network data transmission, web analytics, marketing operations, security, online advertising, and other similar services. GitHub's affiliates also have access to personal data, for example, where they provide services on GitHub's behalf, share common data systems, or where access is needed to operate and provide the Service. GitHub has the responsibility for the processing of personal information it receives under the Data Privacy Framework (DPF) Principles and subsequently transfers to its affiliates.", "84c384ac-2a93-4232-b6de-ac31aa3080d7": "Based on the information provided, the following factors should be considered when deciding on a migration timeline for moving repositories from Bitbucket Server to GitHub:  1. Understanding what data can be migrated by {% data variables.product.prodname_importer_proper_name %} 2. Reviewing the data that's migrated from Bitbucket Server 3. Making a list of any data that needs to be manually migrated or recreated 4. Deciding whether an organization owner or someone else will run the migration 5. Granting the migrator role to someone else if necessary 6. Confirming that the migrator has admin or super admin permissions and SFTP access for the Bitbucket Server instance  These factors will help ensure a smooth and successful migration process.", "f1de3c00-d258-4e55-947e-40fc4f2f07e8": "Global webhooks can be used to notify an external web server when certain events occur within an enterprise, such as the creation, deletion, or modification of repositories or organizations. This allows for the automation of tasks or the enforcement of rules related to user and organization management. For example, an organization may configure a global webhook to send a notification to an external server when a new repository is created. The server can then automatically perform a task, such as creating a new user account or assigning specific permissions to the repository, based on the information provided in the webhook's payload. This can help streamline and simplify the management of user and organization activities within the enterprise.", "5e687138-3069-475e-9339-65f6484d1a96": "Yes, based on the context information provided, a Listing may be removed from GitHub's Marketplace under the following circumstances:  1. Infringement of intellectual property rights: If there is an allegation or actual infringement of any copyright, trademark, trade secret, trade dress, patent, or other intellectual property right of any person.  2. Defamation: If there is an allegation or actual defamation.  3. Violation of third-party rights: If there is an allegation or actual violation of any third party's right of publicity or privacy.  4. Non-compliance with applicable law: If there is an allegation or determination that the Listing does not comply with applicable law.  In such cases, GitHub is not responsible for refunds to affected buyers for Listings removed within a year before the date of takedown, except in cases where the buyer purchased the Listing during the year before the date of takedown. GitHub reserves the right to review Listings for compliance with this agreement, Marketplace program policies, and other applicable terms, obligations, laws, or regulations, and may refuse to include a Listing on Marketplace at its sole discretion.", "3bbec21a-3777-4fc9-878b-28637e02f975": "The \"name\" field in the provided YAML configuration file is used to specify the name of the GitHub Actions workflow, which helps to identify the workflow when running or viewing its logs.  The \"on\" field specifies the events that trigger the workflow to run. In this case, the workflow is configured to run when a new release is published to the repository. The \"types\" field is used to specify the types of release events that should trigger the workflow, in this case, \"published\".  Without further context, it's not possible to provide more detailed information about the workflow or its functionality. However, based on the provided configuration, it appears to be a workflow for automatically building, testing, and publishing a Python package to PyPI when a new release is published to the repository.", "ccdcf7a8-7901-41de-a4af-f8927ec13972": "In this case, the context information provided is not enough to determine the name of the repository. Please provide more context to proceed with creating a new repository.  Instructions:  1. Log in to your GitHub account. 2. Click on the plus sign (+) in the top right corner of the GitHub homepage. 3. Select \"New repository\" from the drop-down menu. 4. Enter the name of the repository according to the context information provided. If you don't have enough context to determine the name, please provide more information before proceeding. 5. Select the repository's visibility (public, private, or internal). 6. Choose whether to initialize the repository with a README file, a license, and a .gitignore file. 7. Click \"Create repository\" to create the new repository.  Note: If you're not sure about the repository's visibility, please consult with your organization's policies or your team's preferences before proceeding.", "484ef955-e26f-4658-9d0c-e02a8ad1a390": "To write a log file with a custom name using the CLI, you can use the flag `--log-file=<path>` to specify the path of the desired log file. This flag allows you to have full control over the name and location of the log file. If you prefer to write the log output to standard error instead, you can use the flag `--log-to-stderr` instead.", "06756291-9015-40a2-90e6-bfbbafc9a383": "Unfortunately, based on the provided context information, it is unclear how to request a feature for {% data variables.product.prodname_copilot %}. The context only mentions requesting a fix for a bug and sharing feedback about {% data variables.product.prodname_copilot_chat %}. If you are looking to request a feature for {% data variables.product.prodname_copilot %}, you may want to check the {% data variables.product.prodname_copilot %} FAQ or contact GitHub support for further guidance.", "c2eac108-54f8-436c-b0a4-b5f15ead99d2": "The title and format for a further reading section should be written as a bulleted list with each item in the list containing the title of the article or external resource in italics or quotation marks, followed by the name of the resource in parentheses if it is an external resource. The section should be titled \"Further reading\" and should be used sparingly when it provides high value.", "39082cac-1363-422f-9387-19ba0361f620": "Without further context, it is unclear what \"AUTOTITLE\" is or what its purpose might be in a document. It could potentially be a formatting command or a placeholder for a title that will be automatically generated by the software or program being used. However, without more information, it is impossible to determine the exact purpose of \"AUTOTITLE\" in this context.", "8f2a82fa-0b5d-4297-9cff-94f0fa4e0f6e": "Based on the context information provided, it is unclear which specific features or functionalities mentioned in the text material can be utilized to create questions for the quiz/examination. The context only provides information about data reusables, profile settings, scheduled reminders, and the option to delete reminders. Without further context or information, it is impossible to determine which of these features can be used to create questions for a quiz or examination.", "150f04db-26a2-4578-9197-4a179bf3ccd2": "If a branch in {% data reusables.desktop.product-name %} is protected and a commit is attempted, Desktop will warn you. To move your changes, click \"switch branches\". To commit your changes to the protected branch, click \"Commit to BRANCH\".", "8436c033-2804-4be3-99ed-650f98c3a6d9": "A team maintainer or organization owner can set a profile picture for a team in GitHub by accessing the organization settings, selecting the specific team, going to the team settings, and uploading a new picture under the \"Profile picture\" section. If a profile picture is not set for the team, the team's profile picture will match the organization's profile picture.", "655f2dcd-151d-42a0-8494-688adbbe4d73": "To export diagnostic information from a CodeQL database for a failed analysis, you can use the `codeql database export-diagnostics` command. This command is available since version `v2.12.6`. The following options are available:  - `<database>`: The path to the CodeQL database under construction. If the `--db-cluster` option is given, this is not a database itself, but a directory that contains databases, and all of those databases will be processed together. - `--format=<format>`: The format in which to write the results. Available formats are:   - `raw`: A list of raw, uninterpreted diagnostic messages as JSON objects.   - `sarif-latest`: Static Analysis Results Interchange Format (SARIF), a JSON-based format for describing static analysis results. This format option uses the most recent supported version (v2.1.0).   - `sarifv2.1.0`: SARIF v2.1.0.   - `text`: A bullet point list of diagnostic messages. - `--[no-]db-cluster`: Indicates that the directory given on the command line is not a database itself, but a directory that contains one or more databases under construction. Those databases will be processed together. - `-o, --output=<output>`: The output path to write diagnostic information to. - `--sarif-exit-code=<sarifExitCode>`: (SARIF formats only) Exit code of the failing process. - `--sarif-exit-code-description=<sarifExitCodeDescription>`: (SARIF formats only) Reason that the failing process exited. - `--sarif-category=<category>`: (SARIF formats only) Specify a category for this analysis to include in the SARIF output. This can be used to distinguish multiple analyses performed on the same commit and repository, but on different languages or different parts of the code.  These options allow you to customize the output format and provide additional context, such as exit codes and categories, to better understand the results of the failed analysis.", "da270634-d7bd-409f-afab-97e4de870862": "To customize your inbox with up to 15 of your own custom filters on GitHub, you can follow these steps:  1. Access your notifications by clicking on your profile picture in the top right corner of the GitHub page and selecting \"Notifications\" from the drop-down menu.  2. In the left sidebar, click on \"Filters\" to open the filter settings.  3. Click on the \"Add filter\" button to create a new filter.  4. Enter a name for your filter and a filter query. For example, to only see notifications for a specific repository, you can create a filter using the query `repo:octocat/open-source-project-name reason:participating`.  5. Click \"Create\" to save your filter.  By default, your inbox has filters for when you are assigned, participating in a thread, requested to review a pull request, or when your username is @mentioned directly or a team you're a member of is @mentioned. These filters are provided by GitHub to help you manage your notifications more efficiently.", "6800d6c0-75c9-4eb5-993d-733032b469ee": "According to the provided context, an organization's billing manager or enterprise owner can add support entitlements to members of their organization. This is mentioned in a note that appears in the support plan. Having a support entitlement is significant because it provides access to {% data variables.contact.premium_support %}, which offers additional benefits such as crisis prevention, technical advisory hours, application upgrade assistance, and cloud planning. Enterprise owners and billing managers automatically have a support entitlement, but they can also extend this entitlement to other members of their organization. This can be useful for ensuring that all members of the organization have the necessary support and resources to address any issues that arise.", "e6e36e61-22c3-406e-9ee2-c88cc327390b": "The different types of conclusions that can be associated with a check run are `action_required`, `cancelled`, `timed_out`, `failure`, `neutral`, `skipped`, and `success`. These conclusions signify the following:  - `action_required`: Indicates that the user needs to take some action to resolve the issue. - `cancelled`: Indicates that the check run was cancelled before it could complete. - `timed_out`: Indicates that the check run timed out before it could complete. - `failure`: Indicates that the check run failed to complete successfully. - `neutral`: Indicates that the check run did not produce any significant results. - `skipped`: Indicates that the check run was skipped for some reason. - `success`: Indicates that the check run completed successfully.  The \"stale\" conclusion is different from the others because it is automatically assigned to a check run if it is in an incomplete state for more than 14 days. This conclusion signifies that the check run has been inactive for an extended period of time and may no longer be relevant. Only {% data variables.product.prodname_dotcom %} can mark check runs as \"stale\".", "e22ca88b-f4b8-4e5b-ae1c-ccf715a668e5": "The two types of custom domains supported by {% data variables.product.prodname_dotcom_the_website %} for {% data variables.product.prodname_pages %} are \"www\" subdomains and apex domains.  Examples: - \"www\" subdomain: `www.example.com` - Custom subdomain: `blog.example.com` - Apex domain: `example.com`  It is recommended to use a \"www\" subdomain even if an apex domain is also used for a {% data variables.product.prodname_pages %} site because when you create a new site with an apex domain, {% data variables.product.prodname_pages %} automatically attempts to secure the \"www\" subdomain for use when serving your site's content. This ensures that your site can be accessed using both the \"www\" subdomain and the apex domain. If you configure a \"www\" subdomain, {% data variables.product.prodname_pages %} automatically attempts to secure the associated apex domain.  When configuring a \"www\" subdomain, you need to make the necessary DNS changes to use it. After configuring a custom domain for a user or organization site, {% data variables.product.prodname_pages %} replaces the \".github.io\" or \".github.io\" portion of the URL for any project sites owned by the account that are published publicly and do not have a custom domain configured with the custom domain. For example, if the custom domain for your user site is \"www.octocat.com\", and you have a project site with no custom domain configured that is published from a repository called \"octo-project\", the {% data variables.product.prodname_pages %} site for that repository will be available at \"www.octocat.com/octo-project\".  {% note %}  **Note:** The URLs for project sites that are privately published are not affected by the custom domain for your user or organization site.  {% endnote %}", "533843f9-0493-4eee-bdfa-2df9a84d9b02": "The \"ActionsRunnerService\" mentioned in the context information appears to be a service related to GitHub Actions runners. It is responsible for listening for job requests, running jobs, and providing status updates. The service also connects to GitHub and manages updates. Based on the log messages provided, it seems to be running successfully and has recently completed a job.", "c3b65de2-038a-4310-bd8b-a990c51d4ee0": "The HTTP method used for the \"Create an issue\" endpoint in {% data variables.product.prodname_dotcom %}'s REST API is `POST`.", "a193b72d-662d-41c7-bd18-40997b353c79": "The purpose of the \"ze CodeQL\" line in the provided context information is to indicate that the GitHub Action being used is the CodeQL Action, which is an analysis tool for code quality and security. The \"ze\" prefix is likely a custom naming convention for this specific usage of the CodeQL Action. The \"debug: true\" parameter is being passed to the Action, which may enable additional logging or debugging output for troubleshooting purposes.", "e795161d-57fe-486c-8971-8817cbccdbaf": "To export a list of all the members in your enterprise from GitHub's web UI, follow these steps:  1. Access your enterprise organization on GitHub. 2. Click on the \"People\" tab in the left-hand menu. 3. To the right of \"Members\", click the \"Download CSV Report\" icon ({% octicon \"download\" aria-hidden=\"true\" %}). 4. If your enterprise has less than 1,000 members, the report will download automatically.  Note: The membership information report includes information such as username, display name, whether two-factor authentication is enabled, organization ownership, and pending invitations. Additional information may be included based on your enterprise's configuration. You can also use GitHub's APIs to retrieve this information.", "28950cc1-40fe-4f8f-8cd3-4f540e950354": "Yes, here's an example:  ```yaml env:   MY_ENV_VAR: ${{ github.ref == 'refs/heads/main' ? 'main' : 'develop' }} ```  In this example, the value of `MY_ENV_VAR` will be set to `main` if the current GitHub reference is `refs/heads/main`, and `develop` otherwise. This eliminates the need for separate if-else blocks to handle different reference names.", "9a57bb3b-b85d-4a7f-8d28-55214483f0d6": "Deploy keys in GitHub can be used to automate deployment processes by allowing read-only or read-write access to a repository from a server. This means that a deployment process can be set up to automatically pull changes from the repository to the server without requiring the user to enter their GitHub credentials. This can be achieved by adding the deploy key to the server's SSH configuration file and specifying the appropriate repository URL. This allows for a more streamlined and automated deployment process, as it eliminates the need for manual intervention to push changes to the server.", "ca84f25b-5363-48fc-bac9-6b52b9dd0c0b": "A sponsor profile on {% data variables.product.prodname_sponsors %} can help potential sponsors understand why they should support an open source developer by providing them with information about the developer's open source work and their commitment to open source development. Specifically, the profile should include:  1. Open source work: The developer should list the open source projects they have contributed to, as well as any notable contributions they have made. This can include the name of the project, a brief description of what it does, and the developer's role in the project.  2. Why they are committed to open source development: The developer should explain why they choose to contribute to open source projects and what they hope to achieve through their work. This can include their personal philosophy on open source, their motivation for contributing, and any goals they have for the projects they work on.  By providing this information, potential sponsors can get a better sense of the developer's skills, experience, and values, which can help them decide whether they want to support the developer financially.  Opting-in to being featured on {% data variables.product.prodname_sponsors %} can provide additional benefits for developers. When a developer opts-in, their profile may be featured on the {% data variables.product.prodname_sponsors %} homepage or in other promotional materials. This can help the developer attract more potential sponsors and increase their visibility in the open source community.  To make the most of this opportunity, developers should:  1. Ensure their profile is complete and up-to-date: This includes providing a clear and concise introduction, listing their open source work, and explaining their commitment to open source development.  2. Choose high-quality profile images: The developer's profile image should be clear, professional, and representative of their brand.  3. Use keywords and tags: By using relevant keywords and tags, the developer can make it easier for potential sponsors to find their profile.  4. Engage with the community: By participating in open source discussions and contributing to projects, the developer can build a reputation as a knowledgeable and active member of the community.  5. Promote their profile: The developer can share their profile on social media, their personal website, and other relevant platforms to attract more potential", "d97069b6-aac2-4e94-995e-ef45ea193ce9": "To create a new branch using the Subversion bridge to GitHub, you can use the `svn copy` command to create a new branch from the current branch (in this case, `trunk`). Here's an example:  ```shell $ svn copy trunk branches/more_awesome > A    branches/more_awesome $ svn commit -m 'Added more_awesome topic branch' > Adding    branches/more_awesome  > Committed revision 2. ```  In the Subversion bridge to GitHub, the `HEAD` branch is equivalent to the default branch in Git, which is typically called `main` or `master`. Other branches in the repository are created using the `svn copy` command, as shown in the example above. These branches are separate lines of development that can be worked on independently of the `HEAD` branch. When you're ready to merge your changes back into the `HEAD` branch, you can use the `svn merge` command to integrate the changes from your branch into the `HEAD` branch.", "47c8135a-f6e9-42a4-8b7a-b7e336ba2df5": "Self-hosted runners can be organized into groups to restrict access to specific workflows, organizations, and repositories in {% ifversion ghec or ghes or ghae %}GitHub Enterprise Cloud (GHEC), GitHub Enterprise Server (GHES), and GitHub Advanced Security (GHAS) {% endif %}. This feature is not available for self-hosted runners used with {% data variables.product.prodname_dotcom %}. For more information, refer to the documentation on restricting access to workflows, organizations, and repositories for self-hosted runners in GHEC, GHES, and GHAS.", "6751c815-958e-434e-8e31-c4a77f644c08": "To publish a package to both the Maven Central Repository and GitHub Packages using the provided workflow, you need to follow these steps:  1. Ensure that your _pom.xml_ file includes distribution management repositories for both your {% data variables.product.prodname_dotcom %} repository and your Maven Central Repository provider.  2. Set up the Java JDK using the `setup-java` action in your workflow.  3. Use the `publish` job in your workflow to publish the package to both registries. Here's an example of how the workflow might look:  ```yaml name: Publish package to the Maven Central Repository and GitHub Packages on:   release:     types: [created] jobs:   publish:     runs-on: ubuntu-latest     permissions:       contents: read       packages: write     steps:       - uses: {% data reusables.actions.action-checkout %}       - uses: {% data reusables.actions.action-setup-java %}         with:           java-version: '11'           distribution: 'temurin'       - name: Publish package to the Maven Central Repository         run: mvn --batch-mode deploy -DaltDeploymentRepository='https://oss.sonatype.org/content/repositories/releases/' -DaltDeploymentRepositoryLayout='default'         env:           GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}       - name: Publish package to GitHub Packages         run: mvn --batch-mode deploy -DaltDeploymentRepository='https://maven.pkg.githubusercontent.com/YOUR_ORGANIZATION/YOUR_REPOSITORY/maven-metadata.xml' -DaltDeploymentRepositoryLayout='default'         env:           GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %} ```  In this example, the `publish` job runs on an U", "d1732e4f-ce0c-4eeb-ba81-9e82a12a9f36": "To suspend a user in {% data variables.product.prodname_ghe_server %}, follow these steps:  1. Access the site admin dashboard by following the instructions provided in the context information. 2. Search for the user you want to suspend. 3. Click on the user's name to open their profile. 4. Click on the \"Admin\" tab at the top of the page. 5. In the \"Account suspension\" section, click \"Suspend\" in the \"Danger Zone\" area. 6. Type a reason for suspending the user in the \"Log reason\" field. 7. Click \"Suspend\" to confirm the suspension.  When a user is suspended in {% data variables.product.prodname_ghe_server %}, they cannot sign in to your instance, nor can they push or pull code. Their issues, comments, repositories, gists, and other data will still be preserved, but they will not be able to access or modify them until their account is unsuspended. The suspension takes effect immediately, and the user will not be notified of the suspension. If the user attempts to access a repository, they will receive an error message indicating that their account is suspended.", "bfd515ea-59e8-48eb-af17-14aeb65ae234": "If you encounter issues with publishing from a branch, such as the removal of a folder or the need for an admin to push changes, there are a few steps you can take to troubleshoot and prevent these problems:  1. If you remove the `/docs` folder from a branch that you've previously selected as your publishing source, your site won't build and you'll get a page build error message for a missing `/docs` folder. To prevent this, make sure you don't remove the `/docs` folder from any branch that you've selected as a publishing source.  2. If you're using GitHub Actions to build and deploy your site, make sure you've configured your workflow to handle the removal of the `/docs` folder. This can be done by adding a step to your workflow that checks if the `/docs` folder exists and skips the build if it doesn't.  3. If you're using a custom workflow, make sure you've configured your workflow to handle the removal of the `/docs` folder. This can be done by adding a step to your workflow that checks if the `/docs` folder exists and skips the build if it doesn't.  4. If you're having issues with publishing from a branch due to the need for an admin to push changes, make sure you've granted the necessary permissions to the user who will be pushing changes. This can be done by adding the user to the repository's collaborators list and granting them the necessary permissions.  5. If you're still having issues with publishing from a branch, you can contact GitHub Support for further assistance. They can help you troubleshoot the issue and provide guidance on how to prevent similar problems in the future.  By following these steps, you can ensure that your {% data variables.product.prodname_pages %} site is built and deployed correctly, even if you make changes to your repository's branch structure.", "66233499-2c1e-4bf7-b6a1-e2e8861f6e4a": "The `--config-file-path` argument in {% data variables.product.prodname_actions_importer %} can be used with the `audit`, `dry-run`, and `migrate` subcommands to specify a source file or a list of source files instead of fetching pipeline contents from source control by default. This is useful when working with large numbers of pipelines or when dealing with source control systems that are difficult to interact with.  For example, in an audit, the `--config-file-path` argument can be used to specify a YAML configuration file that lists the repository slugs and paths of the pipelines to be audited. This is especially useful when dealing with multiple pipelines in an Azure DevOps instance, as the configuration file can be used to specify the repository slugs and paths of each pipeline.  Here's an example of using the `--config-file-path` argument with the `audit` subcommand:  ```shell gh actions-importer audit azure-devops pipeline --output-dir ./output/ --config-file-path ./path/to/azure_devops/config.yml ```  In this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file to perform an audit. The configuration file lists the repository slugs and paths of the pipelines to be audited.  To use the `--config-file-path` argument with the `dry-run` subcommand, the pipeline to be converted is selected by matching the `repository_slug` in the configuration file to the value of the `--azure-devops-organization` and `--azure-devops-project` options. The `path` is then used to pull the specified source file.  Here's an example of using the `--config-file-path` argument with the `dry-run` subcommand:  ```shell gh actions-importer dry-run azure-devops pipeline --output-dir ./output/ --config-file-path ./path/to/azure_devops/conf ```  In this example, {% data variables.product.prodname_actions_importer %} uses the specified YAM", "460ec051-7d15-41cc-b14f-5c4ed28b91a9": "Yes, based on the context information provided, it seems that the lines from the {% data variables.product.prodname_codeql %} workflow should be removed because they are no longer necessary and may cause an issue. The warning message suggests that analyzing the merge commit for best results, and removing the lines that checkout the head of the pull request instead of the merge commit will fix the problem. This implies that the lines were included in the workflow to checkout the head of the pull request instead of the merge commit, but this step is no longer necessary or recommended by {% data variables.product.prodname_codeql %}. Therefore, removing these lines will likely improve the performance and accuracy of the {% data variables.product.prodname_codeql %} analysis.", "d18a47eb-c288-4d8a-b424-c7e046834ac9": "An organization owner in a {% data variables.product.prodname_github_app %} instance can register an app by using the manifest or URL parameters. This involves providing the necessary details about the app, such as its name, description, and authorization scopes. Once registered, the organization can install the app within their instance. If the organization makes the app public, other organizations within the same instance can also install it. This can be beneficial for apps that provide commonly used functionality, as it allows for easier adoption and reduces the need for each organization to develop and maintain their own version of the app. However, it's important for organizations to carefully consider the apps they install and ensure that they are trustworthy and secure.", "85316f08-c5cd-441f-9962-8ed63fa8f0ed": "People who must contribute from two user accounts on {% data variables.location.product_location %} using one workstation can configure Git to simplify the process, as mentioned in the context information. However, for more detailed information on how to do this, we suggest referring to the resource \"AUTOTITLE.\" provided in the context. If this resource is not available or does not provide sufficient guidance, we recommend reaching out to Git's official documentation or support channels for further assistance.", "3b60820f-367a-418b-b350-a90cfcffddcc": "To add organization members to a team in GitHub, follow these steps:  1. Access your organization's settings. 2. Click on Teams. 3. Select the team you want to add members to. 4. Click on the Members tab. 5. Click on the Add a member button above the list of team members. 6. Enter the user's GitHub username or email address, and optionally a customizable message. 7. Click on the Add button. 8. Review the team repository access for the new member. 9. Click on the Invite button to send the invitation.  Note: If you're using GitHub Enterprise Cloud (GHEC) or GitHub Enterprise Server (GHES), you may also be able to map teams to LDAP groups for easier management of team members. In this case, you'll need to add the user to the corresponding LDAP group instead of directly adding them to the team in GitHub.", "4842032b-5a2e-4052-ad05-0fecf8d7a252": "The \"--github-auth-stdin\" option can be used to authenticate to the github.com Container registry by passing a github.com GitHub Apps token or personal access token via standard input. This option is used to authenticate to the github.com Container registry specifically, while the \"--registries-auth-stdin\" option is used to authenticate to GitHub Enterprise Server Container registries by passing a comma-separated list of \\=\\ pairs. Both options override their respective environment variables.", "a672dd23-1292-44a1-b6dc-4f38ecfb28d1": "According to the context information provided, the \"The {% data variables.product.prodname_copilot %} Trust Center\" outlines the privacy measures in place for {% data variables.product.prodname_copilot_for_business %}. However, the context doesn't provide any specific details about these measures. Therefore, to answer the question, it's necessary to refer to the \"The {% data variables.product.prodname_copilot %} Trust Center\" for more information. If the context doesn't provide enough information to answer a question, it's best to say that you weren't given enough information to provide an answer.", "b07e7569-d42c-4319-9987-353267b94fdf": "To configure prebuilds for a specific branch in your repository, follow these steps:  1. Go to your repository in GitHub. 2. Click on the settings icon in the top right corner of the page. 3. Select \"Codespaces\" from the left-hand menu. 4. In the \"Prebuild configuration\" section, click \"Set up prebuilds\". 5. Select the branch for which you want to set up prebuilds. 6. Follow the remaining prompts to complete the prebuild configuration.  Note: If you're using a personal account, prebuilds will consume storage space that may incur a billable charge or use some of your monthly included storage. For organization-owned repositories, you must have added a payment method and set a spending limit for GitHub Codespaces on the organization account or its parent enterprise to use prebuilds.", "25adabe5-3b4d-4123-9bd2-1ced796b01d8": "The error message received when a request cannot be processed is \"Validation Failed\". The response body will include an `errors` property, which includes a `code` property to help diagnose the problem. The possible codes and their descriptions are:  - `missing`: A resource does not exist. - `missing_field`: A parameter that was required was not specified. Review the documentation for the endpoint to see what parameters are required. - `invalid`: The formatting of a parameter is invalid. Review the endpoint documentation for more specific information. - `already_exists`: Another resource has the same value as one of your parameters. This error is specific to resources that have unique constraints, such as usernames or repository names.  By checking the `code` property in the `errors` object, you can quickly identify the nature of the error and take appropriate action to resolve it.", "f2f13ea2-5f22-4535-900a-42679624d0ad": "After purchasing or upgrading a license for {% data variables.product.prodname_enterprise %} from {% data variables.contact.contact_enterprise_sales %}, you need to upload the new license file to {% data variables.location.product_location_enterprise %} to unlock the new user licenses. To do this, sign into {% data variables.location.product_location_enterprise %} as a site administrator, access the Enterprise settings, click on the License tab, and then click \"Update license\" in the \"User licenses\" section. Next, click \"License file\" and either drag the license file onto the page or click \"License file\" to select it from your computer. Finally, click \"Upload\" to complete the process.", "a68acbd9-37b8-4916-afe3-53a3486b7b69": "The reason behind the deprecation of the legacy global node ID format in the {% data variables.product.product_name %} GraphQL API is not explicitly stated in the given context. However, it is mentioned that by migrating to the new format, response times of requests remain consistent and small, and application functionality continues to work once the legacy IDs are fully deprecated. To ensure consistency and continuity of application functionality, users should determine if they need to take action by checking if they store references to GraphQL global node IDs. If they do, they should follow the migration steps, which involve using the `X-Github-Next-Global-ID` header in their GraphQL API requests to force the response payload to always use the new ID format for any object that they requested the `id` field for. If they don't store any global node IDs, they can continue to interact with the API with no change. Additionally, if they currently decode the legacy IDs to extract type information, they should migrate their service to treat these IDs as opaque strings, as the format of the IDs has changed.", "a858cc7f-455c-4270-aae6-50ba802bd6fa": "To create a repository, follow these steps:  1. Log in to your {% data variables.product.product_name %} account. 2. Click on the plus sign (+) in the top right corner of the page. 3. Select \"New repository\" from the drop-down menu. 4. In the \"Repository name\" box, type `hello-world`. 5. Leave the \"Initialize this repository with a README\" checkbox selected. 6. Click on the green \"Create repository\" button.  To start a new branch, follow these steps:  1. Go to your `hello-world` repository. 2. Click on the green \"Code\" button to open the GitHub interface for your repository. 3. Click on the \"Branch: main\" drop-down menu in the top left corner of the page. 4. Select \"New branch\" from the drop-down menu. 5. In the \"Create a new branch\" dialog box, type `new-branch` in the \"Branch name\" box. 6. Click on the green \"Create branch\" button.  To make changes to a file and push commits to {% data variables.product.product_name %}, follow these steps:  1. Go to your `hello-world` repository. 2. Click on the \"Code\" button to open the GitHub interface for your repository. 3. Click on the \"main\" branch to switch to it. 4. Click on the \"Create new file\" button in the top left corner of the page. 5. In the \"Create new file\" dialog box, type `README.md` in the \"Filename\" box. 6. Type `# Hello, World!` in the \"Content\" box. 7. Click on the green \"Commit new file\" button. 8. In the \"Commit changes\" dialog box, type `Initial commit` in the \"Commit message\" box. 9. Click on the green \"Commit changes\" button. 10. Click on the \"Compare & pull request\" button in the top right corner of the page. 11. In the \"Create a pull request\" dialog box, type `Add a README file` in the \"Title\" box. 12. Type `Added a README", "1069e253-9d47-41eb-994d-5311c2294f59": "The syntax for dependencies between jobs differs between Azure Pipelines and {% data variables.product.prodname_actions %}. In Azure Pipelines, the `needs` field is used to specify the jobs that a job depends on, and the jobs are run in parallel by default. For example:  ```yaml jobs:   initial:     steps:       - run: echo \"This job will be run first.\"   fanout1:     needs: initial     steps:       - run: echo \"This job will run after the initial job.\"   fanout2:     needs: initial     steps:       - run: echo \"This job will run after the initial job.\"   fanin:     needs: [fanout1, fanout2]     steps:       - run: echo \"This job will run after fanout1 and fanout2 have finished.\" ```  In {% data variables.product.prodname_actions %}, the `needs` field is also used to specify the jobs that a job depends on, but the jobs are run sequentially by default. For example:  ```yaml jobs:   initial:     runs-on: ubuntu-latest     steps:       - run: echo \"This job will be run first.\"   fanout1:     needs: initial     runs-on: ubuntu-latest     steps:       - run: echo \"This job will run after the initial job.\"   fanout2:     needs: initial     runs-on: ubuntu-latest     steps:       - run: echo \"This job will run after the initial job.\"   fanin:     needs: [fanout1, fanout2]     runs-on: ubuntu-latest     steps:       - run: echo \"This job will run after fanout1 and fanout2 have finished.\" ```  Actions can be used to perform tasks and customize workflows in {% data variables.product.prodname_actions %}. They are reusable components that can be used in multiple workflows. You can find actions in the {% data variables.product.prodname_marketplace", "d8030a7f-235f-4f01-b02a-be8a0d4f5554": "Yes, according to the context information provided, the membership information report contains details such as username, display name, two-factor authentication status, organization ownership or membership, pending invitations, email addresses for verified domains, SAML `NameID`, and license status for {% data variables.visual_studio.prodname_vss_ghe %}. It also mentions that additional information may be included based on the enterprise's configuration. The context also mentions that this information can be retrieved using GitHub's APIs, specifically the GraphQL API and REST API documentation.", "5c33d656-e684-4573-9d7d-a7708cf67df3": "Yes, based on the provided table, GitHub collects and processes personal information such as registration information and demographic information for purposes including providing and personalizing their services, product improvement and development, helping, securing, and troubleshooting, and marketing. They share this information with service providers and user-directed entities, but the specific data included in each category can be found in the GitHub Privacy Statement.", "29d69e85-c545-491d-a0d8-81273df0ab2e": "The purpose of waiting for a VM to enter ReadyRole status before configuring it, as mentioned in the context information, is to ensure that the VM is fully initialized and ready for configuration. ReadyRole is a state that a VM enters after it has been provisioned and is ready to be configured. This state indicates that the VM's operating system and any necessary extensions have been installed and configured, and that the VM is ready to receive further configuration commands. Waiting for the VM to enter ReadyRole status before configuring it helps to prevent any issues that may arise from attempting to configure a VM that is still in the process of being provisioned.", "27be1aa1-3450-4f5f-9e89-74f91564f3ee": "In the provided context, lists should be formatted by capitalizing the first letter in each line and using periods only at the end of lines if they contain a complete sentence. For items with primary and secondary text, a colon delimiter should be used, and the secondary text should be capitalized as if it was the beginning of the line. For example:  - `foo`: Something that provides bar. - `bar`: Something provided by foo.  When introducing a list, a descriptive sentence should be used that clearly conveys the subject of the list, yet allows the list to scale or change without having to update the description.  Unordered lists should be alphabetized if the order of items is not important, and ordered by importance to the reader if the order is important. Short, nonspecific sentences using terms like \"the following\" or \"these\" should be avoided, as they are difficult to localize without context.  Placeholder text should be styled in all caps and connected with dashes (kebab-case) if it is multiple words. If a placeholder is used, it should be explained what someone might replace it with to help people modify examples to fit their needs and identify placeholders for people who use assistive technology.", "b4fa267c-c9e0-47b2-9f42-8e38aaabbc11": "Some words commonly used by attackers that are not permitted in the `label` of input or textarea elements in GitHub Issues to minimize the risk of private information and credentials being posted publicly include \"password\", \"login\", \"username\", and \"secret\". However, the specific list of forbidden words may vary depending on the organization's security policies. It's always best to consult the organization's documentation or contact their support team for a complete list of prohibited terms.", "ef6b5cfe-3678-4f00-b1a7-3e046b559eff": "{% data variables.product.company_short %} recommends using the Octokit.rb SDK to interact with the REST API using Ruby. This SDK implements best practices and makes it easier to interact with the API via Ruby. It works with all modern browsers, Node.rb, and Deno. To use Octokit.rb, you need to install and import the `octokit` gem. This guide assumes that you are familiar with Ruby and the REST API. If you want to use the REST API for personal use, {% data variables.product.company_short %} suggests creating a personal access token (PAT). The guide provides instructions on how to create a PAT and how to authenticate using it in Octokit.rb. However, it also warns about the importance of keeping your authentication credentials secure and suggests storing them securely using services like {% data variables.product.prodname_actions %}, {% data variables.product.prodname_codespaces %}, or another CLI service.", "15ec7b1a-c787-470e-a1ef-e5750e058559": "The purpose of running the command \"docker run --name data pre-receive.dev /bin/true\" in this context is not immediately clear as the provided context information does not provide enough context to determine its purpose. However, based on the naming convention of the image \"pre-receive.dev\", it is possible that this command is being run to create a data container named \"data\" that will be used by the \"pre-receive.dev\" image for some purpose related to pre-receiving code changes. However, without further context, this is purely speculative.", "4bea301f-153a-4c08-8347-fbb10ef542d1": "To add a {% data variables.product.prodname_codeql %} pack as a dependency in another {% data variables.product.prodname_codeql %} pack, you need to update the `dependencies` section in the `qlpack.yml` file of the pack that will be using the dependencies. Here's an example:  Let's say you have two packs: `my-custom-queries` and `my-custom-queries-tests`. The `my-custom-queries` pack contains custom queries, and the `my-custom-queries-tests` pack contains tests for those queries.  In the `my-custom-queries-tests` pack, you need to add the following to the `dependencies` section in the `qlpack.yml` file:  ```yaml dependencies:   my-custom-queries: \"*\" ```  This tells {% data variables.product.prodname_codeql %} to download and install the `my-custom-queries` pack as a dependency for the `my-custom-queries-tests` pack.  Make sure to run `codeql pack install` in the root of the `my-custom-queries-tests` pack to install the dependencies. This will generate a `codeql-pack.lock.yml` file that lists all of the dependencies required to run queries in both packs.  Now you can use the queries from the `my-custom-queries` pack in the tests in the `my-custom-queries-tests` pack.", "c2aebd1b-63c1-44a0-b796-cb62585bee11": "To import a ruleset into a repository using GitHub's settings page, follow these steps:  1. Go to the repository for which you want to import a ruleset. 2. Click on the settings icon in the top right corner of the page. 3. Click on \"Settings\" from the drop-down menu. 4. Click on \"Code and automation\" in the left-hand menu. 5. Click on \"Rulesets\" under \"Code scanning\". 6. Click on \"Import a ruleset\" button. 7. Select the ruleset file you want to import from your local machine or from GitHub. 8. Click on \"Upload file\" to import the ruleset.  Note: This feature is currently in beta and may not be available for all repositories.", "0f92009a-092d-4cd3-9b77-9e5ca5d52b23": "Organization owners can disable SAML SSO for an organization, as stated in the context information. Therefore, the required permissions are possessed by organization owners.", "cb191e47-9b2a-4afd-914f-b0d8a90f5b60": "Based on the given context, it is unclear how the concept of \"AUTOTITLE\" differs from \"AUTOTITLE\" and \"AUTOTITLE\". Without further information, it is impossible to provide a detailed explanation. However, in general, \"AUTOTITLE\" refers to a feature in Microsoft Word that automatically generates a document title based on the contents of the document. \"AUTOTITLE\" and \"AUTOTITLE\" could potentially have the same meaning, but without any context, it is uncertain. If the context provided more information, such as the software or program in which these terms are being used, it may become clearer how they differ. However, without this information, it is best to assume that they are all referring to the same concept or feature.", "a21ac9d4-6a52-4b33-a24b-e9068d0d6058": "To manage maintenance mode for a cluster using SSH, follow these steps:  1. Connect to your {% data variables.product.prodname_ghe_server %} instance using SSH.  2. Navigate to the directory where `ghe-cluster-maintenance` is installed. By default, it's located in `/opt/github/ghe-cluster-maintenance`.  3. To check the current maintenance mode, run the following command:     ```shell    $ ghe-cluster-maintenance -q    ```     This will display whether the cluster is currently in maintenance mode or not.  4. To set maintenance mode, run the following command:     ```shell    $ ghe-cluster-maintenance -s    ```     This will set maintenance mode for all nodes in the cluster.     {%- ifversion custom-maintenance-mode-message %}    Alternatively, you can set a custom maintenance mode message using the `-m` flag:     ```shell    $ ghe-cluster-maintenance -s -m \"Custom maintenance mode message\"    ```    {%- endif %}  5. To unset maintenance mode, run the following command:     ```shell    $ ghe-cluster-maintenance -u    ```     This will unset maintenance mode for all nodes in the cluster.     The purpose of setting or unset maintenance mode for a cluster is to perform maintenance tasks, such as software updates or hardware replacements, without disrupting normal operations. During maintenance mode, new pull requests, issues, and comments are not accepted, and existing ones are read-only. This ensures that no new work is started while the cluster is being maintained.     The benefits of using this feature include minimizing downtime, reducing the risk of data loss, and ensuring data consistency. For example, if you need to replace a faulty server, you can set maintenance mode, replace the server, and then unset maintenance mode without affecting the availability of the cluster.     The drawbacks of using this feature include potential delays in responding to user requests and the possibility of missing important updates or issues during maintenance mode. It's important to communicate the maintenance schedule to users and provide alternative solutions", "3430efdf-ee47-4b35-8cb8-6fc0b2e120ce": "To configure Gradle to publish a package to both the Maven Central Repository and GitHub Packages using environment variables for authentication, you can follow these steps:  1. Create a Gradle build file (e.g., `build.gradle.kts`) with the necessary configurations for publishing to both repositories. Here's an example:  ```groovy plugins {     id(\"maven-publish\") }  repositories {     mavenCentral()     maven(\"https://maven.pkg.github.com/your-org/your-repo\") {         credentials(\"github\")     } }  publishing {     publications {         mavenJava(MavenPublication) {             from(components.java)         }     }     repositories {         maven {             url = uri(\"https://oss.sonatype.org/service/local/staging/deploy/maven2/\")             credentials(\"ossrh\")         }         maven(\"https://maven.pkg.github.com/your-org/your-repo\") {             credentials(\"github\")         }     } }  task sourceJar(type: Jar) {     classifier = \"sources\"     from(sourceSets.main.source) }  task javadocJar(type: Jar) {     classifier = \"javadoc\"     from(sourceSets.main.javadoc) }  artifacts {     archives sourceJar, javadocJar }  task publishToMavenCentral(type: PublishToMavenRepository) {     repository(url = uri(\"https://oss.sonatype.org/service/local/staging/deploy/maven2/\"))     credentials {         username = System.getenv(\"OSSRH_USERNAME\")         password = System.getenv(\"OSSRH_TOKEN\")     } }  task publishToGitHubPackages(type: PublishToMavenRepository) {     repository(url = uri(\"https://maven.pkg.github.com/your-org", "a7702f65-a3af-4e53-868f-96876727121c": "According to the context information provided, the process of migrating to a different Git Large File Storage server involves the following steps:  1. Configure {% data variables.large_files.product_name_long %} to use a third party server, as explained in \"Configuring {% data variables.large_files.product_name_long %} to use a third party server.\"  2. Add a new remote to the repository with the URL of the new server.  3. Set up the environment variables for the new remote.  4. Fetch all objects from the old remote using `git lfs fetch origin --all`.  5. Push all objects to the new remote using `git lfs push NEW-REMOTE --all`.  It's important to note that these steps assume that the repository already uses Git Large File Storage (GLFS) and that the user has prior knowledge of how to use GLFS. If the context didn't provide this information, then the user would need to enable GLFS for the repository before proceeding with the migration process.", "58af2911-54a1-4096-91f9-771ab3f168c2": "The difference between a public and private installation flow for a {% data variables.product.prodname_github_app %} is that a public installation flow allows anyone to install the app in their repositories, while a private installation flow only allows the app owner or organization owners and app managers to install the app on their personal or organization account, respectively.  For a public installation flow, when a user visits the landing page of a public {% data variables.product.prodname_github_app %}, they will see an **Install** button that they can click to add the app to their repository. This is useful for apps that are widely used and can benefit many users.  For a private installation flow, when a user visits the landing page of a private {% data variables.product.prodname_github_app %}, they will see limited information about the app, but the **Install** button will only be available to organization owners and app managers for the organization that owns the app, or the personal account if the app is owned by an individual user. This is useful for apps that are intended for use within a specific organization or by a specific group of users.  To make your {% data variables.product.prodname_github_app %} available to an organization on {% data variables.product.prodname_ghe_cloud %} if the organization is owned by an enterprise, you need to follow these steps:  1. Go to your {% data variables.product.prodname_github_apps %} settings page. 2. Click on the organization for which you want to make the app available. 3. Click on the \"Installation restrictions\" tab. 4. Select \"Enterprise\" as the type of organization. 5. Select the enterprise that owns the organization for which you want to make the app available. 6. Click \"Save\".  To make your app available to other {% data variables.product.prodname_ghe_server %} instances, you need to follow these steps:  1. Go to your {% data variables.product.prodname_github_apps %} settings page. 2. Click on the organization for which you want to make the app available. 3. Click on the \"Installation restrictions\" tab. 4. Select \"GitHub Enterprise Server\" as the type", "cd871a4b-4d2a-47d4-9eaf-7925f16c9b16": "To enable default setup for {% data variables.product.prodname_code_scanning %} for multiple repositories in an organization using security overview, follow these steps:  1. Go to your organization's Security tab in the GitHub web interface. 2. Click on \"Security coverage\" to view a list of repositories in your organization. 3. Use the search bar to narrow down the list of repositories based on name or enablement status of security features. 4. Select the repositories you want to enable {% data variables.product.prodname_code_scanning %} for by checking the boxes next to their names. 5. Click \"Security settings\" next to the number of selected repositories. 6. In the side panel, select \"No change\" in the \"{% data variables.product.prodname_codeql %} Default Setup\" section. 7. Optionally, choose a different query suite than your organization's default query suite by selecting \"Query suite: SUITE NAME\" and then selecting the query suite your configuration of default setup should use. 8. Click \"Enable\" to confirm the enablement of {% data variables.product.prodname_code_scanning %} for the selected repositories. 9. Click \"Apply changes NUMBER\" to save your changes, or click the {% octicon \"x\" aria-label=\"Close\" %} to close the panel without applying your changes.  Note that enabling {% data variables.product.prodname_code_scanning %} for multiple repositories in an organization using security overview will override any existing {% data variables.product.prodname_code_scanning %} configurations for the selected repositories, including any previous query suite selections and workflows for advanced scanning.", "68d7790b-37b7-47e0-829b-3f30ba5da102": "Query packs in {% data variables.product.prodname_codeql %} analysis contain a set of pre-compiled queries that can be evaluated on a {% data variables.product.prodname_codeql %} database, while library packs are designed to be used by query packs (or other library packs) and do not contain queries themselves. The main difference between query packs and library packs is that query packs are intended to be run, while library packs are intended to be used by other packs. Library packs do not contain queries and are not compiled separately, while query packs are compiled and include a compilation cache for consistent and efficient execution of the queries in the pack.", "fdb2b03e-02b1-4083-9288-843bb78a4682": "To add an issue or pull request to a project using the command palette in GitHub, follow these steps:  1. Open your project in GitHub. 2. Press the `Shift` + `Command` + `P` (Mac) or `Ctrl` + `Shift` + `P` (Windows, Linux) keys to open the command palette. 3. Type \"Add items\" and press `Enter`. 4. Select \"Bulk create\" to add multiple issues or pull requests at once, or select \"Create issue\" to add a single issue. 5. Follow the prompts to fill in the necessary information for your issue or pull request, such as the title, description, and assignee. 6. Once you've finished filling in the information, click \"Create issue\" or \"Create pull request\" to add it to your project.  Alternatively, you can also add an issue or pull request to a project by clicking on the \"New issue\" or \"New pull request\" button in the project's sidebar, or by navigating to the \"Issues\" or \"Pull requests\" tab and clicking the \"New issue\" or \"New pull request\" button there.", "945a857c-6730-45c0-b640-9d32733047ed": "To set up streaming of context information to Google Cloud Storage, you need to follow these steps:  1. Create a service account in Google Cloud. You can do this by going to the Google Cloud Console, selecting your project, and then navigating to IAM & Admin > Service accounts. Click the Create service account button, give it a name, and then click Create.  2. Create a JSON key for the service account. Go to the Service accounts page, select the service account you just created, and then click the Keys tab. Click the Add Key button, select JSON, and then click Create. Save the JSON key securely.  3. Grant the Storage Object Creator role to the service account for the bucket. Go to the Storage browser, select the bucket you want to use, and then click the Properties tab. Click the Edit button next to the Access control list (ACL) section, and then click the Add members button. Enter the email address of the service account you created, and then select the Storage Object Creator role. Click the Save button.  4. In {% data variables.product.prodname_dotcom %}, navigate to the Log Management tab and select the Log Streaming tab. Click the Configure stream dropdown menu and select Google Cloud Storage.  5. Enter the project ID and bucket name for your Google Cloud Storage account. You can find these values in the Google Cloud Console by going to the Projects page and selecting your project, and then navigating to Storage > Browser.  6. Upload the JSON key you created in step 2 to {% data variables.product.prodname_dotcom %}. You can do this by going to the Settings tab, selecting the Developer settings section, and then clicking the New API key button. Select the Google Cloud Platform API, and then click the Generate API key button. Enter a name for the API key, and then click the Create key button. Copy the API key, and then go back to the Log Streaming tab in {% data variables.product.prodname_dotcom %}. Click the Edit button next to the API key field, and then paste the API key you copied.  7. Click the Check endpoint button to verify that {% data variables.product.prodname_dotcom %} can connect and write to the Google Cloud Storage endpoint.", "62c37bab-8b34-42a1-8644-7b64e56452c7": "To specify the output file for the decompiled code using the `codeql query decompile` command, you can use the `--output` option followed by the desired output file path. If you don't provide this option, the decompiled code will be written to standard output by default.", "d1828820-37e6-4a66-9a55-0997a51b3edd": "To enable or disable {% data variables.product.prodname_dependabot_security_updates %} for an individual repository, follow these steps:  1. Go to the repository for which you want to enable or disable {% data variables.product.prodname_dependabot_security_updates %}. 2. Click on the repository's settings icon in the top right corner of the page. 3. Click on \"Code security and analysis\" in the left-hand menu. 4. Under \"Code security and analysis\", locate \"{% data variables.product.prodname_dependabot %} security updates\". 5. Click on the \"Enable\" or \"Disable\" button next to \"{% data variables.product.prodname_dependabot %} security updates\" to enable or disable the feature, respectively.  Note: For public repositories, the \"Disable\" button may be disabled if {% data variables.product.prodname_dependabot_security_updates %} is always enabled.", "cb026ded-bdaa-4de7-8e7c-18846762e5f1": "A diagnostic file contains information about the settings and environment of a GitHub Enterprise Server instance, while a support bundle includes a diagnostic file as well as sanitized log files from the past two days by default. The support bundle provides a more comprehensive set of data for troubleshooting purposes.", "6e37188c-3d18-4f8f-817e-08b406226252": "To view the progress of a workflow run using the {% data variables.product.prodname_cli %}, you can use the `run watch` subcommand and select the run from the interactive list. Here's an example command:  ```shell gh run watch ```  This will open an interactive list of workflow runs, and you can select the one you want to monitor by entering its ID and pressing Enter. The output of the selected run will be displayed in real-time, allowing you to track its progress.  If you want to view the progress of a specific workflow run without using the interactive list, you can pass the ID of the run as an argument to the `run watch` command. For example:  ```shell gh run watch RUN_ID ```  Replace `RUN_ID` with the ID of the workflow run you want to monitor.  Note that the `run watch` command only works with workflow runs that are currently in progress. If you want to view the status of a completed workflow run, you can use the `run view` subcommand instead.  I hope that helps! Let me know if you have any other questions.", "2a9fb419-78cb-4aa6-bc29-58bead25ed6f": "The format of the condition required for the \"OIDC 1\" mapping in the configuration process for OIDC in {% data variables.product.prodname_ghe_server %} when connecting to Google Cloud Storage is `google.subject == \"HOSTNAME\"` where `HOSTNAME` is the public hostname for {% data variables.location.product_location_enterprise %} without the protocol. This condition is added under \"Attribute Conditions\" in the \"Configure provider attributes\" step of the configuration process.", "27027fd7-8c24-4a20-937f-f1297e834794": "To install the latest version of the runner, you can follow the installation instructions provided. The context information provided includes a note that if you do not perform a software update within 30 days, the {% data variables.product.prodname_actions %} service will not queue jobs to your runner. However, the specific instructions for installing the latest version are not provided. You may need to refer to the official documentation or contact support for further assistance.", "e40e4c70-4931-4646-add0-9ca383312428": "The error message displayed when a registered callback URL does not match the redirect URI provided is \"The redirect_uri MUST match the registered callback URL for this application.\" This error can be corrected by either providing a redirect URI that matches what was registered or leaving out the redirect URI parameter altogether to use the default one registered with the application.", "e36808a4-9391-4ff5-9a9c-f9c5f28fbfc6": "Markdown can be utilized to add summaries to GitHub workflows by adding Markdown content to the `GITHUB_STEP_SUMMARY` environment file, which is unique for each step in a job. This can be done using Bash or PowerShell scripts that append the Markdown content to the file. Multiline Markdown content can be added by continuously appending content using the `>>` operator in Bash or by appending each line separately using the `Out-File` cmdlet in PowerShell. A blank line should be added between the lead-in sentence and the list to ensure proper formatting. The summaries for all steps in a job are grouped together into a single job summary and are shown on the workflow run summary page.", "2a790e1f-4510-45e6-92e6-949ad5b6739c": "The variable `$(System.PullRequest.PullRequestNumber)` in Azure DevOps represents the number of the current pull request. It provides information about the specific pull request that is being built or tested. On the other hand, the variable `$(System.PullRequest.SourceBranch)` represents the name of the branch from which the pull request was created. It provides information about the branch that contains the changes being proposed in the pull request. In summary, `$(System.PullRequest.PullRequestNumber)` provides information about the pull request itself, while `$(System.PullRequest.SourceBranch)` provides information about the branch that the pull request is based on.", "058ccf5e-50e2-4503-9383-b33b4006efba": "To ensure that user network traffic is switched from an old instance to a new instance after completing a restore process in GitHub Enterprise Server, you can use either DNS or IP address assignment. This step should be taken after reviewing and adjusting the configuration information and settings on the new instance, and saving those settings. The specific method used to switch user network traffic will depend on your organization's network configuration. It's recommended to consult your network administrator for guidance on the best approach for your environment.", "2b17524c-85f7-4f16-b864-f8037ea5d141": "The index of the current job in the current context can be obtained using the variable `CI_NODE_INDEX`. This variable holds the index of the current job being executed in the current context, which is determined by the `strategy` used in the workflow file. If the `parallelism` strategy is used, this variable will hold the index of the current job within the parallel group. If the `matrix` strategy is used, this variable will hold the index of the current job within the matrix. If the `max-parallelism` strategy is used, this variable will hold the index of the current job within the maximum number of parallel jobs allowed. If the context doesn't provide enough information to determine the strategy used, then it's not possible to determine the index of the current job.", "f15f0573-dc32-4f91-bc7a-c5a9db83101a": "The steps to retrieve a user's email address with the 'user:email' scope are:  1. The user is authenticated and the access token is stored in the session. 2. The access token is retrieved from the session. 3. A request is made to the GitHub API to retrieve the user's information with the access token. 4. The 'X-OAuth-Scopes' response header is checked to ensure that the 'user:email' scope is included. 5. If the 'user:email' scope is included, a separate request is made to retrieve the user's email addresses. 6. The user's information and email addresses are passed to the 'advanced' view for display.  If the 'user:email' scope is not included, the access token is invalidated and the user is prompted to start the OAuth flow again.", "f335ab9d-7815-42a6-9e9c-f9829c5c9a50": "Unfortunately, the provided context information does not offer enough insight into the relevant topic to determine how the presented information contributes to our understanding of it. Without prior knowledge or context about the topic, it is impossible to analyze the significance of the \"AUTOTITLE\" statement in relation to the topic at hand. Please provide additional context or clarify the topic in question for a more detailed response.", "8ba489bd-e7f4-400f-b616-4ef16c0233dc": "To enable {% data variables.product.prodname_dependabot_security_updates %} for a specific repository owned by your personal account or organization, follow these steps:  1. Go to the repository for which you want to enable {% data variables.product.prodname_dependabot_security_updates %}.  2. Click on the Settings tab in the repository navigation menu.  3. In the left-hand menu, click on Dependabot.  4. In the Dependabot settings page, scroll down to the Security updates section.  5. Toggle the switch next to Enable security updates to turn on {% data variables.product.prodname_dependabot_security_updates %} for this repository.  6. Click Save changes to save your settings.  7. {% data variables.product.prodname_dependabot_security_updates %} will now automatically update your dependencies with security fixes as they are released.  Note: If you want to enable {% data variables.product.prodname_dependabot_security_updates %} for multiple repositories at once, you can do so by following the instructions in \"Managing {% data variables.product.prodname_dependabot_security_updates %} for your repositories.\"    Managing {% data variables.product.prodname_d  Given the context information and not prior knowledge, answer the following question.   How can {% data variables.product.prodname_dependabot_security_updates %} be disabled for a specific repository owned by your personal account or organization  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  To disable {% data variables.product.prodname_dependabot_security_updates %} for a specific repository owned by your personal account or organization, follow these steps:  1. Go to the repository for which you want to disable {% data variables.product.prodname_dependabot_security_updates %}.  2. Click on the Settings tab in the repository navigation menu.  3. In the left-", "0bf6897d-7bc3-4d00-9290-98ef0041297c": "To create a new _CONTRIBUTING_ file in your repository, follow these steps:  1. Open your repository in your preferred code editor or Git client. 2. Create a new file named `_CONTRIBUTING.md` (note the leading underscore) in the root directory of your repository. 3. Add your contribution guidelines to the new file. Here are some ideas:    - Explain how to create a good issue or pull request, including any specific formatting or information you require.    - Provide links to external resources, such as documentation, mailing lists, or a code of conduct.    - Outline community and behavioral expectations, such as how to communicate with other contributors or how to handle conflicts. 4. Save the file and commit your changes. 5. Push your changes to the repository.  Remember to follow the style and formatting guidelines for Markdown files in your repository.  Some examples of contribution guidelines that you can refer to include the {% data variables.product.prodname_docs %} contribution guidelines, the Ruby on Rails contribution guidelines, and the Open Government contribution guidelines.  For further reading, you can check out the \"Starting an Open Source Project\" section of the Open Source Guides, as well as the {% ifversion fpt or ghes or ghec %}{% data variables.product.prodname_learning %}{% endif %}{% ifversion fpt or ghes or ghec %} \"AUTOTITLE\"{% endif %} documentation.", "8bbe2a80-771c-4cf4-9f43-0acda9b2e93b": "The load balance algorithm tries to ensure that each team member reviews an equal number of pull requests in any 30 day period by distributing the pull requests among the team members in a way that aims to balance the workload. It takes into account the number of pull requests each member has already reviewed during the past 30 days and tries to distribute the new pull requests in a way that ensures each member has a similar number of pull requests to review during the current 30 day period. This helps to prevent some team members from being overburdened with reviews while others have fewer reviews to do.", "73f1eea5-5a89-4389-95af-f1ac3e15a63c": "To ensure that the data in a table is best represented in a table format and not another format, such as a list, you should consider the following:  1. Check if the data has multiple attributes or needs to be compared. If yes, then a table is the best format. 2. Make sure that the data is tabular and not just a list. A table should have a header row and each row should begin and end with a pipe (`|`). 3. Avoid using tables for simple lists. Use lists instead. 4. Ensure that the table's data and why it is important are clear from any preceding content, the column headers, and (if needed) the row headers. 5. Use proper markup for row and column headers to help assistive technology understand relationships between cells.  By following these guidelines, you can ensure that your tables are clear, concise, and easy to read and maintain.", "8ae1507a-a619-4d71-8b03-d614029d52a2": "CAP (Context-Aware Policy) is a security feature that allows organizations to define and enforce access policies based on user and device context. In the context of {% data variables.product.prodname_github_apps %} and {% data variables.product.prodname_oauth_apps %}, CAP validates the IP address of the app's server to ensure that it is authorized to access the user's data. When {% data variables.product.prodname_github_apps %} and {% data variables.product.prodname_oauth_apps %} sign in and make requests on behalf of a user, {% data variables.product.prodname.dotcom %} sends the IP address of the app's server to the organization's Identity Provider (IdP) for validation. If the IP address is not validated by the IdP's CAP, the request will fail, and the user will be unable to access the app's features. This helps to prevent unauthorized access to sensitive data and ensures that only trusted apps and devices are able to access the organization's resources.", "729b3c2b-a379-4a95-aad2-cc42554438ab": "Based on the provided context information, it seems that the document being referred to is related to converting a Bamboo deployment project to a GitHub Actions workflow. The command provided in the context information can be used to migrate a Bamboo deployment project to a GitHub Actions workflow. The command takes several arguments, including the repository URL, the ID of the Bamboo deployment project, the URL of the target GitHub repository, and the output directory for the converted workflow. The command outputs the URL to the pull request that adds the converted workflow to the repository. The context information also mentions the use of environment variables to connect to the Bamboo and GitHub instances. Overall, the context information suggests that the document being referred to is likely a guide or tutorial on how to convert a Bamboo deployment project to a GitHub Actions workflow using {% data variables.product.prodname_actions_importer %}.", "ae4e8154-d36f-4a59-8739-74f29911b44f": "The purpose of adding topics to a repository on GitHub is to help users discover and navigate related projects based on their interests and needs. Topics allow repository owners to categorize their project's content and make it more discoverable to potential contributors and users. By adding relevant topics to a repository, it becomes easier for others to find and contribute to the project, as well as to discover similar projects in the same subject area. Additionally, topics can help users find solutions to specific problems or learn about new technologies and tools in a particular field.", "82033873-4b78-448a-b5b6-5d384613ea59": "An unused license must be available before inviting a new member to join an organization with a paid per-user subscription on GitHub.", "9ba84ecc-8df5-401c-b55a-a32f6cf694d9": "{% ifversion fpt or ghec %}  The pricing model for {% data variables.product.prodname_code_scanning %} is included in the subscription for {% data variables.product.prodname_ghe_cloud %} or {% data variables.product.prodname_ghe_server %}. There is no additional charge for using {% data variables.product.prodname_code_scanning %}.  Each run of a {% data variables.product.prodname_code_scanning %} workflow consumes minutes for {% data variables.product.prodname_actions %}, and these minutes are included in the subscription for {% data variables.product.prodname_ghe_cloud %} or {% data variables.product.prodname_ghe_server %}. For more information, see \"AUTOTITLE.\"  {% else %}  The pricing model for {% data variables.product.prodname_code_scanning %} is not explicitly stated in the provided context information. Without further information, it's unclear whether {% data variables.product.prodname_code_scanning %} is a paid feature or included in a subscription.  {% endif %}", "dbe83175-f1c5-439a-b468-03dfd2b3f0e8": "To insert a Markdown code block using a slash command, you can use the `/code` command in any description or comment field where slash commands are supported. You can choose the language for the code block by following the prompts after typing the command. The available languages for the code block may vary depending on the specific context or platform being used. However, the context provided doesn't specify which platforms or contexts are being referred to, so it's unclear which languages are available for the `/code` command. If you need more information about which languages are supported, you may need to consult the documentation or settings for the specific platform or context being used.", "19f06f8c-4bc2-4f70-a30b-94a54203bc2b": "Tasklists in GitHub can be utilized to monitor the progress of specific tasks by creating a list of tasks related to a particular project or issue. Here's a step-by-step guide on how to create a tasklist, add tasks to it, and mark them as completed:  1. Go to the repository containing the project you want to track tasks for. 2. Click on the issue you want to create a tasklist for, or create a new issue. 3. Click on the \"Tasklist\" tab at the top of the issue. 4. Click on the \"Create tasklist\" button. 5. Enter a name for your tasklist and click \"Create tasklist\". 6. To add tasks to your tasklist, click on the \"Add task\" button. 7. Enter the task name and click \"Add task\". 8. To mark a task as completed, go to the tasklist tab of the issue, find the task you want to mark as completed, and check the box next to it.  To integrate these tasklists with a project and view their progress, follow these steps:  1. Go to the project you want to integrate the tasklist with. 2. Click on the \"New view\" button in the project's sidebar. 3. Select \"Table\" as the view type. 4. Click on the \"Add column\" button and select \"Tracked by\". 5. Save the view. 6. Go to the view you just created and click on the \"Group by\" button. 7. Select \"Tracked by\" from the dropdown menu. 8. The issues in your project will now be grouped by the issue that is tracking them, making it easy to see the progress of each tasklist.  Converting draft issues into regular issues in GitHub allows you to add more details and collaborate with others on the issue. Here's how to convert a draft issue and close it:  1. Go to the draft issue you want to convert. 2. Click on the \"Convert draft\" button in the top right corner of the issue. 3. Fill in the necessary details, such as the title and description. 4. Click \"Convert draft\" to convert the issue. 5. To close the issue, go to the issue page and click on the \"", "e5a571c7-8032-4245-9863-0ef55e5cb832": "To verify that your private key is being used by SSH on your system, you can follow the steps provided in the context information. This involves starting the SSH authentication agent (if necessary), and then running the `ssh-add` command to list the private keys that are currently loaded into the agent. If your private key is being used, it should be listed when you run this command. Additionally, you can try connecting to a GitHub repository using SSH and checking the debug output to confirm that your private key is being used. This can be done by running the `ssh -vT git@{% data variables.command_line.codeblock %}` command, which should display debug messages that include the path to your private key. If your private key is being used, you should see a message that indicates the type of key being used (e.g., \"identity file /path/to/key type -1\").", "008b3def-176b-4bba-936d-5ff956781087": "According to the context information provided, the repository management feature you should use to add a repository to a team is \"Add a repository to a team (see 'AUTOTITLE' for details)\". The feature is listed under the \"Manage team and collaborator access to the repository\" section, indicating that it allows you to associate a repository with a team and manage access to it for team members. The \"AUTOTITLE\" mentioned in the context is not defined, but it's likely a link or button that provides more details on how to add a repository to a team.", "a162a445-2448-45c8-b16a-c2c8381e03e2": "To manage your role within an organization owned by your enterprise using the enterprise settings, follow these steps:  1. Access your enterprise settings by clicking on your profile picture in the upper right-hand corner of GitHub.com and selecting the organization you want to manage your role in from the list of organizations. 2. Click on the {% octicon \"gear\" aria-label=\"Organization settings\" %} dropdown menu and select \"Join as an organization owner\" or \"Join as an organization member\".  If the organization enforces SAML single sign-on (SSO), you cannot use the enterprise settings to join the organization. Instead, you must join the organization using that organization's identity provider (IdP). To do this, you must be assigned access in your IdP to the application for GitHub Enterprise Cloud (GHEC) that is used by the organization. Then, you can authenticate to the organization using SAML SSO.", "85792ef6-1758-4fab-b418-45f56612ba97": "In high availability, the scale of the appliance is dependent exclusively on the primary node and the load is not distributed to the replica server, whereas in clustering, the scale is distributed across multiple nodes, providing horizontal scaling for environments with tens of thousands of developers. This configuration requires careful planning and additional administrative overhead. {% data variables.product.company_short %} will work with you to determine your eligibility for clustering.", "d2a5ffaa-c7c0-4cff-b340-448a6689a872": "When you ask {% data variables.product.prodname_copilot_chat %} to describe the purpose of a file in {% data variables.product.prodname_vscode %}, it will analyze the code and provide a summary of what the file appears to be doing based on its contents. However, the accuracy of this description may depend on the quality and completeness of the code in the file, as well as any additional context that may be available. If the file is incomplete or poorly documented, the description may not be accurate or comprehensive. In such cases, you may need to provide additional information or context to help {% data variables.product.prodname_copilot_chat %} better understand the file's purpose.", "117ab3d5-cc72-4770-ae98-418e9331f2fa": "Based on the context provided, to set up the necessary environment variables for requesting a JWT in your GitHub Actions workflow, you can define the token settings using `actions/github-script` with the `core` toolkit, as shown in the second example provided. This will generate environment variables `ACTIONS_RUNTIME_TOKEN` and `ACTIONS_ID_TOKEN_REQUEST_URL` that you can then use to retrieve a JWT from the {% data variables.product.prodname_dotcom %} OIDC provider using `curl`. The `script` step in the example provided demonstrates how to set these environment variables.", "2d639a5d-9468-4f15-a911-2d0cb71c24dd": "To ensure that the \"label\" key within the \"attributes\" block of a block in the \"body\" list is a string, as opposed to a Boolean value, a string value should be provided for the \"label\" key.  Example:  ```yaml body: - type: textarea   attributes:     label: Please describe the issue you are experiencing. ```  In this example, the \"label\" key is provided with a string value, ensuring that it is a string, as opposed to a Boolean value.", "1dcc8803-0b77-4fbf-ab53-2466068fe3aa": "To mask and pass a secret between jobs or workflows using GitHub Actions, you can follow these steps:  1. Generate the secret in one job or workflow using the `set-output` or `GITHUB_OUTPUT` feature to store it in a variable. 2. Store the secret in a secure store like Vault or AWS Secrets Manager. 3. Retrieve the secret from the store in the subsequent job or workflow using the `set-output` or `GITHUB_OUTPUT` feature to store it in a variable. 4. Use the masked secret in your workflow as needed.  Here's an example of how to do this using Vault:  1. Generate the secret in the first job or workflow:  ```yaml name: Generate Secret  jobs:   generate-secret:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Generate Secret         id: generate-secret         run: |           echo \"::add-mask::$(openssl rand -hex 32)\" >> $GITHUB_OUTPUT           echo \"SECRET=$(openssl rand -hex 32)\" >> $GITHUB_OUTPUT        - name: Store Secret in Vault         uses: hashicorp/vault-action@v2         with:           vault_address: \"https://vault.example.com\"           vault_token: \"s3cr3t\"           path: \"secrets/github-actions/$(date +%Y-%m-%d)\"           mode: \"0600\"           value: \"SECRET\" ```  2. Retrieve the secret in the subsequent job or workflow:  ```yaml name: Use Secret  jobs:   use-secret:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Retrieve Secret from Vault         uses: hashicorp/vault-action@v2         with:           vault_address", "76e5c248-9ef4-4af5-81e5-57083fa4e370": "Without further context, it is unclear what \"AUTOTITLE\" is or what its purpose might be in a document. It could potentially be a formatting command or a placeholder for a title that will be automatically generated by the software or program being used. However, without more information, it is impossible to determine the exact purpose of \"AUTOTITLE\" in this context.", "78a38d28-85b5-41d4-95b5-7b1619d64566": "To rename an organization runner group, follow these steps:  1. Go to your organization's settings by clicking your organization's name in the top left corner of GitHub and selecting Settings from the drop-down menu.  2. In the left-hand menu, click Runner groups.  3. Find the runner group you want to rename and click the three dots to the right of the runner group's name.  4. Select Rename from the drop-down menu.  5. Enter the new name for the runner group and click Rename runner group.    Changing the name of an enterprise runner group  Given the context information and not prior knowledge, answer the following question.   How can I rename an enterprise runner group  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  To rename an enterprise runner group, follow these steps:  1. Go to your enterprise's settings by clicking your enterprise's name in the top left corner of GitHub and selecting Settings from the drop-down menu.  2. In the left-hand menu, click Runner groups.  3. Find the runner group you want to rename and click the three dots to the right of the runner group's name.  4. Select Rename from the drop-down menu.  5. Enter the new name for the runner group and click Rename runner group.    Changing the name of a runner group  Given the context information and not prior knowledge, answer the following question.   How can I rename a runner group  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  To rename a runner group, follow these steps:  1. Go to the runner groups page by clicking your organization's name in the top left corner of GitHub and selecting Settings from the drop-down menu, then clicking Runner groups in the left-hand menu.  2. Find the runner group you want to rename and click the three dots to the right of the runner", "a4c8dca5-1a15-44ba-8cb6-df60819493f1": "To cancel your {% data variables.product.prodname_copilot_business_short %} subscription for your organization account, you need to follow these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu.  2. In the left-hand menu, click on \"Billing & plans\".  3. Click on \"Manage seats\" under the {% data variables.product.prodname_copilot_business_short %} subscription.  4. Remove all assigned {% data variables.product.prodname_copilot %} seats by clicking on the \"x\" next to each user's name.  5. Once all seats have been removed, you will be prompted to confirm the cancellation of your subscription. Click \"Confirm cancellation\" to complete the process.  Note: If you have any remaining {% data variables.product.prodname_copilot %} credits, they will be forfeited when you cancel your subscription.  Further reading:  - \"Managing seats for {% data variables.product.prodname_copilot %}\" (https://docs.github.com/en/billing/managing-billing-for-your-organization/managing-seats-for-github-copilot) - \"Cancelling {% data variables.product.prodname_copilot %} for your organization\" (https://docs.github.com/en/billing/managing-billing-for-your-organization/cancelling-github-copilot-for-your-organization)", "d110722d-7f4a-47c4-9eed-0d2cdd82b8c6": "To approve workflow runs on a pull request from a private fork, you can follow the steps mentioned in the context information. As explained in the \"About workflow runs from private forks\" section, workflow runs can be triggered from a private fork. To approve these workflow runs on a pull request, you can refer to the instructions provided in the \"Approving workflow runs on a pull request from a private fork\" section. This involves accessing the pull request, reviewing the workflow runs, and approving them if they pass all the required checks. Once approved, the pull request can be merged into the main branch.", "6c686d32-8af1-4266-9cdf-e3d66f409599": "The Service agreement between GitHub and its customers outlines the terms and conditions for using GitHub's services, which include the right for GitHub to modify or discontinue the service at any time with or without notice. The agreement also specifies the level of technical support provided by GitHub, which is standard technical support at no additional charge for twenty-four hours a day, five days a week, excluding weekends and national U.S. Holidays. Premium or dedicated technical support may be provided at an additional fee and subscription term specified in an Order Form or SOW. The governing law for the agreement depends on the location of the customer's principal office, with California law applying for customers in the Americas and Irish law applying for customers outside the Americas. The agreement also mentions that the United Nations Convention on Contracts for the International Sale of Goods and the Uniform Computer Information Transactions Act will not apply to this agreement.", "8fad05ae-af41-4bf9-8f36-5f29ea5ddf3e": "{% data variables.product.prodname_actions_importer %} supports variable templates in Azure Pipelines by allowing the use of variable expressions in the file path names of template files. This feature allows for dynamic and customizable template files based on the values of variables.  Here's an example:  ```yaml   File: azure-pipelines.yml variables: - name: environment   value: staging  steps: - template: \"./templates/${{ variables.environment }}/build.yml\" ```  In this example, the `environment` variable is used to dynamically set the file path of the template file. The `build.yml` file in the `staging` folder of the `templates` directory will be used for this step.  Regarding iterative insertion, {% data variables.product.prodname_actions_importer %} partially supports this feature in Azure Pipelines. It allows for the use of iterative expressions in the file path names of template files, but only for parameters and not for steps.  Here's an example:  ```yaml parameters: - name: steps   type: object   default:     - build_step     - release_step steps: - {% raw %}${{ each step in parameters.steps }}{% endraw %}:     - template: \"${{ step }}-variables.yml\" ```  In this example, the `steps` parameter is used to dynamically set the file path of the template files for each step. The `build_step` and `release_step` values in the `steps` parameter are used to set the file path of the `-variables.yml` template files for each step.  However, note that iterative expressions are not supported for steps in Azure Pipelines, so the `each` expression is only used to set the file path of the template files for each step's variables. The steps themselves cannot be dynamically set using iterative expressions.", "97cc2dd3-ebbc-41c7-934a-521ec4b2940a": "If you're using {% data variables.product.prodname_ghe_server %}, the {% data variables.product.prodname_codeql %} action is already included in your installation. To use it, ensure that your self-hosted runner has access to the internet or download the latest released version of the {% data variables.product.prodname_codeql %} {% data variables.product.codeql_cli_ghes_recommended_version %} bundle using a synchronization tool. If you're using GitHub Actions (GHA), you can add the {% data variables.product.prodname_codeql %} action to your workflow by following the instructions provided in the {% data variables.product.prodname_codeql %} documentation. In either case, you must ensure that Git is in the PATH variable on your self-hosted runner and that your runner meets the {% data variables.product.prodname_codeql %} system requirements. If you're using default setup for {% data variables.product.prodname_code_scanning %}, assign the `code-scanning` label to your self-hosted runner. For more information about using labels with self-hosted runners, see the documentation. If you're using {% data variables.product.prodname_code_scanning %} to analyze Python code, you must ensure that your self-hosted runner has Python 3 installed.", "f02ddb96-0498-48a6-8df0-a737b1f0160e": "The feature of eating was introduced in version `v2.15.2`.", "681281c9-1a71-467a-b63a-20c870cca340": "The purpose of the `filePatterns` field in a starter workflow is to allow the workflow to be triggered only if the user's repository contains a file with a specific pattern in its root directory. This feature can be useful to ensure that the workflow is only executed for repositories that meet certain criteria, such as having a specific configuration file or a specific naming convention for certain files. The `filePatterns` field is optional, meaning that it is not required to include it in a starter workflow.", "92f9f9c8-7149-4a15-ae44-4d0d741d17e1": "Yes, according to the provided context, GitHub allows one request for security and compliance risk-related information per year, and the response will be provided in writing within thirty days of receipt of the request, pending needed clarifications of any request.", "6fc412d0-6089-440f-a9b0-2255e79b6621": "The nodes designated as storage nodes in the context provided are \"ghes-storage-node-1\", \"ghes-storage-node-2\", and \"ghes-storage-node-3\". These nodes provide the following services:  - git-server: This service allows users to manage their Git repositories. - pages-server: This service allows users to host their static websites. - storage-server: This service provides storage for Git repositories and user data. - metrics-server: This service collects and reports system metrics for monitoring and analysis purposes.  All of these services are marked as true in the configuration file for the respective storage nodes.", "31fa50db-331e-4cb3-a2b0-d2cb2f05b759": "The security overview feature in {% data variables.product.prodname_GH_advanced_security %} can be utilized by an organization's security team to monitor the adoption of security features across their organization or by a specific team by using the \"Coverage\" view in the security overview dashboard. This view displays the percentage of repositories in the organization that have a specific security feature enabled, such as {% data variables.product.prodname_secret_scanning %} or {% data variables.product.prodname_code_scanning %}. The security team can use this view to identify which teams or repositories have not yet enabled a particular security feature and take appropriate action to ensure that all repositories are protected.  Another view that can be used for this purpose is the \"Risk\" view, which displays a list of repositories with open security alerts, sorted by severity level. The security team can use this view to identify which teams or repositories have a high number of open alerts and take appropriate action to address the issues.  The \"Secret type\" filter in the {% data variables.product.prodname_secret_scanning %} alert view can be used to narrow down the results of security alerts for a specific type of secret, such as a {% data variables.product.pat_generic %} on {% data variables.product.prodname_dotcom %}. This filter allows the security team to view only the alerts related to a specific type of secret, making it easier to identify and address the issues. For example, the security team can use this filter to view only the alerts related to {% data variables.product.pat_generic %} secrets and take appropriate action to address the issues related to these secrets.", "45de6f73-b007-438d-8350-dcbc34e276a4": "Based on the context provided, discussion tools in {% data variables.product.prodname_dotcom %} can be utilized for tasks other than tracking bugs, enhancements, and feedback. These tools can be used for planning, analysis, design, user research, and general project decision making. For example, a team can use a discussion tool to brainstorm ideas and collaborate on a project outside the codebase. This can be especially useful for tasks that don't have a clear owner or result in an actionable task. In this scenario, instead of creating an issue or pull request, the team can hold a discussion in a team discussion to gather input and feedback from all members before moving forward with the project. This can help ensure that everyone is on the same page and that the project is aligned with the team's overall goals and objectives.", "1bdae6c7-7994-44aa-8251-09277a2ed471": "{% data variables.product.prodname_github_apps %} provide more control over app access and permissions compared to authenticating with the built-in `GITHUB_TOKEN` in a {% data variables.product.prodname_actions %} workflow because they allow for more granular permissions and access control. With {% data variables.product.prodname_github_apps %}, you can specify which repositories and organizations the app has access to, as well as which events and actions the app can trigger. This level of control is not available when using the built-in `GITHUB_TOKEN`, which has more limited permissions and access control.  For example, with {% data variables.product.prodname_github_apps %}, you can specify that the app only has access to a specific repository, and only has read-only access to that repository. This level of granularity is not available when using the built-in `GITHUB_TOKEN`, which has more global permissions and access control.  When accessing repositories on {% data variables.product.product_name %} from the command line using HTTPS, you can authenticate with {% data variables.product.prodname_cli %} by running `gh auth login` in your terminal. This will open a web browser and prompt you to log in to your {% data variables.product.product_name %} account. Once you've logged in, you'll be prompted to grant {% data variables.product.prodname_cli %} access to your account. After you've granted access, you can use `gh auth list` to view your tokens, and `gh auth use <token-name>` to switch between tokens.  Alternatively, you can authenticate without {% data variables.product.prodname_cli %} by running `git config --global credential.helper store` in your terminal. This will store your credentials in your local Git configuration, and prompt you to enter your credentials every time you push or pull from a repository.  When accessing repositories on {% data variables.product.product_name %} from the command line using SSH, you'll need to generate an SSH key pair and add the public key to your {% data variables.product.product_", "52cdb6f0-742d-4084-ae11-3b8894abad50": "According to the context information provided, AI-powered generic secret detection may generate more false positive alerts when compared to the existing {% data variables.product.prodname_secret_scanning %} feature. The existing feature, which detects partner patterns, has a very low false positive rate. This excess noise from the AI-powered generic secret detection can be mitigated by grouping alerts in a separate list and requiring security managers and maintainers to triage each alert to verify its accuracy.", "58e56010-252a-4c22-918c-44684ab15cd0": "To define tags for a specific job in a workflow using GitHub Actions, you can use the `tags` input in the job configuration. Here's an example:  ```yaml name: My Workflow  jobs:   my-job:     runs-on: ubuntu-latest     tags:       my-tag: my-value     steps:       - name: Checkout         uses: actions/checkout@v2       - name: Run my job         uses: my-action/my-action@v1         with:           my-input: my-value ```  In this example, we're defining a job called `my-job` that runs on the latest Ubuntu image. We're also defining a tag called `my-tag` with a value of `my-value`. This tag can be used to filter or search for this job in various GitHub Actions tools and services, such as the GitHub Actions dashboard or the GitHub CLI.  Note that the `tags` input is part of the `jobs` configuration, which is a top-level section in the workflow file. This means that you can define tags for multiple jobs in the same workflow, and you can also define tags for the entire workflow using the `tags` input at the workflow level.  Also note that the `tags` input is partially supported, meaning that not all GitHub Actions tools and services may currently support using tags to filter or search for jobs. However, GitHub is actively working to improve support for tags in GitHub Actions, so this may change in the future.", "2a3d4d9a-1f5a-4a62-a55a-d238278b4a0d": "To configure the role and trust policy for the {% data variables.product.prodname_dotcom %} IdP in AWS to allow writes to the S3 bucket created for audit logging, follow these steps:  1. Log in to your AWS account and navigate to the IAM console. 2. Click on \"Roles\" in the left-hand menu and then click on \"Create Role\". 3. Select \"AWS service\" as the trusted entity and then select \"Amazon S3\" as the service. 4. Click \"Next: Permissions\" to continue. 5. Search for \"AmazonS3FullAccess\" and then click \"Attach permissions policies\" to add it to the role. 6. Click \"Next: Tags\" to continue. 7. You can add tags to the role if you want, but this is optional. Click \"Next: Review\" to continue. 8. Enter a name for the role, such as \"GitHub Audit Log Streaming Role\", and then click \"Create Role\".  To create the permissions policy that allows writes to the S3 bucket, follow these steps:  1. Log in to your AWS account and navigate to the IAM console. 2. Click on \"Policies\" in the left-hand menu and then click on \"Create Policy\". 3. Select \"JSON\" as the policy type and then copy and paste the following JSON into the editor:  ```json {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Effect\": \"Allow\",       \"Action\": [         \"s3:PutObject\",         \"s3:GetObject\",         \"s3:ListBucket\"       ],       \"Resource\": [         \"arn:aws:s3:::auditlog-streaming-test\",         \"arn:aws:s3:::auditlog-streaming-test/*\"       ]     }   ] } ```  Replace \"auditlog-streaming-test\" with the name of the S3 bucket you created for audit logging.  4. Click \"Review Policy\" to continue. 5. Enter a name for the policy, such as \"G", "2f0a72ae-8568-46c5-9768-1fbb3fb352f1": "For a self-hosted runner on GitHub Enterprise Cloud (GHEC), the following domains should be allowed for network access during a build process:  1. Essential operations:     - github.com    - api.github.com    - *.actions.githubusercontent.com     For example, the runner must be able to communicate with `github.com` to authenticate and fetch repositories, `api.github.com` to interact with the GitHub API, and `*.actions.githubusercontent.com` to download and upload artifacts.  2. Downloading actions:     - codeload.github.com     For example, the runner must be able to communicate with `codeload.github.com` to download action packages.  These requirements are different for essential operations versus downloading actions. The domains required for essential operations are necessary for the basic functionality of the runner, while the domain required for downloading actions is specific to using GitHub Actions.", "ce8f9f16-f115-4783-b9f0-67bc570c55a3": "The limitations for macOS {% data variables.actions.hosted_runner %}s include the fact that all actions provided by {% data variables.product.prodname_dotcom %} are compatible with arm64 {% data variables.product.prodname_dotcom %}-hosted runners, but community actions may not be compatible with arm64 and need to be manually installed at runtime. To ensure compatibility, it's recommended to test community actions on arm64 runners before using them in workflows. Additionally, due to a limitation of Apple's Virtualization Framework, nested-virtualization is not supported by arm64 runners.", "d7e727a8-5208-44a8-8897-e8a3ed1c2cdd": "To publish a paid plan for your app on {% data variables.product.prodname_marketplace %}, your app must be owned by an organization that is a verified publisher. If your app is already published and you're a verified publisher, then you can publish a new paid plan from the pricing plan editor. You must also meet the following requirements: your app should have a minimum of 100 installations for {% data variables.product.prodname_github_apps %} or a minimum of 200 users for {% data variables.product.prodname_oauth_apps %}. All paid apps must handle {% data variables.product.prodname_marketplace %} purchase events for new purchases, upgrades, downgrades, cancellations, and free trials. When you are ready to publish the app on {% data variables.product.prodname_marketplace %}, you must request verification for the app listing. To transfer an app to an organization for verification, refer to the provided note. For information on how to integrate {% data variables.product.prodname_marketplace %} purchase events into your app, see the provided link. Apps must support both monthly and annual billing for paid subscriptions purchases, and listings may offer any combination of free and paid plans. Free plans are optional but encouraged.", "6bee7169-d38b-4a91-8041-c0d9df1185aa": "You can choose the email address associated with your Git commits by following the instruction provided in the context information, which is to {% data reusables.files.choose-commit-email %}. This step is optional and can be skipped if you want to use the default email address associated with your Git configuration. However, if you want to use a different email address for a specific commit, you can provide it during the commit process. The email address you choose will be displayed in the commit log and can be useful for tracking changes and communicating with collaborators.", "0c6d43bf-0164-4ded-84e2-03e0397c978c": "To obtain the total size of all files in your Git repository using `git-sizer`, follow these steps:  1. Ensure that `git-sizer` and `jq` are installed. 2. Clone your Git repository from the migration origin using `git clone --mirror`. 3. Navigate to the directory where the repository was cloned. 4. Run the following command to calculate the total size of all files in the repository:     ```    git-sizer --json --size-limit=0 --no-progress | jq '.repositories[0].size'    ```     This command will output the total size of all files in the repository in bytes.     - `--json`: Outputs the results in JSON format.    - `--size-limit=0`: Disables the size limit, which is set to 100MB by default.    - `--no-progress`: Disables the progress bar, which can be useful for large repositories.    - `jq '.repositories[0].size'`: Uses `jq` to extract the total size of all files from the JSON output.     Note that `jq` is used to extract the total size because `git-sizer` outputs the results in JSON format by default. If you prefer to see the results in a human-readable format, you can replace `jq '.repositories[0].size'` with `cat` to print the total size directly to the console.     For example:     ```    git-sizer --json --size-limit=0 --no-progress | cat    ```     This command will output the total size of all files in the repository in bytes, without the JSON formatting.     Alternatively, you can use the `--human` flag to output the total size in a human-readable format:     ```    git-sizer --json --size-limit=0 --no-progress --human | jq '.repositories[0].size'    ```     This command will output the total size of all files in the repository in a human-readable format, such as \"1.23 GB\" or \"", "b2ca0fab-1c5d-414d-bac3-33fac77ba6bb": "The significance of indirectly granting broad access to all write-access users of a repository through the use of SSH keys in the context of GitHub Actions is that it allows these users to clone and push to the repository using SSH instead of HTTPS, which can be more secure and faster. However, it also means that any user with write access to the repository can access and modify the repository's contents, potentially compromising sensitive information or introducing unwanted changes. As a result, it's important to carefully manage and control access to SSH keys and ensure that they are only granted to trusted users with a legitimate need for this level of access.", "16278d26-8eb1-4fe6-8d96-906c6583cfe4": "The purpose of fetching branches and their respective commits from an upstream repository in Git is to synchronize your local repository with the remote repository. This allows you to keep track of changes made by other contributors and merge them into your local branch. When you fetch branches, Git downloads the commits and metadata for each branch from the remote repository, but it doesn't automatically merge them into your local branches. Instead, you can merge the fetched branches into your local branches using the `git merge` command.  In the context provided, the command to fetch branches from the upstream repository is:  ```shell $ git fetch upstream ```  This command downloads all the branches and their respective commits from the remote repository `upstream` and stores them in local branches with the prefix `upstream/`. For example, if you fetch the branch `main` from the upstream repository, Git creates a local branch `upstream/main` that points to the same commit as the remote branch. You can then merge this branch into your local branch to keep it up-to-date with the upstream repository.  Example output of the `git fetch` command:  ```shell $ git fetch upstream > remote: Counting objects: 75, done. > remote: Compressing objects: 100% (53/53), done. > remote: Total 62 (delta 27), reused 44 (delta 9) > Unpacking objects: 100% (62/62), done. > From https://{% data variables.command_line.codeblock %}/ORIGINAL_OWNER/ORIGINAL_REPOSITORY >  * [new branch]      main     -> upstream/main ```  In this example, the `git fetch` command downloads all the branches from the remote repository `upstream` and creates a local branch `upstream/main` that points to the same commit as the remote branch `main`. The output also shows that Git created a new local branch `upstream/main` because it didn't find a local branch with the same name. If you already have a local branch with the same name, Git updates its tracking information instead of creating a new", "b896fd32-68c0-4bf9-a831-92038188b5e9": "To store your npm authentication token as a secret for publishing packages to npmjs, follow these steps:  1. Go to your GitHub repository settings. 2. Click on \"Secrets\" in the left-hand menu. 3. Click on \"New repository secret\" at the top of the page. 4. Enter \"NPM_TOKEN\" as the name of the secret. 5. Enter your npm authentication token in the \"Value\" field. 6. Click on \"Add secret\" to save the secret.  To publish a package to npmjs, follow these steps:  1. Make sure your package is properly configured with a `package.json` file. 2. Navigate to your package directory in your terminal. 3. Run `npm login` to log in to your npm account. 4. Follow the prompts to enter your email and password. 5. If you're publishing a package to the global namespace, run `npm publish`. 6. If you're publishing a package with a scope prefix, run `npm publish --access public`. 7. In your `package.json` file, make sure the `name` field includes the scope prefix if applicable. 8. In your GitHub workflow file (`.yaml`), add the following steps to store your npm authentication token as a secret and publish your package:  ```yaml name: Publish Package to npmjs on:   release:     types: [published] jobs:   build:     runs-on: ubuntu-latest     steps:       - uses: {% data reusables.actions.action-checkout %}       - uses: {% data reusables.actions.action-setup-node %}         with:           node-version: '20.x'           registry-url: 'https://registry.npmjs.org'       - run: npm ci       - run: npm publish         env:           NODE_AUTH_TOKEN: {% raw %}${{ secrets.NPM_TOKEN }}{% endraw %} ```  In the example above, the `setup-node` action creates an `.npmrc` file on", "e6f59a1f-d431-4e95-8372-be218ddd02db": "To prioritize open issues and pull requests in a milestone using GitHub, you can click to the left of an issue or pull request's checkbox, drag it to a new location, and drop it. This feature is available as long as there are less than 500 open issues in the milestone. If there are more than 500 open issues, you won't be able to prioritize issues in that milestone.", "0e507355-f977-40ba-805c-d95190af3b74": "To view a pull request related to a security advisory in the GitHub Advisory Database, you can follow these steps:  1. Go to the GitHub Advisory Database repository at https://github.com/github/advisory-database. 2. Search for the security advisory you're interested in. 3. Click on the advisory to open it. 4. Scroll down to the \"Pull Requests\" section and click on the pull request you want to view. 5. You can now see the details of the pull request, including the changes made, the comments, and the reviewers.  As for notifications, you can choose to receive them by following these steps:  1. Go to your GitHub profile settings by clicking on your profile picture in the top right corner of the page and selecting \"Settings\" from the drop-down menu. 2. Click on \"Notifications\" in the left-hand menu. 3. Scroll down to the \"Pull requests\" section and select the checkbox next to \"Let me review pull requests for all repositories I have access to.\" 4. You can also choose to receive notifications for specific repositories by clicking on the repository name in the \"Pull requests\" section and selecting the checkbox next to \"Let me review pull requests for this repository.\" 5. Save your changes by clicking on the \"Save changes\" button at the bottom of the page.  Now you'll receive notifications whenever there's a new pull request related to a security advisory in the GitHub Advisory Database, or in any other repository you have access to.", "06672fde-b849-4aba-a1e4-15dcc36ce022": "Yes, according to the GitHub policy, users are not allowed to use the platform to organize, promote, encourage, threaten, or incite acts of violence, and they cannot post content that depicts or glorifies violence or physical harm against human beings or animals. This includes threatening another individual or group with abuse, harm, sexual violence, or death, as well as posting text, imagery, or audio content glorifying or containing a graphic depiction of violence toward oneself, another individual, group, or animal. However, there may be legitimate reasons to post violent content, such as for educational or documentary purposes, creative works, or depictions of historical events, but a clear warning or disclaimer is required to help users make an educated decision as to whether or not they want to engage with such content. GitHub may also decide to limit the visibility of such content to those who choose to opt in.", "8b248393-db20-41cc-a1ce-94f706805a46": "The context information provided mentions three sponsorship tiers: Bronze, Silver, and Gold. However, it doesn't specify the benefits of each tier. To learn more about the benefits of each tier, you can refer to the \"AUTOTITLE,\" \"AUTOTITLE,\" and \"AUTOTITLE\" links provided in the context information.  To set up a range of different sponsorship options, including monthly and one-time tiers, on {% data variables.product.prodname_sponsors %}, you can create different sponsorship levels with varying amounts and durations. For example, you could offer a monthly sponsorship level for $5 per month, a quarterly sponsorship level for $15, and an annual sponsorship level for $60. You could also offer one-time sponsorship levels for specific projects or milestones, such as a $50 sponsorship for a major feature release. By providing a variety of sponsorship options, you make it easy for people to support your work in a way that fits their budget and schedule.  The context information states that there are no fees associated with sponsorship payouts on {% data variables.product.prodname_sponsors %}. This means that 100% of the funds you receive through {% data variables.product.prodname_sponsors %} go directly to you. For more information about sponsorship payouts, you can refer to the \"AUTOTITLE\" link provided in the context information.", "748afc55-e5f6-41fb-9fe8-7124581b84e7": "Unfortunately, without further context, it's unclear what specific open source projects are being referred to. In order to provide a more specific answer, please provide more details about the open source projects in question.", "91fca23b-c572-4e49-b5f1-ab4261a2f734": "Unfortunately, based on the provided context, it doesn't seem possible to search for starred repositories on Github using keywords. The context only explains how to star a repository, view starred repositories, and view who has starred a repository. It doesn't mention anything about searching for starred repositories using keywords. If you're looking for a specific repository that you've starred, you can go to your starred page and browse through the list of repositories you've starred. However, if you're looking for a repository related to a specific topic or keyword, you can use the search function on Github to search for repositories based on keywords.", "0492d3eb-53d5-4d56-9dd4-2ce94fe70958": "Passing the `--no-debug-info` option in the provided context information will result in the QL evaluator not emitting source location information in RA for debugging purposes. This is different from passing the `--[no-]fast-compilation` option, which is deprecated and was used to omit particularly slow optimization steps in the QL evaluator. The `--[no-]fast-compilation` option is no longer recommended for use as it may negatively impact the performance and compatibility of the QL compiler.", "db5b840c-07b7-409b-8b51-6eed340d6fc2": "To access your newly configured {% data variables.product.prodname_ghe_server %} instance, you should follow the instruction provided in the context information, which is to \"visit your instance\". This likely refers to opening a web browser and navigating to the URL of your {% data variables.product.prodname_ghe_server %} instance, which should be accessible via its public DNS name that was copied earlier. If you're unsure of the URL, you can check the web-based management console where you saved your settings, as it may have provided you with the URL or instructions on how to access it.", "a3657fca-126b-4b93-9c6a-6d62d2cc4f1d": "Changing your password in response to a security incident prevents any unintended future access to your account and projects, as your previous password may have been compromised during the incident. This is important to ensure the security and confidentiality of your data and prevent any unauthorized access or misuse of your account.", "d2bc8298-e794-4ce9-b067-dd9ae2b52b56": "To authenticate ARC using a {% data variables.product.prodname_github_app %}, follow these steps:  1. Create a {% data variables.product.prodname_github_app %} that is owned by an organization. 2. Configure the {% data variables.product.prodname_github_app %} with the following access permissions:    - Repository permissions: Read and write    - Metadata permissions: Read-only    - Organization permissions: Self-hosted runners: Read and write 3. Install the app on your organization and note the app installation ID. 4. Set the app installation ID and the {% data variables.product.pat_v1 %} as environment variables on your self-hosted runners.  {% endif %}    If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  To authenticate ARC using a {% data variables.product.pat_v1 %}, follow these steps:  1. Create a {% data variables.product.pat_v1 %} with the necessary permissions for ARC. 2. Set the {% data variables.product.pat_v1 %} as an environment variable on your self-hosted runners.  {% data reusables.actions.arc-app-post-install-set-secrets %}    In both cases, you can also use the `--token` flag to pass the {% data variables.product.pat_v1 %} or app installation ID directly to ARC during installation or registration.  For more information, see \"AUTOTITLE\".    Setting up ARC to use a {% data variables.product.pat_v1 %}  1. Set the {% data variables.product.pat_v1 %} as an environment variable on your self-hosted runner.     ```sh    export GITHUB_TOKEN=YOUR_TOKEN    ```     Replace `YOUR_TOKEN` with the {% data variables.product.pat_v1 %} you created.  2.", "0acdf80d-827e-4a4f-95e9-c79243aff4b7": "To add a repository to your Apache Maven configuration for {% data variables.product.prodname_registry %} using a personal access token, follow these steps:  1. Create a new `~/.m2/settings.xml` file if one doesn't exist. 2. Add a child `server` tag with an `id` under the `servers` tag, replacing USERNAME with your {% data variables.product.prodname_dotcom %} username, and TOKEN with your personal access token. 3. Configure a repository by mapping the `id` of the repository to the `id` you added in the `server` tag containing your credentials under the `repositories` tag. Replace {% ifversion ghes or ghae %}HOSTNAME with the host name of {% data variables.location.product_location %}, and OWNER with the name of the personal account or organization that owns the repository. Because uppercase letters aren't supported, you must use lowercase letters for the repository owner even if the {% data variables.product.prodname_dotcom %} user or organization name contains uppercase letters. 4. If you want to interact with multiple repositories, you can add each repository to separate `repository` children in the `repositories` tag, mapping the `id` of each to the credentials in the `servers` tag.  Note: If your {% data variables.product.prodname_dotcom %} instance has subdomain isolation enabled, you should replace \"github\" with the subdomain for your organization or enterprise. If you're not sure whether subdomain isolation is enabled, you can check by going to your {% data variables.product.prodname_dotcom %} settings and looking for the \"Code and github pages\" section. If you see a \"GitHub Pages\" tab, subdomain isolation is not enabled. If you see a \"Pages\" tab, subdomain isolation is enabled.", "4ec1ec9a-32c5-452d-a099-2f15cb169612": "To replace a node in an emergency in a {% data variables.product.product_name %} cluster, follow these steps:  1. Provision a new VM:    - Create a new virtual machine (VM) with the required resources.    - Install the {% data variables.product.product_name %} appliance on the new VM.    - Configure the VM with the necessary settings, such as network configuration and storage allocation.  2. Configure an IP address:    - Assign a static IP address to the new VM.    - Ensure that the IP address is not already in use by another node in the cluster.  3. Take the failed node offline:    - Identify the failed node in the cluster.    - Remove the failed node from the cluster configuration file.    - Shut down the failed node to prevent any further data loss.  4. Apply configuration:    - Update the cluster configuration file with the new node's details, including the IP address and node name.    - Apply the configuration changes to the cluster.  5. Add the new node to the cluster configuration file:    - Open the cluster configuration file.    - Add a new node heading with the necessary configuration details, such as the IP address, node name, and node role.    - Save the configuration file.  6. Initialize the cluster:    - Run the initialization process on the new node to join it to the cluster.    - Follow the prompts to complete the initialization process.  7. Apply configuration:    - Apply the configuration changes to the cluster to ensure that the new node is recognized by the cluster.  8. Optionally, evacuate the failed node:    - If you want to move the data from the failed node to the new node, you can use the evacuate feature to move the data.    - Follow the steps to evacuate the failed node and move the data to the new node.  To replace the primary MySQL node in a {% data variables.product.product_name %} cluster, follow these steps:  1. Add a new node to the cluster:    - Follow the steps to add a new node to the cluster, as described in the previous section.    - Ensure that the new node has sufficient", "884adf69-0ad1-4a00-b3a1-46526ea2e4e0": "To use JupyterLab in a GitHub Codespace, it needs to be installed in the codespace. This can be done by following the instructions provided in the context information. If the context doesn't specify whether JupyterLab is already installed in the default image, you can check by opening the codespace and looking for the JupyterLab application in the integrated development environment (IDE). If it's not installed, you can follow the instructions provided in the context information to install it. Alternatively, you can create a new codespace with JupyterLab already installed by selecting it as the development environment when creating the codespace.", "4a9feac6-1701-4905-b84f-d35a76d1976a": "To publish an app to GitHub Marketplace, you need to follow these steps:  1. Review your app carefully to ensure that it will behave as expected in other repositories and that it follows best practice guidelines. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"  2. Add webhook events to the app's settings. This allows the app to receive webhook events from GitHub, which can trigger actions in your app.  3. Create a listing for your app in the GitHub Marketplace. This involves providing a description, screenshots, and pricing information (if applicable).  4. Submit your app for review. GitHub will review your app to ensure that it meets the marketplace's guidelines and requirements.  5. Once your app is approved, it will be published to the marketplace and made available to other GitHub users.  To publish paid plans for your app and display a marketplace badge, you must complete the publisher verification process. For more information, see \"AUTOTITLE\" or \"AUTOTITLE.\"  Organizations can publish paid plans for their apps, but individuals cannot. Each app with a paid plan also goes through a financial onboarding process to enable payments.  To publish apps with free plans, you only need to meet the general requirements for listing any app. For more information, see \"AUTOTITLE.\"  If you're new to creating apps for GitHub Marketplace, you can refer to \"AUTOTITLE\" or \"AUTOTITLE\" for guidance.  {% data variables.product.prodname_github_apps %} are preferred over {% data variables.product.prodname_oauth_apps %}, although you can list both OAuth and {% data variables.product.prodname_github_apps %} in the marketplace. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"", "25409afe-b538-4165-a01b-627df127f72a": "The purpose of the ellipsis icon highlighted with an orange outline next to a custom role in the \"es\" settings is to indicate that the user can perform an action on that specific role by clicking on the ellipsis icon. However, without further context, it's unclear what action can be taken. The dialog that appears when clicking on the ellipsis icon confirms that the action is to delete the role. Therefore, the ellipsis icon with an orange outline serves as a visual cue to the user that they can delete the custom role by clicking on the icon.", "fb7f3b90-313a-4954-8611-75bd6b535790": "To publish your package to both the Maven Central Repository and GitHub Packages using the provided workflow, you need to follow these steps:  1. Ensure that your _pom.xml_ file includes distribution management repositories for both your {% data variables.product.prodname_dotcom %} repository and your Maven Central Repository provider.  2. Set up the Java JDK using the `setup-java` action in your workflow.  3. Use the `publish` job in your workflow to publish your package to both the Maven Central Repository and GitHub Packages. Here's an example of how your workflow might look:  ```yaml name: Publish package to the Maven Central Repository and GitHub Packages on:   release:     types: [created] jobs:   publish:     runs-on: ubuntu-latest     permissions:       contents: read       packages: write     steps:       - uses: {% data reusables.actions.action-checkout %}       - uses: {% data reusables.actions.action-setup-java %}         with:           java-version: '11'           distribution: 'temurin'       - name: Publish package to the Maven Central Repository         run: mvn --batch-mode deploy -DaltDeploymentRepository='https://oss.sonatype.org/content/repositories/releases/' -DaltDeploymentRepositoryLayout='default'       - name: Publish package to GitHub Packages         run: mvn --batch-mode deploy -DaltDeploymentRepository='https://maven.pkg.githubusercontent.com/YOUR_ORGANIZATION/YOUR_REPOSITORY/maven-metadata.xml' -DaltDeploymentRepositoryLayout='default'         env:           GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %} ```  In this example, the `publish` job runs on an Ubuntu machine, and it checks out a copy of your repository using the `action-checkout` action.", "d528cdc9-3afc-4cd8-9217-82c3f6313951": "The location to access the settings for managing member privileges in an organization is provided in the context information. It is under \"Organization settings\" and then \"Member privileges\". To access these settings, follow the steps provided in the context information: go to the organization's page, click on the settings icon (the gear icon), select \"Member privileges\" from the drop-down menu, and then customize the member privileges as needed.", "7dfe9bae-b38e-4784-9ee4-ec0ccbada7db": "To view information for a specific assignment, you can use the `gh classroom assignment` subcommand followed by the assignment ID or slug. For example:  ```shell gh classroom assignment 123456 ```  This command displays the assignment ID, title, description, due date, and other information about the assignment with ID `123456`.  Alternatively, you can use the assignment slug instead of the ID. For example:  ```shell gh classroom assignment my-assignment ```  This command displays the assignment ID, title, description, due date, and other information about the assignment with slug `my-assignment`.    List accepted assignments  ```shell gh classroom accepted ```  List of assignments that have been accepted by students in your classrooms.    Clone an assignment's starter code repository  ```shell gh classroom clone ```  Clones the starter code repository for an assignment.    Clone a student\u2019s assignment repository  ```shell gh classroom clone --student ```  Clones the repository for a student's assignment submission.", "d72c5507-2e55-47a7-9bc5-5fe413f4f928": "A job referencing an environment with required reviewers will wait for an approval before starting. Approval is required for jobs in workflows that reference an environment with required reviewers. If the job is not approved within 30 days, it will automatically fail.", "44c90ff5-ef98-4d5c-93e0-8e551f817ed2": "To sign commits using GPG, follow these steps as outlined in the context information:  1. Check for existing GPG keys 2. Generate a new GPG key 3. Add a GPG key to your GitHub account 4. Tell Git about your signing key 5. Sign commits  Note: The context information provided assumes that you already have GPG installed on your computer. If you don't have GPG, you'll need to download and install it before proceeding with these steps.", "0711c777-631b-49ce-ba85-7b96ea1a452d": "To encourage new contributors to follow your contribution guidelines, you can:  1. Clearly communicate your guidelines: Make sure your guidelines are easily accessible and prominently displayed in your repository. You can add a link to your guidelines in your repository's README file, and you can also include a link in your repository's email notifications.  2. Be consistent: Make sure you follow your own guidelines and set an example for others. This will help build trust and respect among your contributors.  3. Provide feedback: When reviewing pull requests, provide constructive feedback and suggestions for improvement. This will help contributors understand your expectations and improve their contributions over time.  4. Be responsive: Respond to questions and concerns in a timely manner. This will help build trust and show that you value your contributors' time and effort.  As for consequences for those who do not follow your guidelines, this will depend on the severity of the issue. Some possible consequences could include:  1. Gentle reminders: If a contributor makes a minor mistake, you can gently remind them of your guidelines and offer to help them make the necessary changes.  2. More serious consequences: If a contributor repeatedly ignores your guidelines, you may need to take more serious action. This could include closing their pull request, asking them to rewrite it in accordance with your guidelines, or even banning them from your repository if their behavior becomes disruptive.  Ultimately, the goal is to strike a balance between being firm about your guidelines and being fair and understanding with your contributors. By being clear, consistent, and responsive, you can help build a positive and productive community around your project.", "ebd09be8-c7e3-4a2d-be00-688224bfe975": "The `actions-sync` tool can only download actions from public repositories on {% data variables.product.prodname_dotcom_the_website %}.", "ce1856c6-8400-4cd3-8d90-fe4fbade5fb2": "To set a maximum codespace retention period for your organization's codespaces using GitHub's organization settings, follow these steps:  1. Log in to your GitHub account and navigate to your organization's settings page. 2. Click on \"Codespaces\" in the left-hand menu. 3. Click on \"Codespaces policies\" to view the existing policies. 4. Click on \"Add constraint\" and select \"Retention period\" from the drop-down menu. 5. Enter the maximum number of days codespaces can remain stopped before they are automatically deleted, and click \"Save\". 6. You can add additional constraints to the policy by clicking \"Add constraint\" and selecting another constraint. 7. Click \"Save\" to apply the policy to all new codespaces that are billable to your organization.  The retention period constraint is only applied to codespaces when they are created. Existing codespaces are not affected by this policy.  The valid range for the retention period is 0-30 days. Setting the period to 0 will result in codespaces being immediately deleted when they are stopped or when they time out due to inactivity.  It's essential to consider the potential impact of setting a maximum codespace retention period. Codespaces that exceed the maximum retention period will be automatically deleted, which could result in data loss if the codespace was not properly stopped or archived. It's crucial to ensure that all necessary data is backed up and that codespaces are properly stopped or archived before they exceed the maximum retention period. Additionally, it's essential to communicate the retention period policy to all users in your organization to ensure that they are aware of the policy and understand the implications of exceeding the maximum retention period.", "cfbe8eca-337e-4be9-8b31-9f6a661efefc": "The \"codeql generate log-summary\" command is used to create a summary of a structured JSON evaluator event log. This command produces a more stable output across different versions of the CLI than the event logs themselves. When implementing scripts that use output from the logs, it is strongly recommended to use the output of this command instead of the event logs directly.  The command's syntax is as follows:  ```shell copy codeql generate log-summary <input> [<result>] [--minify-output] [--utc] [--format=<format>] [--deduplicate-stage-summaries] ```  Here's an example of how to use the command:  ```shell copy codeql generate log-summary event_log.json output.json --minify-output --utc --format=overall ```  In this example, the command reads the event log from the file \"event_log.json\", writes the summary to the file \"output.json\", and uses the \"--minify-output\" and \"--utc\" options to minimize whitespace and force all timestamps to be in UTC, respectively. The \"--format\" option is set to \"overall\", which produces a JSON file containing overall information about the evaluation run, including summary statistics and information about the most time-consuming evaluations that were performed.  The \"--deduplicate-stage-summaries\" option is also available, but it only works in conjunction with the \"text\" format. If passed, this option moves all the summary tables containing the most expensive predicates to the end of the output, preventing them from being repeated for stages that are shared between queries.  The purpose of the \"codeql generate log-summary\" command is to provide a more stable and human-readable output than the raw event logs. The event logs can be complex and difficult to parse, especially when dealing with large-scale evaluations. The summary output produced by this command is more stable across different versions of the CLI, making it easier to use in scripts and other automation tools.  The advanced options provided by the command, such as \"--minify-output\" and \"--utc\", allow users to customize the output to suit their specific needs. The \"--minify-output\" option minimizes", "4ab12d11-e8a8-41bf-bbb2-c0ccc5ef2039": "Troubleshooting questions should be structured to provide clear and specific symptoms, error messages, or steps that lead to the issue. This allows people to easily identify if their issue matches the question and provides a starting point for finding a solution. Avoid using technical jargon or assumptions about the user's knowledge of the product or feature. Use simple and concise language to explain the issue and potential solutions. Provide screenshots or examples where helpful to illustrate the issue and steps to resolve it.", "f1d61f28-dedc-4c67-8d92-d8d4c546a925": "To find all references of a function or method within a repository using GitHub's symbols pane, follow these steps:  1. Select a repository and navigate to a file containing the function or method. 2. Click the {% octicon \"code-square\" aria-label=\"The code square icon\" %} above the file content to open the symbols pane. 3. Click the function or method call within the file to open the function window. 4. In the function window, you'll see a section titled \"References\" with a number indicating how many references there are. Click \"References\" to see a list of all references to the function or method within the repository.  Alternatively, you can search for the function or method in the symbols pane by clicking \"Search for this symbol in this repository\" or \"all repositories\" in the symbols pane. This will bring up a search results page with all occurrences of the function or method within the repository or across all repositories on GitHub. From there, you can click on a specific reference to navigate to it.  Note that precise code navigation is only available for certain languages supported by GitHub, such as Java, JavaScript, and Python. If your language isn't supported, you'll need to use search-based navigation instead.", "a523eae4-bfaa-41f9-bb35-92764c743530": "You should check for existing keys before adding a public GPG key to your personal account on GitHub. This can be done by following the instructions provided in the context information. If you don't find any existing keys, you can proceed with generating and copying a new key. This will ensure that you are not accidentally adding a duplicate key, which could cause confusion or errors in the verification process.", "51e2d8d1-2a54-45ca-ba3d-aaa3a124637e": "To delete a codespace created by a specific user in your organization using the GitHub CLI, follow these steps:  1. Make sure you have the GitHub CLI installed on your computer. You can download it from the official GitHub CLI website.  2. Log in to your GitHub account by running the following command in your terminal:     ```    gh auth login    ```  3. Navigate to the repository that contains the codespace you want to delete.  4. List the codespaces created by the specific user by running the following command:     ```    gh codespace list --user USER`     Replace `USER` with the username of the user who created the codespace you want to delete.  5. Find the codespace you want to delete in the list and take note of its name.  6. Delete the codespace by running the following command:     ```    gh codespace delete NAME`     Replace `NAME` with the name of the codespace you want to delete.  7. If the codespace contains unsaved changes, you will be prompted to confirm the deletion. Type `y` to confirm or any other key to cancel.  To delete a codespace using the GitHub REST API, follow these steps:  1. Go to the GitHub Developer Documentation website and sign in with your GitHub account.  2. Navigate to the \"REST API\" section and select \"Codespaces\" from the left-hand menu.  3. Scroll down to the \"Delete a codespace\" section and take note of the endpoint and required parameters.  4. Use a tool like Postman or cURL to send a DELETE request to the endpoint with the required parameters.  5. In the request body, include the name of the codespace you want to delete.  6. If the codespace contains unsaved changes, you will be prompted to confirm the deletion. Type `y` to confirm or any other key to cancel.  7. If the request is successful, the codespace will be deleted.  Note: The REST API requires authentication, so make sure you include your access token in the request headers.", "f4a96181-7546-4ebd-a30d-283098b03c12": "To upgrade the number of paid seats an organization has, you need to follow these steps:  1. Go to the organization's billing settings. 2. Click on \"Add seats\". 3. Enter the number of seats you want to add. 4. Confirm the addition of seats.  The cost of adding seats will depend on the number of seats you're adding and the remaining time in your billing cycle. The payment method on file for the organization will be charged a pro-rated amount based on these factors.  Note that if you're downgrading the organization to a free plan, you'll need to follow a different process, which is outlined in the context information provided.", "a77f35bd-afa8-424e-86f1-9baabc17f5cc": "From the Rule Insights page, you can gather information about the actions taken by rulesets in your organization's repositories. These insights include the number of times a rule was triggered, the number of times a rule passed or failed, and the number of times a rule was skipped. You can filter these insights by ruleset, repository, actor, and time period using the dropdown menus at the top of the page. This allows you to focus on specific rulesets, repositories, or actors, and to view insights for a specific time period. By analyzing these insights, you can identify trends, patterns, and potential issues with your rulesets, and make informed decisions about how to improve your organization's code quality and security.", "be6957bf-1d6e-43b3-b9f2-d46e3846b492": "The purpose of setting up a migration source in GitHub Enterprise Cloud (GHEC) as described in the context information is to identify the organization on GitHub Enterprise Server (GHES) that will be migrated to GitHub Enterprise Cloud (GHEC). This step is necessary to initiate the migration process using the GitHub Enterprise Migration Tool. The migration source provides the necessary details, such as the URL and name of the organization, to create a migration source in GHEC. This migration source will be used in subsequent steps to generate migration archives and initiate the migration process.", "82336888-bab1-47c0-96ac-19a6892d8717": "The {% data variables.product.prodname_container_registry %} currently supports Docker Image Manifest V2, Schema 2 and Open Container Initiative (OCI) Specifications. When installing or publishing a Docker image, the {% data variables.product.prodname_container_registry %} supports foreign layers, such as Windows images.", "aaae1eb3-72d6-4336-b637-e7ec0158b43a": "To fix the number of results reported by queries, the following options are recommended in order of complexity:  1. Check if the results are dominated by alerts identified by a single query. Try excluding that query from analysis. You can reintroduce it when other alerts are fixed. This can be achieved using the query configuration options provided by the tool, such as excluding a query from analysis in {% data variables.product.prodname_codeql %}.  2. Check if there are dataflow queries with many deep paths. Try omitting dataflow paths from the output. This can be achieved using the analysis configuration options provided by the tool, such as omitting dataflow paths from the output in {% data variables.product.prodname_codeql %}.  3. Exclude non-production code from analysis for interpreted languages. This can be achieved using the `paths` and `paths-ignore` options in the workflow file for {% data variables.product.prodname_codeql %} advanced setup for {% data variables.product.prodname_code_scanning %}.  Once you have fixed all of these alerts, you can update the configuration to expand the analysis to cover more code or run more queries.", "156b9320-f1e6-4bba-b416-e028b319e10c": "According to the context information provided, GitHub's responsibilities in relation to security safeguards and Data Breach response and notification are outlined in the Security Exhibit and Applicable Data Protection Laws. Specifically, GitHub will implement appropriate technical and organizational measures to ensure a level of security appropriate to the risks, regularly monitor compliance with these measures, and continue to take appropriate safeguards throughout the duration of the Agreement. In the event of a Data Breach, GitHub will comply with the Information Security obligations in the Security Exhibit and the Applicable Data Protection Laws, including Data Breach notification obligations. Additionally, GitHub represents and warrants that it will take reasonable steps to ensure that all GitHub personnel processing Protected Data have agreed to keep the Protected Data confidential and have received adequate training on compliance with the DPA and the Applicable Data Protection Laws. GitHub will also maintain complete, accurate, and up to date written records of all categories of processing activities carried out on behalf of Customer containing the information required under the Applicable Data Protection Laws. To the extent that assistance does not risk the security of GitHub or the privacy rights of individual Data Subjects, GitHub will make these records available to Customer on request as reasonably required, such as to help Customer demonstrate its compliance under the Applicable Data Protection Laws.", "d1faf4b8-162c-4271-8939-c0f27997a3b7": "In Go, the `import` statement is used to import packages into the current package. It takes the form of `import \"package_path\"` and can also be used to import multiple packages at once by separating the paths with spaces. This statement is used to access the functionality and variables defined in other packages within the Go program. The concept of modules in Go is related to this as a module is a collection of packages that are versioned and distributed together, and the `go.mod` file at the top level of the module's source tree defines the module's path, version, and dependencies.", "1f981493-cdf3-43cc-b9dd-7554677179ff": "Based on the context information provided, \"Edit custom properties values at the organization level\" allows you to set custom property values for all repositories in the organization, while \"Manage the organization's custom properties definitions\" grants access to create and edit custom property definitions for the organization as a whole. In other words, \"Edit custom properties values at the organization level\" involves actually modifying the values of custom properties for all repositories in the organization, while \"Manage the organization's custom properties definitions\" refers to managing the definitions or templates for custom properties that can be applied to repositories within the organization.", "7f34d34b-d98e-43dc-912c-c578e34725b5": "The purpose of the Security Exhibit mentioned in the context information is to outline GitHub's information security program, including measures to protect customer content and personal data, as well as incident management procedures in the event of a security incident. This exhibit is incorporated by reference into the Standard Contractual Clauses and is binding on the data importer as if it were set forth in its entirety. The Security Exhibit provides written descriptions of security controls and other information requested by the customer regarding GitHub's security practices and policies for the Online Service.", "c1c65ad4-ede9-4935-a5a4-6d959dc987cc": "To ensure that all provisioned users in Azure AD have a unique normalized alias, you can follow these steps:  1. Log in to the Azure AD portal and navigate to the {% data variables.product.prodname_emu_idp_application %} application. 2. In the left sidebar, click \"Provisioning\". 3. Click \"Edit Provisioning\". 4. Expand \"Mappings\" and click \"Provision Azure Active Directory Users\". 5. Click the {% data variables.product.prodname_dotcom %} \"userName\" attribute mapping. 6. Change the attribute mapping.    - To map an existing attribute in Azure AD to the \"userName\" attribute in {% data variables.product.prodname_dotcom %}, click your desired attribute field. Then, save and wait for a provisioning cycle to occur within about 40 minutes.    - To use an expression instead of an existing attribute, change the Mapping type to \"Expression\", then add a custom expression that will make this value unique for all users. For example, you could use `FIRST NAME]-[LAST NAME]-[EMPLOYEE ID]`. For more information, see [Reference for writing expressions for attribute mappings in Azure Active Directory in Microsoft Docs.  By following these steps, you can ensure that all provisioned users in Azure AD have a unique normalized alias, which is necessary to avoid conflicts and ensure that all users can access their accounts without any issues.", "cb490f3a-72e4-439d-b8f2-acf09de53dfa": "Repositories in your organization that are eligible for default setup for {% data variables.product.prodname_code_scanning %} are those that meet all the following criteria: {% data variables.product.prodname_code_scanning_caps %} is not already enabled, {% data variables.product.prodname_actions %} are enabled, and the repository uses a {% data variables.product.prodname_codeql %}-supported language (Go, JavaScript/TypeScript, Python, or Ruby) or is publicly visible (if you're using {% data variables.product.prodname_GH_advanced %}).  To configure advanced setup for repositories that are not eligible, you can either enable default setup at the repository level or use a script to configure advanced setup at the organization level. For more information on configuring advanced setup, see \"AUTOTITLE.\"", "75f9487c-932c-42e5-bdf0-e44e23cf1874": "The required options for logging in to Docker Hub using the `login-action` in GitHub Actions are `username` and `password`. These options should be stored as secrets to avoid exposing them in the workflow file.", "80800fa8-7ad0-4e36-88f8-485b1d173e2c": "In the context of {% data variables.product.prodname_actions %} workflows, a runner is a machine that executes jobs. {% data variables.product.prodname_dotcom %} provides runners that can be used to run jobs, or users can host their own runners. Each {% data variables.product.prodname_dotcom %}-hosted runner is a new virtual machine hosted by {% data variables.product.prodname_dotcom %} with the runner application and other tools preinstalled, and is available with Ubuntu Linux, Windows, or macOS operating systems. When using a {% data variables.product.prodname_dotcom %}-hosted runner, machine maintenance and upgrades are taken care of for the user.", "6bc2f5a4-b1b1-4067-af85-7e825b69f05b": "Masked project or group variable values and artifact reports must be migrated manually when using GitHub Actions Importer with GitLab.", "91b68bd9-c861-4ae8-bdaa-6555c233666b": "To ensure that your webhook settings are correctly configured to receive webhook deliveries from {% data variables.product.company_short %}, you should follow these steps:  1. Check your webhook settings on {% data variables.product.company_short %}. Make sure that the URL you provided is correct and matches the URL where your server is listening.  2. Make a test webhook delivery on {% data variables.product.company_short %}. You can do this by navigating to the repository where you want to receive webhook deliveries, clicking on \"Settings,\" and then clicking on \"Webhooks\" in the left-hand menu. Click on the \"Add webhook\" button, and then enter the URL where your server is listening. Make sure that the \"Content type\" is set to \"application/json,\" and then click on the \"Let's go\" button.  3. Trigger an event on {% data variables.product.company_short %} that should trigger a webhook delivery. This could be a push event, a pull request event, or any other event that you want to receive webhook deliveries for.  4. Check your server logs to make sure that you received the webhook delivery. If you're using a language like Node.js, you can use a package like `morgan` to log incoming requests.  5. Check {% data variables.product.company_short %} to verify that the webhook delivery was successful. You can do this by navigating to the repository where you want to receive webhook deliveries, clicking on \"Settings,\" and then clicking on \"Webhooks\" in the left-hand menu. Click on the webhook you just created, and then click on the \"Deliveries\" tab. You should see a list of all the webhook deliveries that have been sent to your server.  If you're experiencing issues with receiving webhook deliveries from {% data variables.product.company_short %}, here are some troubleshooting steps you can take:  1. Check your server logs to make sure that you're receiving incoming requests. If you're not seeing any requests, it could be a problem with your server configuration or your network setup.  2. Check the webhook delivery logs on {% data variables.product.company", "11c82c78-8eb9-45ae-ae41-ddbbf4a2b163": "If you are using SAML SSO and are not able to see your organization's resources after authorizing a GitHub app, it is likely due to an issue with SAML sessions. To resolve this issue, you should follow these steps:  1. Go to `https://github.com/orgs/ORGANIZATION-NAME/sso` to start an active SAML session for your organization. Replace `ORGANIZATION-NAME` with the name of your organization. 2. Revoke your authorization of the GitHub app. 3. Reauthorize the GitHub app. The app's authorization process may vary, so refer to the app's documentation for specific instructions.  By following these steps, you should be able to see your organization's resources after authorizing a GitHub app while using SAML SSO.", "685aff3e-078e-442d-b441-a789812f261b": "Without further context, it is unclear what \"AUTOTITLE\" refers to. If \"AUTOTITLE\" is referring to a specific document feature, it could potentially differ from other document features in terms of functionality based on the specific software or document type being used. More information is needed to accurately answer this question.", "b64800d4-82f6-4728-a547-40d2a6e9a2c7": "To remove a single file from the repository's Git history using either the `filter-repo` command or BFG Repo-Cleaner, follow these steps:  1. First, make sure that the file you want to remove is not currently checked out. You can do this by running the following command in your repository:     ```shell    git status    ```     If the file you want to remove is listed as \"Untracked files\", it's not currently checked out. If it's listed as \"Changes not staged for commit\", it's currently checked out and you'll need to check it out again after removing it.  2. Next, choose whether you want to use `filter-repo` or BFG Repo-Cleaner to remove the file. Both tools are designed to remove large files from Git history, but they have different features and usage.     - `filter-repo` is a Git tool that's included in Git LFS 2.x. It's designed to be more lightweight and easier to use than BFG Repo-Cleaner, and it's included in the Git LFS installation process. To use `filter-repo`, follow these steps:       a. Install Git LFS 2.x by following the instructions in the Git LFS documentation.       b. Navigate to your repository.       c. Run the following command to remove the file:         ```shell        filter-repo --path PATH_TO_FILE --force        ```         Replace `PATH_TO_FILE` with the path to the file you want to remove.       d. Follow the prompts to confirm the removal.     - BFG Repo-Cleaner is a separate tool that's not included in Git LFS. It's designed to be more powerful and flexible than `filter-repo`, but it's also more complex to use. To use BFG Repo-Cleaner, follow these steps:       a. Download BFG Repo-Cleaner from the BFG Repo-Cleaner website.       b. Navigate to your repository.       c. Run the following command to remove the file:         ```shell", "91c38709-71dd-4b0c-804c-4c2f76c84fd8": "To enable, disable, or set individual package ecosystems to read-only on {% data variables.location.product_location %}, follow these steps:  1. Access the settings page of the Management Console by following the instructions provided in the context information. 2. Click on the \"Packages\" tab. 3. Under \"Ecosystem Toggles\", select \"Enabled\", \"Read-Only\", or \"Disabled\" for each package type. 4. Save the settings.  Note: Subdomain isolation must be enabled to toggle the {% data variables.product.prodname_container_registry %} options.  This will allow you to manage which package ecosystems are available on your instance. By setting an ecosystem to \"Read-Only\", you can prevent new packages from being uploaded while still allowing existing packages to be downloaded.", "12051d0b-5705-4e59-8290-73502f2b6bb9": "To create a workflow that performs a Docker build and publishes Docker images to Docker Hub or {% data variables.product.prodname_registry %} using the provided guide, follow these steps:  1. Ensure that you have a complete definition for a Docker image stored in a {% data variables.product.prodname_dotcom %} repository, including a _Dockerfile_ and any other necessary files.  2. Create a new workflow file in your repository, or edit an existing one.  3. Add the following YAML configuration to your workflow file:  ```yaml name: Docker Build and Publish  on:   push:     branches:       - main  jobs:   build-and-push:     runs-on: ubuntu-latest      steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up Docker Buildx       uses: docker/setup-buildx-action@v1       with:         platform: linux/amd64,linux/arm64      - name: Login to Docker Hub       uses: docker/login-action@v1       with:         username: ${{ secrets.DOCKERHUB_USERNAME }}         password: ${{ secrets.DOCKERHUB_PASSWORD }}      - name: Build and push       uses: docker/build-push-action@v2       with:         tag: ${{ github.sha }}         file: Dockerfile         push: true         arch: amd64         platform: linux/amd64         registry: dockerhub.com ```  4. Replace the following placeholders with your own values:  - `DOCKERHUB_USERNAME`: Your Docker Hub username. - `DOCKERHUB_PASSWORD`: Your Docker Hub password.  5. Save the workflow file and commit the changes to your repository.  6. Trigger the workflow by pushing a change to the `main` branch.  The workflow will now perform a Docker", "b2bd1a0e-47ca-420a-8297-ed426f891d58": "To sign tags using GPG, follow these steps after completing steps 1-4 in the \"To sign commits using GPG and have those commits verified on {% data variables.product.product_name %}\" section:  1. Tag a commit using `git tag <tagname>` 2. Sign the tag using `git tag -s <tagname>` 3. Push the tag to {% data variables.product.product_name %} using `git push --tags`  Your signed tag will now be cryptographically verifiable against your public key on {% data variables.product.product_name %}.", "da59f65d-a6d6-43bf-828e-debc91835104": "In the context of {% data variables.product.prodname_dependabot %}, a security update refers to a pull request that addresses a security vulnerability in a dependency, while a version update refers to a pull request that updates a dependency to a newer version, regardless of whether it addresses a security vulnerability or not.", "3ee0ff45-eece-4216-85b0-29ff26b2c549": "To configure dependency groups in the `dependabot.yml` file for {% data variables.product.prodname_dependabot_version_updates %}, you must specify groups per package ecosystem. Here's an example configuration for `groups`:  ```yaml version: 2 updates:   # ...  groups:   # Group for all Ruby dependencies   ruby:     # ...    # Group for all JavaScript dependencies   javascript:     # ...    # Group for all Python dependencies   python:     # ...    # Group for all PHP dependencies   php:     # ...    # Group for all Go dependencies   go:     # ...    # Group for all Rust dependencies   rust:     # ...    # Group for all Elixir dependencies   elixir:     # ...    # Group for all TypeScript dependencies   typescript:     # ...    # Group for all Kotlin dependencies   kotlin:     # ...    # Group for all Swift dependencies   swift:     # ...    # Group for all Java dependencies   java:     # ...    # Group for all Scala dependencies   scala:     # ...    # Group for all Clojure dependencies   clojure:     # ...    # Group for all C++ dependencies   cpp:     # ...    # Group for all R dependencies   r:     # ...    # Group for all Julia dependencies   julia:     # ...    # Group for all Crystal dependencies   crystal:     # ...    # Group for all Dart dependencies   dart:     # ...    # Group for all Nim dependencies   nim:     # ...    # Group for all OCaml dependencies   ocaml:     # ...    # Group for all Haxe dependencies   haxe:     # ...    # Group for all F# dependencies   fsharp:     # ...    # Group for all A", "45f36622-95a6-48b2-bc4d-71161ae3ea6e": "To limit the use of public actions and reusable workflows in your organization's GitHub Actions, you can manage the permissions for your organization by following these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. Click on \"Actions\" in the left-hand menu. 3. Under \"Policies\", select \"Limit use of public actions and reusable workflows\". 4. Choose whether you want to allow the use of local actions and reusable workflows only, or if you want to allow the use of both local and public actions and reusable workflows. 5. Click \"Save\" to apply the changes.  The benefits of limiting the use of public actions and reusable workflows include:  1. Increased security: By limiting the use of public actions and reusable workflows, you can ensure that only trusted and verified actions and workflows are used in your organization's GitHub Actions. This reduces the risk of malicious or unintended actions being executed in your workflows. 2. Improved compliance: Many organizations have compliance requirements that restrict the use of public actions and workflows. By limiting the use of these resources, you can ensure that your organization's GitHub Actions are in compliance with these requirements. 3. Better control over workflow execution: By allowing only local actions and reusable workflows, you can have greater control over the execution of your workflows. This allows you to ensure that your workflows are executed in a consistent and predictable manner, and that any necessary customizations are applied. 4. Reduced maintenance: By limiting the use of public actions and reusable workflows, you can reduce the amount of maintenance required to manage your organization's GitHub Actions. This is because you won't need to keep track of and manage the updates and maintenance of external actions and workflows.  Overall, limiting the use of public actions and reusable workflows can help to improve the security, compliance, and control of your organization's GitHub Actions, while also reducing maintenance requirements.", "0be93a17-5735-4455-8653-14a5859c4ea2": "Dynamic text should be formatted in bold in IBE.", "833411ee-cc97-4b5c-a7b9-222e204bde72": "According to the context information provided, organizations can disable automatic inheritance of access permissions for all new packages scoped to their organization. However, no specific steps or instructions are given on how to do this. It's possible that this feature is available through the organization's settings or administration console, but further research or contact with GitHub support may be necessary to determine the exact process.", "f30bfe3d-1faa-4a85-a70d-7f81a8ca6bd1": "Tags in {% data variables.product.prodname_desktop %} serve as annotations that mark specific points in a repository's history, such as version numbers for releases. They are associated with commits, allowing users to easily identify and reference specific versions of their code. Tags can be created, viewed, and deleted within {% data variables.product.prodname_desktop %}, and they can be pushed to a remote repository to share with collaborators. It's important to note that tags can only be deleted for commits that have not yet been pushed. For more information about tags and their usage, refer to the Git documentation on tagging.", "15813d7d-4010-4661-b054-c684003d2446": "The user should be aware that {% data variables.product.prodname_copilot_for_prs %} is currently in beta, and while it can provide helpful context for pull requests, it should be used as a supplement to a human's work, not a replacement. Some limitations and issues that users may encounter include:  1. Accuracy: While {% data variables.product.prodname_copilot_short %} is designed to be accurate, it may not always capture all the necessary context, especially for complex changes. Users should review the summary and the code changes carefully to ensure that everything is accurate and complete.  2. Dependency on code structure: {% data variables.product.prodname_copilot_short %} relies on the structure of the code to generate the summary and the list of changes. If the code structure is unclear or inconsistent, the summary may not be as helpful. Users should ensure that their code is well-structured and documented to make it easier for {% data variables.product.prodname_copilot_short %} to generate an accurate summary.  3. Size of pull request: {% data variables.product.prodname_copilot_short %} can take a couple of minutes to generate a summary for larger pull requests. Users should be aware of this and plan accordingly.  To provide feedback on {% data variables.product.prodname_copilot_for_prs %}, users can click on the text link that appears in the UI after a summary is generated. This will take them to a survey where they can provide feedback on the feature's accuracy, usefulness, and any other issues or limitations they have encountered. Users can also contact GitHub support if they have any questions or concerns about {% data variables.product.prodname_copilot_for_prs %}.", "cfd3776e-ec61-4e74-9589-e43d36217a9d": "The purpose of adding a registry as a source in RubyGems is to make a specific RubyGems registry accessible to the gem installer. This allows you to install gems from that registry, which can be useful if you have private gems that you want to use in your projects.  To add a registry as a source, you can use the `gem sources` command with the `--add` flag. In the provided context, the command to add the registry as a source is:  ```shell gem sources --add https://USERNAME:TOKEN@{% ifversion fpt or ghec %}rubygems.pkg.github.com{% else %}REGISTRY_URL{% endif %}/NAMESPACE/ ```  Replace `USERNAME` with your GitHub username, `TOKEN` with your GitHub token (which can be generated in your GitHub settings), `REGISTRY_URL` with the URL for your instance's RubyGems registry (if you're using GitHub Enterprise Server, replace `HOSTNAME` with your server's hostname), and `NAMESPACE` with the namespace of the organization or personal account that owns the repository containing the gem (if you're using GitHub Packages for RubyGems, replace `rubygems.pkg.github.com` with the URL provided by GitHub).  After running this command, the specified registry will be added as a source, and you can install gems from it using the `gem install` command.", "0c9cb364-ca17-480e-b0c6-cfca64b0b0dc": "{% data reusables.enterprise_installation.hardware-considerations-all-platforms %} should be taken into account while installing {% data variables.product.prodname_ghe_server %} on OpenStack. This includes having enough CPU, memory, and storage resources to support the desired number of users and repositories, as well as ensuring that the hardware is compatible with the OpenStack and {% data variables.product.prodname_ghe_server %} software versions being used. It's also important to consider factors such as network bandwidth and latency, as well as any specific requirements for the hardware platform being used (such as virtualization or containerization technologies).", "a98e366c-7a55-4402-a61d-829c92352d40": "Question: What are the key factors that contribute to successful supply chain collaboration and communication?  Answer: In the context provided, the key factors that contribute to successful supply chain collaboration and communication include:  1. Clear communication: All parties involved in the supply chain should have a clear understanding of their roles and responsibilities, as well as the expectations for communication and collaboration.  2. Trust: Trust is essential for successful collaboration and communication. All parties should be able to rely on each other to deliver on their commitments and to be transparent about any issues or challenges that arise.  3. Technology: Technology can facilitate collaboration and communication by providing a platform for real-time communication, sharing of data and information, and tracking of performance metrics.  4. Flexibility: Supply chain collaboration and communication should be flexible and adaptable to changing circumstances. All parties should be willing to adjust their strategies and approaches as needed to meet the needs of the supply chain.  5. Continuous improvement: Collaboration and communication should be focused on continuous improvement, with a commitment to identifying and addressing issues and opportunities for improvement.  6. Sustainability and social responsibility: Collaboration and communication should be guided by principles of sustainability and social responsibility, with a focus on minimizing environmental impact and promoting social welfare.  These factors are important for ensuring that all parties involved in the supply chain are working together effectively to achieve common goals and to deliver value to customers.", "496ae98e-07d1-409b-bc21-d2c41accad05": "You can find information about open security advisories and licenses in GitHub Enterprise Server (GHES) in the **Open security advisories** and **Licenses** graphs, respectively. These graphs can be accessed through the Security tab in the organization settings.", "d789d67d-7130-4975-adf2-738586a17068": "To ensure that your proposed file change is accepted by others using Git, you can follow these steps:  1. Make sure your proposed change is relevant and necessary for the project. 2. Write a clear and concise commit message that explains the reason for the change and its benefits. 3. Choose a meaningful commit email address that is associated with your Git account. 4. Choose the correct branch to propose the change on, typically the main or development branch. 5. Propose the file change using Git's pull request feature, which allows others to review and comment on your proposed change before it is merged into the main branch. 6. Respond to any feedback or suggestions from other collaborators in a constructive and professional manner. 7. Make any necessary adjustments to your proposed change based on the feedback received. 8. Once the pull request is approved, merge the change into the main branch. 9. Test the new code to ensure that it does not introduce any bugs or issues. 10. Document the change in the project's changelog or release notes.  By following these steps, you can increase the likelihood that your proposed file change is accepted by others using Git and that it is integrated into the project in a timely and efficient manner.", "1c74a105-012a-4254-b79e-246b667a1be1": "The purpose of the \"--common-caches\" option is to control the location of cached data on disk that will persist between several runs of the CLI, such as downloaded QL packs and compiled query plans. This option allows for customization of the directory where this cached data will be stored, which can be useful for managing disk space or sharing cached data between multiple users or machines. If not set explicitly, this option defaults to a directory named \".codeql\" in the user's home directory, which will be created if it doesn't already exist. This feature was introduced in version 2.15.2 of the CLI.", "5f16a81b-7594-4382-a863-aa6c305bcc20": "Custom properties in GitHub repositories allow developers to add additional information to their repositories, such as compliance frameworks, data sensitivity, or project details. These properties are private and can only be viewed by people with read permissions to the repository. Custom properties can be used to target specific repositories with rulesets by integrating them with GitHub's automated security tools, such as CodeQL or Dependabot. By setting values for custom properties for repositories in an organization, developers can ensure that their repositories are properly categorized and targeted by these tools. This can help to improve the overall security and compliance of the organization's codebase.", "ce27d35e-72b6-4116-ae1b-6252cb3d3228": "The purpose of global webhooks in the context provided is to notify an external web server when events occur within an enterprise using {% data variables.location.product_location %}. This allows for the configuration of the server to receive the webhook's payload and run an application or code to monitor, respond to, or enforce rules for user and organization management for the enterprise. This feature can be used to automatically perform tasks after receiving the webhook, such as creating, deleting, or modifying repositories or organizations within the enterprise.", "43cf846d-c171-4bfa-bc60-eccba2fb11c7": "Custom transformers for items in {% data variables.product.prodname_actions_importer %} can be created by defining them using a DSL built on top of Ruby. Each custom transformer file must contain at least one `transform` method, which returns a Hash, an array of Hashes, or nil. This returned value corresponds to an action defined in YAML. The example provided converts a build step with the \"buildJavascriptApp\" identifier to run various npm commands. The resulting workflow step in {% data variables.product.prodname_actions %} is comprised of converted build steps with the same identifier. Note that custom transformer files must reside in the same directory or subdirectories from where the `gh actions-importer` command is run.", "f85370c7-6d81-4913-84e0-367388fad79b": "To use a specific endpoint, the following requirements need to be met based on the context provided:  1. The user has the necessary scopes and permissions for the endpoint. This may vary depending on the endpoint and the resources it affects. For example, if the endpoint requires organization-level permissions, the user must be an owner of the affected organization.  2. The {% data variables.product.prodname_github_app %} (if used) has access to the required repositories. This is necessary for endpoints that affect resources owned by the app.  3. The user has access to the required repositories. This is necessary for endpoints that affect resources owned by the user.  4. If using an OAuth app user access token, the token must have the required scopes and permissions, the user who authorized the token must have the necessary permissions, and the organization (if applicable) has not blocked OAuth app access.  5. If using `GITHUB_TOKEN` in a {% data variables.product.prodname_actions %} workflow, the endpoint should only affect resources owned by the repository where the workflow is running. If access to resources outside of that repository is required, a {% data variables.product.pat_generic %} or an OAuth app user access token should be used instead.  6. The token (if using an OAuth app user access token or `GITHUB_TOKEN`) must not be expired or revoked.  If any of these requirements are not met, the user may not be able to use the endpoint or may encounter errors when attempting to do so.", "4ff27c2b-35b5-428d-a497-4519ab132292": "The user should wait for the package to become visible after the workflow concludes, as no further action is required on the user's part. However, if the context provides additional information, such as a specific process or action that needs to be taken for the package to become visible, then the user should follow those steps. Without further context, it's unclear what needs to be done for the package to become visible after the workflow concludes.", "ae09d0a5-ee18-423b-a07c-c32154e80b73": "To use PowerShell and Pester for continuous integration (CI) in {% data variables.product.prodname_dotcom %}-hosted runners, follow these steps:  Prerequisites:  1. Familiarity with YAML and {% data variables.product.prodname_actions %} syntax. 2. Basic understanding of PowerShell and Pester. 3. {% data reusables.actions.enterprise-setup-prereq %} (if using {% data variables.product.prodname_ghe_cloud %}).  Adding a workflow for Pester:  1. Create a new YAML file in your repository's `.github/workflows/` directory. 2. Name the file `test-powershell.yaml`. 3. Add the following content to the file:  ```yaml name: Test PowerShell on Ubuntu on: push  jobs:   pester-test:     name: Pester test     runs-on: ubuntu-latest     steps:       - name: Check out repository code         uses: {% data reusables.actions.action-checkout %}       - name: Perform a Pester test from the command-line         shell: pwsh         run: Test-Path resultsfile.log | Should -Be $true       - name: Perform a Pester test from the Tests.ps1 file         shell: pwsh         run: |           Invoke-Pester Unit.Tests.ps1 -Passthru ```  4. Save the file and commit your changes.  Explanation:  - `name: Test PowerShell on Ubuntu` - The name of the workflow. - `on: push` - Triggers the workflow when a push event occurs. - `jobs: pester-test:` - Defines a new job called `pester-test`. - `name: Pester test` - The name of the job. - `runs-on: ubuntu-latest` - Runs the job on the latest Ubuntu runner. - `steps:` - Defines the", "58a6b0ae-dc02-416d-b997-ae78f699a2df": "Based on the provided context information, it seems that the script is retrieving an OIDC token and an ID token URL using the steps.script.outputs variable. The script then makes a request to the ID token URL using the `curl` command and parses the response using the `jq` command to extract the ID token. The `jwtd` function is defined to decode and verify the JWT signature using the `jq` command. The ID token is then echoed to the console. However, it's not clear how this ID token will be used to obtain an access token from a cloud provider. More context is needed to determine the specific steps for exchanging the ID token for an access token and accessing resources in the cloud provider.", "ca1e3215-fd64-4d08-b328-08595c599abc": "{% data variables.product.prodname_actions_importer %} uses a mapping to convert GitLab environment variables to their closest equivalent in {% data variables.product.prodname_actions %}. For example, `CI_API_V4_URL` in GitLab is converted to `{% raw %}${{ github.api_url }}`{% endraw %} in {% data variables.product.prodname_actions %}. This allows GitLab workflows to be easily migrated to {% data variables.product.prodname_actions %}.  The difference between `CI_COMMIT_REF_NAME` and `CI_COMMIT_REF_SLUG` in GitLab environment variables is that `CI_COMMIT_REF_NAME` represents the full name of the branch or tag, including the prefix (such as \"refs/heads/\" or \"refs/tags/\"), while `CI_COMMIT_REF_SLUG` represents only the name of the branch or tag, without the prefix. For example, if the current branch is \"feature/my-feature\", then `CI_COMMIT_REF_NAME` would be \"refs/heads/feature/my-feature\", and `CI_COMMIT_REF_SLUG` would be \"feature/my-feature\". This distinction can be useful in certain scenarios, such as when working with multiple repositories with similar branch or tag names.", "4cbc2f31-944d-46b5-a497-a20a5078cadb": "Subdomain isolation for {% data variables.location.product_location %} after configuring a hostname is recommended to help mitigate various cross-site scripting vulnerabilities, as stated in the context information provided. This feature isolates subdomains from each other, preventing malicious scripts from executing on other subdomains. By enabling subdomain isolation, you can enhance the security of your {% data variables.location.product_location %} instance and protect your users from potential cross-site scripting attacks.", "d8688312-e96e-4c40-a58d-bb32282b7cb1": "In {% data variables.product.prodname_github_codespaces %}, a prebuild is a build that is created in advance and stored as an image, which can be used to quickly create a codespace without the need for a full build. This can significantly reduce the time it takes to create a codespace, especially for complex or time-consuming builds. In contrast, a regular build is a build that is performed on demand when a codespace is created or when changes are pushed to the repository. While regular builds can still be optimized for speed, they may take longer than prebuilds, especially for large or complex projects.", "3344fb28-7710-4300-b671-d91881e686ba": "Follow the troubleshooting steps provided in the context information to resolve the issue of being unable to connect to a codespace from {% data variables.product.prodname_vscode %} desktop. These steps include checking for the latest version of the {% data variables.product.prodname_github_codespaces %} extension and verifying the connection status of the {% data variables.product.prodname_github_codespaces %} service at githubstatus.com. If the service is available and the extension is up-to-date, contact support for further assistance.", "791b3b51-63a3-4380-af5c-3e0b2651bd29": "To ensure that your {% data variables.product.prodname_pages %} site is published from a specific branch, you need to follow these steps:  1. Make sure the branch you want to use as your publishing source already exists in your repository. 2. Go to your repository on {% data variables.product.prodname_dotcom %}. 3. Click on the settings icon in the top right corner of the page. 4. Click on \"Pages\" in the left-hand menu. 5. Under \"Build and deployment\", select \"Deploy from a branch\". 6. Select the branch you want to use as your publishing source from the drop-down menu. 7. Optionally, select a folder for your publishing source from the drop-down menu. 8. Click \"Save\".  If you're using GitHub Actions to build and deploy your site, you'll need to update your workflow file to use the selected branch as the source for your site. For more information, see the documentation on building and deploying your site with GitHub Actions.  If you're not using GitHub Actions, you'll need to push your changes to the selected branch and wait for the site to be automatically built and deployed.  Note that if you choose the `docs` folder on any branch as your publishing source, and later remove the `/docs` folder from that branch in your repository, your site won't build and you'll get a page build error message for a missing `/docs` folder. To avoid this, make sure you don't remove the `/docs` folder from the branch you've selected as your publishing source.  If you're having trouble publishing your site from a branch, you can refer to the troubleshooting guide for more information.", "44e892bb-543e-4eb1-a439-ae54433d4e9c": "To communicate an intended change to the development team and prevent wasted time in debugging failed runs of code scanning, the following steps can be taken:  1. Notify the development team: Communicate the intended change to the development team through email, chat, or any other preferred communication channel. This ensures that they are aware of the change and can prepare accordingly.  2. Provide details: Provide detailed information about the intended change, including the reason for the change, the expected outcome, and any potential impacts. This helps the development team understand the context and implications of the change.  3. Coordinate with the team: Coordinate with the development team to ensure that the change is implemented smoothly and does not cause any unintended consequences. This may involve scheduling a meeting, providing training, or providing additional resources.  4. Test the change: Test the intended change thoroughly to ensure that it works as expected and does not cause any issues. This helps prevent failed runs of code scanning and ensures that the development team can focus on other tasks.  5. Document the change: Document the intended change and its implementation to provide a reference for future reference. This helps ensure that the change is properly understood and can be easily replicated or reversed if necessary.  By following these steps, the development team can be informed of the intended change and can avoid wasted time in debugging failed runs of code scanning.", "6f9a0dca-05ab-4379-a7e1-309ee7beefd6": "To add notes to a {% data variables.projects.projects_v1_board %}, follow these steps:  1. Navigate to the {% data variables.projects.projects_v1_board %} where you want to add notes. 2. In the column you want to add a note to, click the 'add a note to this column' button, which is represented by {% octicon \"plus\" aria-label=\"Add a note to this column\" %}. 3. Type your note, then click 'Add'.  Note: You can reference an issue or pull request in your note by typing its URL in the card.  To convert a note to an issue, follow these steps:  1. Navigate to the note that you want to convert to an issue. 2. Click the 'more options' button, which is represented by {% data reusables.project-management.project-note-more-options %}. 3. Click 'Convert to issue'. 4. If the card is on an organization-wide {% data variables.projects.projects_v1_board %}, in the drop-down menu, choose the repository you want to add the issue to. 5. Optionally, edit the pre-filled issue title, and type an issue body. 6. Click 'Convert to issue'. 7. The note is automatically converted to an issue.  To edit or remove a note, follow these steps:  1. Navigate to the note that you want to edit or remove. 2. Click the 'more options' button, which is represented by {% data reusables.project-management.project-note-more-options %}. 3. To edit the note, click 'Edit note'. 4. Make the necessary changes, then click 'Save'. 5. To remove the note, click 'Remove note'. 6. Confirm that you want to remove the note by clicking 'Remove'.  Note: The steps for editing and removing a note are the same for both personal and organization-wide {% data variables.projects.projects_v1_board %}.", "cfdda304-29e3-4519-864d-e7c9a33ce3c8": "The variable `$(Release.Reason)` in the provided context represents the reason for which the release is being triggered. This could be due to various events such as the completion of a build, the merging of a pull request, or the manual triggering of a release. The value of this variable can be used to customize the release process based on the reason for the release, such as deploying to a different environment or performing additional tests.", "87f0a88a-af8e-48eb-9db8-a74e7f3e9016": "The purpose of GitHub's bug bounty program is to better engage with security researchers by providing them with a financial reward for finding and reporting vulnerabilities through their coordinated disclosure process. This program benefits security researchers by recognizing the significant effort they put forth when hunting down bugs and providing them with a monetary incentive to continue their work in improving the security of GitHub's software. It also helps GitHub to address vulnerabilities in a timely and responsible manner, ultimately improving the overall security of their platform.", "e3b7b8c9-4527-4536-b90b-1e08cf2466c0": "Variables and secrets can be configured in both GitLab CI/CD and {% data variables.product.prodname_actions %} through the use of configuration files and the respective UI. In GitLab CI/CD, variables can be set in the pipeline configuration file, while secrets can be created through the GitLab UI. Similarly, in {% data variables.product.prodname_actions %}, variables can be set in the workflow configuration file, and secrets can be created through the {% data variables.product.product_name %} UI.", "a734a011-d442-4d05-83dc-33b9ac67e25e": "To search for repositories based on the number of stars they have, you can use the `stars` qualifier provided in the context information. Here's an example:  - `stars:500` matches repositories with exactly 500 stars.  To find repositories that were created or last updated within a specific time frame, you can use the `created` and `pushed` qualifiers, respectively. Here are examples:  - `created:2021-01-01` matches repositories with the word \"webos\" that were created on or before January 1, 2021. - `pushed:>2021-01-01` matches repositories with the word \"css\" that were pushed to after January 1, 2021. - `pushed:>=2021-03-06 fork:only` matches repositories with the word \"case\" that were pushed to on or after March 6, 2021, and that are forks.", "c155cb4d-5aec-44d5-927f-602e7c7bcd7e": "To verify the successful deployment of the containerized web application using Kubernetes, you can follow these steps:  1. Run the \"Deploy\" step in the GitHub Actions workflow provided in the context information. This will deploy the Docker image to the GKE cluster.  2. Check the status of the deployment using the following command:     ```    kubectl rollout status deployment/$DEPLOYMENT_NAME    ```     Replace `$DEPLOYMENT_NAME` with the name of the deployment you created. The output should show that the deployment is \"Rolling update complete\".  3. Get the list of services in the cluster using the following command:     ```    kubectl get services -o wide    ```     This will display a list of services running in the cluster, along with their IP addresses and ports. Look for the service associated with your application.  4. Access the application by connecting to the service's IP address and port. You can find the IP address and port by running the following command:     ```    kubectl get services -o jsonpath=\"{.items[*].spec.clusterIP} {.items[*].spec.ports[0].nodePort}\" | awk '{print $1 \" \" $2}'    ```     This will display the cluster IP and node port for each service. Find the service associated with your application and use its IP address and node port to access the application.  5. Verify that the application is running by accessing it through a web browser or a command-line tool like `curl`.  If you encounter any issues during the deployment process, you can refer to the \"Deploying a containerized web application\" documentation provided in the context information for troubleshooting tips.", "f18e0572-cabb-412b-935c-21cd992128b7": "If the `marketplace_purchaseon_free_trial` attribute is `true` in the `marketplace_purchase` webhook event, the app developer should track the free trial start date (`effective_date`) and the date the free trial ends (`free_trial_ends_on`). To display the remaining days left in a free trial in the app's UI, the developer can use the `free_trial_ends_on` date. They can calculate the number of days left in the free trial by subtracting the current date from the `free_trial_ends_on` date. The developer can then display this number of days in the app's UI, such as in a banner or in the billing UI.  If a free trial expires and the customer does not cancel the app before the end of the free trial, the developer should automatically transition the customer to a paid plan. This can be done by handling the `marketplace_purchase` webhook event with the `renewed` action, which occurs when a customer renews their subscription. The developer can use the `effective_date` and `marketplace_purchase` object from this webhook event to determine which plan the customer is now on and when the next billing cycle begins. The developer can then update the app's functionality and UI accordingly. If the customer does not want to continue using the app, they can cancel it at any time in their {% data variables.product.prodname_github_marketplace %} account settings.", "bbdacffd-85e0-4b15-8ffe-3ac45168823d": "To deploy a {% data variables.product.product_name %} cluster in an environment, you need to install {% data variables.product.prodname_ghe_server %} on each node's virtual machine, upload a cluster-enabled license, configure the first node, and initialize the node with a configuration file. Additionally, {% data variables.product.prodname_ghe_server %} requires HTTPS for clustering. The specific software that needs to be installed on each node's virtual machine is {% data variables.product.prodname_ghe_server %}.", "31ce5933-98c9-4393-b415-48f8bc2d458d": "To edit or delete a default label for a repository in an organization on GitHub, follow these steps:  1. Access the organization on GitHub. 2. Click on the organization's settings icon (the gear icon) in the organization's repository list. 3. Click on \"Repository defaults\" in the left-hand menu. 4. Scroll down to the \"Repository labels\" section. 5. To edit a label, click on the label you want to edit. 6. Update the label name, description, and color (optional) as desired. 7. Click \"Save\" to save the changes. 8. To delete a label, click on the label you want to delete. 9. Click \"Delete this label\" in the confirmation dialog. 10. Click \"Delete\" to confirm the deletion.  Note that deleting a default label will remove it from all new repositories created in the organization, but it will not affect existing repositories or labels that have already been created. To delete a label from existing repositories, you'll need to delete it individually from each repository.", "92823779-a115-40f8-9d2a-5e0c4f516834": "To ensure that users receive notifications of failed prebuilds, you can add them as recipients in the \"Failure notifications\" setting when creating or editing a prebuild configuration. When creating or editing a prebuild configuration, scroll down to the \"Failure notifications\" section and begin typing a username, team name, or full name. Click the name once it appears to add them to the list. The users or teams you add will receive an email when prebuild failures occur, containing a link to the workflow run logs to help with further investigation. Note that people will only receive notifications of failed prebuilds if they have enabled notifications for failed Actions workflows in their personal settings.", "22f2c617-55de-488a-9568-5997bef9862f": "To enable interactive maps in your GitHub Enterprise Server instance using the provided context information, follow these steps:  1. Go to your GitHub Enterprise Server instance and log in as an administrator. 2. Click on your profile picture in the top right corner and select \"Settings\" from the drop-down menu. 3. In the left-hand menu, click on \"Management console\". 4. In the \"Settings\" sidebar, click on \"Privacy\". 5. To enable interactive maps, select \"Enable GeoJSON rendering\". 6. If you want to configure authentication for Azure Maps, under \"Azure Maps API Token\", paste your token. 7. Click \"Save settings\" to apply the changes.  Note: The context information provided only mentions how to enable interactive maps and configure authentication for Azure Maps. If you need further information on how to obtain an Azure Maps authentication token, you should refer to the Microsoft Docs mentioned in the context information.", "df27a329-5e0f-475c-abe7-faa9b33551ff": "The Unified view in {% data variables.product.prodname_desktop %} displays changes linearly, while the Split view shows old content on the left side and new content on the right side when displaying diffs. To change between these views, in the top-right corner of the diff view, click the gear icon and select \"Diff display\". From there, you can choose between Unified and Split views.", "c10c9af6-29db-4ec8-8050-963e2f2fcad7": "The purpose of adding a passphrase to an SSH key is to provide an additional layer of security to prevent unauthorized access to your systems in case your computer is compromised. When a passphrase is added to an SSH key, it requires the user to enter a password or passphrase each time they attempt to use the key to authenticate themselves to a remote server. This ensures that even if someone gains access to your private key, they cannot use it to log in without the passphrase. By securely saving the passphrase in the SSH agent, users can avoid having to enter the passphrase every time they connect, providing a more convenient and secure experience.", "1bb553d2-ef59-4a1f-b6c4-aff91f846830": "Your client should have the owner role in the organization after inviting them, in order for you to transfer ownership to them.", "c2e059a0-ae03-4f49-bab7-97b7f39f17d9": "To generate a new SSH key using the Ed25519 algorithm, you can use the following command:  ```shell ssh-keygen -t ed25519 -C \"your_email@example.com\" ```  When prompted to save the key, you can press Enter to accept the default file location. If you have created SSH keys previously, ssh-keygen may ask you to overwrite another key. In this case, we recommend creating a custom-named SSH key by typing the default file location and replacing `id_ed25519` with your custom key name.  For example:  ```shell > Enter a file in which to save the key (/Users/YOU/.ssh/id_ed25519): [Press enter] ```  In this case, you can replace `/Users/YOU/.ssh/id_ed25519` with a custom file name, such as `/Users/YOU/.ssh/my_ed25519_key`.  If you want to add a passphrase to your SSH key, you can do so by entering a secure passphrase when prompted. This is optional, but it adds an extra layer of security to your key.  ```shell > Enter passphrase (empty for no passphrase): [Type a passphrase] > Enter same passphrase again: [Type passphrase again] ```  After entering your passphrase, you will be asked to confirm it. Once you have confirmed your passphrase, your new SSH key will be generated.  Note: If you are using a legacy system that doesn't support the Ed25519 algorithm, you can use the RSA algorithm instead by replacing `ed25519` with `rsa` in the command.  ```shell ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" ```  In this case, you can use the same steps to save and secure your key as described above.", "fa197564-69fa-4bc7-8a38-80142c8c57e3": "Base64 encoding can be used to store small binary blobs as secrets in GitHub by encoding the binary file into a Base64 string using a command like `base64 -i file.bin -o file.base64` on MacOS or `base64 -w 0 file.bin > file.base64` on Linux. The Base64 string can then be stored as a secret in GitHub using the `gh secret set` command. To access the Base64 string from a runner, it can be piped to `base64 --decode` to decode it back into the original binary format. This is a useful technique for storing small binary blobs as secrets in GitHub, as it allows them to be easily accessed and used in workflows without having to store the actual binary file in a public repository. However, it should be noted that Base64 encoding is not a substitute for actual encryption, as it only converts binary to text and does not provide any additional security.", "976c206b-0ad7-4824-83a7-cecbcce2597b": "The agreement outlines the terms and conditions for participating in the Program, which includes access to the API and Confidential Information. The parties involved agree to keep the Confidential Information confidential and not disclose it to third parties without prior written consent. The agreement also outlines the fees and payment terms for using the Program, as well as the consequences of breaching the agreement or the Program Terms. The agreement includes warranties and disclaimers from the participating party, and specifies that certain rights and obligations will survive the termination or expiration of the agreement. The Program, API, and Confidential Information are provided \"as is\" without any warranties or guarantees.", "40e20878-fd21-47fd-b298-e5775fc3084a": "The purpose of adding labels to issues is to categorize and prioritize them based on their status, severity, or type. For example, \"help wanted\" could be added to issues that are looking for contributions, \"good first issue\" could be added to issues that are suitable for new contributors, and \"bug\" could be added to issues that report a problem with the software.  To automate this process using the {% data variables.product.prodname_cli %} in a workflow, you can create a YAML file with a workflow that runs when an issue is opened or reopened. The workflow uses the {% data variables.product.prodname_cli %} to add labels to the issue using the `gh issue edit` command.  Here's an example of how to create such a workflow:  1. Create a new file named `.github/workflows/label-issues.yml` in your repository. 2. Copy and paste the following YAML code into the file:  ```yaml name: Label issues on:   issues:     types:       - opened       - reopened jobs:   label-issues:     runs-on: ubuntu-latest     permissions:       issues: write     steps:       - name: Add labels         run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"         env:           GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}           GH_REPO: ${{ github.repository }}           NUMBER: ${{ github.event.issue.number }}           LABELS: triage,help wanted ```  In this example, the workflow runs when an issue is opened or reopened, and it adds the \"triage\" and \"help wanted\" labels to the issue using the `gh issue edit` command. You can customize the `LABELS` variable to add different labels based on your needs.  3. Save the file and commit it to your repository.  Now, every time an issue is opened or reopened, the workflow will automatically add the specified labels to the issue, making it easier to categorize and prioritize them", "137ebd77-1472-4cea-ae14-54381fbb3a4f": "To obtain a GitLab personal access token with the \"read_api\" scope, follow these steps:  1. Log in to your GitLab account and go to your profile settings. 2. Click on the \"Settings\" tab and then select \"Access tokens\" from the left-hand menu. 3. Click on the \"New token\" button. 4. In the \"Note\" field, enter a description for the token, such as \"GitHub Actions Importer\". 5. In the \"Scopes\" section, select the \"read_api\" scope. 6. Click on the \"Create token\" button to generate the token. 7. Copy the token and save it in a secure location for later use.  After creating the token, you should use it when configuring the {% data variables.product.prodname_actions_importer %} CLI tool to connect to your GitLab instance. This will allow the tool to access the necessary data and resources for importing your GitLab CI/CD configuration into {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %}.", "e2d30d43-5f1b-4130-8179-4390636b240b": "The purpose of creating an organization profile README for both public users and members of the organization is to provide them with useful information about the organization. This can include an \"About\" section that describes the organization, guidance for getting help in the organization, and any other relevant information. By creating separate READMEs for public users and members, organizations can tailor the information to the specific needs of each group.  Text, images, and GIFs can be formatted using {% data variables.product.company_short %} Flavored Markdown, which allows for the use of Markdown syntax to add formatting, such as bold, italic, and underline text, as well as images and GIFs. This can help to make the README more visually appealing and easier to read. For more information on how to use {% data variables.product.company_short %} Flavored Markdown, you can refer to the documentation provided by {% data variables.product.company_short %}.", "eeed8cd9-80e8-41fe-af38-818e3ab6830b": "To signal to readers that you've added or changed a note, or to indicate the publication date of errata, append a datestamp in the format \"[Updated: YYYY-MM-DD]\" to the end of the release note or documentation. This format follows the ISO 8601 standard for dates and is easily recognizable by users. By using this format consistently, users can easily identify when changes have been made and ensure they are using the most up-to-date information.", "7247205d-a935-42b2-984e-4b63b9406f04": "To list public events and return only two issues instead of the default 30, and fetch only the first page of results, we can modify the request by adding the `per_page` and `page` query parameters to the URL. Here's the modified request:  ```shell curl --request GET \\ --url \"{% data variables.product.api_url_code %}/events?per_page=2&page=1\" \\ --header \"Accept: application/vnd.github+json\" \\ --header \"X-GitHub-Api-Version: 2022-11-28\" \\ https://api.github.com/events ```  Replace `{% data variables.product.api_url_code %}` with the actual API URL code for your GitHub instance. This will return only two issues instead of the default 30, and fetch only the first page of results.", "dad183c1-d245-4360-a3c1-a3772406831c": "Users have several options to manage and disable non-essential cookies on GitHub Enterprise Marketing Pages and on the web in general. On GitHub pages that serve non-essential cookies, users can click on a link in the page's footer to access cookie settings and update their preferences. Some users may also see a cookie consent banner with options to accept, manage, or reject all non-essential cookies. Users can also control cookies using browser tools such as Do Not Track signals, cookie controls, browser extensions, and the Global Privacy Control signal. These options allow users to limit the types of cookies they encounter online and manage their privacy preferences.", "864e4ef8-2c8f-45e3-af02-3b1b84aff900": "Based on the context information provided, an organization owner on GitHub can manage the following features:  - Publish packages (for certain package managers) - Manage topics - Enable wikis and restrict wiki editors - Enable project boards - Configure pull request merges - Configure a publishing source for GitHub Pages (if the organization has a GitHub Pages site)  The context information does not explicitly state whether an organization owner can view content exclusion settings for GitHub Copilot, so it is unclear whether this feature can be managed by an organization owner.  If the context information had included a statement about whether an organization owner can view content exclusion settings for GitHub Copilot, then the answer would have been:  - View content exclusion settings for GitHub Copilot (if the context information stated that this feature can be managed by an organization owner)  Otherwise, if the context information had not included any statement about GitHub Copilot, then the answer would have been:  - View content exclusion settings for GitHub Copilot (if the context information stated that this feature can be managed by an organization owner) or \"No, based on the provided context information\" (if the context information did not explicitly state whether this feature can be managed by an organization owner).", "5440123d-7576-4d85-82c3-b7e70fdc8751": "Environment variables stored in an environment can be accessed by workflow jobs that reference the environment using the `vars` context. This allows for the dynamic configuration of workflows based on environment-specific values. These variables are only accessible within the context of the environment and are not available outside of it. For more information, refer to the documentation on environment variables and the `vars` context in GitHub Actions.", "55efcd28-5436-4ad6-bb85-48184d5da7f4": "In repositories where {% data variables.product.prodname_code_scanning %} is configured as a pull request check, alerts generated by changes in the pull request are reported in the following places: the check results in the pull request, the **Conversation** tab of the pull request (if enabled), and the **Files changed** tab of the pull request. The default branch for this check is the branch that the pull request targets, but this configuration can be changed within {% data variables.product.prodname_actions %} or in a third-party CI/CD system.", "b50a1751-f653-449b-b955-8d6eab2797e1": "GitHub ensures the security and privacy of its users' data by implementing various measures to prevent unauthorized access or misuse. Some of these measures include:  1. Encryption: GitHub uses encryption to protect users' data both in transit and at rest. This ensures that data is secure and cannot be accessed by unauthorized parties.  2. Access controls: GitHub has strict access controls in place to ensure that only authorized personnel can access users' data. This includes multi-factor authentication, role-based access control, and regular security audits.  3. Regular security updates: GitHub regularly updates its systems and software to ensure that they are secure and up-to-date with the latest security patches.  4. Data backup and recovery: GitHub regularly backs up users' data to prevent data loss in the event of a system failure or other disaster.  5. Data minimization: GitHub only collects and processes the minimum amount of data necessary to provide its services. This helps to reduce the risk of data breaches and misuse.  6. User privacy: GitHub provides users with control over their data, including the ability to delete their accounts and data at any time.  7. Third-party security: GitHub works with trusted third-party security providers to ensure that its systems and data are secure.  Overall, GitHub takes a proactive and comprehensive approach to data security and privacy, and is committed to protecting its users' data at all times.", "a64f8ab5-c610-4af0-ad10-94b2eabec440": "Customer grants to GitHub the rights it needs to use Customer Content without attribution and to make reasonable adaptations of Customer Content as necessary to provide the Service, as stated in Section D of the context information. GitHub can use this content in accordance with the license granted by Customer to External Users, as outlined in Section E, and in the case of Customer Content added to a repository with a license notice, under the same terms. GitHub's use of Customer Content is limited to providing the Service and is not otherwise permitted without Customer's prior written consent.", "4aba59fe-989a-4613-8424-3d75557d34dd": "To set the intro and permission statement for an article in GitHub documentation, you can add the following keys to the YAML front matter of the Markdown file:  - `intro`: Sets the intro for the page. This string will render after the `title`. - `permissions`: Sets the permission statement for the article. This string will render after the `intro`.  Both `intro` and `permissions` are optional keys. If you omit them, GitHub will use a default value.  Here's an example of how to use these keys:  ```yaml title: Contributing to projects with GitHub Desktop shortTitle: Contributing to projects intro: This article explains how to contribute to projects using GitHub Desktop. permissions: This article is available to all GitHub users. ```  In this example, the `intro` is set to a custom message, and the `permissions` statement is also set to a custom message. If you omit these keys, GitHub will use a default value.", "2b9bbab9-89d9-481b-b3b5-2b31f52220c1": "Based on the context information provided, to determine which repositories in your organization are eligible for default setup for code scanning, you can use the following criteria:  1. The repository must have languages suitable for default setup. This means that the repository contains one or more of the following languages: JavaScript, TypeScript, Java, Python, Ruby, Go, C#, C++, or PHP.  2. For public repositories, the repository must be eligible for default setup. This means that the repository is visible to the public.  3. For private or internal repositories, the repository must meet one of the following criteria:     a. The repository must have {% data variables.product.prodname_GH_advanced_security %} enabled. This can be checked in the repository settings.     b. The repository does not have {% data variables.product.prodname_GH_advanced_security %} enabled, but you plan to enable it in the future. In this case, the repository will become eligible for default setup once {% data variables.product.prodname_GH_advanced_security %} is enabled.  If a repository meets these criteria, it is eligible for default setup for code scanning. You can then enable or disable default setup for all eligible repositories at once using the steps outlined in the documentation provided.  If you have a large number of repositories in your organization, you may want to consider using the multi-repo enablement feature, which allows you to enable or disable default setup for multiple repositories at once. This can save a significant amount of time and effort compared to enabling or disabling default setup for each repository individually.  For more information on configuring default setup at scale for multiple repositories in your organization, see the documentation provided.", "21cfbf98-ffd9-498e-aaa1-8f035a321def": "To switch between multiple accounts on {% data variables.product.prodname_dotcom_the_website %} without always being required to reauthenticate, you can sign in to your accounts and use the account switcher. This feature is available for personal accounts and service accounts, as well as for switching between personal accounts and {% data variables.enterprise.prodname_managed_users %} in an enterprise that uses {% data variables.product.prodname_emus %}. When you add a new account to the account switcher, both the account you are currently signed in to and the new account will become available in the switcher, and you will be signed in to the new account immediately. If you access {% data variables.product.prodname_dotcom_the_website %} on another computer or browser, the same accounts will not be available until you add them. If you follow a link to {% data variables.product.product_name %} from an external source, you will first be prompted to choose which account you want to use. Your SSO sessions will persist when you switch away from an account and return, but if you're a member of an enterprise that uses {% data variables.product.prodname_emus %} and add your {% data variables.enterprise.prodname_managed_user %} to the account switcher, the {% data variables.enterprise.prodname_managed_user %} will appear grayed out if your session has expired. Selecting the expired account will send you to reauthenticate with your IdP.", "c1941a62-4159-4a3d-93ce-2b4afd0160fd": "The \"Repository reports\" section provides the following key pieces of information:  1. `created_at`: When the repository was created (as an ISO 8601 timestamp) 2. `owner_id`: ID of the repository's owner 3. `owner_type`: Type of the repository's owner (either a user or an organization) 4. Additional information about the repository, such as the number of commits, branches, and tags, may also be included in this section, but it is not explicitly mentioned in the context provided.  Note: If the context provided did not include the `owner_type` key, then it would not be listed as a key piece of information in this answer.", "27379c3d-43b6-4cfb-8b66-a9c57061522e": "To filter search results across all of {% data variables.product.prodname_dotcom_the_website %} to only show matching apps and actions from {% data variables.product.prodname_marketplace %}, follow these steps:  1. Go to https://github.com/search. 2. Type any keywords and press Enter. 3. In the \"Filter by\" sidebar, click More to see all available filters for your search. 4. Click Marketplace to show results from {% data variables.product.prodname_marketplace %}.  This will narrow down your search results to only show matching apps and actions from {% data variables.product.prodname_marketplace %}.", "67a6b612-5be5-436f-8933-dd3dce2b909b": "To request a new password on {% ifversion fpt or ghec %}{% data variables.product.product_url %}{% else %}{% data variables.product.product_url %}{% endif %}, you need to visit the password reset page (https://{% data variables.product.product_url %}/password_reset) and enter the email address associated with your account. After clicking \"Send password reset email,\" you will receive a link to reset your password via email. You must click on this link within 3 hours of receiving the email, as the time limit for clicking on the link is 3 hours. If you didn't receive the email, you should check your spam folder. Additionally, if you have enabled two-factor authentication, you will be prompted to enter your authentication code or recovery code, or use your security key or {% data variables.product.prodname_mobile %} app.", "f357bf88-f314-438e-8901-8e528d4776a0": "The article lists two identity providers, Active Directory Federation Services (AD FS) and all identity providers that implement the SAML 2.0 standard, as officially supported for SAML SSO with {% data variables.product.product_name %}. Limited support is provided for non-supported identity providers that implement the SAML 2.0 standard.", "f6df692d-8f3c-48be-b9cd-d57117a50f8c": "The {% data variables.product.prodname_command_palette %} can be used to navigate quickly through {% data variables.product.product_name %} by providing optimized suggestions based on your current context and recently used resources. You can open the command palette from anywhere on {% data variables.product.product_name %} using a keyboard shortcut, which saves time and keeps your hands on the keyboard. The suggestions are optimized to give easy access to top-level pages like the Issues page, and you can refine the suggestions by starting to enter the name or number for the location you want. This makes it easy to navigate quickly through {% data variables.product.product_name %} without having to navigate through a series of menus.", "243ce393-9f54-4b8a-9ae9-6f956fd39a06": "To change the `target-branch` for Docker version updates in the `dependabot.yml` file, you can add the following configuration under the Docker update section:  ```yaml target-branch: \"develop\" ```  In this example, the `target-branch` is set to \"develop\". You can replace \"develop\" with the name of the branch you want to use for Docker version updates.  Note that this change will only affect version updates for Docker, as security updates are always made against the default branch. If the new `target-branch` does not contain a Dockerfile to update, this change will have the effect of disabling version updates for Docker.    How can custom labels be set for all pull requests raised for a package manager using the `dependabot.yml` file?  Answer:  To set custom labels for all pull requests raised for a package manager using the `dependabot.yml` file, you can add the following configuration under the update section for the package manager:  ```yaml labels:   - \"custom-label-1\"   - \"custom-label-2\" ```  In this example, the `labels` array contains the names of the custom labels you want to use. You can replace \"custom-label-1\" and \"custom-label-2\" with the names of the labels you want to use.  Note that you cannot create new labels in the `dependabot.yml` file, so the custom labels must already exist in the repository.    How can the default labels be overridden for all pull requests raised for a package manager using the `dependabot.yml` file?  Answer:  To override the default labels for all pull requests raised for a package manager using the `dependabot.yml` file, you can add the following configuration under the update section for the package manager:  ```yaml labels:   - \"custom-label-1\"   - \"custom-label-2\" ```  In this example, the `labels` array contains the names of the custom labels you want to use instead of the default labels. You can replace \"custom-label-1\" and \"", "4d1b5bf2-37e8-4f2a-b973-1666c6d0b53f": "To change the branch range and destination repository while creating a pull request on GitHub, you can use the drop-down lists provided in the pull request creation process. By default, the pull request is based on the parent repository's default branch, but you can change both the parent repository and the branch using the drop-down lists. You can also swap your head and base branches using the drop-down lists to establish diffs between reference points. Remember that the base branch is where changes should be applied, and the head branch contains what you would like to be applied. When you change the base repository, you also change notifications for the pull request. The Commit and Files changed preview areas will update to show your new range. Additionally, project maintainers can add a pull request template for a repository, which includes prompts for information in the body of a pull request.", "5eae9ba7-1d88-424e-8580-84c9a538a9de": "To access and manage environment secrets in the context of deployment environments, you can refer to the \"AUTOTITLE\" documentation provided. This documentation should provide more information about environments and how to manage environment secrets. Additionally, the text mentions that GitHub offers this feature as a gated feature, which means it may require a paid subscription or additional setup to access. If you don't have access to this feature, you won't be able to manage environment secrets in this way.", "3ed8610b-f06e-4de3-a3b0-d51ff16ddee8": "If you omit required parameters or use the wrong type for a parameter, you may receive a \"Invalid request\" error message. This error can be avoided by carefully reviewing the reference documentation for the endpoint to ensure that you are using the correct parameter types and that you are including all of the required parameters. It's also a good idea to test your requests with sample data to ensure that they are being processed correctly.", "8831334b-cde8-4abf-a858-f8ad1fae3eb3": "To authenticate to GitHub Enterprise Server Container registries using the CLI tool provided, you can pass the `--registries-auth-stdin` option or use the `CODEQL_REGISTRIES_AUTH` environment variable. This overrides the `GITHUB_TOKEN` environment variable.", "f060b546-d636-44c3-b792-f9b17d75e759": "According to the text material provided, the parties involved in this agreement are independent contractors and are not employees, agents, partners, franchisors, franchisees, or legal representatives of each other for any purpose. This legal status is outlined in section 4 of the agreement.", "de39aeac-08c5-4fe9-a0d5-ee89fad8be7b": "An enterprise owner can view a list of IdP groups, their memberships, and connected teams on GitHub by accessing the Enterprise account settings, clicking on the Identity provider tab, and then clicking on the name of the desired IdP group. From there, they can view the members of the group and the teams that are connected to it by clicking on the \"Members\" or \"Teams\" tab, respectively. If a team is unable to sync with the group on the IdP, an error will be displayed.", "8aa66ee4-247a-489c-bea8-59513c12c855": "{% data variables.product.prodname_desktop %} differs from using Git on the command line in that it provides a graphical user interface for simplifying Git commands and visualizing changes, making it easier for beginners to get started with Git and {% data variables.product.prodname_dotcom %}. Advanced users can also benefit from {% data variables.product.prodname_desktop %} as it provides easy access to less-common Git commands and encourages best practices for creating accurate and easy-to-follow commit histories. Additionally, {% data variables.product.prodname_desktop %} is specifically designed for use with {% data variables.product.prodname_dotcom %}, allowing for quick authentication and pull request checking without the need for a separate browser.", "85dee2f6-7dcf-46ac-856b-6c5038644b12": "The significance of being able to specify multiple affected version ranges in the \"Affected products\" section of a security advisory is that it allows for more detailed and accurate reporting of the impacted software versions. This is important because it helps developers and users quickly identify which versions of a software are affected by a security vulnerability, and which versions are not. By providing a clear and comprehensive list of affected versions, it also helps to ensure that appropriate actions are taken to address the vulnerability in a timely manner.", "e55405a0-66dc-4f7e-a95d-916cb1c256e5": "A small number of extractor errors during database creation indicates a good state of analysis because it suggests that the majority of source files were successfully extracted and analyzed. This means that the database contains accurate and comprehensive information about the codebase, and the analysis results are likely to be reliable. However, it's still important to investigate any extractor errors that do occur, as they may indicate underlying issues with the codebase or the extraction process that need to be addressed.", "6497b088-d47c-45c8-b2d0-b7a3010b0b8c": "To set up a {% data variables.product.prodname_copilot %} subscription for your personal {% ifversion fpt %}or organization{% else %}, organization, or enterprise{% endif %} account, follow these steps:  1. Go to the {% data variables.product.prodname_dotcom %} website and sign in to your account.  2. If you have a personal account, go to your profile and click on the \"Settings\" tab. If you have an organization or enterprise account, go to your organization or enterprise settings.  3. Click on the \"Billing\" tab.  4. Click on the \"Subscriptions\" tab.  5. Click on the \"Add subscription\" button.  6. Select the {% data variables.product.prodname_copilot %} subscription plan that best fits your needs.  7. Follow the prompts to enter your payment information and complete the subscription process.  8. Once your subscription is active, you can install the {% data variables.product.prodname_copilot %} extension in {% data variables.product.prodname_vscode %} and start using {% data variables.product.prodname_copilot %}.  If you have an organization or enterprise account, you will also need to assign {% data variables.product.prodname_copilot %} seats to your team members. To do this, go to your organization or enterprise settings, click on the \"Members\" tab, and assign {% data variables.product.prodname_copilot %} seats to your team members.  {% endif %}    Installing the {% data variables.product.prodname_copilot %} extension in {% data variables.product.prodname_vscode %}  After you have set up your {% data variables.product.prodname_copilot %} subscription, you can install the {% data variables.product.prodname_copilot %} extension in {% data variables.product.prodname_vscode %}.  1. Open {% data variables.product.prodname_vscode %} and go to the \"Extensions\" tab.  2. Search for", "8d5ac199-4bef-4233-9f5f-a36e45c041ae": "According to the context information provided, it should be avoided to describe table data unnecessarily as it may make the table confusing and unclear. This is because the table's data and importance should be clear from any preceding content, column headers, and row headers (if needed). If the data in a table is unclear without a lengthy description, it may indicate that the information would be better communicated in a different way or that the table needs row headers. Therefore, it's best to avoid lengthy descriptions of the data in a table and let the table's structure and headers do the explaining.", "e4f090b4-d45c-42e8-8e6c-48b2eeba90aa": "To copy a tasklist, including its content and context, from a {% data variables.product.product_name %} issue using the UI, follow these steps:  1. Go to the {% data variables.product.product_name %} issue containing the tasklist you want to copy. 2. Click the horizontal kebab icon (\u22ee) in the top-right corner of the tasklist. 3. Select \"Copy markdown\" from the drop-down menu. 4. The tasklist and its context will be copied to your clipboard, including the issue title, so you can paste it outside of GitHub without losing context.  Note: This feature allows you to easily duplicate a tasklist in other issues or share it with others who may not have access to the original issue.", "e22ce5c1-f034-4f82-a0e6-8bd51365904e": "To navigate to a specific codespace using the arrow keys in the terminal, follow these steps:  1. Open your terminal and ensure that you have GitHub CLI (gh) installed. 2. Run the command `gh codespace list` to view a list of your available codespaces. 3. Use the arrow keys to navigate to the codespace you want to open. 4. Press Enter to open the selected codespace.  Note: The exact steps may vary depending on the specific version of GitHub CLI you are using. For more information, refer to the GitHub CLI documentation.", "740f0150-a42a-466b-a3bf-a5a106610bb9": "According to the context provided, there are limits to the number of codespaces that can be created and run simultaneously, and if you exceed these limits, a message is displayed telling you that you must remove an existing codespace before you can create a new one. However, the specific limits are not specified in the context. It's recommended to refer to the documentation or contact support for more information on these limits.", "a378c9ed-b104-459f-9735-52fedf957357": "For {% ifversion ghes or ghae %}my enterprise{% elsif ghec %}{% data variables.product.product_name %}{% endif %}, the custom footer will be displayed to all users and on all pages {% ifversion ghes or ghae %}for my enterprise{% elsif ghec %}for repositories and organizations that belong to the enterprise, and for all enterprise members and collaborators{% endif %}.", "f470664b-bb87-41bb-8104-84b92614427b": "{% data variables.product.prodname_cli %} can be used to automate {% data variables.product.prodname_dotcom %} operations by creating scripts that interact with the {% data variables.product.prodname_dotcom %} API using the {% data variables.product.prodname_cli %} commands. For example, you could create a script that automatically creates a new repository, adds a remote origin, and pushes an existing local repository to the new remote repository. This script could be run from the command line using the {% data variables.product.prodname_cli %} commands, making it easy to automate repetitive tasks and streamline your workflow.", "ec70e160-082e-4400-bab8-09129c6306f5": "Checking for a license that includes GitHub Advanced Security is a prerequisite for using some additional features of dependency review, such as license checks and blocking of pull requests. Enabling the dependency graph for an instance is also a prerequisite for using dependency review, but it is required for all instances that want to use dependency review, regardless of whether they are using the additional features or not. In short, checking for a license is a requirement for specific features, while enabling the dependency graph is a requirement for using dependency review as a whole.", "2fad7441-ff1c-4e4e-9cae-474b82da82c7": "The relationship between the author and the subject of the comment in a discussion is represented by the \"authorAssociation\" field, which is of type \"CommentAuthorAssociation!\" and is required. This field indicates whether the author is the original poster (OP) of the discussion, a participant in the discussion, or a bystander. The possible values for this field are \"AUTHOR\", \"PARTICIPANT\", and \"BYSTANDER\", respectively.", "5d81c7f2-0a12-4534-be55-8e30f801e8ac": "To upload the tool cache for an Ubuntu 22.04 environment using the {% data variables.product.prodname_actions %} workflow feature on {% data variables.product.prodname_dotcom_the_website %}, follow these step-by-step instructions:  1. Go to your repository on {% data variables.product.prodname_dotcom_the_website %}. 2. Create a new workflow file in the repository's `.github/workflows` folder. 3. Paste the following YAML code into the new workflow file:  ```yaml name: Upload Node.js 14 and 16 tool cache on: push jobs:   upload_tool_cache:     runs-on: ubuntu-22.04     steps:       - name: Clear any existing tool cache         run: |           mv \"${{ runner.tool_cache }}\" \"${{ runner.tool_cache }}.old\"           mkdir -p \"${{ runner.tool_cache }}\"       - name: Setup Node 14         uses: {% data reusables.actions.action-setup-node %}         with:           node-version: 14.x       - name: Setup Node 16         uses: {% data reusables.actions.action-setup-node %}         with:           node-version: 16.x       - name: Archive tool cache         run: |           cd \"${{ runner.tool_cache }}\"           tar -czf tool_cache.tar.gz *       - name: Upload tool cache artifact         uses: {% data reusables.actions.action-upload-artifact %}         with:           path: \"${{runner.tool_cache}}/tool_cache.tar.gz\" ```  4. Save the workflow file and commit your changes. 5. Go to the Actions tab on your repository on {% data variables.product.prodname_dotcom_the_website %}. 6. Click on the workflow you just created to view its run.", "b8809405-b654-4983-82a1-e4bcb2081367": "In the context of Advanced Security, a \"Unique Committer\" refers to a licensed user of GitHub Enterprise, GitHub Enterprise Cloud, GitHub Enterprise Server, or GitHub AE who has made a commit in the last 90 days to any repository with any GitHub Advanced Security functionality activated. The licensing requirements for Advanced Security are impacted by this concept because customers are required to acquire a GitHub Advanced Security User license for each of their Unique Committers. This means that the number of licenses required for Advanced Security is based on the number of active committers using the feature, rather than the number of users or repositories in the organization. This licensing model ensures that customers only pay for the number of users who are actively using Advanced Security features, rather than being charged based on the size of their organization or the number of repositories they have.", "043c33f8-5291-427f-9bb2-3bc470936348": "Based on the context information provided, you can continue learning about {% data variables.product.prodname_actions %} by referring to the documentation titled \"AUTOTITLE.\" This documentation is not explicitly mentioned in the context, but it is implied that it contains further information about {% data variables.product.prodname_actions %}. Without prior knowledge, it's unclear what \"AUTOTITLE.\" refers to, but it's possible that it's the title of a specific resource, such as a user guide, tutorial, or reference manual, that is available on the {% data variables.product.prodname_dotcom %} or {% data variables.product.prodname_ghe_managed %} websites or documentation portals. To access this resource, you can search for \"AUTOTITLE.\" on the relevant website or consult the documentation index or table of contents. Alternatively, you can contact {% data variables.product.prodname_dotcom %} or {% data variables.product.prodname_ghe_managed %} support for assistance in locating the appropriate resource.", "32397954-d798-47c5-be3e-1d360dcf85ea": "A matrix strategy can be used to automate the testing process for a software application by running the same set of tests on different environments or configurations. This allows developers to ensure that their application works consistently across different platforms, browsers, or operating systems.  For example, a single-dimension matrix configuration for testing a web application on different browsers could look like this:  name: Browser Testing  on:   push:     branches:       - main  jobs:   browser-tests:     runs-on: ubuntu-latest     strategy:       matrix:         browser: [chrome, firefox, safari]     steps:       - name: Checkout         uses: actions/checkout@v2       - name: Set up Node.js         uses: actions/setup-node@v2         with:           node-version: '14'       - name: Install dependencies         run: npm install       - name: Run tests         run: npm test -- --browser=${{ matrix.browser }}  In this example, the matrix strategy is used to run the same set of tests on three different browsers: Chrome, Firefox, and Safari. The tests are executed in parallel on separate runners, and the results are reported separately for each browser.  Contexts can be used to create matrices by defining a set of variables that can be used to generate different matrix configurations. For example, a matrix configuration created using contexts for testing a web application on different operating systems and browsers could look like this:  name: Cross-platform Testing  on:   push:     branches:       - main  jobs:   cross-platform:     runs-on: ubuntu-latest     strategy:       matrix:         os: [ubuntu-latest, macos-latest, windows-latest]         browser: [chrome, firefox, safari]     steps:       - name: Checkout         uses: actions/checkout@v2       - name: Set up Node.js         uses: actions/setup-node@v2         with:           node-version: '14'       - name: Install dependencies", "c4db3404-8324-458e-ae74-8cb030cdb131": "The configurations files for dependency managers like `dep`, `glide`, and `go get` in the `autobuild` process for Go codebases are used to automatically install dependencies. If these files are not found, the `autobuild` process rearranges the repository directory structure suitable for addition to `GOPATH` and uses `go get` to install dependencies. The directory structure reverts to normal after extraction completes. This step is skipped if configurations files are found.", "1511a6cd-942d-402c-be31-2957beb5896c": "{% data variables.product.prodname_dependabot %} checks whether it's possible to upgrade the vulnerable dependency to a fixed version without disrupting the dependency graph for the repository by analyzing the dependency graph of the repository. It ensures that the new version of the vulnerable dependency does not introduce any conflicts with other dependencies in the graph. If a conflict is detected, {% data variables.product.prodname_dependabot %} does not raise a pull request to update the dependency. For more information, see \"AUTOTITLE.\"  {% endnote %}  {% endif %}  {% ifversion dependabot-security-updates-unlock-transitive-dependencies %}  {% note %}  **Note**: For npm, {% data variables.product.prodname_dependabot %} also considers transitive dependencies when determining whether it's possible to upgrade a vulnerable dependency without disrupting the dependency graph for a repository. This means that {% data variables.product.prodname_dependabot %} checks whether the new version of the vulnerable dependency and its transitive dependencies do not introduce any conflicts with other dependencies in the graph. If a conflict is detected, {% data variables.product.prodname_dependabot %} does not raise a pull request to update the dependency. For more information, see \"AUTOTITLE.\"  {% endnote %}  {% endif %}  {% ifversion dependabot-security-updates-unlock-transitive-dependencies %}  {% note %}  **Note**: For Maven, {% data variables.product.prodname_dependabot %} also considers transitive dependencies when determining whether it's possible to upgrade a vulnerable dependency without disrupting the dependency graph for a repository. This means that {% data variables.product.prodname_dependabot %} checks whether the new version of the vulnerable dependency and its transitive dependencies do not introduce any conflicts with other dependencies in the graph. If a conflict is detected, {% data variables.product.prodname_dependabot %} does not raise a pull request to update the dependency. For more information, see \"AUTOTITLE.\"  {% endnote %}  {% endif %", "774a76dc-5d9f-409d-8352-6ae9e9880d7e": "To download the log archive for a specific job in a workflow using the GitHub web interface, follow these steps:  1. Go to your repository in GitHub. 2. Click on the Actions tab. 3. Find the workflow that contains the job you want to download the log archive for. 4. Click on the workflow to view its runs. 5. Find the run that contains the job you want to download the log archive for. 6. Click on the run to view its details. 7. Click on the job you want to download the log archive for. 8. Click on the gear icon in the upper-right corner of the log. 9. Click on \"Download log archive\" to save the archive to your computer.  Note: If the workflow was partially re-run, the archive will only contain the logs for the re-run jobs. To download logs for all jobs, you may need to download the log archives for previous run attempts as well.", "4d50af42-ff5c-44fd-85cc-4ef1b07392dd": "The text material provides a table that shows the support for certain features of secret scanning and push protection for different versions of Github Enterprise Server (GHES). Here's a breakdown of the features and their support:  - {% data variables.product.prodname_secret_scanning_caps %} alert: This feature scans secrets in repositories for potential security issues. It's supported for private repositories with GitHub Apps (GHA) for GHES versions 3.7 and 3.8, and for all versions of GHES starting from 3.5.  - Push protection: This feature prevents the pushing of secrets to repositories. It's supported for all versions of GHES starting from 3.7.  - Validity check: This feature checks the validity of secrets before they're used in workflows. It's supported for all versions of GHES starting from 3.8.  Overall, the support for these features has improved over time, with more features being added to newer versions of GHES.", "44adefa7-ed2f-41b8-b003-819c45a1f983": "To select up to six public, private, or internal repositories to display as pinned repositories on your organization's page on {% data variables.product.product_name %}, follow these steps:  1. Go to your organization's settings on {% data variables.product.product_name %}. 2. Click on \"Edit pinned repositories\" in the left-hand menu. 3. In the \"Edit pinned repositories\" dialog box, select the combination of up to six public, private, or internal repositories you want to display. 4. Click \"Save pins\" to apply your changes.  Note: If you're using {% data variables.product.product_name %} for Free, you can only pin public repositories. If you're using {% data variables.product.product_name %} for Business or Enterprise, you can also pin private and internal repositories.", "91697da8-49dc-4eca-96ef-a6804771a696": "Without further context, it is unclear how the concept of \"AUTOTITLE\" differs from \"AUTOTITLE\" in the given context. Please provide additional information to help clarify the difference between these two concepts.", "0bd860b3-ddfb-4c71-9604-ce4fa22651eb": "To disconnect your Patreon account from your GitHub Sponsors profile and stop receiving new sponsorships through Patreon, you can follow the steps provided in the context information. This will not cancel existing sponsorships through Patreon, but your sponsors will have to cancel those sponsorships themselves. For more information, you can refer to the Patreon documentation on how to cancel sponsorships.", "504f9811-fe6a-424a-87ae-2104c2b8a8fd": "To delete your site on GitHub Pages, you can either delete the repository or change the source to the `None` branch.  Deleting the repository will permanently remove all the content and history of your site from GitHub. This method is irreversible, and you should make sure that you don't want to keep the site before proceeding.  Changing the source to the `None` branch will remove the current deployment of your site without deleting the repository or the site itself. This method allows you to keep the content and history of your site on GitHub, but it will not be accessible through the `github.io` domain.  To remove the current deployment of your site without deleting it, follow these steps:  1. Go to your repository on GitHub. 2. Click on the settings icon in the top right corner of the page. 3. Click on Pages in the left-hand menu. 4. Under \"Build and deployment,\" click on the branch that your site is currently deployed from. 5. Click on the \"Unpublish\" button. 6. Confirm that you want to unpublish your site by clicking on the \"Unpublish\" button again.  Your site will no longer be accessible through the `github.io` domain, but the content and history of your site will still be available in your repository.", "87c8c253-c418-40af-be01-ff61a26dd0c6": "The purpose of the `--db-cluster` option in the configuration file for the CodeQL CLI is to create a \"cluster\" of databases for different languages instead of creating a single database. This option is used when running queries from the configuration file in a later step, and it can appear multiple times or have a comma-separated list of languages if the `--db-cluster` option is given. This option is used to analyze specific languages when creating a new database, and it can be omitted if the source root being analyzed is a checkout of a GitHub repository, in which case the CodeQL CLI will attempt to automatically determine what languages to analyze using a GitHub PAT token.", "1132ed20-afcb-4bb9-bb4e-335a83ca2196": "To obtain artifacts to help you debug {% data variables.product.prodname_codeql %}, you can enable step debug logging in {% data variables.product.prodname_actions %} or create {% data variables.product.prodname_codeql %} debugging artifacts using a workflow flag. The debug artifacts will be uploaded to the workflow run as an artifact named `debug-artifacts` and contain the {% data variables.product.prodname_codeql %} logs, {% data variables.product.prodname_codeql %} database(s), and any SARIF file(s) produced by the workflow. These artifacts will help you debug problems with {% data variables.product.prodname_codeql %} {% data variables.product.prodname_code_scanning %}. If you contact GitHub support, they might ask for this data. If you want to create {% data variables.product.prodname_codeql %} debugging artifacts by re-running jobs, you need to select **Enable debug logging**. If you want to create {% data variables.product.prodname_codeql %} debugging artifacts using a flag in your workflow, you need to modify the `init` step of your {% data variables.code-scanning.codeql_workflow %} file and set `debug: true`.", "c5bda417-1db0-4794-9e83-c9cfb582c9be": "In the context provided, the \"Manage\" dropdown for {% data variables.product.prodname_GH_advanced_security %} licensing can be found under \"{% data variables.product.prodname_GH_advanced_security %}\" in the licensing screen. This information is only applicable if you are using {% data variables.product.prodname_GH_Enterprise_Cloud %} with {% data variables.product.prodname_GH_advanced_security %}. If you are using a different version of {% data variables.product.prodname_GH %}, the steps to access the \"Manage\" dropdown may be different.", "be29f708-defc-4cfb-9665-3ac6b1213021": "When triggered by a public fork's pull_request_target event, the `GITHUB_TOKEN` has read/write repository permission for actions, checks, contents, deployments, issues, metadata, repository-projects, and pull-requests. It also has read permission for packages and pages, and read/write permission for security-events and statuses. Note that when a workflow is triggered by the `pull_request_target` event, the `GITHUB_TOKEN` is granted read/write repository permission, even when it is triggered from a public fork. Private repositories can control whether pull requests from forks can run workflows, and can configure the permissions assigned to `GITHUB_TOKEN`. It's recommended to grant the `GITHUB_TOKEN` the least required access as a good security practice.", "15e6d575-e2ab-4bee-8e4e-8fdae8cb0efd": "To initialize an enterprise in {% data variables.product.product_name %}, you first need to purchase the product. After receiving an invitation email, you must have a SAML identity provider (IdP) and provide its Entity ID (SSO) URL, Issuer ID URL, and public signing certificate (Base64-encoded) during initialization. During the process, the enterprise owner will name the enterprise, configure SAML SSO, create policies for all organizations, and configure a support contact for users. The initial username and password for {% data variables.product.prodname_ghe_managed %} should be stored securely in a password manager. If the initial password expires before initialization is complete, a password reset can be requested from the invitation email.", "5807de12-28f4-4c3a-ae4d-8a435c8f1956": "To use {% data variables.product.prodname_copilot_chat %} in {% data variables.product.prodname_vscode %} for your {% data variables.product.prodname_dotcom %} account with access to {% data variables.product.prodname_copilot %}, you need to meet the following prerequisites:  1. Have an active subscription to {% data variables.product.prodname_copilot %}. 2. Have the latest version of {% data variables.product.prodname_vscode %} installed. 3. Be signed into {% data variables.product.prodname_vscode %} with the same {% data variables.product.prodname_dotcom %} ID that has access to {% data variables.product.prodname_copilot %}.  If you haven't met these prerequisites, you won't be able to use {% data variables.product.prodname_copilot_chat %} in {% data variables.product.prodname_vscode %}.", "24ea0454-3b52-4369-8890-648ca440f827": "The context provided highlights two complexities of copyright law that should be considered when dealing with copyright infringement allegations. Firstly, distributing software can sometimes be copyright infringement if it induces end-users to use it to infringe copyrighted works. Secondly, a copyright complaint may also be based on non-literal copying of design elements in the software, rather than the source code itself. These complexities make it especially important to seek professional advice in such cases because there are many nuances to the law and some unsettled questions in these types of cases. It is crucial to understand the legal implications and potential consequences of the allegations, as a counter notice is a legal statement that could lead to a court order seeking to restrain the user from engaging in infringing activity relating to the content on GitHub, potentially resulting in legal action and liability. Therefore, it is advisable to consult a legal professional to evaluate the merit of the claims and provide guidance on the best course of action.", "53f61524-5224-4926-ab1c-7bba64be4f02": "To organize and prioritize issues in a repository, you can use projects, task lists (if you're using {% ifversion fpt or ghec %}GitHub Enterprise Cloud or GitHub Enterprise Server {% endif %}), labels, and milestones. Projects allow you to group issues together based on a specific project or initiative. Task lists help you break down larger issues into smaller, more manageable tasks. Labels and milestones can be used to categorize related issues based on their status, priority, or other relevant criteria. By using these features, you can better manage and prioritize your work in the repository.", "ec591ec2-e465-4646-ad46-be78dd2d0119": "To remove sensitive data from your Git repository using the BFG tool, follow these steps:  1. Install the BFG tool by downloading the appropriate binary for your operating system from the BFG Repo-Cleaner website (https://rtyley.github.io/bfg-repo-cleaner/) and adding the binary to your system's PATH environment variable.  2. Clone your repository to a new directory:     ```shell    $ git clone --mirror https://{% data variables.command_line.codeblock %}/YOUR-USERNAME/YOUR-REPOSITORY.git NEW-REPO    ```  3. Navigate to the new repository directory:     ```shell    $ cd NEW-REPO    ```  4. Run the BFG tool to remove the sensitive data:     ```shell    $ java -jar bfg.jar --strip-blobs-regfile sensitive-data.txt --strip-blobs-regex 'sensitive-data' --replace-renamed --output NEW-REPO-CLEAN    ```     In this command, replace `sensitive-data.txt` with the path to your list of sensitive filenames or regular expressions, and replace `sensitive-data` with the regular expression to match sensitive data. The `--replace-renamed` option ensures that renamed files are also removed.  5. After the BFG tool has finished running, navigate to the new repository directory:     ```shell    $ cd NEW-REPO-CLEAN    ```  6. Remove the original repository directory:     ```shell    $ rm -rf NEW-REPO    ```  7. Rename the new repository directory:     ```shell    $ mv NEW-REPO-CLEAN NEW-REPO    ```  8. Add, commit, and push the changes to your GitHub repository:     ```shell    $ git add .    $ git commit -m \"Removed sensitive data using BFG tool\"    $ git push origin --force --tags    ```  To remove sensitive data from your Git repository using `git filter-repo`, follow these steps:", "c8906f66-6cd7-48ad-b535-30b0583d4fd7": "On {% data variables.product.prodname_mobile %}, you can manage, triage, and clear notifications, read, review, and collaborate on issues and pull requests, and search for, browse, and interact with users, repositories, and organizations. These features are available on {% data variables.product.prodname_mobile %}.", "28adcad8-2089-4384-9737-ecbc5d73f9aa": "In GitHub, both teams and organizations can have a profile picture that represents them. By default, a team's profile picture will match the organization's profile picture, but a team maintainer or organization owner can set a separate profile picture for the team by accessing the team's settings, clicking \"Profile picture,\" and uploading a new image. This allows teams to have a unique visual identity within the organization.", "b0775caa-ca70-4eea-82bb-4ee086514841": "Check out your branch locally and use the following command to launch the local server:  ```shell npm start ```  When the server is running, navigate to the problematic article on `https://localhost:4000` in your browser. The staging server will just show an \"Oops\" error, but the local server should show a stack trace for debugging.", "f6c5d17f-3694-4b7f-8060-96a13a78c38a": "According to the context information provided, up to 10 custom organization roles can be created in a GitHub organization.", "d273811e-a7a0-48cd-b5d5-64b0f19d1be9": "The CodeQL CLI can automatically determine the languages to analyze when analyzing a GitHub repository by omitting the `-l` or `--language` option and supplying a GitHub PAT token either in the environment variable `GITHUB_TOKEN` or via standard input using the `--github-auth-stdin` option. This feature is available only when analyzing a GitHub repository.", "a99aba5d-49d0-4e0d-9311-38ae08681d9c": "In the given context, panels should be described in relation to UI elements by describing what someone needs to do instead of referring to panels directly. This approach is recommended to avoid confusion and ensure clarity in the documentation. Therefore, instead of saying \"click the View charts and graphs panel to open it,\" you should say \"click View charts and graphs for your repository, then select the time period you want to view from the dropdown menu.\"", "296cb482-c129-4211-8470-9c75eec78bbc": "The REST API provided by {% data variables.product.company_short %} can be utilized to manage repositories by sending HTTP requests to specific endpoints. These requests can be used to perform various operations such as creating, deleting, updating, and querying repositories. By leveraging the REST API, developers can automate repetitive tasks, integrate their tools and workflows with {% data variables.product.company_short %}, and access repository data programmatically. The context information provided in this case indicates that the REST API can be used to manage repositories on {% data variables.product.company_short %}, which includes GitHub Enterprise Cloud (GHEC), GitHub Enterprise Server (GHES), GitHub Advanced Security (GHAE), and GitHub.com (fpt).", "50b52179-00f3-4533-a7f8-75401d1698aa": "The first step required to authenticate a nearby device using a tablet is to continue to follow the prompts to start the authentication process. This may involve choosing to scan a QR code or trigger a push notification to the nearby device.", "5e12bf39-0687-41bc-9a5e-25ee95c5025c": "To ensure that workflows in GitHub Actions do not create recursive or unintended runs, and also consider usage costs, we can follow these best practices:  1. Use environment variables to store authentication tokens, such as personal access tokens (PATs) or OAuth tokens, instead of hardcoding them in the workflow file. This allows us to easily rotate or revoke tokens without having to update the workflow.  2. Use the `GITHUB_TOKEN` variable with caution, as it has full access to the repository and can trigger recursive workflow runs. Instead, use a PAT or OAuth token stored as a secret to authenticate with the API.  3. Avoid using workflow triggers that can trigger recursive workflow runs, such as pushing to a branch that already has a workflow configured to run on push events. Instead, use event filters to target specific branches or events.  4. Use event activity types and filters to further control when the workflow runs. For example, we can use the `pull_request` event with the `types: [opened, synchronize, reopened]` filter to only run the workflow when a pull request is opened, synchronized, or reopened.  5. Use the `uses` step to reuse existing workflows instead of duplicating workflow logic. This reduces the number of workflows running and helps to avoid recursive workflow runs.  Here's an example of how to use multiple events and filters to control when a workflow runs:  ```yaml name: My Workflow  on:   push:     branches:       - main       - my-branch   pull_request:     types:       - opened       - synchronize       - reopened     branches:       - my-branch  jobs:   my-job:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Run tests         uses: my-org/my-action@v1         with:           TEST_ENV: \"staging\"        - name: Deploy to staging         uses", "533ca009-ab76-4d58-b5fd-d0d921290f6c": "The section in the repository settings on GitHub Pages where you can configure publishing source is \"Source\" (located under the \"Pages\" tab).", "9b8791de-499a-4e53-a198-8c288c0e304a": "To give your token an expiration in [PRODUCT NAME], select \"Expiration\" and choose a default option or click \"Custom\" to enter a date while generating a new token or editing an existing one in your developer settings. This feature is available in [PRODUCT NAME] with [VERSION NUMBER] or higher. If you're not sure which version you're using, you can check in your account settings. If you're not currently using [PRODUCT NAME], you may not have this option available.", "789a327e-d75f-4352-98eb-7669dd5213db": "To edit a webhook created in a specific repository using the REST API in {% data variables.product.prodname_dotcom %}, follow these steps:  1. Make a GET request to the `/repos/{owner}/{repo}/settings/webhooks` endpoint to retrieve the list of webhooks for the repository. 2. Find the webhook you want to edit in the list and make a note of its `id`. 3. Make a PUT request to the `/repos/{owner}/{repo}/settings/webhooks/{id}` endpoint, passing the updated webhook configuration in the request body. 4. The updated webhook will now be reflected in the list of webhooks for the repository.  Here's an example request body for updating a webhook:  ```json {   \"name\": \"My updated webhook\",   \"active\": false,   \"events\": [\"push\"],   \"url\": \"https://example.com/webhooks\",   \"content_type\": \"json\",   \"secret\": \"my-secret\",   \"insecure_skip_verify\": false,   \"dispatch\": false,   \"active_status\": \"success\" } ```  In this example, we're updating the name, active status, and URL of the webhook.  {% endif %}    {% ifversion ghec %}    Editing a global webhook for a {% data variables.product.prodname_ghe_cloud %}  Enterprise owners and organization administrators can edit a global webhook to change any of the settings that were selected when the webhook was initially created.  You can use the {% data variables.product.company_short %} web interface or the REST API to edit a global webhook. For more information about using the REST API to edit a global webhook, see \"AUTOTITLE.\"  {% data reusables.repositories.navigate-to-admin-settings %} {% data reusables.repositories.sidebar-settings %} {% data reusables.webhooks.sidebar_webhooks %} {% data reusables.webhooks.", "d0c1937e-5506-4a3a-9d3d-36f235ca66b0": "No prompt appears in the lower-right corner of {% data variables.product.prodname_vscode_shortname %} when you add features while working locally, without being connected to a codespace. The prompt only appears if you're working in a codespace and make changes to the dev container configuration while in {% data variables.product.prodname_vscode_shortname %}. In that case, a prompt will appear in the lower-right corner asking if you want to rebuild the container and apply the changes to the codespace you're working in.", "35bd7eda-2100-499e-b800-e9d8497a75c6": "The ports required for {% data variables.product.prodname_actions %} to connect to {% data variables.location.product_location %} are 443 for HTTPS and 80 for HTTP (required if TLS is not configured). On the other hand, for {% data variables.product.prodname_github_connect %}, the connection between {% data variables.product.product_name %} and {% data variables.product.prodname_dotcom_the_website %} uses HTTPS over ports 443 or 80, and TLS is required. Therefore, while both services require HTTPS, the specific ports required for {% data variables.product.prodname_actions %} and {% data variables.product.prodname_github_connect %} are different.", "c6b4810e-a4ed-49ea-a843-90ef0fc5f1d1": "The process for switching an organization's SAML configuration to an enterprise account involves enforcing SAML SSO for the enterprise account, optionally removing existing SAML configurations for organizations owned by the enterprise account, hiding organization-level apps in the IdP if kept, and advising enterprise members about the change. Members will need to authorize any PATs or SSH keys that were not previously authorized for use with SAML SSO for the organization. SCIM provisioning is not currently supported when SAML SSO is configured for an enterprise account, and organizations should consider removing organization-level SAML configurations to prevent unexpected issues in the future. The decision to implement SAML SSO at the organization or enterprise level should also be carefully considered.", "a41bfc29-db6d-4be9-aa6e-787755d03338": "To add or update a setup URL for a {% data variables.product.prodname_github_apps %}, follow these steps:  1. Go to your {% data variables.product.prodname_github_apps %} settings by clicking on your app's name in the list of apps and then clicking on the Settings tab. 2. Under \"Post installation,\" enter the URL where you'd like to redirect users after they install your app in the \"Setup URL\" field. 3. Optionally, if you want to redirect users to the setup URL after they update an installation, select \"Redirect on update.\" An update includes adding or removing a repository for an installation. If \"Setup URL\" is blank, this will be ignored.  If you select \"Request user authorization (OAuth) during installation,\" users will instead be redirected to the Callback URL as part of the authorization flow, where you can describe additional setup. This means that you won't be able to enter a setup URL in this case.", "731d26c6-4733-4dda-8536-0f0919a3d784": "The event \"automatic_base_change_failed\" indicates that GitHub attempted to automatically change the base branch of a pull request but was unsuccessful. This event is available for pull requests.", "52c01293-1b7b-43aa-8fec-c373ccc82077": "Cloning a repository creates a local copy of an existing remote repository on your computer, allowing you to work on it offline and push changes back to the original repository. Forking a repository creates a new repository based on the original repository, allowing you to make changes independently without affecting the original repository. This is useful for contributing changes to an existing project or working on a separate project based on the original repository. When you clone a repository, you have write access to the original repository, while when you fork a repository, you have write access to your own fork.", "0465dffe-8ec8-4b83-819b-8c6adc723370": "If you prefer to use a different service mechanism other than the default launchd service configuration provided in the context information, you can create a customized service or use any other service mechanism you prefer. The context suggests using the `plist` template at `actions-runner/bin/actions.runner.plist.template` as a reference. However, if you choose to use a customized service, it's essential to ensure that the self-hosted runner service is always invoked using the `runsvc.sh` entry point, as mentioned in the context information.  2. The context provides a reference to the `serviced` template at `actions-runner/bin/actions.runner.service.template` for customizing the self-hosted runner service. However, if you prefer to use a different operating system, such as macOS, the context suggests using the `plist` template at `actions-runner/bin/actions.runner.plist.template` as a reference instead. Regardless of the operating system you choose, it's crucial to ensure that the self-hosted runner service is always invoked using the `runsvc.sh` entry point, as mentioned in the context information.  Answer:  If you're using macOS and want to customize the self-hosted runner service, the context suggests using the `plist` template at `actions-runner/bin/actions.runner.plist.template` as a reference. However, it's essential to remember that the self-hosted runner service must always be invoked using the `runsvc.sh` entry point, as mentioned in the context information, regardless of the operating system you choose.", "28c37ab3-4095-4cda-b798-f82f28fe50f4": "Someone can suggest improvements on a global security advisory in the {% data variables.product.prodname_advisory_database %} by navigating to the advisory, clicking the \"Suggest improvements for this vulnerability\" link on the right-hand side of the page, and making desired improvements in the \"Improve security advisory\" form. If the improvement is accepted and published by the {% data variables.product.prodname_security %} curation team, the person who submitted the improvement will automatically be assigned a credit type of \"Analyst.\" This means that their contribution will be recognized and acknowledged in the {% data variables.product.prodname_advisory_database %}.", "4fb68818-9a1f-4c4c-a8e7-6f96cb60a83c": "To track user billing requests using the {% data variables.product.prodname_marketplace %} API and webhook events, you can follow these steps:  1. First, you need to create a {% data variables.product.prodname_marketplace %} listing and add a pricing plan. For detailed instructions, refer to the \"AUTOTITLE\" links provided in the context information.  2. Once your listing is published, you can use the {% data variables.product.prodname_marketplace %} API to retrieve billing information for your users. The API provides endpoints for managing subscriptions, invoices, and payments. For more information, see the \"AUTOTITLE\" link provided in the context information.  3. To receive real-time notifications about billing events, you can set up webhook events. Webhook events allow you to receive notifications about subscription cancellations, failed payments, and other billing-related events. To set up webhook events, refer to the \"AUTOTITLE\" link provided in the context information.  4. By using the {% data variables.product.prodname_marketplace %} API and webhook events, you can track user billing requests and manage subscriptions and payments more efficiently.  If you have any questions about using the {% data variables.product.prodname_marketplace %} API or webhook events, you can contact {% data variables.contact.contact_support %} for assistance.", "63415af4-13ee-4126-8754-43e51236498b": "To add a policy to set a maximum codespace retention period, you can follow these steps:  1. Log in to your GitHub account and navigate to your repository. 2. Click on the \"Codespaces\" tab in the left-hand menu. 3. Click on the \"Policies\" tab at the top of the page. 4. Click on the \"Add policy\" button. 5. Select \"Maximum retention period\" from the policy type dropdown menu. 6. Enter the number of days you want to set as the maximum retention period for codespaces. 7. Click on the \"Save\" button to add the policy.  You can find the \"Codespaces policies\" page by following the steps mentioned above. It's located under the \"Codespaces\" tab in the left-hand menu of your repository.", "184ea72d-167f-40a5-81e2-60a1b04799e6": "{% data variables.product.prodname_copilot_for_business %} subscriptions are billed monthly, based on the number of {% data variables.product.prodname_copilot %} seats assigned to users within your enterprise. The seats are assigned to individuals and teams by the admins of the organizations that have been granted access to {% data variables.product.prodname_copilot %} by the enterprise owner.", "3b35b559-0a7a-44a3-81af-a36dd6bfa718": "The board view in Projects can be utilized to schedule and quickly see the status of work items by arranging issues into columns for different iterations. This allows for easy visualization of what's currently planned, what's coming up, and the ability to quickly drag items to the appropriate iteration. To group issues by iterations in this view, the user can create a board view and then group the issues by iterations. This can be done by selecting the \"Group by\" option in the view settings and choosing the iteration custom field. This will group the issues into columns based on their iteration, making it easy to see the status of each iteration and quickly move issues between iterations as needed.", "05bff397-aa70-4bfe-a777-79e7b10c67ef": "For ghes = 3.7, only the list of keys is updated in step 5 using the command `ghe-config secrets.github.encrypted-column-keying-material \"KEY-LIST\"`.  For ghes = 3.8 or ghes = 3.9, in step 5, the list of keys and the current encryption key are updated separately using the following commands:  - `ghe-config secrets.github.encrypted-column-keying-material \"DECRYPTION-KEY-LIST\"` to update the list of keys. - `ghe-config secrets.github.encrypted-column-current-encryption-key \"ENCRYPTION-KEY\"` to update the current encryption key.  Therefore, the difference in the commands run in step 5 for ghes = 3.7 and ghes = 3.8 or ghes = 3.9 is that for ghes = 3.7, only the list of keys is updated, while for ghes = 3.8 or ghes = 3.9, both the list of keys and the current encryption key are updated separately.", "69d1bb6c-9e59-4300-8c32-26651ecabc8b": "To access the \"Billing\" tab in your enterprise account settings on GitHub, follow these steps: 1. Log in to your GitHub account. 2. Click on your profile icon in the top right corner of the page. 3. Select \"Settings\" from the drop-down menu. 4. Click on \"Organizations\" in the left-hand menu. 5. Select the organization for which you want to view the billing information. 6. Click on the \"Settings\" tab for the selected organization. 7. Click on the \"Billing\" tab in the left-hand menu.  Alternatively, you can also access the \"Billing\" tab by going to https://github.com/[your-organization]/settings/billing directly in your web browser. Replace [your-organization] with the name of your organization on GitHub.", "32e294f5-cb8b-4e7a-ac0b-75e4a9b25b01": "To disable SAML single sign-on (SSO) for an organization, an organization owner can follow these steps:  1. Access the organization settings by clicking on their profile, selecting the organization, and then clicking on the settings icon in the top right corner. 2. Click on the Security tab. 3. Under \"SAML single sign-on\", deselect the \"Enable SAML authentication\" option. 4. Click Save.  As a result of disabling SAML SSO, all external identities for the organization will be removed. This means that users will no longer be able to sign in using their external identity provider (IdP) and will need to sign in using their GitHub username and password instead. If the organization's IdP is unavailable, users can still sign in using a recovery code.", "11a95f6b-cee3-46d2-a2c2-9e03c879fc51": "The command used to open a codespace in the browser is `gh codespace code --web`. If the context didn't provide this information, then it's not possible to give an answer.", "5167c36e-39c4-48a0-bf39-c6a7cbd5d15a": "Based on the concept map provided, it is not immediately clear whether there is a hierarchy between the concepts labeled A, B, C, and D. The lines connecting the concepts could represent various types of relationships, such as causality, influence, or association. Without further context or labels on the lines, it is difficult to determine the hierarchy.  2. The quadrant diagram provided does not include the blue square labeled \"Diagrams in the {% data variables.product.prodname_docs %}\" to determine which quadrant it falls into. More information is needed to answer this question.", "74a6e7ee-08a3-4fcb-a0c8-4a315ec3a3dd": "Environment variables are used to provide authentication credentials for accessing the Maven Central Repository through the OSSRH hosting project. In a GitHub Actions workflow, these variables should be set up as secrets in the repository's settings, and then referenced in the workflow using the `${{ secrets.VARIABLE_NAME }}` syntax. In the provided workflow example, the `MAVEN_USERNAME` and `MAVEN_PASSWORD` variables are set up as secrets and passed to the `publish` step as environment variables. This allows the Maven `deploy` command to authenticate and publish the package to the Maven Central Repository.", "4c540c44-4ea5-4a78-81fc-3327ea1ee424": "Content can be hidden from the rendered Markdown using HTML comments by placing the content between the comment tags `<!--` and `-->`. This is explained in the context provided.", "e9a79589-90b9-4125-9524-09a1a438b92b": "If you want to edit a security advisory in the GitHub Advisory Database, and you have {% data variables.product.prodname_github_connect %} enabled for your organization, you can follow these steps:  1. Navigate to `https://HOSTNAME/advisories` to view the list of advisories. 2. Select the security advisory you would like to contribute to. 3. On the right-hand side of the page, click the \"Suggest improvements for this vulnerability on {% data variables.product.prodname_dotcom_the_website %}\" link. 4. Edit the advisory, following the contribution guidelines. 5. Submit a pull request to the github/advisory-database repository.  Alternatively, if you don't have {% data variables.product.prodname_github_connect %} enabled, you can still open a pull request directly on the advisory file in the github/advisory-database repository. For more information, see the contribution guidelines.  If the advisory originated from a {% data variables.product.prodname_dotcom %} repository, the curation team will also be tagged for optional commentary. You can view the pull request and get notifications when it is updated or closed.", "995437d4-cfc6-43c7-85f9-6920f060f94b": "The purpose of adding a file to the local repository and staging it for commit, as explained in the context information, is to prepare the changes made to the file for tracking and eventual pushing to a remote repository. This process ensures that the changes are not lost and can be easily managed and reviewed before being shared with others. It also allows for clear and organized version control of the project.", "bae2f9a1-7a1c-4774-ad65-4aef497067fd": "To fetch source code from non-{% data variables.product.prodname_dotcom %} repositories using a credentials file with a `provider` value of `gitlab` and an access token `super_secret_token`, you can add the following configuration to your credentials file:  ```yaml - url: https://gitlab.com   access_token: super_secret_token   provider: gitlab ```  This will tell {% data variables.product.prodname_actions_importer %} to use the `super_secret_token` to retrieve source code from repositories hosted on `https://gitlab.com` using the `gitlab` provider.", "6aea4195-9f9e-4aa5-b8e4-8e27f51a9462": "{% data variables.product.prodname_ghe_cloud %} is a cloud-based version of GitHub Enterprise, which is a self-hosted GitHub experience for organizations. It offers additional features such as SAML single sign-on (SSO), internal repositories, audit log streaming, and included {% data variables.product.prodname_actions %} minutes. These features can benefit an organization by providing enhanced security, better collaboration and management of code, and access to powerful automation tools. By setting up a trial of {% data variables.product.prodname_ghe_cloud %}, an organization can evaluate these features and determine if they are a good fit for their needs.", "893f7408-b625-4682-be56-7784382653fa": "In the event of a breach by a subprocessor of their obligations under Clause 3 or Clause 11, the data subject is entitled to receive compensation from the data exporter for the damages suffered (Clause 6.1). However, if the data exporter has factually disappeared or ceased to exist in law or has become insolvent, the data subject may issue a claim against the data importer as if it were the data exporter, unless a successor entity has assumed the entire legal obligations of the data exporter by contract or by operation of law (Clause 6.2). The data importer may not rely on a breach by a subprocessor to avoid its own liabilities (Clause 6.3). If both the data exporter and the data importer have factually disappeared or ceased to exist in law or have become insolvent, the subprocessor agrees that the data subject may issue a claim against the subprocessor with regard to its own processing operations under the Clauses as if it were the data exporter or the data importer (Clause 6.4).", "b682c67f-de6f-47dd-8a2d-901ae33afa72": "The purpose of an interface in object-oriented programming is to define a common behavior or structure that multiple classes or objects can implement. It serves as a contract between the interface and its implementing objects, specifying the methods and properties that must be implemented by the objects. This allows for greater flexibility and reusability in code, as objects can be easily swapped in and out based on their implementation of the interface.  For example, let's say we have a `Lockable` interface that defines a `lock()` and `unlock()` method. The `Issue` and `PullRequest` classes can then implement this interface, allowing them to be treated as interchangeable objects with the same behavior.  ```javascript interface Lockable {   lock(): void;   unlock(): void; }  class Issue implements Lockable {   lock(): void {     // Implementation   }    unlock(): void {     // Implementation   } }  class PullRequest implements Lockable {   lock(): void {     // Implementation   }    unlock(): void {     // Implementation   } } ```  2. GitHub Enterprise (GHE) utilizes GraphQL as a query language for accessing data from its REST APIs. GraphQL allows for more efficient and flexible data retrieval, as clients can specify exactly what data they need in a single request, rather than making multiple requests for different data points.  For example, instead of making separate requests for an issue's title, comments, and assignee, a client can make a single request for all of this information using GraphQL. This reduces the number of requests needed and the amount of data transferred, resulting in faster and more efficient data retrieval.  GHE also offers benefits such as better performance, as GraphQL can optimize queries based on the client's needs, and better scalability, as GraphQL can handle complex queries and mutations without overloading the server.  Here's an example of how GraphQL is used in GHE to retrieve an issue's details:  ```graphql query {   issue(number: 123) {     title     body     comments(first: 10) {       nodes {", "6a5eb01a-6b89-4c91-b537-fcb89bbc8d3b": "A journey map can be used to track changes in a specific measurement over time by plotting the measurement on the vertical axis and time on the horizontal axis. The vertical axis should represent the measurement being tracked, such as temperature, stock prices, or customer satisfaction scores. The horizontal axis should represent time, with a clear label for the time units being used. Blue dots or other markers can be used to indicate specific measurements at different points in time, and a line can be drawn to connect the dots and illustrate the trend over time. This type of diagram is useful for identifying patterns, trends, and outliers in the data, and for making predictions or forecasts based on the trend line.", "ed837a1d-67fb-4b23-84a8-8bdad10b2a56": "In {% data variables.product.prodname_actions %}, billing for reusable workflows is always associated with the caller workflow. The assignment of {% data variables.product.prodname_dotcom %}-hosted runners is always evaluated using only the caller's context, and the caller cannot use {% data variables.product.prodname_dotcom %}-hosted runners from the called repository. This policy applies to users on FPT, GHES, and GHEC plans.    How can I configure the artifact and log retention period for my repository, organization, or enterprise account in {% data variables.product.prodname_actions %}?  Answer:  To configure the artifact and log retention period for your repository, organization, or enterprise account in {% data variables.product.prodname_actions %}, you can follow the steps outlined in the following resources:  - \"AUTOTITLE\" - \"AUTOTITLE\" - \"AUTOTITLE\"    What is the usage policy for {% data variables.product.prodname_actions %} in {% data variables.product.prodname_dotcom %}, and how does it relate to the GitHub Terms of Service?  Answer:  The usage policy for {% data variables.product.prodname_actions %} in {% data variables.product.prodname_dotcom %} requires users to ensure that they use {% data variables.product.prodname_actions %} within the GitHub Terms of Service. For more information on {% data variables.product.prodname_actions %}-specific terms, see the GitHub Additional Product Terms. This policy applies to users on FPT, GHES, and GHEC plans.    How can I disable or limit {% data variables.product.prodname_actions %} for my repository or organization in {% data variables.product.prodname_dotcom %}?  Answer:  To disable or limit {% data variables.product.prodname_actions %} for your repository or organization in {% data variables.product.prodname_dotcom %}, you can follow the steps outlined in the \"AUTOTITLE\" resource. This resource provides", "c901e7c8-543b-496e-bb9e-2f8b2d6e3d3b": "You should get a wildcard Transport Layer Security (TLS) certificate for `*.HOSTNAME` with a Subject Alternative Name (SAN) for both `HOSTNAME` and the wildcard domain `*.HOSTNAME`. For example, if your hostname is `github.octoinc.com`, get a certificate with the Common Name value set to `*.github.octoinc.com` and a SAN value set to both `github.octoinc.com` and `*.github.octoinc.com`.", "ef3dd055-33d6-4f9a-8224-632b2301a6c6": "In rare cases, errors might not be propagated to the corresponding error handlers and are not logged in the regular locations. If you encounter errors and there is nothing in the logs, you may try to see the logs from the process running {% data variables.product.prodname_vscode %} and the extension.  1. Open the {% data variables.product.prodname_vscode_command_palette_shortname %}    - For Mac:       - Use: Shift+Command+P    - For Windows or Linux:       - Use Ctrl+Shift+P 1. Type \"Toggle\", and then select **Developer: Toggle Developer Tools** from the list. 1. In the Developer Tools window, select the **Console** tab to see any errors or warnings.  Further reading:  - \"AUTOTITLE\" - \"Network Connections in {% data variables.product.prodname_vscode %}\" in the {% data variables.product.prodname_vscode %} documentation  Note: If your error is related to certificates, it helps to check these logs for the `Custom Certificates:` line. If this line says `disabled`, you are not using {% data variables.product.prodname_copilot_for_business %}, so custom certificates are not supported. For more information, see \"AUTOTITLE.\"", "e898d8d8-5150-48cd-9f73-37f154ac753e": "Deployment protection rules in {% data variables.product.prodname_actions %} workflows can be used to restrict access to environments by requiring specific conditions to pass before a job referencing the environment can proceed. This can include requiring manual approval, delaying the job, or restricting the environment to certain branches. By implementing these rules, you can ensure that only authorized changes are deployed to your environments, providing an additional layer of security and control over your workflow.", "3727648e-7a75-42a2-b4a3-1d93a9147598": "To make the app code work with a {% data variables.product.prodname_ghe_server %} instance instead of {% data variables.product.prodname_free_user %}, {% data variables.product.prodname_pro %}, {% data variables.product.prodname_team %}, and {% data variables.product.prodname_ghe_cloud %}, you should update the URLs used by the app code to match the ones provided for {% data variables.product.prodname_ghe_server %}. This involves replacing `https://api.github.com` with `https://HOSTNAME/api/v3` for API requests, `https://github.com/login/oauth/authorize` with `https://HOSTNAME/login/oauth/authorize` for OAuth authorization, and `https://github.com/login/oauth/access_token` with `https://HOSTNAME/login/oauth/access_token` for OAuth token requests. The URL for GraphQL requests, `https://api.github.com/graphql`, should also be updated to `https://HOSTNAME/api/v3/graphql`. Replace `HOSTNAME` with the hostname of the {% data variables.product.prodname_ghe_server %} instance.", "432dde9d-1207-4664-8aa8-1e922f2d1fbf": "Without further context, it is unclear what \"AUTOTITLE\" is or what its purpose might be in a document. It could potentially be a formatting command or a placeholder for a title that will be automatically generated by the software or program being used. However, without more information, it is impossible to determine the exact purpose of \"AUTOTITLE\" in this context.", "aa5883a8-9270-41d8-8d96-e7679799f22b": "To fix the error \"body[i]: `x` is not a permitted key\" in a YAML file with multiple body blocks, you need to identify the specific body block with the error (i.e., the value of `i`) and then remove the key-value pair that is causing the error. This can be done by finding the block with the error and removing the key-value pair that is causing the error, or by ensuring that only permitted attributes are used in the `attributes` block.  For example, if the error is \"body[2]: `x` is not a permitted key\", you would need to find the third body block in the YAML file and remove the key-value pair that is causing the error. Alternatively, you could ensure that only permitted attributes are used in the `attributes` block of the third body block.  In general, to avoid this error, it's best to only use permitted attributes in the `attributes` block and to ensure that all keys used in the YAML file are permitted. This will help to ensure that your YAML file is valid and can be processed without errors.", "c08be43d-8ed4-42cb-9e48-8b0c437932c3": "Yes, you can allow members to create {% data variables.projects.projects_v1_boards %} in repositories in your organization's GitHub account. To enable this feature, follow these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. In the left-hand menu, click on \"Code planning, and automation\" and then click on \"Projects\". 3. Under \"Projects (classic) only\" (or \"Repository projects\" if you're using GitHub Enterprise Server version 3.7 or earlier), select \"Allow members to enable Projects (classic) for all repositories in the organization\". 4. Click \"Save\" to apply the changes.  To prevent members from creating project boards in repositories, you can follow these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. In the left-hand menu, click on \"Code planning, and automation\" and then click on \"Projects\". 3. Under \"Projects (classic) only\" (or \"Repository projects\" if you're using GitHub Enterprise Server version 3.7 or earlier), unselect \"Allow members to enable Projects (classic) for all repositories in the organization\". 4. Click \"Save\" to apply the changes.  Note that disabling project boards in repositories will also disable the ability to create new repositories with project boards enabled. If you want to allow members to create new repositories with project boards enabled, you can do so by creating a new repository with project boards enabled and then sharing it with your organization's members.", "b1db8e4c-4cd0-4c8a-9c27-b6950da63154": "The context information provided suggests that $(System.JobId) and $(System.JobName) are related to the current job being executed in the Azure DevOps pipeline. $(System.JobId) appears to represent a unique identifier for the job, while $(System.JobName) appears to represent the name of the job. It's possible that these variables are used to track and manage multiple jobs running concurrently in the pipeline. However, without further context or documentation, it's difficult to provide a definitive answer.", "ae2629a7-9af4-4feb-a843-ee9b9a7d2008": "To search for specific repositories or issues on GitHub using keywords and filters, you can use the search function on the GitHub website or the GitHub CLI. Here's an example query:  - `repo:github/learn-github language:python`: This query searches for repositories on GitHub with the name \"learn-github\" and the language \"python\". - `is:issue state:open label:bug`: This query searches for open issues with the label \"bug\".  To search for code that contains a specific phrase but not another phrase, you can use quotation marks and the NOT operator. Here's an example query:  - `\"hello world\"` NOT `readme`: This query searches for code that contains the phrase \"hello world\" but not the word \"readme\".  To search for specific repositories or issues related to a username, you can use the qualifier `user`, `actor`, or `assignee` and specify a username or `@me` to search for the current user. Here's an example query:  - `author:nat`: This query searches for commits authored by the user `nat`. - `is:issue assignee:@me`: This query searches for issues assigned to the user viewing the results.  Note that some non-alphanumeric symbols, such as spaces, are dropped from code search queries within quotation marks, so results can be unexpected in some cases.", "ab67b6b9-ccf9-49e6-a60e-07a731e4720a": "To search for code based on its language using GitHub's advanced search feature, you can use the `language` qualifier. For example, `language:python` will search for code written in Python.  2. To search for code based on its file size using GitHub's advanced search feature, you can use the `size` qualifier. For example, `size:>10000` will search for code in files larger than 10,000 bytes.  3. To search for code based on its filename using GitHub's advanced search feature, you can use the `filename` qualifier. For example, `filename:README.md` will search for files named README.md.  4. To search for code based on its file extension using GitHub's advanced search feature, you can use the `extension` qualifier. For example, `extension:js` will search for code with the .js file extension.", "a8349eef-45ca-4421-ab16-af9b5e095f77": "According to the Marketplace Terms provided, modifications to these terms can only be made through a written amendment signed by an authorized representative of GitHub. No other method of modification is permitted. Therefore, to modify these Marketplace Terms, a written document signed by an authorized representative of GitHub is required.", "75a5c32d-c9ef-4314-a2f6-99954a2d2d92": "To view the usage of GitHub Actions for your personal account, follow these steps:  1. Go to your GitHub profile by clicking on your profile picture in the top right corner of the GitHub page and selecting \"Profile\" from the drop-down menu. 2. Click on \"Settings\" in the left-hand menu. 3. Click on \"Billing & plans\" in the left-hand menu. 4. Scroll down to the \"GitHub Actions\" section. 5. Here, you can view your usage of GitHub Actions minutes, packages, storage, and report downloads.  Note: If you're not currently subscribed to a GitHub plan that includes GitHub Actions, you won't see any usage data in this section.", "32b7c43c-0464-4282-ba39-c3c4ec642afd": "To grant your backup host access to your primary instance in {% data variables.product.prodname_ghe_server %}, you need to open your primary instance's settings page at `http(s)://HOSTNAME/setup/settings` and add the backup host's SSH key to the list of authorized SSH keys. This is mentioned in step 3 of the given context information.", "c38f7d0d-251e-442a-8625-526b41eec313": "Based on the provided context, it is not explicitly stated which specific categories of data the data importer collects and processes. However, it is mentioned that the data importer will adhere to the requirements set forth in the DPA and the GitHub Privacy Statement, which may provide further information on the types of data collected and processed.  Regarding the processing of sensitive personal data, the data importer is required to obtain the explicit consent of the data subject prior to processing such data, as stated in the context information. This requirement is in line with the provisions of the GDPR, which mandates that sensitive personal data should be processed with additional safeguards and restrictions.", "888116e9-116d-41e9-b692-2af08e991ffc": "Sharing private information on GitHub Community can pose several risks, such as identity theft, phishing scams, and unauthorized access to sensitive data. To mitigate these risks, users should avoid sharing any personal or confidential information, such as passwords, credit card numbers, or social security numbers, on the platform. Instead, users can share links to their repositories or projects, along with a brief description of what they need help with, without revealing any sensitive information. Additionally, users can enable two-factor authentication (2FA) on their GitHub accounts to add an extra layer of security and prevent unauthorized access. Finally, users should be cautious when interacting with unfamiliar users and avoid clicking on suspicious links or downloading unknown files. By following these best practices, users can help ensure the security and privacy of their personal information on GitHub Community.", "67f2cfe4-e509-463f-8d51-adfa8a38a263": "To restrict email notifications to a verified domain, an organization must first verify ownership of the domain. This feature is only available for organizations using {% data variables.product.prodname_ghe_cloud %}. Once the domain is verified, the organization can configure email notifications to only be sent to email addresses within the verified domain. This helps to prevent unauthorized access to the organization's email notifications and ensures that sensitive information is only shared with authorized users.", "9d314cbe-5af3-4196-8fca-16ac1d4cb5bb": "To restore privileges for a user in an organization on GitHub, follow these steps:  a. Go to the organization's settings. b. Click on the \"People\" tab. c. Find the user whose privileges you want to restore. d. Click on the three dots next to their name. e. Select \"Restore\" from the drop-down menu.  2. To restore privileges for a user in an organization on GitHub, follow these steps:  a. Log in to your GitHub account. b. Go to the organization's settings. c. Click on the \"People\" tab. d. Find the user whose privileges you want to restore. e. Click on the three dots next to their name. f. Select \"Restore\" from the drop-down menu.  Answers:  1. To add a user to a team in an organization on GitHub, follow these steps:  a. Go to the organization's settings. b. Click on the \"Teams\" tab. c. Click on the team you want to add the user to. d. Click on the \"Members\" tab. e. Click on the \"Add members\" button. f. Type in the user's GitHub username or email address. g. Select the user from the list that appears. h. Choose the user's role in the team. i. Click on the \"Add members\" button.  2. To add a user to a team in an organization on GitHub, follow these steps:  a. Log in to your GitHub account. b. Go to the organization's settings. c. Click on the \"Teams\" tab. d. Click on the team you want to add the user to. e. Click on the \"Members\" tab. f. Click on the \"Add members\" button. g. Type in the user's GitHub username or email address. h. Select the user from the list that appears. i. Choose the user's role in the team. j. Click on the \"Add members\" button.  Answers:  1. To cancel an invitation to join an organization on GitHub, follow", "4ef719fd-39dd-475b-a045-7ceebeb72e31": "Iranian GitHub users will be able to use paid services under the license received from OFAC, as long as they are able to make payments using major credit cards, which are subject to the terms and conditions of GitHub's payment processors. However, third parties may include restrictions that block payments from Iran, which could limit the availability of paid services for Iranian users.", "c94e77b5-b32e-4e71-9d8a-ea96956fef07": "The environment variables used for authentication in the provided workflow when publishing to the Maven Central Repository are `MAVEN_USERNAME` and `MAVEN_PASSWORD`. These variables are defined in the `publish-to-maven` step and are referenced in the `settings.xml` file generated by the `setup-java` action during the first setup.  The environment variables used for authentication in the provided workflow when publishing to GitHub Packages are `GITHUB_TOKEN`. This variable is defined in the `publish-to-packages` step and is referenced in the `settings.xml` file generated by the `setup-java` action during the second setup.  Note: The values of these variables are not explicitly provided in the context information, but they are referenced as environment variables that are defined in the next step. It is assumed that these variables are defined and set elsewhere in the workflow or in the repository's settings.", "84b9a362-8eeb-4e3f-b3f1-3b2bca8aa257": "Without further context, it is unclear which software applications FPT and GHEC refer to. Please provide more information or clarify which software applications are being referred to in order to provide an example of how the \"AUTOTITLE\" feature is utilized in a specific scenario within the context of FPT or GHEC.", "00e3a13f-eeb4-4741-993f-2663daa10da3": "The purpose of the Security Overview feature in GitHub is to provide a centralized view of an organization's security posture by showing which security features are enabled for repositories and including repository and alert-focused views to quickly investigate security issues and take action to remediate them. It also provides risk and coverage information about GitHub's security features, such as Dependabot and advanced security features like Code Scanning and Secret Scanning, for enterprises that use GitHub Advanced Security and for public repositories. Additionally, it offers an organization-level dashboard of insights from security features for enterprise-owned organizations that use GitHub Advanced Security. The views are interactive with filters that allow for detailed analysis and identification of high-risk or low-feature coverage areas.", "8aa7721f-63d3-4a5e-8067-a491f1dd7499": "The value of the variable `GITLAB_USER_LOGIN` in the provided context information is `{% raw %}${{ github.actor }}`{% endraw %}. This is because the context information provided is for GitHub Actions, and `GITHUB_ACTOR` represents the username of the user who triggered the workflow. Therefore, `GITLAB_USER_LOGIN` would have the same value in this context, as it is being used as a substitute for `GITHUB_ACTOR` in GitHub Actions.", "f42a5b91-7b3e-462b-8fb4-8fa9eb2e2cc8": "To modify your {% data variables.product.prodname_copilot_business_short %} subscription for your organization or enterprise account, you can follow these steps:  1. Log in to your {% data variables.product.prodname_dotcom %} account.  2. Click on your profile icon in the top right corner of the page and select \"Billing\" from the drop-down menu.  3. If you have an organization account, select your organization from the list of organizations. If you have an enterprise account, select your enterprise from the list of enterprises.  4. Click on the \"Subscriptions\" tab.  5. Locate the {% data variables.product.prodname_copilot_business_short %} subscription and click on the \"Manage\" button.  6. From here, you can modify various aspects of your subscription, such as the number of seats, the billing cycle, and the payment method.  7. Once you have made the desired changes, click on the \"Save\" button to apply them.  For customers under a Microsoft Enterprise Agreement:  1. Log in to your {% data variables.product.prodname_dotcom %} account.  2. Click on your profile icon in the top right corner of the page and select \"Billing\" from the drop-down menu.  3. Select your organization from the list of organizations.  4. Click on the \"Subscriptions\" tab.  5. Locate the {% data variables.product.prodname_copilot_business_short %} subscription and click on the \"Manage\" button.  6. From here, you can modify various aspects of your subscription, such as the number of seats and the billing cycle.  7. Once you have made the desired changes, click on the \"Save\" button to apply them.  For customers under a direct GitHub contract:  1. Log in to your {% data variables.product.prodname_dotcom %} account.  2. Click on your profile icon in the top right corner of the page and select \"Billing\" from the drop-down menu.  3. Select your organization from the list of organizations.  4. Click on the \"Subscriptions", "457e4b35-ab6c-40ee-993d-7cc2a1d97706": "A user can authorize an app to make API requests on their behalf by installing the app on their account or organization. During the installation process, the user will grant the app permission to access the organization and repository resources that it requested. If the app is installed on an organization that includes multiple members, each member will need to authorize the app before the app can act on their behalf. The app does not need to be installed in order for a user to authorize it.", "410fdec2-d6bb-47a8-acc2-fc4fb077ccf6": "A migration log is a record of the steps taken during a migration process using {% data variables.product.prodname_importer_proper_name %}. It includes information such as who initiated the migration, the source of the migration, the duration of the migration, and any resources that did not migrate as expected. This information is useful because it provides transparency and traceability into the migration process, allowing for easier identification and resolution of any issues that may arise during the migration. It also serves as a reference for future migrations or for auditing purposes. Additionally, the migration log can be used to discuss any warnings with the team and record any decisions.", "822d233a-8b19-492d-aee8-485dd2ab3e3b": "The purpose of running the audit command in the {% data variables.product.prodname_actions_importer %} tool for Bitbucket is to get a high-level view of pipelines in a Bitbucket instance and to generate a report that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}. This command can be executed by running the following command in the terminal, replacing `:workspace` with the name of the Bitbucket workspace to audit and `:project_key` with the key of the project to be audited:  ```bash gh actions-importer audit bitbucket --workspace :workspace --project-key :project_key --output-dir tmp/audit ```  The output of the command will be saved in the `tmp/audit` directory. This command can be helpful in determining the feasibility and complexity of migrating pipelines from Bitbucket to GitHub Actions using {% data variables.product.prodname_actions_importer %}.", "b200f1dc-bb16-4da2-81d1-07f3841af464": "Based on the context information provided, if you are enrolled in an informal learning program that is not part of the {% data variables.product.prodname_campus_program %} and not enrolled in a degree or diploma granting course of study, you are not eligible for the {% data variables.product.prodname_student_pack %}. This is because the {% data variables.product.prodname_student_pack %} is specifically designed for students who are enrolled in degree or diploma granting courses of study or students at coding schools or bootcamps that are part of the {% data variables.product.prodname_campus_program %}. However, your instructor may still apply for a {% data variables.product.prodname_education %} discount for classroom use.", "91c36799-fc4a-48fa-9fe7-18eb485c3dab": "In the provided context, if you encounter issues with log forwarding, you can troubleshoot them by following these steps:  1. Access your GitHub Enterprise Server (GHES) instance by visiting its URL in a web browser. 2. Log in with your administrator credentials. 3. Go to the Settings tab in the left-hand menu. 4. Click on Logs in the left-hand menu. 5. Scroll down to the Log forwarding section. 6. Click on the Setup log forwarding button. 7. Follow the on-screen instructions to configure log forwarding. 8. Once log forwarding is set up, you can test it by accessing the diagnostics page. To do this, go to the URL `http(s)://[hostname]/setup/diagnostics` in a web browser, replacing `[hostname]` with the hostname or IP address of your GHES instance. 9. Click on the Download diagnostics button to download a file containing diagnostic information. 10. Attach this file to a message to GitHub Support by visiting {% data variables.contact.contact_ent_support %}. This will provide them with the necessary information to help you troubleshoot any issues you may be experiencing with log forwarding.  If you encounter any issues during the setup process, refer to the official GitHub Enterprise Server documentation for further guidance: https://docs.github.com/en/enterprise-server@3.x/admin/guides/installing-on-premises-enterprise-server/configuring-log-forwarding.", "4acdbff0-6515-465f-9fd2-cd06829b46e3": "The consequences of not following the policy regarding gifts and entertainment for prospective customers and government officials, as outlined in the context information provided, include fines and jail time for both the company and the individual involved. This is because failing to adhere to the policy could result in GitHub paying for extravagant gifts or bribes, which is considered a violation of anti-bribery laws. These laws have strict penalties, and both the company and the individual could face fines and imprisonment, regardless of whether the customer or government official actually submitted an order or picked up the gift. It's essential to follow the policy and report any gifts or entertainment over a certain amount to Legal in writing, including the recipient and the purpose of the gift, as well as a copy of the email approval. Failure to do so could result in severe consequences.", "23c0bfe6-cc04-4655-9a5a-5b208c940c51": "The command \"sed -i 's/#AuthorizedKeysFile/AuthorizedKeysFile/' /etc\" in the context provided is significant because it modifies the configuration file \"/etc\" by replacing the hash symbol (#) before the \"AuthorizedKeysFile\" line with no hash symbol. This change allows the SSH key generated by the \"ssh-keygen -A\" command to be automatically added to the authorized keys file for the root user, without requiring the user to manually edit the file. This is likely being done as part of the Docker image build process to ensure that the SSH key is properly configured for the container's intended use.", "74131f4a-690b-462e-8793-17b07d230594": "The context information provides references to a feature called \"AUTOTITLE\" without explaining its purpose. Without further context, it is unclear what \"AUTOTITLE\" does. However, based on the context, it appears to be related to auditing and monitoring in a management system. It could potentially be a feature that automatically generates titles or labels for audit and system logs, or it could be a feature that automatically generates titles or labels for other types of documents or records in a management system. Without more information, it is impossible to say for certain.", "fd71c559-14c2-4918-aa36-180c4f856629": "Without prior knowledge, it is unclear how the `v2.15.2` version differs from the previous version in terms of functionality. The context provided only mentions the version numbers, and no information is given about the specific changes or updates made in the `v2.15.2` release. It is recommended to refer to the release notes or documentation for `v2.15.2` to understand the differences between this version and the previous one.", "ad9668d7-45eb-4cab-988c-23a935a6bedd": "External authentication refers to a method of logging in to {% data variables.product.prodname_dotcom %} using an external identity provider (IdP) instead of a username and password. This feature is only available for {% data variables.product.prodname_ghes %} and is configured by the site administrator. The specific steps for configuring external authentication may vary depending on the IdP being used. For more information, you can refer to the documentation provided by {% data variables.product.prodname_ghes %}.", "002277b4-a63c-46d3-9997-f9dd3ee8a6b7": "The different types of contributions that are counted towards a user's profile on GitHub are committing to a repository's default branch or `gh-pages` branch, creating a branch, opening an issue, opening a discussion, answering a discussion, proposing a pull request, and submitting a pull request review (for GitHub Enterprise Server and GitHub Advanced Security users). Pull request merges are also counted as contributions, but only for the repository's default branch or `gh-pages` branch. Co-authoring commits in a repository's default branch or `gh-pages` branch are also counted as contributions for GitHub Enterprise Server users.", "f4636cb1-5fe2-47cd-bb57-7d170912bbc6": "To create and share a support bundle using the web-based management console in GitHub Enterprise, follow these steps:  1. Access the web-based management console by following these steps:     a. Go to your GitHub Enterprise organization's website.    b. Click the gear icon in the upper-right corner of the page.    c. Select \"Settings\" from the drop-down menu.    d. Click \"Management console\" in the left-hand menu.    e. Enter your management console password when prompted.  2. Once you're in the management console, click the \"Support\" link in the left-hand menu. 3. Click \"Download support bundle\" to create a support bundle. 4. Follow the prompts to save the support bundle to your local machine. 5. Go to the GitHub Enterprise support page (https://enterprise.github.com/support) and sign in with your organization's credentials. 6. Click \"Upload support bundle\" to share the support bundle with GitHub Enterprise support.  To create and share a support bundle using SSH in GitHub Enterprise, follow these steps:  1. Connect to your GitHub Enterprise server via SSH using the following command:     ```shell    ssh -p 122 admin@HOSTNAME    ```     Replace \"HOSTNAME\" with the IP address or hostname of your GitHub Enterprise server.  2. Once you're connected to the server, run the following command to download the support bundle:     ```shell    ghe-support-bundle -o > support-bundle.tgz    ```     This command will create a support bundle in the current directory and save it as a .tgz file named \"support-bundle.tgz\".  3. Go to the GitHub Enterprise support page (https://enterprise.github.com/support) and sign in with your organization's credentials. 4. Click \"Upload support bundle\" to share the support bundle with GitHub Enterprise support.  Note: The `ghe-support-bundle` command is a GitHub Enterprise-specific command that's used to create support bundles. It's included in the GitHub Enterprise server software and is not available in standard GitHub install", "49018b6b-19d5-4ada-b021-878acf586692": "Only organization owners can delete parent teams in an organization, as mentioned in the tip provided in the context information.", "5a5d622d-942e-4954-a901-4e410c6f0106": "The `$(Agent.ToolsDirectory)` variable in the context information refers to the directory where tools and dependencies are installed and cached by the Azure DevOps agent, while the `$(Agent.WorkFolder)` variable refers to the directory where the agent performs the build and test tasks specified in the build definition. In other words, the `$(Agent.ToolsDirectory)` is used to store and manage tools and dependencies, while the `$(Agent.WorkFolder)` is used to execute the build and test tasks.", "2a4264d0-1631-4125-9677-18f4052f6864": "Release notes for changes should be written in the present tense to reflect the current behavior, and should avoid passive language to clarify actors and impact. To reduce repetition and unnecessary words, \"now\" is usually implied. These guidelines help to make the release notes more concise and clear for the audience.", "ae286b85-d783-44ec-81a1-39f7d186ed3f": "To set the name of the default branch using Git's symbolic-ref command, you can use the following command:  ```shell git symbolic-ref HEAD refs/heads/main ```  This command sets the HEAD reference to point to the main branch, which becomes the default branch.  Answer:  To add all the files in your local repository and stage them for the first commit in Git, you can use the following command:  ```shell git add . ```  This command adds all the files in the current directory and its subdirectories to the staging area, preparing them for the first commit. The dot (.) at the end is a wildcard that matches all the files in the current directory.", "21cbb304-fd14-499e-a702-132acee40abe": "To convert environment variables from a string to a JSON data type using the `fromJSON` function in a GitHub workflow, you can follow these steps:  1. Define the environment variables as a JSON string in the workflow's `env` section. For example:  ```yaml name: My Workflow  on:   push:     branches:       - main  jobs:   build:     runs-on: ubuntu-latest      steps:       - name: Checkout         uses: actions/checkout@v2        - name: Set up JDK 11         uses: actions/setup-java@v2         with:           java-version: '11'           distribution: 'adopt'        - name: Build         run: mvn package        - name: Deploy         uses: your-org/your-repo::main         with:           JAR_FILE: ${{ fromJSON(env.artifacts) }}.jar      env:       artifacts: '{\"my-artifact\": \"my-artifact-value.jar\", \"my-other-artifact\": \"my-other-artifact-value.jar\"}' ```  2. In the `deploy` step, use the `fromJSON` function to convert the `artifacts` environment variable from a JSON string to a JSON object. This allows you to access the individual artifacts by their keys.  In this example, we're deploying two artifacts, `my-artifact` and `my-other-artifact`, and passing the `my-artifact` artifact to the `your-org/your-repo` action.  To calculate a final SHA-256 hash for a set of files that match a specific pattern using the `hashFiles` function in a GitHub workflow, you can follow these steps:  1. Define the pattern for the files you want to hash. For example:  ```yaml name: My Workflow  on:   push:     branches:       - main  jobs:   build:     runs-on: ubuntu-latest", "35704f9c-b976-4c6c-81b2-bc6703d92e46": "Failure to comply with the provisions of the Data Protection Addendum would constitute a breach, as outlined in section c. This could include, but is not limited to, failing to implement appropriate technical and organizational measures to ensure the security of the Protected Data, failing to notify GitHub of a breach in a timely manner, or failing to cooperate with GitHub in investigating and addressing any breaches. The specific provisions that must be complied with will be outlined in the Data Protection Addendum itself.", "eabb19ee-f29b-400a-b444-2f80aaa2d2f6": "Under \"Repository permissions\", next to \"Pull requests,\" select \"Read & write\". This permission is required because the app will be creating and updating pull requests.  Under \"Subscribe to events\", select \"Pull request\". This event will trigger the app's webhook, which will then update the pull request's status.  These settings are required for the app to function properly. Without these permissions, the app will not be able to create, update, or delete pull requests, and it will not be able to receive pull request events.", "f3d3223d-f3dc-4976-9429-ab116e9102f6": "Creating new projects, repositories, and organizations in GitHub:  a. Creating a new project board:  - Command: `New project` - Behavior: This command creates a new project board in your GitHub organization. You can use this command from the organization's homepage. Once you click on the command, you'll be prompted to enter a name for the project and select a template (if available). After that, you'll be taken to the new project board, where you can start adding tasks and issues.  b. Creating a new repository from scratch:  - Command: `New repository` - Behavior: This command creates a new repository in your GitHub organization or user account. You can use this command from the organization's or user's homepage. Once you click on the command, you'll be prompted to enter a name for the repository, select a template (if available), and choose whether to initialize the repository with a README file. After that, you'll be taken to the new repository, where you can start adding files and commits.  c. Creating a new team:  - Command: `New team` - Behavior: This command creates a new team in your GitHub organization. You can use this command from the organization's homepage. Once you click on the command, you'll be prompted to enter a name and description for the team, and select the team's privacy settings. After that, you can add members to the team and assign them to repositories and projects.  2. Cloning a repository using GitHub's command-line tool, HTTPS, or SSH:  a. Using GitHub's command-line tool (GitHub CLI):  - Step-by-step instructions:    - Install the GitHub CLI by following the instructions on the official GitHub CLI website.    - Open your terminal or command prompt and log in to your GitHub account by running `gh auth login`.    - Navigate to the directory where you want to clone the repository.    - Run `gh repo clone <repository-url>` to clone the repository. Replace `<repository-url>` with the URL of the repository you want to clone.", "1a18a3ee-b8cc-4f8d-b702-2fe43b3ae468": "To write detailed logs to a specific directory using the CLI tool, you can use the `--logdir` option. This option allows you to specify a directory where detailed logs will be written with generated names that include timestamps and the name of the running subcommand. This option overrides the default location of `.codeql` in the user's home directory. The logs will be persisted between several runs of the CLI.  Example usage:  ``` codeql analyze --logdir=/path/to/logs ```  This will write detailed logs to the specified directory instead of the default location.", "227cb3d4-09b1-4787-97bc-4f5c23c71e64": "To pin repositories to an organization's profile, you need to follow these steps:  1. Go to your organization's profile page. 2. Select the \"View as\" dropdown menu in the right sidebar and choose \"Public\" or \"Member\". 3. If you already have pinned repositories, click \"Customize pins\" in the \"Pinned\" section. If you haven't pinned any repositories yet, click \"Pin repositories\" in the right sidebar. 4. Select the repositories you want to pin by checking the boxes next to their names. 5. Click \"Save changes\" to save your selections.  Only organization owners have the authority to pin repositories to an organization's profile.", "41bad3f1-df51-48d8-afb2-e1e96564b844": "An organization owner can manage policies for the use of {% data variables.product.prodname_copilot %} within their organization by selecting the desired policy settings for {% data variables.product.prodname_copilot %} in their organization. These policy settings will determine the behavior of {% data variables.product.prodname_copilot %} for all organization members granted access to {% data variables.product.prodname_copilot_short %} through the organization. If explicit settings have been selected at the enterprise level, the organization owner cannot override those settings at the organization level. For more information on managing policies at the enterprise level, refer to the documentation provided by {% data variables.product.prodname_copilot %}.", "45854dbf-ecb0-4249-9df9-6830e5326eee": "The purpose of adding labels to issues is to help with managing and prioritizing issues in a repository by categorizing them based on their status, severity, or type. For example, labels like \"bug\", \"enhancement\", \"help wanted\", and \"good first issue\" can be used to indicate the nature of the issue and make it easier to find and address.  The workflow created in this tutorial uses the {% data variables.product.prodname_cli %} to automatically add the \"triage\" label to newly opened or reopened issues. This can help with quickly identifying issues that require initial review and assessment, and can make it easier to prioritize and manage the backlog of issues in the repository. By filtering for issues with the \"triage\" label, team members can easily see which issues need to be reviewed and addressed first.  To implement this functionality, follow these steps:  1. Create a new workflow file in your repository, and copy the YAML contents provided in the tutorial into it. 2. Customize the `env` values in the workflow file to match your repository and the labels you want to add. The `GITHUB_TOKEN`, `GH_REPO`, and `NUMBER` values are automatically set, but you may need to create a new secret called `GITHUB_TOKEN` to store your GitHub access token. 3. Save the workflow file and commit it to your repository. 4. Test the workflow by opening or reopening an issue in your repository. The \"triage\" label should be automatically added to the issue, and you should be able to see it by filtering for issues with the \"triage\" label in your repository's issue list.  Overall, using the {% data variables.product.prodname_cli %} in a workflow to automatically add labels to issues can help with managing and prioritizing issues in a repository, and can save time and effort by automating a repetitive task.", "001cbf42-24c7-4bbc-bf2b-d5d2bbd5b409": "{% data variables.product.prodname_actions_importer %} supports variable templates in Azure Pipelines with relative or dynamic file paths that can contain variable, parameter, and iterative expressions. However, a default value must be set for templates with dynamic file paths.  For example, in the following YAML file, {% data variables.product.prodname_actions_importer %} can extract the `vars.yml` template with a variable expression in the file name:  ```yaml  File: azure-pipelines.yml variables: - template: 'templates/vars.yml'  steps: - template: \"./templates/${{ variables.one }}\" ```  In the `templates` folder, there is a `vars.yml` file with variables defined:  ```yaml  File: templates/vars.yml variables:   one: 'simple_step.yml' ```  {% data variables.product.prodname_actions_importer %} also supports parameter expressions in file paths:  ```yaml parameters: - name: template   type: string    default: simple_step.yml  steps: - template: \"./templates/{% raw %}${{ parameters.template }}{% endraw %}\" ```  Iterative expressions can also be used in file paths:  ```yaml parameters: - name: steps   type: object   default:   - build_step   - release_step steps:  - {% raw %}${{ each step in parameters.steps }}{% endraw %}:     - template: \"${{ step }}-variables.yml\" ```  In this example, {% data variables.product.prodname_actions_importer %} can extract the `build_step-variables.yml` and `release_step-variables.yml` templates for each step defined in the `steps` parameter.  However, conditional and iterative insertion are only partially supported by {% data variables.product.prodname_actions_importer %}. This means that while conditional expressions can be used to include or exclude steps based on certain conditions, iter", "25748bec-326b-4a9b-a085-32a9532b46bc": "To ensure that {% data variables.product.prodname_codeql %} accurately scans all source files for compiled languages, the following steps can be taken:  1. Replace the `autobuild` step with the same build commands used in production. This ensures that {% data variables.product.prodname_codeql %} knows exactly how to compile all of the source files that need to be scanned.  2. Use recognized compilers for C code, such as GCC, Clang, or MSVC, to avoid issues during the analysis process. Most vendor-specific C compilers will not be recognized by {% data variables.product.prodname_codeql %}.  3. Inspect the copy of the source files included in the {% data variables.product.prodname_codeql %} database to understand why some source files haven't been analyzed. This can help identify any issues with the build process or the way {% data variables.product.prodname_codeql %} is configured.  Potential issues that may arise during the analysis process include:  1. Incomplete analysis due to the `autobuild` feature not compiling all of the source files correctly. This can happen when multiple `build.sh` commands exist in a single repository, and the `autobuild` step only executes one of them.  2. Issues with the compiler used to compile the source code, such as compatibility issues with {% data variables.product.prodname_codeql %}. Most vendor-specific C compilers will not be recognized by {% data variables.product.prodname_codeql %}.  3. Incorrect configuration of {% data variables.product.prodname_codeql %}, such as incorrect compiler settings or incorrect paths to the source files. This can result in some source files not being analyzed.  By following these steps and being aware of these potential issues, you can ensure that {% data variables.product.prodname_codeql %} accurately scans all source files for compiled languages.", "aff4058b-5efc-45ea-8133-4cd3af284e8d": "In this case, without prior knowledge, it's unclear what the relevant section is or what the \"Settings\" page is. Therefore, we cannot generate the next steps based on the given context. Please provide more information for us to proceed.", "908ad735-f0ce-4818-a088-c01cfdc854c5": "SAML (Security Assertion Markup Language) single sign-on (SSO) is a standardized protocol that allows users to authenticate once and access multiple applications without having to enter their credentials separately for each application. In the context provided, SAML SSO can be configured at both the organization and enterprise account levels. When configured at the organization level, each organization requires a unique SSO tenant in the identity provider (IdP), resulting in unique SAML identity records for each organization. In contrast, when configured at the enterprise account level, all organizations owned by the enterprise account share a single SAML identity. After enabling SAML SSO for an enterprise account, any existing SAML SSO configurations for organizations owned by the enterprise account will be overridden, and team synchronization settings will be removed. Enterprise members will not be notified of this change, but they will need to select the new enterprise-level app tile in their IdP dashboard instead of the old organization-level app tile. Any previously authorized items, such as personal access tokens, SSH keys, OAuth apps, and GitHub apps, will continue to be authorized for the organization.", "be93302b-7f67-4ce2-8003-3198eaced03d": "To make shell scripts more secure when used in {% data variables.product.prodname_actions %}, it's recommended to use double quote shell variables to avoid word splitting, and to store the value of expressions like {% raw %}${{ github.event.issue.title }} {% endraw %} in memory and use them as variables, rather than interacting with the script generation process. This approach can be applied to the script injection attempt mentioned in the context information by ensuring that the value of the PR title is stored in a variable and used in the script, rather than being directly concatenated with the command. For example:  ```shell title=$(echo ${{ github.event.issue.title }} | sed 's/^\"//; s/\"$//') env:   TITLE: \"$title\" PR title did not start with 'octocat' ```  This script stores the value of the PR title in the `title` variable, removes the quotes around it, and then uses it in the `env` section of the script. This helps prevent script injection attacks, as the value of the variable is now treated as a literal string, rather than being interpreted as a command.", "21003e4e-2572-4022-9443-ed96626efac5": "Before pushing changes to GitHub, you should update your local branch to include any commits that have been added to the remote repository. This can be done by fetching the new commits before pushing your changes to avoid merge conflicts. {% data variables.product.prodname_desktop %} will prompt you to do this if someone has made commits on the remote that are not on your local branch.", "caa0963c-2d89-4d1d-8a18-fe20dbd961af": "The \"Changes\" and \"History\" views in the left sidebar of the GitHub Desktop app are used to manage the changes and history of a repository. The \"Changes\" view shows the changes made to files in the current branch that have not been committed to the local repository. This view allows the user to commit these changes to the repository. The \"History\" view shows the previous commits on the current branch of the repository. This view allows the user to view the changes made in each commit and compare them using diffs. In summary, the \"Changes\" view is used to manage changes that have not been committed, while the \"History\" view is used to manage the history of committed changes.", "077ecc1c-d4f9-43f3-8265-59ac12f567de": "The purpose of setting a `GITHUB_TOKEN` environment variable in a workflow is to grant specific permissions to the workflow, as specified in the `permissions` key. This allows the workflow to authenticate and perform actions on behalf of the user, such as creating or updating repositories, without requiring the user's actual GitHub credentials. This is useful for automating tasks and workflows that require access to GitHub resources. The `GITHUB_TOKEN` secret is used to securely store and manage the token, and it is automatically set as an environment variable during the workflow's execution. For more information on using secrets in workflows, refer to the GitHub documentation on the topic.", "8189d452-442c-449b-a1ad-16cb732387d7": "Without further context, it is unclear what \"AUTOTITLE\" is or what its purpose might be in a document. It could potentially be a formatting command or a placeholder for a title that will be automatically generated by the software or program being used. However, without more information, it is impossible to determine the exact purpose of \"AUTOTITLE\" in this context.", "898c79c6-220e-4443-b58c-31df6f5fa3be": "To directly download a specific version of a CodeQL pack, you can use the `codeql pack download` command followed by the pack name and the version you want to download. For example, to download version 1.0.2 of the `octo-org/optional-security-queries` pack, you can run the following command:  ```shell $ echo $OCTO-ORG_ACCESS_TOKEN | codeql pack download octo-org/optional-security-queries@1.0.2 ```  This will download the pack to the `~/.codeql/packages` directory by default. You can then use the downloaded pack in your CodeQL analysis by specifying its path using the `--pack` option, like this:  ```shell $ echo $OCTO-ORG_ACCESS_TOKEN | codeql database analyze --pack /path/to/pack \\     --format=sarif-latest --output=/temp/example-repo-js.sarif ```  Note that the `--pack` option can be used multiple times to specify multiple packs.  For more information on downloading and managing CodeQL packs, see the CodeQL documentation on packs.", "7a5d681a-a8ae-477b-8657-fe12fef46939": "To fix the error \"Error: Permission to user/repo denied to user/other-repo\" when pushing with a key that is attached to another repository as a deploy key and does not have access to the repository you are trying to push to, you should remove the deploy key from the repository and add it to your personal account instead.  Here's a step-by-step guide on how to do this:  1. Go to the repository where the deploy key is currently attached. 2. Click on the Settings tab in the top right corner of the page. 3. Click on Deploy keys in the left-hand menu. 4. Find the deploy key that is causing the error and click on the three dots to the right of it. 5. Click on Remove from the drop-down menu. 6. Confirm that you want to remove the deploy key by clicking on Remove deploy key in the pop-up window. 7. Go to your personal account by clicking on your profile picture in the top right corner of the page and selecting Your profile. 8. Click on SSH and GPG keys in the left-hand menu. 9. Click on New SSH key to add a new deploy key. 10. Enter a title for the key and paste the contents of the deploy key into the Key field. 11. Click on Add SSH key to save the new deploy key. 12. Go back to the repository where you were getting the error and try pushing your changes again.  This should fix the error \"Error: Permission to user/repo denied to user/other-repo\" and allow you to push your changes to the repository.", "e71f32d8-355d-4e38-99e7-2827ea4248d1": "Yes, it is possible to use the REST API to view workflows for a repository in {% data variables.product.prodname_actions %}. Here are the steps involved:  1. Obtain an access token with the `repo` and `workflow` scopes. You can do this by following the instructions in the {% data variables.product.prodname_dotcom %} documentation.  2. Send a GET request to the `/repos/{owner}/{repo}/actions/workflows` endpoint, replacing `{owner}` and `{repo}` with the appropriate values for your repository.  3. In the request headers, include an `Authorization` header with the format `Bearer <access_token>`.  4. The response will contain a list of all the workflows for the repository, along with their names, statuses, and other details.  Here's an example request using cURL:  ``` curl -X GET \\   https://api.github.com/repos/octocat/hello-world/actions/workflows \\   -H 'Authorization: Bearer <access_token>' ```  Replace `<access_token>` with the access token you obtained in step 1.  Note that the {% data variables.product.prodname_actions %} documentation provides more detailed information on how to use the REST API for workflows, including how to create, update, and delete workflows.", "0ad0b260-0715-4517-8413-9965dfc8ee91": "To display the sum of a specific number field on the roadmap layout, you need to have the numeric summary feature enabled in your GitHub Enterprise Cloud organization. After that, you can follow the steps provided in the context information under \"Showing the sum of a number field\" to display the sum of a specific number field on the roadmap layout. If you're not sure whether your organization has this feature enabled, you can check with your GitHub Enterprise Cloud administrator. If the context doesn't provide information about whether this feature is enabled, then you'll need to ask your administrator or check your organization's documentation to confirm.", "64d8a2d2-8159-4198-a98a-2d1c72329593": "To check your current {% data variables.product.prodname_enterprise %} license, search for users and repositories, and query the audit log using the provided context information, you can refer to the **License info & search** section of the site admin dashboard. This section provides access to the necessary tools and resources for managing your {% data variables.product.prodname_enterprise %} license, as well as the ability to search for users and repositories, and query the audit log.", "cc69c906-4c11-40a1-bfa0-79a9a2db5fd8": "To create a new issue using the REST API in {% data variables.product.prodname_dotcom %}, you can follow these steps:  1. Make sure you have a personal access token with read and write permissions for repository issues. You can generate a new token in your account settings.  2. Use the Octokit.js library to make a `POST` request to the `/repos/{owner}/{repo}/issues` endpoint. Replace `{owner}` and `{repo}` with the owner and repository names, respectively.  3. In the request body, include the issue title and body. Here's an example:  ```javascript const octokit = require('octokit').rest({   auth: 'YOUR-TOKEN' });  octokit.issues.create({   owner: 'OWNER',   repo: 'REPO',   title: 'Created with the REST API',   body: 'This is a test issue created by the REST API' }).then(response => {   console.log(response.data); }).catch(error => {   console.error(error.message); }); ```  Replace `OWNER` and `REPO` with the actual owner and repository names, and replace `YOUR-TOKEN` with your personal access token.  This will create a new issue with the specified title and body. The response will contain the newly created issue data.  For more information, see the Octokit.js documentation.", "ffda9e0d-2451-44df-9333-dcef13ee5725": "The total number of teams present in the GHE environment, including both enabled and disabled organizations, is represented by the variable AA in the context information provided.", "bb29c5b7-eb92-4f7d-9290-91535c64625e": "When you downgrade your personal account from {% data variables.product.prodname_pro %} to {% data variables.product.prodname_free_user %}, you will lose access to advanced code review tools on private repositories. To avoid a domain takeover if you have published a {% data variables.product.prodname_pages %} site in a private repository and added a custom domain, you should remove or update your DNS records before making the downgrade.", "6451c919-6da0-4b5e-ac87-af8a903d4f0e": "{% ifversion ghes %}  {% data variables.product.product_name %} automatically creates a username for each person who authenticates through CAS, LDAP, or SAML external authentication systems for the first time. The username is generated by {% data variables.location.product_location %} and consists of the normalized identifier provided by the external authentication system, with an underscore and short code added. If multiple identifiers are normalized into the same username, a username conflict occurs, and only the first user account is created. To avoid username conflicts, it's recommended to ensure that the normalized usernames are unique and within the 39-character limit.  {% elsif ghec %}  For individuals who authenticate through an enterprise with {% data variables.product.prodname_emus %}, their usernames are automatically created when their user account is provisioned via SCIM, by normalizing an identifier provided by the SAML identity provider (IdP), then adding an underscore and short code. If multiple identifiers are normalized into the same username, a username conflict occurs, and only the first user account is created. To avoid username conflicts, it's recommended to ensure that the normalized usernames are unique and within the 39-character limit.  {% elsif ghae %}  For individuals who authenticate through SAML SSO, their usernames are automatically created when they sign in through the identity provider (IdP) for the first time. The username consists of the normalized identifier provided by the IdP, with an underscore and short code added. If multiple identifiers are normalized into the same username, a username conflict occurs, and only the first user account is created. To avoid username conflicts, it's recommended to ensure that the normalized usernames are unique and within the 39-character limit.", "a2cdae56-dbc6-4f17-9a63-420c11979c51": "Yes, based on the context information provided, we can summarize the guidelines for writing release notes for changes and known issues as follows:  1. Use present tense for changes and clarify actors and impact. 2. Avoid passive language and repetition. 3. Implied \"now\" and clarify audience. 4. Include relevant links and error messages (if applicable). 5. For known issues, describe the issue, its behavior, and any necessary actions.  These guidelines help ensure clear and concise release notes that effectively communicate changes and issues to users.", "7d220d16-3fef-4983-8dea-be591a76445f": "To subscribe a GitHub App to receive webhook payloads for specific events, you need to follow these steps:  1. Go to your GitHub App's registration page. 2. Scroll down to the \"Subscribe to events\" section. 3. Select the events you want your app to receive payloads for. For example, if you want your app to receive a payload whenever a new issue is opened, select \"Issues\". 4. Save your changes.  The permissions required for each event are listed below:  - Issues: Your app needs the \"Read & write\" permission for the repository where the event occurred. - Pull requests: Your app needs the \"Read & write\" permission for the repository where the event occurred. - Push: Your app needs the \"Write\" permission for the repository where the event occurred. - Release: Your app needs the \"Read\" permission for the repository where the event occurred. - Deployment: Your app needs the \"Read\" permission for the repository where the event occurred. - Page build: Your app needs the \"Read\" permission for the repository where the event occurred. - Status: Your app needs the \"Read\" permission for the repository where the event occurred.  Make sure to grant your app the necessary permissions before subscribing to the events.", "1431ef6b-1abf-49cc-8f8a-6a3b245980a5": "A diagram can help explain a concept to an audience by providing a visual representation of complex information, making it easier for some people to understand and remember. When evaluating the effectiveness of a diagram, factors to consider include the audience's prior knowledge, the scope of the information being presented, and the purpose of the diagram within the context. The diagram should complement the accompanying text and provide a purpose that it accomplishes with the text. It should also be evaluated by someone else to determine if it explains the expected information to the specified audience with the appropriate level of detail. Different types of diagrams may be more effective for different purposes, and the choice of diagram should be based on the specific acceptance criteria. If the context doesn't provide enough information to answer the question, it's best to say that you weren't given enough information to give an answer.", "435a0c21-94f0-4714-8dbc-fe732dce3fe1": "To write a script to find and redeliver failed deliveries for an organization webhook using {% data variables.product.prodname_actions %}, you'll need a {% data variables.product.pat_generic %} with specific access permissions. Here's how to set it up:  1. Create a {% data variables.product.pat_generic %} with the following access:     - For a {% data variables.product.pat_v2 %}:      - Set resource owner to be the organization where your webhook was created      - Grant the token access to the repository where this workflow will run      - Grant the token write access to the organization webhooks permission      - Grant the token write access to the repository variables permission     - For a {% data variables.product.pat_v1 %}, grant the token the `admin:org_hook` and `repo` scope.  2. Store the {% data variables.product.pat_generic %} as a {% data variables.product.prodname_actions %} secret. To do this:     - Go to your repository's Settings tab    - Click on Secrets in the left-hand menu    - Click on New repository secret    - Enter a name for the secret (e.g., GITHUB_TOKEN)    - Paste the {% data variables.product.pat_generic %} into the Value field    - Click on Add secret  3. Write your script to find and redeliver failed deliveries. Here's an example:     ```javascript    const github = require('@actions/github');    const core = require('@actions/core');    const { Octokit } = require('@octokit/rest');     const octokit = new Octokit({      auth: core.getInput('github-token', { required: true }),    });     const context = github.context;     async function run() {      const webhookId = context.payload.webhook.id;      const webhookDeliveryId = context.payload.webhook.delivery.id;      const webhookDeliveryStatus = context.payload.webhook.del", "61fa1f2f-a796-44cb-b679-4cb93764e9d5": "The nodes designated as search nodes in the context provided are \"ghes-search-node-1\", \"ghes-search-node-2\", and \"ghes-search-node-3\". These nodes provide the service of Elasticsearch server, as indicated by the `elasticsearch-server = true` configuration in their respective cluster definitions.", "f4905520-ba2b-4072-b139-6680c658d019": "In the context provided, the JWT (JSON Web Token) is a token issued by the {% data variables.product.prodname_dotcom %} OIDC provider that contains information about the user, such as their identity and permissions. This token is used to authenticate the user to HashiCorp Vault and fetch an access token, which is then used to access the specific secrets in Vault. The access token is specific to the user's session and has a limited lifespan, while the JWT is a longer-lived token that can be used to fetch multiple access tokens. In summary, the JWT is used to authenticate the user, while the access token is used to authorize access to specific resources in HashiCorp Vault.", "bfd1bc3e-50b3-4421-9d39-97bf1ceaf7dd": "The advantage of using strongly typed schemas in GraphQL is that it ensures data handling safety by preventing errors caused by incorrect data types. This can be demonstrated through an example mutation where we add a comment to an issue or pull request. In the following mutation, we mistakenly specify an integer instead of a string for the value of `clientMutationId`:  ```graphql mutation {   createIssueComment(input: { body: \"This is a comment.\", issueNumber: 123 }) {     clientMutationId     comment {       body     }   } } ```  In this example, the `clientMutationId` field is expected to be a string, but we pass an integer instead. When we execute this mutation, GraphQL will return an error message:  ```json {   \"errors\": [     {       \"message\": \"Expected String, found Int.\",       \"locations\": [         {           \"line\": 3,           \"column\": 11         }       ],       \"path\": [         \"createIssueComment\",         \"clientMutationId\"       ]     }   ] } ```  This error message clearly indicates the issue with our input, making it easier to identify and fix the problem.  In contrast, if we were using a REST API, we might not receive such a clear error message, making it harder to identify and fix the issue.    Example: Querying with variables  GraphQL allows you to pass variables to queries and mutations, making them more flexible and reusable.  For example, instead of repeating the same query multiple times with different values, you can define variables and use them in your queries:  ```graphql query($owner: String!, $repo: String!) {   repository(owner: $owner, name: $repo) {     name     description     url   } } ```  You can then execute this query with different values for the variables:  ```graphql {   \"query\": \"{ repository(owner: \\\"octocat\\\", name: \\\"Hello-World\\\") { name description url } }", "14f76f6b-0adb-4e4f-9290-210ea1a61649": "The `refs/pull/origin/` namespace in Git is significant because it is used to store references to pull requests from a remote repository named \"origin\". These references are local to the current repository and are not directly affected by calls to `git-remote`, which are used to manage remote references. In contrast, remote references are references that point to specific commits or branches in a remote repository. They are managed by `git-remote` and are updated when fetching or pulling changes from the remote repository. The `refs/pull/origin/` namespace is unique in that it is specific to pull requests and is not directly related to remote references.", "7f6d491c-47bc-4770-b0d1-a6e16cefdd5c": "Both CircleCI and {% data variables.product.prodname_actions %} provide mechanisms to persist data between jobs. In CircleCI, this is achieved through the `persist_to_workspace` and `attach_workspace` commands, while in {% data variables.product.prodname_actions %}, it is done using the `upload-artifact` and `download-artifact` actions. The syntax for each system is provided in the context information.", "75fe75de-c039-407f-b4c3-5cfaa2cc8a6d": "In the {% data variables.product.prodname_dotcom %} REST API, there are three types of parameters: path parameters, body parameters, and query parameters.  Path parameters modify the endpoint path and are required in the request. For example, in the \"List repositories\" endpoint, the path parameter `{owner}` represents the owner of the repository, and the path parameter `{repo}` represents the name of the repository. This allows you to specify a specific repository to list.  Body parameters allow you to pass additional data to the API and can be optional or required, depending on the endpoint. For example, in the \"Create an issue\" endpoint, the body parameter `title` is required to create a new issue, and the body parameter `body` is optional to add text to the issue body.  Query parameters allow you to control what data is returned and are usually optional. For example, in the \"List public events\" endpoint, the query parameter `per_page` allows you to specify the number of issues to return per page.  Here's an example of a request that uses all three types of parameters:  GET /repos/{owner}/{repo}/issues?state=open&per_page=100&page=1  In this example, the path parameters `{owner}` and `{repo}` represent the owner and name of the repository, respectively. The query parameters `state`, `per_page`, and `page` allow you to filter the issues to return, specify the number of issues to return per page, and specify the page number to return, respectively.  If you're not given enough information to determine the types of parameters used in a specific endpoint, then you can't provide an answer.", "6b682749-acce-4770-b30f-13110db17f4c": "Unfortunately, without further context, it is unclear how the feature \"AUTOTITLE\" specifically provides contextual information to the user. The given context information only mentions \"AUTOTITLE\" in passing and does not elaborate on its functionality or how it contributes to the overall context of the situation. More context is needed to fully understand the role of \"AUTOTITLE\" in this scenario.", "1f2955f2-b4cd-4d05-8b72-84b98c93eb9b": "Developers can update references to legacy IDs in their applications to use the new ID format provided by the GitHub API's `X-Github-Next-Global-ID` header by making a request to the API with the legacy ID and including the `X-Github-Next-Global-ID` header set to `1`. The response will contain the new ID format for the legacy ID, which can then be used in subsequent requests to the API. This process should be repeated for all references to legacy IDs in the application.", "20a1fd86-3b0b-4bbe-9aca-1e0a2247187b": "To run a script after the container is created, you can use the \"postCreateCommand\" property in the devcontainer.json file. Here's an example:  ```jsonc \"postCreateCommand\": \"sudo chmod a+x \\\"$(pwd)\\\" && sudo rm -rf /var/www/html && sudo ln -s \\\"$(pwd)\\\" /var/www/html\" ```  In this example, the script will change the permissions of the current directory, remove the existing content of the `/var/www/html` directory, and create a symbolic link to the current directory. This script can be customized based on your specific requirements. ```", "fe79cc6f-e7ee-43c1-a5b5-759b8b438213": "Based on the given context, it is unclear how the concept of \"AUTOTITLE\" differs from \"AUTOTITLE\". Both terms are mentioned without any further explanation or context provided. Without prior knowledge or additional information, it is impossible to determine any differences between the two concepts. It is recommended to provide more context or clarify the meaning of these terms to better understand how they differ, if at all.", "4e15c1f0-1485-4582-bb4d-4386d27faa63": "To switch to a different account on {% data variables.product.prodname_mobile %} if you no longer need access to it, you can sign out of the account by accessing the account switcher. First, navigate to the {% octicon \"person\" aria-hidden=\"true\" %} **Profile** tab, then tap {% octicon \"gear\" aria-hidden=\"true\" %}, and then tap **Manage Accounts**. From there, you can log into other accounts or log out of an existing account by swiping left on the account you want to sign out of, and then tapping **Sign out**. Alternatively, if you're already logged into one account, you can long-press {% octicon \"person\" aria-hidden=\"true\" %} **Profile** at the bottom of the app, swipe left on the account you want to sign out of, and then tap **Sign out**.", "5688937f-e212-4d4f-a9ce-7731180f9695": "According to section 1.6.2, GitHub will only provide Support for a given Release of the Software for one (1) year from the original Release date, or six (6) months from the last Update of the Release, whichever is longer. If Customer requires Support for earlier Releases of the Software, then they must pay for that Support in accordance with the terms of a mutually agreed upon Order Form or SOW. Therefore, to obtain Support for earlier Releases, Customer needs to negotiate and agree upon the terms of an Order Form or SOW with GitHub, which will outline the pricing and duration of the Support for the earlier Release.", "58192f81-3bf4-4702-a6cf-c55aaf927eba": "Without further context, it is unclear what \"AUTOTITLE\" refers to and what its meaning is. It could potentially be a technical term related to software or hardware, but without additional information, it is impossible to provide a summary in one sentence.", "95d36f02-f02c-4f1e-aa35-eeef68eac764": "Product callouts should be used to convey the availability of a feature in specific products only when it cannot be conveyed by versioning alone. This is important because it helps readers understand which products the feature is available in and avoid confusion with features that may have similar names but are only available in different products. Product callouts follow a strict format, clearly identifying the feature and which products it's available in, and include links to \"GitHub's products\" and sometimes to other relevant articles. By using product callouts consistently, we can ensure that our content is accurate and helpful to all of our users, regardless of which product they are using.", "a0247e97-3220-4200-b49e-4ce66cbba802": "To specify build commands for a C++ project using the `make` tool in {% data variables.product.prodname_codeql %}, you can use the `--command` option with the `make` command as an argument, like this:  ```shell codeql database create cpp-database --language=c-cpp --command=make ```  This will create a new {% data variables.product.prodname_codeql %} database for your C++ project and use the `make` command to build it. If you need to pass additional arguments to `make`, you can quote the entire argument to ensure that it is interpreted correctly.", "bf5941dc-4fa1-4fdd-948f-00503aa8c1c2": "To review recent commits made to repositories, you can access the repository's commit history through the web interface or a command-line tool like Git. When reviewing commits, you should look for any suspicious activity, such as unauthorized changes to sensitive files, new files added without explanation, or changes made outside of normal working hours. Additionally, you should ensure that all commits have clear and descriptive commit messages that accurately reflect the changes made. If you notice any unusual activity or suspect that a commit was made maliciously, you should investigate further and take appropriate action, such as rolling back the commit or contacting the responsible party.", "0db52453-c624-4ea9-8a02-44c255892044": "A {% data variables.secret-scanning.alerts %} can be closed as \"pattern edited\" when the secret pattern that was detected in the code has been edited or modified in a way that no longer matches the original pattern. This could mean that the secret has been renamed, moved to a different location, or encrypted in a different way. When a secret pattern is edited, the {% data variables.secret-scanning.alerts %} is closed as \"pattern edited\" to indicate that the original issue has been resolved.", "3fcad57a-d1fc-498c-844b-88f42e36b1fc": "Based on the provided context information, it is unclear how the repeated use of \"AUTOTITLE\" contributes to the overall meaning and purpose of the text. Without further context or information, it is impossible to determine whether \"AUTOTITLE\" is being used as a literary device, a formatting instruction, or for some other purpose. More context is needed to fully understand the significance of this repeated phrase.", "9ac53e11-4da9-402c-ba01-2bca49454c49": "GitHub has been appointed as the authorized developer with respect to Apple software included in Actions. This means that GitHub is authorized to work with and access Apple software that is part of Actions. GitHub's responsibility in this regard is to comply with the terms for any such software included in Actions and to keep confidential any confidential information of Apple that is accessed as part of Actions.", "ff86a3aa-9018-404c-afcc-2f858d060aeb": "To authenticate your workflow to publish a package to a different repository in the {% data variables.product.prodname_registry %} registry, you need to use a {% data variables.product.pat_v1 %} that has permission to write to packages in the destination repository. Here's how to set it up:  1. Create a new personal access token (PAT) with the \"write:packages\" permission. You can do this by going to your account settings, selecting \"Developer settings,\" then \"Personal access tokens,\" and creating a new token.  2. Store the PAT as a secret in your repository. Go to your repository settings, select \"Secrets,\" then \"New repository secret,\" and enter a name for the secret (e.g., \"GITHUB_PACKAGES_TOKEN\") and the PAT value.  3. In your workflow file, reference the PAT as an environment variable. Here's an example:  ```yaml name: Publish package to GitHub Packages on:   release:     types: [published] jobs:   build:     runs-on: ubuntu-latest     permissions:       contents: read       packages: write     steps:       - uses: {% data reusables.actions.action-checkout %}       - name: Set up GitHub Packages         uses: {% data reusables.actions.action-setup-github-packages %}         with:           token: ${{ secrets.GITHUB_PACKAGES_TOKEN }}       - name: Install dependencies         run: yarn install       - name: Publish package         run: yarn publish --registry=https://npm.pkg.github.com ```  In this example, we're using the `action-setup-github-packages` action to set up GitHub Packages and pass the PAT as an environment variable. We're also using the `yarn publish` command to publish the package, specifying the registry URL for GitHub Packages.  4. To set up an `.npmrc` file to publish to GitHub Packages using the PAT in your workflow, you can use the `setup", "dbd8edc4-1de0-42c9-a152-78d275111129": "To verify if the SSH configuration file exists in the default location, you can open your terminal and run the following command:  ```shell $ open ~/.ssh/config ```  If the file exists, it will open in your default text editor. If the file doesn't exist, you'll see an error message like this:  ```shell $ open ~/.ssh/config The file /Users/YOU/.ssh/config does not exist. ```  In this case, you can create the file using the `touch` command:  ```shell $ touch ~/.ssh/config ```  After creating the file, you can open it and make the necessary modifications.", "636a61dd-c5cb-4151-b67e-763163c83cfa": "The context information provided doesn't explicitly recommend whether it's recommended to set the \"--target-sha\" option or not. However, it does mention that \"--target-sha\" is an alternative to \"--target-dbscheme\" and provides a brief explanation of what it does. Without further context, it's unclear whether using \"--target-sha\" is preferred over \"--target-dbscheme\" in a specific scenario. It's best to consult the documentation or contact support for guidance on when to use \"--target-sha\" instead of \"--target-dbscheme\". Therefore, I would say that we weren't given enough information to give an answer regarding the recommended approach for setting the \"--target-sha\" option in this context.", "cec1dd98-5866-49d7-9c59-c798181558c3": "To unlock repositories on a {% data variables.product.prodname_dotcom_the_website %} organization using the migration unlock endpoint, follow these steps:  1. Obtain an access token for authentication. You can generate a new token in your {% data variables.product.prodname_dotcom_the_website %} settings by going to \"Developer settings\" > \"Personal access tokens\" > \"Generate new token\".  2. Send a `DELETE` request to the migration unlock endpoint using `curl` or a similar tool. The request should include the following headers and parameters:     - `Authorization`: Set this header to `Bearer GITHUB_ACCESS_TOKEN`, where `GITHUB_ACCESS_TOKEN` is the access token you obtained in step 1.    - `Accept`: Set this header to `application/vnd.github.wyandotte-preview+json`.    - `ID`: Replace `ID` with the unique ID of the migration you want to unlock. You can find this ID in the URL of the migration's page on {% data variables.product.prodname_dotcom_the_website %}. For example, if the migration's URL is `https://github.com/ORG-NAME/migrations/123456789`, then the ID is `123456789`.    - `REPO_NAME`: Replace `REPO_NAME` with the name of the repository you want to unlock.     Here's an example `curl` command:     ```shell    curl -H \"Authorization: Bearer GITHUB_ACCESS_TOKEN\" -X DELETE \\      -H \"Accept: application/vnd.github.wyandotte-preview+json\" \\      https://api.github.com/orgs/ORG-NAME/migrations/ID/repos/REPO_NAME/lock    ```  3. Repeat step 2 for each repository you want to unlock.  4. After unlocking the repositories, you should delete them from the organization using the repository delete endpoint. Here's an example `curl` command:", "ce3878fc-de31-4b07-afa4-7d0e0964574f": "To rebuild a codespace using the {% data variables.product.prodname_cli %}, you can use the `gh codespace rebuild` command. If you want to perform a full rebuild, you can add the `--full` flag at the end of the command. The codespace will use the `devcontainer.json` file that is currently saved in the codespace to rebuild. For more information, you can refer to the {% data variables.product.prodname_cli %} online help or use `gh codespace rebuild --help` for specific subcommand help.", "cfd65ec3-0fec-4fa8-a094-1c42f0f5762f": "According to the context information provided, third parties processing payments for GitHub may include restrictions that block payments from Iran. This means that while GitHub has restored all cloud services to Iranian users, including paid services, the availability of those paid services may still be impacted by the restrictions imposed by the third parties processing payments. Therefore, Iranian users may encounter difficulties in making payments for GitHub's paid services due to these restrictions.", "b492db6b-4158-4994-8ea9-5d0b89dc0f46": "The number of Maven packages owned by organizations currently available in {% data variables.product.prodname_registry %} for {% data variables.location.product_location %} is provided by the variable BL from the context information. Therefore, the answer is BL.", "ff170a48-eda1-484e-9f18-eafeabd3059a": "To ensure that sensitive data is not accidentally committed to a Git repository, you can follow these specific steps and use the following tools:  1. Add a `.gitignore` file to your repository and include the path to the file containing sensitive data. This will prevent the file from being added to the repository during future commits.  2. Use the `git filter-repo` tool to remove the sensitive data from the repository's history. This tool allows you to filter commits based on specific criteria, such as file paths or commit messages.     Here's an example command to remove a file called `secrets.txt` from the repository's history:     ```shell    $ git filter-repo --force --invert-path secrets.txt    ```     This command will remove all commits that added or modified the `secrets.txt` file, but it will preserve all other commits in the repository.  3. Double-check that you've removed everything you wanted to from the repository's history, and that all of your branches are checked out.  4. The `git filter-repo` tool will automatically remove your configured remotes. To restore your remotes, use the `git remote set-url` command, replacing `OWNER` and `REPO` with your repository details:     ```shell    $ git remote add origin https://github.com/OWNER/REPOSITORY.git    ```  5. Once you're happy with the state of your repository, and you have set the appropriate remote, force-push your local changes to overwrite your repository on {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %}, as well as all the branches you've pushed up. A force push is required to remove sensitive data from your commit history.     ```shell    $ git push origin --force --all    ```  To restore remote URLs after using the `git filter-repo` tool to remove sensitive data from a Git repository's history, you can use the `git remote set-url` command to add the remote URL again. Here's an example command:  ```", "bc39e857-cd65-4641-bc97-d9d1a584a1f9": "To control how {% data variables.product.product_name %} displays animated _.gif_ images, you can access the accessibility settings in your user settings. Under the \"Motion\" section, you can manage settings for motion and select from three options for displaying animated images: \"Sync with system\", \"Enabled\", or \"Disabled\". If your operating system has a preference for reduced motion, {% data variables.product.product_name %} will sync with that preference by default. If you want to customize this setting, you can select \"Sync with system\" to follow your operating system's preference, \"Enabled\" to display animated images, or \"Disabled\" to prevent animated images from playing.", "e8f08b52-ab1b-4478-bc43-fbc5149437be": "To manage access to groups of repositories in {% data variables.product.prodname_dotcom %} after migrating from Bitbucket Server, you can create teams on {% data variables.product.prodname_dotcom %}. This allows you to grant access to multiple repositories at once, rather than managing access on a repository-by-repository basis. This approach is recommended because it simplifies access management and reduces the number of permissions that need to be granted. Additionally, it allows you to group repositories based on their functionality or purpose, making it easier to manage and maintain them over time.", "9f769fcd-fb2c-4122-ad06-ff386d9ce3b4": "According to Section Q of the agreement, GitHub provides Expert Services upon request from the customer. These services are detailed in a separate Statement of Work (SOW). GitHub controls the manner and means by which these services are performed and may use third parties to perform them. GitHub retains all right, title, and interest in and to anything used or developed in connection with performing these services, including software, tools, specifications, ideas, concepts, inventions, processes, techniques, and know-how. If GitHub delivers anything to the customer while performing these services, GitHub grants the customer a non-exclusive, non-transferable, worldwide, royalty-free, limited-term license to use those deliverables during the term of the agreement, solely in conjunction with the customer's use of the Service.", "e10603cf-97be-4c09-b78c-00f8c58062f7": "Administrators can control access to runners using runner groups by collecting sets of runners and creating a security boundary around them. They can then decide which organizations or repositories are permitted to run jobs on those sets of machines. Only {% data variables.actions.hosted_runner %}s with Linux or Windows operating systems can be assigned to runner groups. This means that administrators can create a group, add runners to it, and then update workflows to target either the group name or the label assigned to the {% data variables.actions.hosted_runner %}. By doing so, they can ensure that only authorized organizations or repositories can run jobs on those sets of machines.", "7a9e6f3f-0522-4774-91fd-ade9308105bd": "The purpose of verifying a primary email address on a {% data variables.product.company_short %} account is to confirm the ownership of the email address and ensure its authenticity. This is important for security reasons, as it helps prevent unauthorized access to the account and protects against phishing attempts. To verify a primary email address, the user needs to follow the verification process provided by {% data variables.product.company_short %}. This typically involves clicking on a link sent to the email address or entering a verification code received via email or SMS. Once the email address is verified, the user can continue using their {% data variables.product.company_short %} account without any restrictions.", "2b971f7e-bd5a-486e-8ec6-dcb6653e08c4": "Yes, whenever possible, troubleshooting content should be contained within procedural content or guides. This helps people find solutions to their problems more quickly and easily, as they are already in the context of the task they are trying to complete. It also ensures that troubleshooting content is presented in a clear and concise manner, as it is integrated into the overall flow of the content. However, if the troubleshooting issue applies to a specific article or conceptual grouping of features, or if a product or feature has multiple known issues that should be grouped together, it may be appropriate to create a dedicated \"Known issues with NAME\" article. The goal is to provide people with the information they need to resolve their issues as quickly and efficiently as possible.", "e8f49753-149e-4a81-9dcc-ed66cc92ec86": "To merge updates made online with local work, you can use the `git merge` command. The syntax for specifying the branch name is `git merge origin BRANCH_NAME`, where `origin` is the name of the remote repository and `BRANCH_NAME` is the name of the branch you want to merge.  Alternatively, you can use the `git pull` command to fetch updates from the remote repository and merge them with your local work in a single command. The syntax for this command is `git pull origin BRANCH_NAME`.  In both cases, replace `origin` and `BRANCH_NAME` with the appropriate values for your specific repository and branch.", "4a0979e6-9a0a-4e0e-8c27-250056e2566e": "The \"--format\" option in the \"codeql resolve extractor\" command affects the output format of the command. The available options for this option are \"text\" (the default), \"json\", and \"betterjson\". If \"text\" is selected, the path to the found extractor pack is printed to standard output. If \"json\" is selected, the path to the found extractor pack is printed as a JSON string. If \"betterjson\" is selected, details about the found extractor pack are printed as a JSON string.", "f0ae2372-8ced-42f5-bbb4-0905b7ea34a9": "According to the context information provided, there is no specified maximum number of sponsorship tiers that can be created. However, it is recommended to keep the number of tiers manageable and easy to understand for potential sponsors. Each tier should include a clear description of the benefits and rewards that sponsors will receive at that level. This can include access to exclusive features, priority support, or recognition on the project's website or social media channels. It's also important to include a recommended or minimum sponsorship amount for each tier, as well as any custom amounts that sponsors can choose to contribute. Additionally, a welcome message can be added to each tier to provide a personalized greeting to new sponsors.", "60d1319d-4d37-4f58-ac46-fd0a37c17972": "The outcome of \"Rewrite commits for large files\" in the given context is to update the commit history of a repository to replace the original commits that contain large files with new commits that reference the same changes but with the large files replaced by references to their corresponding Git LFS (Large File Storage) objects. This is necessary because Git's default behavior is to store large files in the repository itself, which can lead to slow performance, increased storage usage, and longer pull and push times. By using Git LFS, large files are stored externally and referenced by Git, which improves performance and reduces storage usage. The \"Rewrite commits for large files\" step is typically performed after opting in to Git LFS and getting large files, and before updating the repository on GitHub.", "c9c45c00-635c-4d66-a9f4-d9a657cadd5a": "Organization members with admin permissions to a repository should be granted the permission to delete or transfer repositories if the organization owner has selected the option \"Allow members to delete or transfer repositories for this organization\" under \"Repository deletion and transfer\" in the organization settings. If this option is not selected, then only organization owners will have the ability to delete or transfer repositories.", "b8c0af07-e1f2-49a7-a6d5-9f8816c1d379": "The purpose of setting up blob storage in the context of the enterprise migration tool is to provide an intermediate location for storing data that can be accessed by GitHub.com, as many GitHub Enterprise Server instances are behind firewalls. This allows for the efficient transfer of large amounts of data during the migration process. The supported cloud providers for blob storage include Amazon Web Services (AWS) Simple Storage Service (S3) and Microsoft Azure Blob Storage. The credentials for the storage provider must be configured in either the GitHub Enterprise Server management console or the GitHub CLI, depending on the version of the server being used.", "bcf258b2-798a-433d-a183-40603ba63af5": "The decoding process in BQRS converts the result data from BQRS into other forms using the codeql bqrs decode command. This command allows users to select a particular result set, sort the selected result set by specific columns, and choose the desired output format. The decoding output is written to standard output by default, but users can specify a file to write the output to using the --output option. The available output formats include text, CSV, JSON, and BQRS. Users can also control how result columns of entity type are shown using the --entities option. Additionally, options for pagination are available for interactive front-ends using the --rows option. If no result set is selected, all result sets will be decoded, provided the selected output format and processing options support that. If sort directions are not specified, then ascending order will be used for all columns. The --sort-key and --sort-direction options can be used to sort the selected result set by specific columns and directions, respectively. The codeql bqrs info command can be used to list the available result sets.", "d6fefdbe-e995-4d6d-969e-b0223182ef2f": "To manage scheduled reminders for your personal account in {% data variables.product.company_short %}'s products, you should use the term \"managing scheduled reminders for your personal account\" instead of \"managing scheduled reminders for your user account\". This applies to both {% data variables.product.prodname_ghe_cloud %} and {% data variables.product.prodname_ghe_server %}.", "499597b0-8f87-4182-80d7-db8cf79e5e1f": "The `sort:most-important` and `sort:severity` sorting options both prioritize security issues based on their severity, but there is a difference in how they determine importance.  The `sort:most-important` sorting option takes into account not just the severity of the vulnerability, but also its impact, relevancy, and actionability. This means that it considers factors beyond just the CVSS score, which can provide a more comprehensive view of the urgency and potential impact of the security issue.  On the other hand, the `sort:severity` sorting option only considers the severity of the vulnerability, as determined by its CVSS score. While this can still be a useful metric for prioritizing security issues, it may not capture the full picture of the issue's importance.  In general, the `sort:most-important` sorting option would be more useful for prioritizing security issues because it provides a more holistic view of the issue's importance. This can help developers and security teams to more effectively allocate resources and address the most pressing security issues first.  However, the specific sorting option that is most useful will depend on the organization's specific needs and priorities. Some organizations may prefer to prioritize issues based solely on severity, while others may find the additional factors considered by `sort:most-important` to be more relevant. It's important to choose the sorting option that best aligns with the organization's security strategy and risk management approach.", "81e1a572-37f8-44b4-a4e9-a013def15626": "To configure the scope mapping for a project using an _.npmrc_ file in order to prevent accidental publishing to npmjs.org instead of {% data variables.product.prodname_registry %}, you can follow these steps:  1. Authenticate with {% data variables.product.prodname_registry %} using your GitHub credentials. 2. Create an _.npmrc_ file in the root directory of your project. 3. Add the following lines to the _.npmrc_ file:     ```    @my-org:registry=https://npm.pkg.github.com    ```     Replace `my-org` with your GitHub organization's name.  4. Add the _.npmrc_ file to your project's .gitignore file to prevent it from being committed to the repository. 5. Verify the name of your package in your project's `package.json`. The `name` field must contain the scope and the name of the package. For example, if your package is called \"test\", and you are publishing to the \"My-org\" GitHub organization, the `name` field in your `package.json` should be `@my-org/test`. 6. Publish your package using the `npm publish` command.  By following these steps, you can ensure that your package is published to {% data variables.product.prodname_registry %} instead of npmjs.org, preventing accidental publishing to the wrong registry.", "5f013d6c-6726-47e9-ad89-43fae1ea76a8": "To create a new VM using the appliance image for {% data variables.product.prodname_ghe_server %} on Azure, follow these steps:  1. Find the most recent {% data variables.product.prodname_ghe_server %} appliance image. You can do this by running the following command in your terminal:     ```shell    az vm image list --all -f GitHub-Enterprise | grep '\"urn\": \"GitHub:' | sort -V    ```  2. Create a new VM using the appliance image you found. Run the following command in your terminal, replacing the placeholders with your preferred values:     ```shell    az vm create --name <VM_NAME> --resource-group <RESOURCE_GROUP> --size <VM_SIZE> --location <REGION> --image GitHub-Enterprise:<IMAGE_VERSION> --storage-sku Premium_LRS    ```     Here's an example command:     ```shell    az vm create --name ghe-vm --resource-group ghe-rg --size Standard_DS2_v2 --location eastus --image GitHub-Enterprise:2.22.0 --storage-sku Premium_LRS    ```     For more information about the `az vm create` command, see \"az vm create\" in the Microsoft documentation.", "86a589bd-a95a-41d1-be5d-2c432ed88d12": "According to the provided context information, if the desired username on GitHub is already taken, you should consider other names or unique variations, such as using a number, hyphen, or an alternative spelling, to identify a desirable username that's still available. Additionally, you may want to check if the account is inactive or if the user is violating any policies, such as the Trademark Policy or Name Squatting Policy, before giving up on the desired username.", "40079179-174f-4315-9376-142452cd7609": "Adding labels, milestones, assignees, and projects to an issue in GitHub serves multiple purposes. Labels help categorize the issue, making it easier to search and filter for specific types of issues. Milestones track the progress of the issue towards a specific target date. Assignees communicate responsibility for the issue, making it clear who is working on it. Projects allow issues to be organized and associated with a specific project, making it easier to manage and track related issues. All of these features help improve communication, organization, and efficiency in collaborative development workflows.", "0e56530c-73ad-4805-921a-a67ed4d5c9f0": "After opening a pull request in Step 7: Test your app, you can test your app by navigating to your webhook proxy URL on smee.io. You should see a `pull_request` event, indicating that {% data variables.product.company_short %} successfully sent a pull request event when you created a pull request. In the terminal where you ran `npm run server`, you should see a message like \"Received a pull request event for #1\", where the integer after the `#` is the number of the pull request that you opened. Additionally, in the timeline of your pull request, you should see a comment from your app using the contents of the `message` field.", "a14b3ddd-bdf4-4eb0-9f1b-88b721eed3b4": "To create a query suite that selects queries with a specific kind and precision level from a local directory in CodeQL, you can define a query suite using a YAML file with the following structure:  ```yaml - queries: my-custom-queries - include:     kind: problem     precision: very-high ```  Replace `my-custom-queries` with the name of the directory containing your custom queries, and replace `problem` and `very-high` with the desired query kind and precision level, respectively. This will select all queries with the specified kind and precision level from the `my-custom-queries` directory.  Note that you can also exclude queries with a specific tag or severity level using the `exclude` instruction, and import existing query suite definitions using the `import` instruction. For more information, refer to the CodeQL documentation on query suite definitions.", "49acd9a8-1496-4319-9d5f-e35dd63a2d94": "The conversion process of the pe of properties {% data variables.product.prodname_actions_importer %} differs for Freestyle pipelines and Jenkinsfile pipelines in terms of the supported syntax.  For Freestyle pipelines, the following syntax is supported:  - docker template: `jobs..container` - build: `jobs` - build environment: `env` - build triggers: `on` - general: `runners`  For Jenkinsfile pipelines, the following syntax is supported:  - docker: `jobs..container` - stage: `jobs.` - agent: `runners` - environment: `env` - stages: `jobs` - steps: `jobs..steps` - triggers: `on` - when: `jobs..if`  The `github/gh-actions-importer` repository currently supports the following Jenkins plugins for converting Freestyle pipelines to GitHub Actions:  - Jenkins Pipeline: Declarative Pipeline plugin - Jenkins Pipeline: Pipeline plugin - Jenkins Pipeline: Pipeline Step plugin - Jenkins Pipeline: Pipeline Utility Steps plugin - Jenkins Pipeline: Shared Groovy Steps plugin - Jenkins Pipeline: Step Matrix plugin - Jenkins Pipeline: Step Matrix Plugin for Declarative Pipelines plugin - Jenkins Pipeline: Step Matrix Plugin for Pipeline plugin plugin - Jenkins Pipeline: Step Matrix Plugin for Pipeline Step plugin plugin - Jenkins Pipeline: Step Matrix Plugin for Shared Groovy Steps plugin plugin - Jenkins Pipeline: Step Matrix Plugin for Step Matrix plugin plugin - Jenkins Pipeline: Step Matrix Plugin for Step Matrix Plugin for Declarative Pipelines plugin plugin - Jenkins Pipeline: Step Matrix Plugin for Step Matrix Plugin for Pipeline plugin plugin - Jenkins Pipeline: Step Matrix Plugin for Step Matrix Plugin for Pipeline Step plugin plugin - Jenkins Pipeline: Step Matrix Plugin for Step Matrix Plugin for Shared Groovy Steps plugin plugin - Jenkins Pipeline: Step Matrix Plugin for Step Matrix Plugin for", "010371d9-d55f-470c-b83d-6e818ceab59c": "To move an enterprise runner to a group in GitHub Actions, follow these steps:  1. Access your enterprise account. 2. Go to the Policies tab. 3. Click on the Actions tab. 4. Click on the Runners tab. 5. Find the runner you want to move and click on the three dots next to it. 6. Select Move from the drop-down menu. 7. Select the group you want to move the runner to. 8. Click on Move runner to group.  Alternatively, you can also move an organization runner to a group by following these steps:  1. Go to the organization you want to manage. 2. Click on Settings. 3. Click on Actions in the left-hand menu. 4. Click on Runner groups. 5. Click on the group you want to move the runner to. 6. Click on Add runners. 7. Select the runner you want to move. 8. Click on Add runners.  Note that these steps may vary slightly depending on the version of GitHub Actions you are using. If you are using GitHub Enterprise Cloud (GHEC), the steps may be slightly different, but the overall process is similar.", "859494bf-d63f-4429-9157-9877f2af13e8": "Repository autolinks in the context of GitHub apps serve to streamline workflow by automatically linking external resources such as JIRA issues and Zendesk tickets to a repository. This feature can be utilized through the REST API and is available for use with {% data variables.product.prodname_github_apps %} that have repository administration permissions with read or write access. The purpose of this functionality is to provide a more efficient and connected experience for developers working on related issues or tickets across multiple platforms.", "eaf15b88-32ee-4205-90db-e813890c0e53": "Yes, based on the provided context, I can summarize the responsibilities of the customer regarding user-generated content and ownership of content in the context of using the GitHub service. The customer is solely responsible for any user-generated content they post, upload, link to, or otherwise make available via the service. The customer retains ownership of their own content, and GitHub is not responsible for any public display or misuse of such content.", "0914003d-9e67-40ce-8fb0-036d3b07197b": "Selecting whether to import large files using Git Large File Storage during the import process is significant because it allows for the import of files larger than 100 MB, which may not be possible with traditional Git storage. This feature, called Git Large File Storage, is useful for projects that contain large files, such as media files or databases, as it provides a more efficient and scalable way to manage these files in a Git repository. By selecting this option during the import process, the user can ensure that all of their project's files, including the large ones, are properly imported into the new GitHub repository.", "4ec2ab38-7257-4fc2-ac09-f6dd8454a60a": "Without further context, it is unclear what \"AUTOTITLE\" refers to or what its meaning is. The provided context does not offer any clues or explanations to help determine its significance.", "cbb0fe79-5a38-4213-8846-01a929612759": "Upon termination of service, within 30 days of such termination, Company will provide the federal government with all user-generated content that is publicly visible through the Sites created by the federal government at Company, in a commonly used file or database format as Company deems appropriate, as long as providing the data does not violate Company's privacy policy.", "d6a9de76-9dd2-43eb-9ebe-b7d6bc3e2acf": "Based on the Code of Ethics provided, an example of an acceptable gift for a non-government official in a commercial transaction would be a promotional item with the company logo, such as a branded pen or USB drive, as long as it is of minimal value and does not exceed the company's gift policy limits. However, it's always best to consult with the company's gift policy guidelines for specific details and limitations.", "24bb6e8e-4f77-4b31-9b55-84854153add6": "To create a new repository for your GitHub Pages site, follow these steps:  1. Go to your GitHub account and click on the plus sign (+) in the top right corner of the page. 2. Select \"New repository\" from the drop-down menu. 3. Enter a name for your repository and choose whether you want it to be public or private. 4. Leave the \"Initialize this repository with a README\" option checked. 5. Click on the \"Create repository\" button to create your new repository.  Alternatively, you can also create a new repository from the GitHub desktop app or the command line using the `git init` and `git add` commands.  Remember to replace \"REPOSITORY-NAME\" in the command line instructions with the name you chose for your repository.", "92ef58ef-da0b-4aba-88b3-867be45a5e84": "To update the container image using the {% data variables.product.prodname_actions_importer %} CLI command in your terminal, run the following command:  ```shell gh actions-importer update ```  This command will connect to {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %} and ensure that the container image is updated to the latest version. The output of the command should indicate whether the image is up-to-date or not.", "2db0b760-60e8-443a-8a4a-4b33c0b72df1": "To authenticate your app using the device flow in GitHub, you can follow these steps:  1. Create a new {% data variables.product.prodname_github_app %} in your organization's settings. 2. Generate a new client ID and secret for your app. 3. Register your app's redirect URI. 4. In your app, initiate the device flow by redirecting the user to the authorization URL with the necessary parameters. 5. After the user authorizes your app, GitHub will redirect the user back to your app with an authorization code. 6. Exchange the authorization code for an access token using the device flow token endpoint. 7. Store the access token securely and use it to make API requests on behalf of the user. 8. Periodically refresh the access token using the device flow token endpoint to ensure it remains valid.  In this tutorial, we demonstrated how to write a CLI that uses the device flow to generate a user access token. You can expand this CLI to accept additional commands and adjust the code to meet your app's needs. Remember to update your app's permissions if your app needs additional permissions for the API requests that you want to make. Additionally, you should securely store tokens and follow best practices with your {% data variables.product.prodname_github_app %}.", "258f354a-5d57-459e-8a43-1b9709556980": "The system suggests fixes for {% data variables.product.prodname_code_scanning %} alerts through a combination of static analysis and machine learning. The system identifies potential security vulnerabilities in code using {% data variables.product.prodname_codeql %}, a powerful static analysis tool. It then uses machine learning to suggest potential fixes for these vulnerabilities.  However, developers should consider several limitations when reviewing these suggestions. Firstly, the system does not suggest fixes for all types of {% data variables.product.prodname_code_scanning %} alerts identified by {% data variables.product.prodname_codeql %}. Autofix is supported for a subset of the default {% data variables.product.prodname_codeql %} JavaScript/TypeScript queries, and the LLM (Large Language Model) is limited in its operational capacity. Secondly, each suggested fix is tested before it is added to a pull request. If no suggestion is available, or if the suggested fix fails internal testing, then no autofix suggestion is displayed.  Developers should also consider updating the CI testing and dependency management for a repository before enabling autofix for {% data variables.product.prodname_code_scanning %}. This is because the system's suggestions may not always be perfect, and developers should always review and test the suggested changes before committing them to the repository.  Finally, the system primarily uses English to suggest fixes, so developers should be aware of any potential language barriers when reviewing these suggestions. Developers should also consider the limitations of AI and edit the changes as needed before accepting the changes.  In summary, while the system's suggestions can be valuable in helping developers resolve {% data variables.product.prodname_code_scanning %} alerts, developers should always review and test the suggested changes before committing them to the repository, and consider the limitations of AI and the system's support for specific programming languages and human languages.", "d78d45a1-3f26-49b6-adc1-0972f9f4a868": "The {% data variables.product.pat_generic %} scopes required for downloading a migration log in a {% data variables.product.prodname_dotcom %} repository are `repo` and `read:org` for the destination organization, and `repo` and `read:org` for the repository being migrated.  However, if the context is referring to a migration log for an organization migration, then the required scopes for the destination enterprise are `read:enterprise` and `admin:org` for the organization being migrated, and `repo`, `admin:org`, and `workflow` for the destination organization.  If the context is referring to a migration log for a repository migration, then the required scopes for the destination organization are `repo`, `admin:org`, and `workflow` for the repository being migrated, and `repo` and `read:org` for the destination organization.  If the context is referring to a migration log for a migration from Azure DevOps, then the required scopes for the ADO {% data variables.product.pat_generic %} are `work item (read)`, `code (read)`, and `identity (read)`.  In summary, the required scopes for downloading a migration log depend on the specific type of migration being performed.", "5457c308-340d-4454-9b9d-e02a41955bba": "The best practice for managing secrets in source code, especially in repositories with multiple contributors, is to not include them in the source code. This is crucial for repositories that are publicly accessible and critical for public repositories. Automatic detection of secrets committed to a repository is available for certain plans, and users can also configure additional scanning to alert them about accidentally leaked secrets. It's also recommended to define custom patterns to detect additional secrets at the repository, organization, or enterprise level.", "77a080a7-b71d-429a-871f-d2178d0539e0": "{% data variables.product.prodname_actions %} can be utilized to automate various tasks and streamline the development process for actions. Here are some specific examples:  1. Continuous integration (CI): Use a workflow to automatically build, test, and lint your action whenever a new commit is pushed to a branch or a pull request is created. This ensures that your action is always in a working state and helps catch any issues early on.  2. Dependency updates: Use a workflow to automatically update your action's dependencies to the latest versions. This ensures that your action is always using the latest and most secure versions of its dependencies.  3. Release management: Use a workflow to automatically publish a new version of your action to {% data variables.product.prodname_marketplace %} when a new tag is pushed to your repository. This makes it easy for users to find and use your action.  4. Task automation: Use {% data variables.product.prodname_actions %} to automate repetitive tasks, such as generating documentation, running tests, or deploying your action to a production environment. This saves time and reduces the risk of errors.  To ensure confidence in the action's functionality, automated tests and build badges can be implemented. Here's how:  1. Add a workflow that automatically runs your action's tests whenever a new commit is pushed to a branch or a pull request is created. This ensures that your action's functionality is always tested and helps catch any issues early on.  2. Add a badge to your repository's README that displays the status of your action's tests. This makes it easy for users to see whether your action is currently passing its tests.  3. Use a workflow to automatically publish a new version of your action to {% data variables.product.prodname_marketplace %} only if all of its tests pass. This ensures that your action is always released in a working state.  In terms of community contributions, it's encouraged to welcome issues, pull requests, and vulnerability reports. Here's how to manage these contributions effectively:  1. Add a workflow that automatically runs your action's tests whenever a new pull request is created. This ensures that any changes made in the pull request don'", "3e213dae-d52f-45fe-a986-5034bc0c959c": "The `github.actor` context variable in a workflow run represents the username of the user or app that initiated the workflow run. This can be either the user or organization that created the workflow, or an app that triggered the workflow through an event. The value of this variable is available in all steps of a job, and can be used to perform actions based on the identity of the actor. If the workflow is triggered by an event, such as a push or pull request, the value of `github.actor` will be the username of the user or app that triggered the event. If the workflow is manually started, such as through the {% data variables.product.prodname_dotcom %} web interface, the value of `github.actor` will be the username of the user who started the workflow.", "7493f665-1ec3-405c-b65b-64343304c8a6": "To view {% data variables.product.prodname_dependabot_alerts %} for a specific ecosystem, you can use the qualifier \"ecosystem:ECOSYSTEM-NAME\" in your GitHub search query. Replace \"ECOSYSTEM-NAME\" with the name of the ecosystem you're interested in, such as \"ecosystem:nodejs\" for Node.js or \"ecosystem:python\" for Python. This will display {% data variables.product.prodname_dependabot_alerts %} detected in that specific ecosystem.", "d8d48c2a-63ec-40ca-a7fa-2b4315374544": "To request a team review in the provided context, follow these steps:  1. Click on the **elect **Team review request**. 2. Make the necessary changes to your work and ensure that it meets the team's review criteria. 3. Click on **Save changes**.  This will notify your team members that you have requested a review, and they will be able to access your work and provide feedback.", "88307b66-e8dc-4641-97a7-3b686d2787cb": "As an organization owner on GitHub, to update an existing sponsorship with an end date, you need to follow these steps:  1. Navigate to your organization's page on GitHub. 2. Click on \"Sponsoring\" from the left-hand menu. 3. Find the sponsorship you want to update and click \"Manage\" next to it. 4. Select \"Manage sponsorship\" from the drop-down menu. 5. Scroll down to the \"End date\" section and select the date you want the sponsorship to end. 6. Click \"Update sponsorship\" to save the changes.  Note: These steps assume that you already have an existing sponsorship on GitHub Sponsors as an organization owner. If you don't have a sponsorship yet, you'll need to follow the steps to create a new sponsorship first.", "1feb9d42-d640-4df3-9fa8-86aeb824344f": "A code hosting platform is a web-based service that provides developers with a place to store, manage, and collaborate on their source code. It typically includes features such as version control, code review, issue tracking, and continuous integration/continuous delivery (CI/CD) tools. Some popular examples of code hosting platforms other than {% data variables.product.prodname_dotcom %} include GitLab, Bitbucket, and Azure DevOps.  The main difference between a code hosting platform and other tools used for storing code is the level of collaboration and management features provided. While basic version control tools like Git allow developers to manage their code locally and push changes to a remote repository, code hosting platforms offer additional features that make it easier to collaborate with others, manage large teams, and streamline the development process.  In the context of migrating code from one platform to another, a migration origin refers to the current code hosting platform where the code is currently stored. This could be {% data variables.product.prodname_dotcom %}, GitLab, Bitbucket, or any other code hosting platform. A migration destination, on the other hand, refers to the new code hosting platform where the code will be migrated to. This could also be {% data variables.product.prodname_dotcom %}, or it could be a different platform altogether.  An example of a migration origin that is not {% data variables.product.prodname_dotcom %} could be GitLab. In this case, the migration process would involve exporting the code and associated data from GitLab and importing it into the new code hosting platform, whether that's {% data variables.product.prodname_dotcom %} or another platform altogether.", "dc3acf1f-f484-47a2-ba1b-5a10317bdce6": "Using {% data variables.product.prodname_ghe_cloud %} with a single organization allows for collaboration within that specific organization. Creating an enterprise account, on the other hand, enables collaboration between multiple organizations and provides a single point of visibility and management for administrators. Additional features and support available with an enterprise account include Enterprise Managed Users, {% data variables.product.prodname_ghe_server %}, and invoicing support. To create an enterprise account if you currently have a single organization and do not have any coupons or discounts applied, you can do so through your billing page. If you need Enterprise Managed Users, {% data variables.product.prodname_ghe_server %}, or invoicing support, you should contact {% data variables.contact.contact_enterprise_sales %}. The limitations for creating an enterprise account through a free trial are that it is limited to 50 seats, and if you have an existing organization with more than 50 seats, you should also contact {% data variables.contact.contact_enterprise_sales %}. When adding your existing organization to an enterprise account, the organization's resources remain accessible to members at the same URLs, and all current owners of the organization become owners of the enterprise account. After adding the organization to the enterprise account, {% data variables.product.company_short %} bills the enterprise account for usage within all organizations owned by the enterprise, and the current billing details for the organization become billing details for the new enterprise account.", "11c87bc6-c136-4466-8158-a71f94940164": "The context information provided explains how to use the {% data variables.product.prodname_actions_importer %} tool to migrate workflows from Bamboo to GitHub Actions. The user needs to replace `:my_repository` with their repository name, `:my_deployment_project_id` with the ID of the deployment project they want to convert, and `:target_url` with the URL of their GitHub repository. The command will output the URL to the pull request that adds the converted workflow to the repository. The user can also specify environment variables and optional arguments to customize the migration process. If more information is needed to answer a specific question, it should be requested from the context provider.", "93aad1c1-aaed-42c6-b34a-d0be8ea5d190": "The context provided only mentions {% data variables.actions.hosted_runner %}s with Linux or Windows operating systems, and doesn't provide information about other types of runners. However, based on the documentation, it seems that the architecture of {% data variables.actions.hosted_runner %}s with Linux or Windows operating systems differs from other types of runners in that they can be assigned to runner groups, which provide a security boundary and control access to runners at the organization and enterprise levels. This allows administrators to decide which organizations or repositories are permitted to run jobs on those sets of machines. The role of {% data variables.actions.hosted_runner_caps %}s is to manage these groups at the organization level, where they can contain multiple instances of the runner, and they can also be created at the enterprise level and shared with organizations in the hierarchy. This allows for greater flexibility and control over the use of runners within an organization or enterprise.", "0e1707a6-9fe0-4962-b76e-68b25e01a6de": "The purpose of configuring access to actions on github.com using github connect is to enable automation and workflow customization on github.com using third-party services or tools. This allows developers to extend the functionality of github.com beyond its native features and integrate external services into their workflows. It also provides a secure way to share data and access between github.com and external services, as github connect uses oauth authentication to authorize access. Overall, configuring access to actions on github.com using github connect enables more flexibility and customization in github.com workflows.", "96b22ab6-c85f-448e-812d-4c9d19f4fa48": "To resolve merge conflicts in Git when working on a collaborative project with multiple contributors, follow these step-by-step instructions:  1. On the command line, note the files that contain merge conflicts by running the following command:     ```shell    git status    ```     This will display a list of files with merge conflicts, as well as any other changes that need to be committed.  2. Open the first of these files in your text editor.  3. In the file, look for the merge conflict markers, which will look like this:     ```text    <<<<<<< HEAD    Here are the changes you've made.    =====================    Here are the changes from the main branch.    >>>>>>> main    ```  4. Decide which changes to keep. You can do this by reviewing the changes made by both yourself and the upstream branch, and deciding which changes are necessary.  5. Delete the unwanted changes and the merge conflict markers. For example, you could change the five lines shown in the previous code sample to the single line:     ```text    Here are the changes you want to use.    ```  6. If there are multiple files with merge conflicts, repeat steps 2-5 until you resolve all conflicts.  7. In the terminal, stage the file, or files, that you just modified by running the following command:     ```shell    git add changed-file-1.md changed-file-2.md    ```  8. Commit the files by running the following command:     ```shell    git commit -m \"Resolves merge conflicts\"    ```  9. Push the committed changes to the remote repository on GitHub.com by running the following command:     ```shell    git push    ```  To create a pull request on GitHub for a collaborative project, follow these steps:  1. Go to your repository on GitHub.com.  2. Click the **Pull requests** tab at the top of the page.  3. Click the **New pull request** button.  4. Select the branch you want to create the pull request from by clicking the dropdown menu next to the **", "d9aa4446-b6c1-4bc3-bded-21f9bee4049e": "The \"Cache nextjs build\" step in the provided context information helps in reducing the build time for the workflow by caching the Next.js build using the `actions/cache` action. This means that instead of rebuilding the Next.js application from scratch every time the workflow runs, the action will first attempt to retrieve the cached build from the specified cache key. If the cache is found, the build process will be skipped, and the workflow will continue with the remaining steps. This can significantly reduce the overall build time for the workflow, especially for larger projects with complex build processes.", "1b57b62d-9835-40b2-b93f-69e6e4d8ebd2": "To view the list of LDAP users who have access to your instance and provision new users, follow these steps:  1. Go to your {% data variables.product.prodname_ghe_server %} instance. 2. Sign in as an administrator. 3. Access the settings by clicking your profile picture in the upper-right corner of the page and selecting \"Settings\" from the drop-down menu. 4. In the left sidebar, click \"LDAP users\". 5. To search for a user, type a full or partial username and click \"Search\". Existing users will be displayed in search results. If a user doesn't exist, click \"Create\" to provision the new user account.  Alternatively, you can manually provision a user account by following these steps:  1. Go to your {% data variables.product.prodname_ghe_server %} instance. 2. Sign in as an administrator. 3. Access the settings by clicking your profile picture in the upper-right corner of the page and selecting \"Settings\" from the drop-down menu. 4. In the left sidebar, click \"Users\". 5. Click the \"New user\" button. 6. Enter the user's email address and a strong password. 7. Select the \"LDAP\" authentication method. 8. Enter the user's LDAP username and password. 9. Click \"Create user\" to create the new user account.  Note that unless LDAP Sync is enabled, changes to LDAP accounts are not automatically synchronized with {% data variables.product.prodname_ghe_server %}. In this case, you'll need to manually promote and demote users on {% data variables.product.prodname_ghe_server %} to reflect changes in LDAP, and manually sync accounts using the \"Sync now\" button or the API. Additionally, to remove LDAP accounts, you'll need to suspend the {% data variables.product.prodname_ghe_server %} accounts.", "32f34f8c-5ed3-4f12-bd12-70d1cd81e48a": "The available machine types for codespaces are 2-core, 4-core, and 8-core, depending on the region and the plan you're using. To change the machine type for an existing codespace, you can either do it from the GitHub web interface by clicking on the ellipsis next to the codespace and selecting \"Change machine type\", or you can use the `gh codespace edit --machine MACHINE-TYPE-NAME` command in the GitHub CLI, after finding out the available machine types for your codespace by listing your codespaces or checking the current machine type of a specific codespace using the `gh api` command.", "63c7f816-468a-4a3a-a5d0-f8eb5040c3e3": "The purpose of creating an environment in a GitHub repository is to provide a staging area for testing and validating changes before deploying them to production. This feature is available for all public repositories and for users on {% data variables.product.prodname_pro %} or {% data variables.product.prodname_team %} plans for private repositories. Some features for environments have no or limited availability for private repositories, and organizations with {% data variables.product.prodname_team %} and users with {% data variables.product.prodname_pro %} can create environments in private repositories.", "19f5b05a-21be-493c-a2ef-2d55240e8abb": "The required option for specifying the tags of a Docker image in GitHub Actions using the `build-push-action` is `tags`. The value of this option should be the tag of the new image in the format `DOCKER-HUB-NAMESPACE/DOCKER-HUB-REPOSITORY:VERSION`. This option can take a single tag or a list of tags.", "fbf85dec-6ad0-4fd6-8500-843795cd0d33": "To create a new project board in GitHub, a user can follow these steps:  1. Log in to their GitHub account and navigate to their profile or organization. 2. Click on the \"Projects\" tab (classic projects for older projects) or \"Projects (classic)\" for organization-wide projects. 3. Click the \"New project\" button. 4. Enter a name and description for the project. 5. Choose a template (optional). 6. Select the visibility level for the project. 7. Choose which repositories are linked to the project (optional). 8. Click \"Create project\" to create the project board.  For visibility, a user can choose between making the project board public, private, or internal to their organization. If the project board is internal, the user can also choose to limit visibility to specific teams or members within the organization.  To customize columns, a user can click on the \"Add column\" button and select a column type (such as \"To do,\" \"In progress,\" or \"Done\"). They can also choose to add automation options to the column, such as automatically moving cards to the next column when certain criteria are met. The user can edit the column names and order at any time by clicking on the \"Edit project columns\" button.", "a61e6604-7ab1-4c21-91d3-af9e14c0099b": "According to the context provided, codespaces can be customized both for individual users and for others who contribute to a repository by fully customizing the setup. This can be achieved by using the default image provided, or by creating a custom image that includes specific tools and configurations. This allows for a consistent and optimized development environment for all users, regardless of their local setup. Additionally, custom images can be shared with others in the repository, making it easier for contributors to get started and ensuring that everyone is working with the same tools and configurations.", "320eeaf1-85bc-4f7b-bdca-ee96cb6554a0": "The variable `$TRAVIS_REPO_SLUG` represents the unique identifier for the current repository being used, as it is set to the value of `github.repository_owner/github.repository`, which is the format used by GitHub to represent a repository's URL. This format includes the owner of the repository (`github.repository_owner`) and the name of the repository (`github.repository`), separated by a forward slash (`/`).", "ca127a8e-1d62-46a5-837b-d6f6a5ce7bfc": "The ID number in a pull request's title is significant because it uniquely identifies the pull request. It can be used to create a new branch based on that pull request by fetching the reference to the pull request using its ID number and creating a new branch based on that reference. This allows you to work on the pull request further, test it out, or even open a new pull request with additional changes without affecting the original pull request. The ID number can be found by clicking on the pull request and looking at the sequence of digits right after the pull request's title.", "a3528bfc-35a4-46a8-9896-64f6bfd6e6f3": "To automate tasks in a project using GitHub Actions, you can create workflows that automatically trigger actions based on specific events, such as pushing code to a repository or creating a new issue. These actions can perform tasks such as running automated tests, deploying code to a production environment, or sending notifications to team members. By automating repetitive tasks, you can save time and ensure consistency in your development process. To get started with GitHub Actions, you can refer to the guide \"Automating Projects Using Actions\" in the GitHub documentation.  2. To create a project in GitHub, you can follow these steps:  1. Go to your repository and click on the \"Projects\" tab. 2. Click on the \"New project\" button. 3. Enter a name for your project and select the repository you want to associate with it. 4. Choose a project template (optional) and click on the \"Create project\" button.  Once your project is created, you can add issues, pull requests, and other project-related content to it. You can also customize the project's layout and views to suit your needs. To learn more about creating projects in GitHub, you can refer to the guide \"Creating a Project\" in the GitHub documentation.  Answer:  To create a project in GitHub, you need to follow a simple process. First, go to your repository and click on the \"Projects\" tab. Then, click on the \"New project\" button and enter a name for your project. Next, select the repository you want to associate with it, and choose a project template if you want to. Finally, click on the \"Create project\" button, and your project will be created. With your project created, you can add issues, pull requests, and other project-related content to it, and customize the project's layout and views to suit your needs. To learn more about creating projects in GitHub, you can refer to the guide \"Creating a Project\" in the GitHub documentation.", "e916bc8f-f37f-4d06-90c2-2a9c2e3ba6d2": "The \"--db-cluster\" option in the \"codeql database print-baseline\" command is used when the directory given on the command line contains multiple databases under construction instead of a single database. This option indicates that all of those databases will be processed together. The main difference between using this option and specifying a single database is that with \"--db-cluster\", multiple databases are processed simultaneously, whereas specifying a single database processes only that specific database.", "f36fd473-c5b1-48ac-bb0a-dac1cf265567": "In GitHub's repository protection feature, rulesets and branch protection rules interact by layering with each other. This means that rules defined in multiple sources, such as repository-level rulesets and branch-level protection rules, are aggregated, and all rules apply. However, where multiple different versions of the same rule exist, the most restrictive version of the rule applies.  For example, let's say an administrator of a repository has set up a ruleset targeting a branch that requires signed commits and three reviews on pull requests before they can be merged. Additionally, an existing branch protection rule for the same branch requires a linear commit history and two reviews on pull requests before they can be merged. If an organization administrator also sets up a ruleset targeting the same branch that blocks force pushes and requires one review on pull requests before they can be merged, all rules apply, and the most restrictive version of each rule is enforced. In this case, the branch would require signed commits, a linear commit history, and three reviews on pull requests before they can be merged. Force pushes would also be blocked.  When multiple versions of the same rule exist across different sources, the most restrictive version is determined by comparing the specificity of each rule. The more specific rule takes precedence. For example, a rule that applies to a specific branch takes precedence over a rule that applies to all branches in the repository. If multiple rules have the same level of specificity, the most restrictive version is determined based on the severity of the restriction. For example, a rule that requires three reviews takes precedence over a rule that requires two reviews.  In summary, rulesets and branch protection rules interact by layering with each other, and the most restrictive version of each rule is enforced. This allows administrators to apply multiple layers of protection to their repositories while ensuring that the most restrictive rules are enforced.", "55535244-0353-4c6b-98a7-8983449bfe20": "To delete all log files for a specific workflow run using the GitHub web interface, follow these steps:  1. Go to your repository on GitHub. 2. Click on the Actions tab. 3. Find the workflow run you want to delete logs for. 4. Click on the three dots (...) in the upper-right corner of the log. 5. Select \"Delete all logs\" from the dropdown menu. 6. Confirm the deletion in the pop-up window.  Alternatively, you can also delete logs for multiple workflow runs at once by following these steps:  1. Go to your repository on GitHub. 2. Click on the Actions tab. 3. Click on the workflow you want to delete logs for. 4. Click on the three dots (...) in the upper-right corner of the workflow. 5. Select \"Delete all logs\" from the dropdown menu. 6. Confirm the deletion in the pop-up window.  This will delete all log files for all runs of this workflow.  Note: This feature is only available for workflow runs that have completed. If you want to delete logs for a workflow that is still running, you'll need to do it programmatically.    How can I delete all log files for a specific workflow run using the GitHub web interface?", "e702e032-e9b7-4b4e-9295-3e88d471e575": "To create a GitHub Actions workflow that uses the stored credentials to periodically run a script to find and redeliver failed deliveries, follow these necessary steps:  1. Store your {% data variables.product.pat_generic %} as a {% data variables.product.prodname_actions %} secret in the repository where you want the workflow to run. For more information, see \"AUTOTITLE.\"  2. Copy the provided {% data variables.product.prodname_actions %} workflow into a YAML file in the `.github/workflows` directory in the repository where you want the workflow to run. Replace the placeholders in the \"Run script\" step as follows:     - In the `on` section, you can choose to run the workflow every 6 hours or when manually triggered.     - In the `permissions` section, grant the `GITHUB_TOKEN` permission to access repository contents.     - In the \"Check out repo content\" step, the {% data reusables.actions.action-checkout %} action is used to check out the repository contents.     - In the \"Setup Node.js\" step, the {% data reusables.actions.action-setup-node %} action is used to set up Node.js.     - In the \"Install dependencies\" step, the `npm install octokit` command is used to install the octokit library, which is required by the script.     - In the \"Set environment variables\" step, you can set up any necessary environment variables, such as the URL of the API endpoint to retrieve failed deliveries.     - In the \"Run script\" step, replace the placeholder with the path to the script that you want to run.  3. Save the YAML file and commit the changes to the repository.  4. Test the workflow by manually triggering it or waiting for the scheduled run.  5. Monitor the workflow logs to ensure that it is running successfully and delivering failed messages.  6. Adjust the workflow as needed to meet your specific requirements, such as changing the frequency of runs or modifying the script.  7. Continuously monitor the workflow and make necessary", "01fbacd1-48a6-4405-8fdd-93fb9756246b": "To enable {% data variables.product.prodname_actions %} on {% data variables.product.prodname_ghe_server %}, you need to complete the following prerequisites:  1. Create a Google Cloud Storage bucket for storing data generated by workflow runs. 2. Configure {% data variables.product.prodname_ghe_server %} to use OIDC with Google Cloud Storage (recommended). This involves creating a Google Cloud service account, creating a Google Cloud identity pool and identity provider, and configuring {% data variables.product.prodname_ghe_server %} to use the provider and service account to access your Google Cloud Storage bucket. 3. Follow the steps outlined in the {% data variables.product.prodname_ghe_server %} documentation for enabling {% data variables.product.prodname_actions %}. 4. If you're using {% data variables.product.prodname_ghe_cloud %}, you also need to enable {% data variables.product.prodname_actions %} in your organization's settings. 5. If you're using {% data variables.product.prodname_ghe_server %} with self-hosted runners, you need to install the {% data variables.product.prodname_actions %} runner on your runners. 6. If you're using {% data variables.product.prodname_ghe_server %} with GitHub Apps, you need to create a new GitHub App with the {% data variables.product.prodname_actions %} feature enabled. 7. If you're using {% data variables.product.prodname_ghe_server %} with GitHub Enterprise Cloud, you need to enable {% data variables.product.prodname_actions %} in your organization's settings. 8. If you're using {% data variables.product.prodname_ghe_server %} with GitHub Enterprise Server, you need to enable {% data variables.product.prodname_actions %} in your organization's settings and install the {% data variables.product.prodname_actions %} feature on your runners. 9. If you're using {% data variables.product.prod", "49668dd6-a6a8-4b0e-b0c5-291a3eb526e4": "The `--evaluator-log` option is used to output structured logs about evaluator performance to a specified file. This option allows for detailed analysis of evaluator performance, and the format of the log file is subject to change without notice. The logs are in JSON format and are separated by either two newline characters by default or one if the `--evaluator-log-minify` option is passed. The `--evaluator-log-minify` option is used to minimize the size of the JSON log produced at the expense of making it less human-readable.  Here's an example of how to use these options in the command line:  ``` codeql query --evaluator-log=evaluator.log --evaluator-log-minify=false my_query.ql ```  In this example, the `--evaluator-log` option is used to output structured logs about evaluator performance to a file named `evaluator.log`. The `--evaluator-log-minify` option is set to `false` to ensure that the logs are human-readable.  The `--ram` option is used to set the total amount of RAM the query evaluator should be allowed to use. The default value for this option is not specified in the context information provided. Here's an example of how to set a specific amount of RAM using this option:  ``` codeql query --ram=8192 my_query.ql ```  In this example, the `--ram` option is used to set the total amount of RAM the query evaluator should be allowed to use to 8192 MB (8 GB).", "ef305d06-cfc0-4945-bf83-c6c74d7380dc": "No, the destination repository is not automatically unlocked after a successful import using the \"startImport\" GraphQL mutation. The repository needs to be manually unlocked using the \"Unlock\" button in Enterprise Cloud Importer or the \"unlockImportedRepositories\" GraphQL mutation. This is mentioned in the context information provided.", "afd937f5-77da-4fd0-ad0f-ff1ce0e58757": "To enable streaming of API requests for your enterprise account on GitHub, follow these steps:  1. Access your enterprise account settings by following the instructions provided in the context information. 2. Click on the \"Audit log\" tab. 3. Click on \"Settings\" under \"API Requests\". 4. Select \"Enable API Request Events\". 5. Click \"Save\" to apply the changes.  Note that this feature is currently in public beta and subject to change, as mentioned in the context information.", "c5882071-d604-4abe-9fc4-161f9c7fdfcc": "To create a new branch in GitHub and push your changes to it, follow these instructions:  1. Log in to your GitHub account at https://github.com/  2. Navigate to the repository where you want to create a new branch. You can do this by clicking on the repository name in your list of repositories, or by searching for the repository in the search bar at the top of the page.  3. Once you're in the repository, click the green \"Code\" button to open the repository in your preferred code editor. This will give you a URL that you can use to clone the repository to your local machine.  4. Clone the repository to your local machine by running the following command in your terminal or command prompt:     ```    git clone https://github.com/your-username/repository-name.git    ```  5. Navigate to the newly cloned repository on your local machine:     ```    cd repository-name    ```  6. Create a new branch by running the following command:     ```    git checkout -b new-branch-name    ```  7. Make your changes to the files in the new branch.  8. Stage your changes by running the following command:     ```    git add .    ```  9. Commit your changes by running the following command:     ```    git commit -m \"Adding new feature\"    ```  10. Push your changes to the new branch on GitHub by running the following command:     ```    git push origin new-branch-name    ```  11. Go back to your repository on GitHub and verify that your changes have been pushed to the new branch.  12. If you're ready to merge your changes into the main branch, create a pull request by clicking the \"New pull request\" button on the branch comparison page.  13. Follow the prompts to create the pull request, and make sure to include a clear and concise description of your changes.  14. Once your pull request has been reviewed and approved, merge it into the main branch by clicking the \"Merge pull request\" button.  15. Delete the new branch by running the", "d09ecc28-ac65-4fe2-b41a-9f9e193053c3": "The prerequisites for using OIDC with {% data variables.product.prodname_actions %} workflows include configuring your cloud provider to trust {% data variables.product.prodname_dotcom %}'s OIDC as a federated identity. Permissions settings for the token can be added by specifying the necessary scopes in the `permissions` field of the workflow YAML. For example:  ```yaml name: My workflow  on:   push:     branches:       - main  jobs:   my-job:     runs-on: ubuntu-latest     steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up JWT       uses: actions/setup-jwt@v1       with:         token-url: ${{ secrets.OIDC_TOKEN_URL }}         audience: ${{ secrets.OIDC_AUDIENCE }}         issuer: ${{ secrets.OIDC_ISSUER }}         scopes: read:user read:org admin:org      - name: Run a script       uses: your-org/your-repo/.github/actions/run-script@v1       with:         access-token: ${{ secrets.CLOUD_ACCESS_TOKEN }} ```  In this example, the `set up JWT` step is used to exchange the OIDC token for a cloud access token. The `audience`, `issuer`, and `scopes` variables are set using secrets, which should be stored securely in {% data variables.product.prodname_dotcom %}. The `scopes` variable specifies the necessary permissions for the token.  If your cloud provider offers an official action for using OIDC with {% data variables.product.prodname_actions %}, it is recommended to use that action instead of manually exchanging the token. This will ensure that the action is up-to-date and compatible with the latest versions of {% data variables.product.prodname_actions %}.  If your cloud provider doesn't offer an official action, you can use the Actions core toolkit to exchange the", "88d1bbb9-287e-4220-8a40-74e9396e393e": "Based on the provided context information, the \"Get import progress\" request has a status of \"complete\". This indicates that the import process has finished successfully and there are no outstanding issues or errors.", "0b496e2e-1c4b-4a12-8e05-38a3ed0943b0": "The purpose of the \"ETag\" header in GitHub events is to optimize polling by allowing clients to check if any new events have been triggered since the last request. If the ETag value returned in the response header matches the value sent in the \"If-None-Match\" request header, it indicates that the resource has not been modified, and the server returns a \"304 Not Modified\" response. This saves network bandwidth and server resources, as the client does not have to download the entire response body again. By obeying the \"X-Poll-Interval\" header, which specifies how often the client is allowed to poll, and using the ETag header to avoid unnecessary requests, clients can efficiently receive real-time updates on GitHub events.", "c12e3dae-df6c-4f7c-bc24-004393b57867": "To apply to become a {% data variables.product.prodname_student_leader_program_singular %}, you can visit the official website of GitHub's Student Leader Program and follow the application process provided there. The exact steps to apply may vary from time to time, so it's best to check the current application process on the program's official page. Alternatively, you can reach out to GitHub's support team for further assistance. However, without the specific details of the program, it's difficult to provide a more detailed answer.", "00faf1ee-5dc3-438a-83fa-613075952e16": "To create a local copy of a repository on GitHub using GitHub Desktop, follow these steps:  1. Open GitHub Desktop and sign in to your GitHub account. 2. Click on the \"Clone or create\" button on the top left corner of the window. 3. In the \"Clone or create\" dialog box, you can either clone an existing repository or create a new one. To clone an existing repository, select the \"Clone repository\" option and enter the URL of the repository you want to clone. If you want to create a new repository, select the \"Create a new repository\" option and follow the prompts to create a new repository. 4. Choose a local path where you want to save the repository on your computer. 5. Click on the \"Clone\" button to create the local copy of the repository.  That's it! You now have a local copy of the repository on your computer that you can work on without affecting the original repository on GitHub.", "987210a9-892d-4894-98de-937c46ab368c": "To modify an existing OAuth app in {% data variables.product.prodname_dotcom %}, you need to follow these steps:  1. Access your user settings by clicking on your profile picture in the upper-right corner of the page and selecting \"Settings\" from the drop-down menu. 2. In the left-hand menu, click on \"Developer settings\". 3. Click on \"OAuth apps\" in the left-hand menu. 4. Find the OAuth app you want to modify and click on its name to open its settings page. 5. Modify the information you want to change, such as the app's name, description, or homepage URL. 6. Click \"Save\" to update the app's information.  Note that some changes, such as modifying the app's scopes, may require additional steps or approval from your organization's administrator. Refer to the documentation for more information on modifying OAuth apps in {% data variables.product.prodname_dotcom %}.", "f6aec6b3-c796-40e7-a39d-70c3d584ab7b": "To add new tools to the system in alphabetical order, you should follow the order of the first letter of the tool's name. If there are multiple tools with the same first letter, then continue with the second letter, and so on, until the tool's name is unique in the alphabetical arrangement. It's essential to maintain consistency in the naming convention of the tools to ensure proper sorting and easy navigation within the system.", "f6b351ac-0732-4df0-93ed-e979398f5482": "To configure a custom domain for your {% data variables.product.prodname_pages %} site using a subdomain, follow these steps:  1. Purchase a domain name from a domain registrar. 2. Log in to your domain registrar's website and navigate to your domain's DNS settings. 3. Create a new CNAME record with the following values:     - Host: `www` (for a \"www\" subdomain) or the desired subdomain (for a custom subdomain)    - Points to: `{% data variables.product.prodname_dotcom %}` (replace `{% data variables.product.prodname_dotcom %}` with the URL of your {% data variables.product.prodname_pages %} site)  4. Save the CNAME record and wait for the DNS changes to propagate. 5. In your {% data variables.product.prodname_pages %} site, navigate to Settings > Domains and enter your custom domain. 6. Verify your domain by adding a TXT record with the provided value to your domain's DNS settings. 7. Save your custom domain and wait for the domain verification to complete.  The main difference between using a \"www\" subdomain versus a custom subdomain is the URL structure of your site. A \"www\" subdomain is the most commonly used type of subdomain and is included in the URL before the root domain (e.g., `www.example.com`). Custom subdomains, on the other hand, are distinct sections of your site that do not use the standard \"www\" variant (e.g., `blog.example.com`). Custom subdomains are mostly used when you want two distinct sections of your site that can be customized independently.", "08f2d972-66bf-42e2-96ff-4e770c8fb29c": "The text material outlines the terms related to subprocessors in a data processing agreement between GitHub and a customer. GitHub is required to provide information about its subprocessing agreements if confidential or sensitive information cannot be disclosed to the customer. GitHub must provide 30 days' prior written notice of the addition or removal of subprocessors, including the categories of processing, and the customer has the right to object to new subprocessors with reasonable objections. If GitHub is unable to provide an alternative solution to the customer's objection, the customer may terminate the agreement. Additionally, the customer has the right to temporarily suspend data transfer or prohibit collection and processing of customer personal data if GitHub breaches its security or privacy obligations, or if the transfer, collection, or processing of all customer personal data is suspended for longer than one month. The customer may also terminate the agreement without prejudice to any other claims if GitHub notifies the customer that it can no longer meet its privacy obligations, is in substantial or persistent breach of warranties or representations, or if the customer objects to a subprocessor.", "040c86b4-abc1-4ebf-ac4e-325fa35d56c3": "Deploy keys can be set up in GitHub for project deployment by following these steps:  1. Generate an RSA key pair on the server using the `ssh-keygen` command. 2. Navigate to the repository settings in GitHub. 3. Click on \"Deploy Keys\" in the sidebar. 4. Click \"Add deploy key\". 5. Provide a title for the key and paste the public key from the server. 6. Select \"Allow write access\" if the key should have write access to the repository. 7. Click \"Add key\".  Alternatively, deploy keys can be set up using the REST API.  Note that deploy keys are read-only by default, but can be given write access when adding them to a repository. Deploy keys only grant access to a single repository, and should not be protected by a passphrase as they are not tied to a specific user. If the user who created the deploy key is removed from the repository, the deploy key will still be active.  If multiple repositories are being used on a single server, a dedicated key pair should be generated for each repository, and an alias entry should be added to the server's SSH configuration file for each repository.", "be54c8cf-8ec5-4d72-b625-bff7bd9d4e35": "Based on the context provided, \"actions/cache\" seems to be a feature related to network response caching in {% data variables.product.prodname_actions_importer %}. However, without further information, it's unclear how this feature works or how it differs from other features. It's recommended to consult the official documentation or contact support for more details.", "70356933-be6d-4214-874d-2b40fb5dc726": "In the context provided, the difference between updating a manifest and replacing an outdated dependency directly is that updating a manifest involves checking in a new version of the dependency to the manifest file, while replacing an outdated dependency directly involves replacing the outdated dependency with the new version directly in the repository. This difference is relevant for dependencies that are referenced in a manifest versus dependencies that are vendored (or cached) and checked in to a specific directory in the repository. When {% data variables.product.prodname_dependabot %} identifies an outdated dependency, it raises a pull request to update the manifest for dependencies referenced in a manifest, and raises a pull request to replace the outdated dependency directly for vendored dependencies.", "2dfd6111-fd33-4318-8f14-44c080fbe923": "According to the context information provided, {% data variables.product.prodname_github_apps %} are preferred over {% data variables.product.prodname_oauth_apps %} in terms of security and access control. {% data variables.product.prodname_github_apps %} use fine-grained permissions, which means that they can access specific resources within a repository, rather than having broad access to all repositories. This limits the damage that could be done if the app's credentials were leaked. Additionally, {% data variables.product.prodname_github_apps %} give the user more control over which repositories the app can access, which further enhances security. In contrast, {% data variables.product.prodname_oauth_apps %} may have broader access to all repositories, which could potentially lead to more significant security risks. Overall, the context suggests that {% data variables.product.prodname_github_apps %} offer better security and access control than {% data variables.product.prodname_oauth_apps %}.", "56fd0ae6-2e84-4204-972f-17041c12fd91": "The `GITHUB_TOKEN` can be passed as an input in a GitHub Action workflow by defining it as an input in the YAML configuration file. This allows the token to be securely passed to the workflow without exposing it in the repository's codebase.  Here's an example of how to pass the `GITHUB_TOKEN` as an input:  ```yaml name: My Workflow  on: [ push ]  jobs:   my-job:     runs-on: ubuntu-latest     permissions:       contents: read     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Set up JWT         uses: actions/setup-jwt@v1         with:           token: ${{ secrets.GITHUB_TOKEN }}        - name: Run my script         run: |           # Your script goes here           # You can use the JWT to authenticate API requests           # For example:           # curl --request GET \\           #   --url https://api.github.com/repos/my-org/my-repo/issues \\           #   --header 'Authorization: Bearer ${{ github.token }}'        - name: Create issue using REST API         run: |           curl --request POST \\           --url https://api.github.com/repos/${{ github.repository }}/issues \\           --header 'Authorization: Bearer ${{ secrets.GITHUB_TOKEN }}' \\           --header 'Content-Type: application/json' \\           --data '{             \"title\": \"Automated issue for commit: ${{ github.sha }}\"             \"body\": \"This issue was automatically created by the GitHub Action workflow **${{ github.workflow }}.** \\n\\n The commit hash was: _${{ github.sha }}_.\"             }' \\           --fail ```  In this example, we're using the `actions/setup-jwt` action to set up a JSON web token (JWT) that can be used to authenticate", "5b02d6b6-a3bc-40ea-9fac-4688c9f612f7": "In order to file a DMCA counter notice, one should have a \"good faith belief that the material was removed or disabled as a result of mistake or misidentification of the material to be removed or disabled.\" Some examples of mistakes that could lead to this belief include:  1. The complaining party doesn't have the copyright: If the person who sent the takedown notice doesn't actually own the copyright to the material in question, then it's a mistake to claim that it's being infringed upon.  2. I have a license: If the person who sent the takedown notice is mistaken about whether you have a license to use the material, then that's a mistake.  3. The code has been released under an open-source license that permits my use: If the person who sent the takedown notice is mistaken about the terms of the open-source license that governs the material, then that's a mistake.  4. The complaint doesn't account for the fact that my use is protected by the fair-use doctrine: If the person who sent the takedown notice is mistaken about whether your use of the material falls under the fair-use doctrine, then that's a mistake.  These are just a few examples, but the key is to identify a specific mistake that you believe was made in the takedown notice.", "31b83e16-4344-48ee-a5c1-c3711ef1f851": "In the provided code snippet, rate limit errors can be handled by wrapping the `octokit.request` method call in a `begin` and `rescue` block. If a `RateLimitExceeded` error is raised, the script waits for the specified time before retrying the request. The `x-ratelimit-reset` header is used to determine the time at which the current rate limit window resets, and the script sleeps for the number of seconds until that time. This ensures that the script does not exceed the rate limit and can continue making requests.  Here's the modified code snippet:  ```ruby def request_retry(route, parameters)   begin     response = octokit.request(route, parameters)     return response   rescue Octokit::RateLimitExceeded => error     reset_time_epoch_seconds = error.response.headers['x-ratelimit-reset'].to_i     current_time_epoch_seconds = Time.now.to_i     seconds_to_wait = reset_time_epoch_seconds - current_time_epoch_seconds     puts \"You have exceeded your rate limit. Retrying in #{seconds_to_wait} seconds.\"     sleep(seconds_to_wait)     retry   rescue Octokit::Error => error     puts error   end end  response = request_retry(\"GET /repos/{owner}/{repo}/issues\", owner: \"github\", repo: \"docs\", per_page: 2) ```  In this modified code snippet, the `request_retry` method is defined to handle rate limit errors. The `request` method is wrapped in a `begin` and `rescue` block, and if a `RateLimitExceeded` error is raised, the script waits for the specified time before retrying the request. If any other error is raised, the script prints the error message.  The `request_retry` method is then called with the desired route and parameters, and the response is assigned to the `response` variable. If the request is successful, the response object is returned. If an error occurs, the error message is printed.  This approach ensures that the script does not exceed the", "980131a9-46c2-416b-a356-f49de938afc6": "One can view a combined overview of teams and people with access to a repository in the repository settings by following these steps:  1. Go to the repository in question. 2. Click on the settings icon in the top right corner of the repository page. 3. Select \"People\" from the left-hand menu. 4. This will display a list of all the people and teams that have access to the repository, whether it be through direct invitations or through membership in a team that has been granted access.  Note: The exact steps may vary slightly depending on the GitHub product being used (e.g., GitHub.com vs. GitHub Enterprise). However, the general process should be similar across all GitHub products.", "ab7c693c-afaf-4ea2-8f9b-a0a4833d9045": "Setting the `webhook_active` field to `true` in the context information enables webhook functionality for the {% data variables.product.prodname_github_app %}. This means that the app will receive event notifications from GitHub when certain actions are taken, such as a push event when a new commit is pushed to a repository. When `webhook_active` is set to `false`, webhook functionality is disabled for the app.  The `webhook_url` field specifies the URL where GitHub should send webhook event payloads. This URL should be a valid HTTP(S) endpoint that can receive and process the payload. The `webhook_secret` field (available only for GitHub Enterprise Server) is an optional string that can be used to secure webhook events. When `webhook_secret` is set, GitHub will include the secret in the `X-Hub-Signature` header of the webhook request, which can be used to verify the authenticity of the request.  Some webhook events require specific permissions before they can be selected during registration of a {% data variables.product.prodname_github_app %}. For example, the `push` event requires `write` permission for the repository, while the `issues` event requires `read` or `write` permission for the repository. To select multiple events in a query string, you can use the `events[]` parameter, where `events[]` is the name of the event (e.g., `push`, `pull_request`, `issues`) and you can include multiple events separated by the `[]` syntax. For example, `events[]=push&events[]=pull_request` would select both the `push` and `pull_request` events.", "94347a3b-f214-498b-9ae7-01d5be7bb923": "The recommended action for repositories that are no longer actively maintained, as mentioned in the context information, is to archive them. This will make the repository read-only, and collaborators and teams cannot be added or removed. Contributors with access to the repository can only fork or star the project. This is recommended to prevent unnecessary clutter and costs for legacy per-repository billing plans. It also allows for searching for archived repositories and issues within them. If using GitHub Advanced Security, secret scanning can still be enabled on archived repositories.", "3fceed27-0286-44b9-a22e-a0bdbc907813": "To connect a GitHub Enterprise Server (GHES) account to a GitHub.com account, the user needs to follow these steps:  1. Sign in to both the GHES and GitHub.com accounts. 2. On the GHES account, click the user profile icon in the upper-right corner and select Settings. 3. In the left-hand menu, click the GitHub Connect tab and then click the Connect to GitHub.com button. 4. Log in to the GitHub.com account and authorize the connection. 5. Review the resources that will be accessed from the GitHub.com account and click Authorize. 6. Choose whether to send contribution counts to GitHub.com.  During the connection process, the following resources will be accessed from the GitHub.com account:  - User profile information, such as name, email address, and avatar. - Repository information, such as the list of repositories and their names, descriptions, and URLs. - Pull request information, such as the list of pull requests and their titles, descriptions, and URLs. - Issue information, such as the list of issues and their titles, descriptions, and URLs. - Organization information, such as the list of organizations and their names and URLs. - Team information, such as the list of teams and their names and URLs. - Collaborator information, such as the list of collaborators and their names and email addresses. - Contribution information, such as the number of contributions made by the user to each repository.  Note that the specific resources accessed may vary depending on the permissions granted during the connection process.", "3a4f4a5f-572d-4d06-a91b-77c7efc2ac10": "{% data variables.product.prodname_dependabot %} determines if there is a new version of a dependency by looking at the semantic versioning (semver) of the dependency to decide whether it should update to that version. This means that it checks the major, minor, and patch versions of the dependency to determine if the new version is a patch release (minor change), minor release (major change), or major release (breaking change). If the new version is a patch or minor release, {% data variables.product.prodname_dependabot %} will update the dependency to the new version. However, if the new version is a major release, {% data variables.product.prodname_dependabot %} will not update the dependency automatically, as major releases may introduce breaking changes that could cause issues in the application. In this case, the developer will need to manually review the changelog and release notes to determine if the new major release is compatible with their application before updating the dependency.", "c4d11e2d-4d86-46a0-af7a-42e53e42038c": "Contacting any member of GitHub Staff via unsolicited mentions or pings, or via channels other than GitHub Community itself, or the Support contact form is strongly discouraged and may be considered a violation of our prohibition against harassment.", "8127b4b2-86dd-4c46-ad0a-3bc9d3086dcd": "An organization owner can create an announcement banner for their organization using {% data variables.product.prodname_ghe_cloud %} by accessing the organization settings, navigating to the \"Messages\" section, clicking on \"Announcement\", typing the announcement in the text field, optionally setting an expiration date and allowing users to dismiss the announcement, and saving the message.", "aca4cb89-61f8-4cde-be85-855738f84ea3": "According to the provided context, the responsibility of the Product Provider regarding data security and privacy when using a Developer Product is to delete a user's Personal Information from its systems within its defined window after the user ceases using the Developer Product and revokes access through their account settings. A user can ensure proper termination of their account with the Product Provider by contacting the Product Provider to confirm that their account has been properly terminated. It's the user's responsibility to understand the security and privacy considerations of using a Developer Product for their own security and privacy risk and compliance considerations.", "bfb2c43e-2440-41c4-ac77-893ff45d0d8d": "Alerts generated by {% data variables.product.prodname_code_scanning %} are categorized based on their severity level, which can be `Error`, `Warning`, or `Note`. For security-related alerts, {% data variables.product.prodname_code_scanning %} uses security severity levels, which can be `Critical`, `High`, `Medium`, or `Low`. These security severity levels are calculated using the Common Vulnerability Scoring System (CVSS), an open framework for communicating the characteristics and severity of software vulnerabilities. By default, any {% data variables.product.prodname_code_scanning %} results with a security severity of `Critical` or `High` will cause a check failure if {% data variables.product.prodname_code_scanning %} is enabled as a pull request check. The user can specify which severity level of code scanning alerts causes a check failure.", "ab9cfb06-9b4f-48c3-b52c-e4fa1335b697": "The purpose of running a dry run using the gh actions-importer dry-run subcommand is to convert a pipeline to its GitHub Actions equivalent and output the workflow in YAML format to the local filesystem without creating a pull request. This allows the user to review and verify the generated workflow before actually creating a pull request to merge it into the repository. To execute a dry run, the user can run the following command:  ```bash $ gh actions-importer dry-run [pipeline-type] ```  Replace `[pipeline-type]` with the name of the pipeline tool being migrated, such as `circle-ci`, `travis-ci`, or `jenkins`. This will generate the YAML file for the GitHub Actions workflow and print it to the console. The user can then review the generated workflow and make any necessary adjustments before running the migration with the `migrate` subcommand.", "343d1a30-7197-4614-a4f0-6afa093cdb10": "When installing an app using GitHub Apps, you have the following options:  1. Choose whether to grant the app access to all repositories in your organization or only select specific repositories. 2. If you choose to select specific repositories, you can choose which repositories the app will have access to under the \"Select repositories\" dropdown. 3. Click \"Install\" to complete the installation process.  Note that the specific permissions granted to the app will depend on the app's functionality and the permissions it requests during the installation process. Some apps may also require additional configuration or setup after installation.", "56fd59e4-375a-45c7-8d57-9a406a3317c9": "To revoke a linked identity in the context of SAML single sign-on for an enterprise account, follow these steps:  1. Access the enterprise account. 2. Go to the People tab. 3. Click on the person whose linked identity needs to be revoked. 4. Click on Revoke Identity. 5. Confirm the revocation by clicking on Revoke Identity again.  Note: If the enterprise uses {% data variables.product.prodname_emus %}, changes to managed users should be made through the IdP instead of on {% data variables.product.product_name %}.  Alternatively, to revoke an active SAML session, follow these steps:  1. Access the enterprise account. 2. Go to the People tab. 3. Click on the person whose active SAML session needs to be revoked. 4. Click on Revoke Session.  Note: This will immediately log out the user from all active sessions.  To view and revoke authorized credentials, follow these steps:  1. Access the enterprise account. 2. Go to the Security tab. 3. Click on Authorized Applications. 4. Find the application with the authorized credential that needs to be revoked. 5. Click on the application to view its details. 6. Click on Revoke Credential. 7. Confirm the revocation by clicking on Revoke Credential again.  Note: This will immediately revoke the authorized credential for the selected application.", "d72443f2-4489-4731-a3a8-5423529646fc": "Under the limitation of liability section of the agreement, GitHub is not liable for indirect, incidental, special, exemplary, consequential, punitive, or other similar damages, including lost profits, lost sales or business, lost data, business interruption, or any other loss incurred by the developer or any third party in connection with this agreement, regardless of whether the developer has been advised of the possibility of or could have foreseen such damages. The maximum amount of liability for GitHub in this agreement is six (6) months' spend.", "aca9a778-8de2-4dfa-a1b9-27e67df11fd0": "To delete all log files for multiple workflow runs programmatically using a script, you can use the GitHub API to list all workflow runs for a repository and delete the logs for each run. Here's an example script in Python that demonstrates this:  ```python import os import json import argparse import requests from requests.auth import HTTPBasicAuth  # Authentication token for GitHub API token = os.environ.get('GITHUB_TOKEN')  # Base URL for GitHub API base_url = 'https://api.github.com'  # Function to delete logs for a workflow run def delete_logs(run_id):     url = f'{base_url}/repos/{repo}/actions/runs/{run_id}/logs'     response = requests.delete(url, auth=HTTPBasicAuth('token', token))     response.raise_for_status()  # Function to list workflow runs for a repository def list_runs(repo):     url = f'{base_url}/repos/{repo}/actions/runs'     response = requests.get(url, auth=HTTPBasicAuth('token', token))     runs = response.json()['workflow_runs']     for run in runs:         delete_logs(run['id'])  # Parse command-line arguments parser = argparse.ArgumentParser(description='Delete logs for all workflow runs in a repository') parser.add_argument('repo', help='Repository name or owner/repo') args = parser.parse_args()  # Call function to list workflow runs and delete logs list_runs(args.repo)  print('Logs deleted for all workflow runs in', args.repo) ```  Replace `repo` with the name of the repository you want to delete logs for. You'll also need to replace `GITHUB_TOKEN` with your personal access token with the `repo` and `workflow` scopes.  To run the script, save it as a Python file (e.g., `delete_logs.py`) and run it in your terminal or command prompt:  ```sh python delete_logs", "80f5c315-46d7-4bdd-9cd6-543fb57c3890": "To access the `env` context in a workflow using the expression syntax, you can use the following syntax:  ```yaml name: My Workflow  jobs:   my-job:     runs-on: ubuntu-latest     steps:       - name: Set up environment         uses: actions/checkout@v2       - name: Run script         uses: actions/setup-node@v2         with:           node-version: '14'         env:           MY_VARIABLE: 'my-value'         run: |           echo \"Hello, world!\"           echo \"My variable is: ${{ github.event.name }}\"           echo \"My variable is: ${{ env.MY_VARIABLE }}\" ```  In this example, the `env` context is accessed using the expression syntax `{{ env.MY_VARIABLE }}` in the `run` step of the `my-job`. This will replace the expression with the value of the `MY_VARIABLE` variable that was set in the `env` step.", "99d676da-690a-449a-8dc8-e0ec8bb72e73": "To enable the \"Codespace Performance\" section in the side bar of {% data variables.product.prodname_vscode_shortname %} through the \"Settings\" tab in {% data variables.product.prodname_github_codespaces %}, follow these steps:  1. Click {% octicon \"gear\" aria-label=\"Manage\" %}, located in the bottom-left corner of the Activity Bar, to display the \"Settings\" tab. 2. Expand **Extensions** and click **{% data variables.product.prodname_github_codespaces %}** to display the settings for the extension. 3. Enable **Show Performance Explorer** to display the \"Codespace Performance\" section in the side bar.  Note: If the \"Codespace Performance\" section is already displayed in the side bar, you can still follow these steps to copy the Codespace ID using the clipboard icon.", "ca283a28-06f7-4e13-98fa-953e250bdf37": "After migration, the domain in the pull URL for a Docker image stored in the {% data variables.product.prodname_container_registry %} will be {% data variables.product.prodname_container_registry_namespace %} instead of {% data variables.product.prodname_docker_registry_namespace %}. This change occurs during the migration process for Docker images stored in the Docker registry, which is being replaced by the {% data variables.product.prodname_container_registry %}. No action is required on the part of the user during this process.", "06228041-e5ab-4101-a775-c3280103828b": "The context information provided doesn't give enough information to determine which ecosystem has the highest number of downloads. The metrics provided are the number of downloads for Maven packages (BM) and the number of downloads for npm packages (BX). However, there is no metric provided for the number of downloads for NuGet packages (CCB). Without this information, it's impossible to determine which ecosystem has the highest number of downloads.", "6c75a465-32c4-466e-bbf9-df4041aaa94c": "To test webhook delivery on {% data variables.product.company_short %}, you can follow these steps:  1. Trigger the event that should trigger the webhook delivery. For example, if you are testing a repository webhook, create a new issue in the repository where the webhook is configured.  2. Check {% data variables.product.company_short %} to verify that a webhook delivery was sent. You can do this by going to the repository settings, selecting Webhooks, and checking the Delivery history for the webhook. If the delivery is successful, you should see a green checkmark next to the delivery.  If the delivery is not successful, or if you see a red X next to the delivery, refer to the troubleshooting guide to help diagnose the problem. You can find the troubleshooting guide by clicking the \"Troubleshooting\" link in the webhook settings.  Alternatively, you can use the REST API to trigger a test `ping` event for your organization or repository webhook, or a test `push` event for your repository webhook. For more information, see the documentation for the REST API.  If you want to test your webhook code locally, you can use a webhook proxy URL to forward webhooks from {% data variables.product.company_short %} to your computer or codespace. You can then use your computer or codespace as a local server to receive these forwarded webhooks.  To use a webhook proxy URL, follow these steps:  1. Go to the repository settings and select Webhooks.  2. Click the \"Add webhook\" button and enter the URL for the webhook proxy service.  3. Configure the webhook proxy service to forward the webhook to your local server.  4. Test the webhook by triggering the event that should trigger the webhook delivery.  5. Check your local server to verify that the webhook was received.  By testing webhook delivery and testing your webhook code locally, you can ensure that your webhook is working as expected and that your code is properly handling webhook deliveries.", "d2094b14-06e0-4e3d-9dd6-a1905dd2eb21": "The application form for the {% data variables.product.prodname_student_leader_program %} program should include information about the applicant's motivation, interest, growth potential, and contribution to their campus community. The program wants to learn about the challenges the student community faces, the opportunities the applicant wants to build for their peers, and the potential for growth. This information will help the program select students who are passionate, driven, and committed to making a positive impact on their campus community.", "ca01245c-e9e3-4d0c-85b2-119409e0c152": "A webhook is a feature that allows a {% data variables.product.prodname_github_app %} to receive real-time notifications when certain events occur on {% data variables.product.prodname_dotcom %}. When a webhook is enabled for a {% data variables.product.prodname_github_app %} registration, {% data variables.product.prodname_dotcom %} sends a payload containing information about the event to the URL specified by the developer. The developer's server can then process the payload and take appropriate action based on the event that triggered the webhook. This allows the {% data variables.product.prodname_github_app %} to automate tasks or provide real-time updates to users based on events happening in their repositories.", "47fa230c-7728-47c6-96f3-48549e27d2c7": "The `--logdir` option can be used to write detailed logs to a specific directory during the execution of the CLI by providing the path to the desired directory as an argument to this option. This will cause the CLI to write detailed logs to one or more files in the given directory, with generated names that include timestamps and the name of the running subcommand. This option allows for more fine-grained control over where detailed logs are written, which can be useful for debugging or auditing purposes.", "1c85fb77-a64c-4ef2-814c-801d48e25358": "Without further context, it is unclear how \"AUTOTITLE\" is utilized in the provided context. Please provide additional information or clarify the context to assist in answering this question.", "9042fa51-4ddd-46c2-82d0-72f9263c2224": "Filters can be used in a security overview to narrow down the focus based on specific factors such as alert risk level, alert type, and feature enablement. Some examples of filters that can be used in the \"Security risk\" and \"Security coverage\" views are:  - `code-scanning:enabled`: Displays repositories that have configured {% data variables.product.prodname_code_scanning %}. - `dependabot:enabled`: Displays repositories that have enabled {% data variables.product.prodname_dependabot_alerts %}. - `secret-scanning:enabled`: Displays repositories that have enabled {% data variables.product.prodname_secret_scanning %}. - `repo:REPOSITORY-NAME`: Displays data for the specified repository. For example, `repo:test-repository` would display data for the repository named \"test-repository\".  In the \"Security risk\" view, you can also filter by alert risk level using the `risk` qualifier. For example, `risk:high` would display only high-risk alerts.  In the \"Security coverage\" view, you can filter by the type of security feature using the `security-feature` qualifier. For example, `security-feature:code-scanning` would display only repositories that have configured {% data variables.product.prodname_code_scanning %}.  You can also use free text search to search for repositories by name using the `repo` qualifier. For example, `repo:test` would display repositories with \"test\" in their name.  Overall, filters can be a powerful tool for quickly identifying and addressing security issues in your organization's repositories.", "94c4a07b-4f6e-4e67-ac8e-b5b462b9cf52": "Only site admins are able to access the global configuration for these hooks, as mentioned in the object attributes section.", "c1ff52dc-23ea-425b-9027-04806c5d6870": "To authenticate a request to access the GitHub REST API, one can provide an authentication token with the required scopes or permissions. This can be done in several ways, such as creating a personal access token (PAT), generating a token with a GitHub app, or using the built-in `GITHUB_TOKEN` in a GitHub Actions workflow. By authenticating the request, one can make more requests per hour and access resources that are not available to unauthenticated requests.  For example, to make a request using an authentication token, one can include the token in the `Authorization` header of the request, as shown below:  ``` GET /users/myusername Authorization: Bearer <YOUR_TOKEN> ```  When obtaining an authentication token, it's essential to treat it as a sensitive credential and follow best practices for handling it. Some precautions to take include:  1. Store the token securely: Avoid storing the token in plain text or easily accessible locations. Instead, store it in a secure location, such as a password manager or environment variable.  2. Limit access to the token: Only grant access to the token to authorized users or services. Avoid sharing the token with others or storing it in publicly accessible locations.  3. Rotate the token regularly: Periodically rotate the token to ensure that it remains secure. This can help prevent unauthorized access in the event that the token is compromised.  4. Use the appropriate scopes: Only grant the token the necessary scopes to access the resources required. Avoid granting excessive scopes, as this can increase the risk of unauthorized access.  5. Monitor access to the token: Regularly monitor access to the token to ensure that it's being used appropriately. This can help detect any unauthorized access or suspicious activity.  By following these best practices, one can help ensure the security and confidentiality of their authentication tokens and the resources they access.", "7efa9f1a-8f27-43fc-81ef-83ca24a0e689": "The '--threat-model' option in CodeQL is used to control the evaluation of queries based on specific threat models. This feature allows developers to analyze their code from different security perspectives and identify potential vulnerabilities.  The available options for this feature include:  1. Enabling a specific threat model: To enable a specific threat model, simply provide its name after the '--threat-model' option. For example, '--threat-model local' will enable the 'local' threat model.  2. Disabling a specific threat model: To disable a specific threat model, prepend an exclamation mark ('!') to its name. For example, '--threat-model !environment' will disable the 'environment' threat model.  3. Enabling or disabling all threat models: The 'all' threat model can be used to enable or disable all threat models. For example, '--threat-model all' will enable all threat models, while '--threat-model !all' will disable all threat models.  The 'default' threat model is enabled by default, but can be disabled by specifying '--threat-model !default'.  The '--threat-model' options are processed in order, allowing for fine-grained control over which threat models are evaluated for a given query.  This feature is only available for languages that support threat models. It was introduced in CodeQL version 2.15.3.", "e64739ab-7236-4480-89fc-ce3b9149970f": "During a migration to {% data variables.product.product_name %}, actions that should be avoided to prevent interruption include modifying settings for your enterprise (if you're using {% data variables.product.ghes %}) or running `ghe-config-apply` from an administrative SSH session (if you're using {% data variables.product.ghee %}). These actions can trigger a configuration run, which can restart services and potentially interrupt the migration.", "86a0f72a-e57c-474b-8e81-dcfe7e55430a": "On most systems, the default private key names used by SSH are `~/.ssh/id_rsa` and `~/.ssh/identity`. If these keys were generated automatically by your system or by Git, they should be automatically added to the SSH authentication agent, and you shouldn't need to run the `ssh-add` command with a specific path to your key. However, if you generated your key using a different name or location, you may need to run `ssh-add path/to/key` to add it to the agent.", "d8f81272-dd7a-489c-9973-3e1197f932e6": "If you want your app to take actions on behalf of a user, you should build a {% data variables.product.prodname_github_app %} that acts on behalf of a user and authenticate it using a user access token. This type of request is sometimes called \"user-to-server,\" and it means that the app will be limited by the permissions that have been given to the app as well as the user's permission. With this pattern, the user must authorize the app before the app can take action. For more information, see \"AUTOTITLE.\"", "abfaf503-d8a8-436f-a23f-f33586c33973": "Without further context, it is unclear what \"AUTOTITLE\" is or what its purpose might be in a document. It could potentially be a formatting command or a placeholder for a title that will be automatically generated by the software or program being used. However, without more information, it is impossible to determine the exact purpose of \"AUTOTITLE\" in this context.", "57e77d10-c011-42a3-bd8d-317f1c06f065": "The recipient of this agreement is restricted from reselling the GitHub Products, charging any service or other fee to Qualified Users in connection with their use of the GitHub Products, and is responsible and liable for Qualified Users' use of the GitHub Products. They are also required to cooperate with GitHub to enforce the terms of this Agreement in connection with Qualified Users' use of the GitHub Products, including sending appropriate notices to and terminating access to the GitHub Products for Qualified Users who misuse the GitHub Products in any way. Additionally, only the Designated Admin shall communicate with GitHub directly about program membership and administration of the Products and Services. The recipient must immediately terminate access to GitHub Products and Services for any Qualified User who no longer qualifies as such under this Agreement.", "26d0efb5-a694-4189-80ac-e4b4e3241372": "The \"--format\" option in the \"codeql resolve upgrades\" command affects the output format. The available choices for this option are \"lines\" (default) and \"json\". In \"lines\" mode, the upgrade scripts are printed on one line each, while in \"json\" mode, a JSON array of upgrade script paths is printed. This option allows the user to choose the output format based on their preference or use case.", "80aba685-2348-487e-a034-faaa39a2cbcf": "To join someone else's codespace for collaborative work, you need to be invited by the person who created the codespace. They can set up a Live Share session and send you a link to join. Here are the steps you can follow to join:  1. Click the link provided by the person who created the codespace. 2. If prompted, sign in to your GitHub account. 3. In the Live Share session, you'll see the person's workspace and be able to collaborate in real-time. 4. Make your changes and contributions to the codebase. 5. When you're done, leave the Live Share session by clicking the \"Leave\" button in the top right corner of the session.  Note that the idle timeout feature only applies to the person who started the codespace, not to collaborators working in the codespace via Live Share. So, as long as someone is actively working in the codespace, it won't be automatically stopped due to inactivity.", "21018d05-2afa-4478-beec-3ddf5a878790": "{% data variables.product.prodname_code_scanning %} can detect potential vulnerabilities and errors in code, such as security issues, logic errors, and performance bottlenecks. It prioritizes these findings for developers based on the severity and impact of the issue, as well as the likelihood of exploitation. Developers can then focus on addressing the most critical issues first. Additionally, {% data variables.product.prodname_code_scanning %} provides suggestions for remediation and links to relevant resources to help developers resolve the issues.", "571900d6-129a-4bab-9b52-3046399b9f87": "The command \"::notice file=app.js,line=1,col=5,endColumn=7::Missing semicolon\" can be used to create a notice message in an action by specifying the file name, line number, column number, and end column number where the issue is located, followed by a descriptive message in the format \"::notice file={name},line={line},col={col},endColumn={endColumn}::{message}\". This command is used to draw attention to potential issues or important information in the context of the specified file and location.", "665cbc81-0261-4b07-ad01-cfc7484e87b9": "The provided code handles user authentication using OAuth by implementing the OAuth flow to obtain an access token and scopes, and storing them in the session. It also includes a method to check if the user is already authenticated, and redirects to the callback URL to complete the authentication process. The code then renders a view with the user's information, including their public and private email addresses, with permission from the user. The code simplifies the roundtrip routing by changing the callback URL in {% data variables.product.product_name %} to the root URL, but this is not necessary since both the server and the advanced app are relying on the same callback URL.", "c4577b7b-e1c8-4efb-9ff8-d84766ad9cf2": "Generating a new repository from a template in GitHub involves selecting the template repository and then following the prompts to create a new repository with the same directory structure and files as the default branch of the template repository. The user can also choose to include all the other branches in the template repository. Branches created from a template have unrelated histories, meaning that they cannot create pull requests or merge between the branches. This is a limitation of branches created from a template, as they are essentially new repositories with their own histories, separate from the history of the template repository.", "5a5ad7ed-6482-4342-8b21-9b7620e7b2ba": "To provide a visual representation of the status of your workflows, you can use the workflow status badge. This feature allows you to display the status of your workflows directly in your repository's README file or other public-facing pages.  To implement this feature, you can follow the instructions provided in the GitHub documentation. Here's a brief overview of the steps involved:  1. Ensure that your repository is public or that you have a GitHub Enterprise Cloud (GEC) account with the necessary permissions.  2. Navigate to your repository's Settings tab and select the Pages option.  3. Choose the source for your status badge. You can either use the default branch or a specific branch or tag.  4. Select the \"Automatically deploy from the main branch\" option to enable the status badge.  5. Copy the status badge code provided by GitHub and paste it into your README file or other public-facing page.  6. Save your changes and refresh your repository's page to see the status badge in action.  For more detailed instructions and troubleshooting tips, you can refer to the GitHub documentation on workflow status badges.", "ddddfc3f-6fe9-4695-9e5a-deb3b32acae2": "Yes, based on the context provided, I can explain the process of implementing \"persistent\" authentication using sessions for storing tokens, and how it makes authentication transparent to the user.  In order to implement \"persistent\" authentication, we can use sessions to store the access token and scope information obtained during the OAuth flow. This allows us to bypass the need for the user to re-authenticate every time they access the application, as the session data is automatically passed to the server.  Here's how it works:  1. The user logs in to {% data variables.product.product_name %} and grants permission to the application. 2. The application receives the access token and scope information, which it stores in the session. 3. The user navigates away from the application and returns later. 4. When the user accesses the application again, the session data is automatically passed to the server, and the application can use the stored access token and scope information to make authenticated requests on behalf of the user.  This makes authentication transparent to the user, as they don't have to re-authenticate every time they access the application. Instead, they can simply navigate to the application and start using it immediately.  To handle caching of scopes within the session, we can store the scopes in the session data and check them against the current scopes obtained during the OAuth flow. If the scopes have changed, we can inform the user of the changes and ask for permission to access the new scopes. This ensures that the user is always aware of the application's access to their data.  Overall, implementing \"persistent\" authentication using sessions for storing tokens provides a more seamless and convenient user experience, while also ensuring that the user's data is protected and they are aware of the application's access to their data.", "b7aed02e-a97c-4058-ae59-8179f8600b6e": "The \"exclude\" instruction in a {% data variables.product.prodname_codeql %} pack can be used to filter queries by specifying a condition that must not be met for a query to be included in the pack. The syntax for specifying a condition is similar to that used in query definitions, with the addition of the \"exclude\" keyword at the beginning of the instruction.  For example, to exclude all queries that contain the string \"deprecated\" in their name, you can use the following instruction:  ```yaml - exclude:     name contain: deprecated ```  To specify a version of a {% data variables.product.prodname_codeql %} pack to use, you can include the version number in the pack's URL or in a \"qlpack\" instruction at the beginning of your pack definition. If you don't specify a version, the most recent version of the pack will be used.  For example, to use a specific version of a pack, you can include the version number in the URL like this:  ```yaml - qlpack: my-org/my-pack@v1.2.3 ```  Alternatively, you can use a \"qlpack\" instruction at the beginning of your pack definition:  ```yaml - qlpack: my-org/my-pack - version: v1.2.3 ```  In both cases, the specified version will be used for all queries in the pack, unless overridden by a \"version\" instruction within a query definition.", "15e44e55-6cbf-4012-874e-2984de6b5e03": "In GitHub, when multiple rulesets target the same branch or tag in a repository, all rules in all active rulesets are evaluated. The order in which the rules are evaluated is not specified, and there is no priority assigned to individual rules or rulesets. This means that a user may encounter multiple rules from different rulesets when interacting with a protected branch or tag. However, since rulesets can be easily managed and viewed, developers can understand why they are encountering a rule and which ruleset it belongs to. This provides greater transparency and control over the security and compliance constraints in a repository.", "26674501-5bfd-464d-a1b3-1fa59cafd536": "To download specific artifacts across all runs in a repository using the `gh run download` subcommand in GitHub CLI, you can use the following command:  ```shell gh run download -n ARTIFACT_NAME-1 ARTIFACT_NAME-2 ```  Replace `ARTIFACT_NAME-1` and `ARTIFACT_NAME-2` with the names of the specific artifacts you want to download. This command will download those artifacts from all runs in the repository. If you want to download artifacts from a specific run, you can use the `RUN_ID` parameter instead, as shown in the context information.", "ebbb6ffe-7638-41a9-a7f0-4f7061bf30dc": "To enable {% data variables.product.prodname_actions %} with MinIO storage in the Enterprise Management Console, follow these steps:  1. Access the Enterprise Management Console by following the instructions provided in the context information. 2. Click on \"Settings\" to access the settings page. 3. Click on \"Actions\" to access the Actions settings page. 4. Check the box next to \"Enable {% data variables.product.prodname_actions %}\" to enable {% data variables.product.prodname_actions %}. 5. If you're using GHES with Actions Storage OIDC, follow these steps:    a. Under \"Authentication\", select \"Credentials-based\".    b. Enter your storage bucket's details, including the access key and secret key.    c. Click \"Save Settings\".    d. Click the \"Test Storage\" button to ensure that the storage configuration is correct. 6. If you're using GHES without Actions Storage OIDC, follow these steps:    a. Under \"Artifact & Log Storage\", select \"Amazon S3\".    b. Enter your storage bucket's details, including the bucket name, region, and access key and secret key.    c. Select \"Force path style\".    d. Click \"Save Settings\".    e. Click the \"Test Storage\" button to ensure that the storage configuration is correct. 7. Click \"Save Settings\" to save your changes. 8. Follow the post-installation steps provided in the context information to complete the setup process.  Note: For MinIO, you cannot use OpenID Connect (OIDC) authentication. You must use credentials-based authentication.", "7cca2609-52b0-4bd5-b484-e5f68eaf1da6": "{% data variables.product.prodname_dotcom %} supports semantic versioning syntax. For more information, see \"Using semantic versioning\" and the \"Semantic versioning specification.\"", "64be7529-cad8-4652-b7a8-4f1bda3bdbd0": "A workflow can access up to 100 environment-level variables. The total combined size limit for organization and repository variables is 256 KB per workflow run. If this limit is exceeded, additional variables can be accessed by defining them in an environment and accessing them using the `env` context.", "0d64e55f-f051-4942-b83b-4f36a57b6c10": "If the `repository` key is not provided in the `package.json` file, then the package will not be linked to a repository when it is published, but it can still be published to npmjs.org using {% data variables.product.prodname_registry %}. This means that the package will not have a direct association with a specific repository on {% data variables.product.prodname_dotcom %}.  On the other hand, if the `repository` key is provided in the `package.json` file, then the specified repository becomes the destination npm registry for {% data variables.product.prodname_registry %}. This means that the package will be published to the specified repository on {% data variables.product.prodname_dotcom %}. However, if the `url` specified in the `repository` key is invalid, the package may still be published, but it won't be linked to the repository source as intended.  In summary, providing the `repository` key in the `package.json` file allows for a direct association between the package and a specific repository on {% data variables.product.prodname_dotcom %}, while not providing the `repository` key allows for the package to be published to npmjs.org using {% data variables.product.prodname_registry %} without a direct association with a specific repository on {% data variables.product.prodname_dotcom %}.", "b474c5f5-57ed-43a7-965e-18447bc4c6d9": "The roles that allow for the creation of new repositories in the given context are \"Owners\" and \"n\".   Explanation:  The context provides a list of roles with corresponding permissions. The role \"Owners\" has the permission to \"Create repositories\", while the role \"n\" also has this permission. Therefore, both \"Owners\" and \"n\" have the ability to create new repositories in this context.", "8663481e-6712-4ed9-8b9c-8069bc78feff": "Maintaining a README file with usage examples and guidance in an open source project is significant because it provides clear instructions and guidance to potential users, making it easier for them to understand how to use the project. This, in turn, encourages community involvement because it makes the project more accessible and user-friendly, which can lead to more contributions, bug reports, and feedback from the community. Additionally, a well-written README can help potential users understand the project's purpose, features, and limitations, which can lead to more informed decisions about whether to use the project or not. Overall, a comprehensive README is an essential part of any open source project, as it helps to foster a healthy and engaged community.", "f63648ab-2aeb-454e-87b8-b9ea989df0c1": "To provision users and manage groups for SCIM integration in your enterprise, you should use the following REST API endpoints:  - To create a user, make a `POST` request to the Provision a SCIM enterprise user endpoint. - To retrieve a user, make a `GET` request to the Get SCIM provisioning information for an enterprise user endpoint. - To update all of a user's attributes, make a `PUT` request to the Set SCIM information for a provisioned enterprise user endpoint. - To update an individual user attribute, make a `PATCH` request to the Update an attribute for a SCIM enterprise user endpoint. - To list all users, make a `GET` request to the List SCIM provisioned identities for an enterprise endpoint. - To delete a user, make a `DELETE` request to the Delete a SCIM user from an enterprise endpoint.  To manage groups, you should use the following REST API endpoints:  - To create a group, make a `POST` request to the Provision a SCIM enterprise group endpoint. - To retrieve a group, make a `GET` request to the Get SCIM provisioning information for an enterprise group endpoint. - To update all of a group's attributes, make a `PUT` request to the Set SCIM information for a provisioned enterprise group endpoint. - To update an individual group attribute, make a `PATCH` request to the Update an attribute for a SCIM enterprise group endpoint. - To list all groups, make a `GET` request to the List provisioned SCIM groups for an enterprise endpoint. - To delete a group, make a `DELETE` request to the Delete a SCIM group from an enterprise endpoint.  When making requests to these endpoints, you should be aware of the following rate limits:  - 100 requests per minute per user. - 1000 requests per minute per organization. - 10,00 requests per minute per GitHub App.  If you exceed these rate limits, GitHub will return a `429 Too Many Requests` error. To avoid this error, you should implement rate limiting in your SCIM integration. For more information, see \"AUTOTITLE.\"", "ae896d11-17c8-4e50-b525-f11243a27589": "To create service containers in a job using the `services` keyword in a workflow, you can define the service containers under the `services` section of the job configuration. Here's an example from the context information:  ```yaml services:   redis:     image: redis ```  This creates a service container called `redis` using the `redis` image from Docker Hub.  2. The purpose of mapping ports on the Docker host and service container in a job that runs directly on the runner machine is to enable communication between the service container and the job running on the runner machine. By default, the service container's ports are not exposed to the runner machine, so you need to map them using the `ports` keyword. For example:  ```yaml services:   redis:     image: redis     ports:       - \"6379:6379\" ```  This maps the `6379` port on the `redis` container to the `6379` port on the Docker host, allowing the job running on the runner machine to connect to the `redis` container. If the service container uses UDP, you can also map UDP ports using the `ports` keyword. For example:  ```yaml services:   redis:     image: redis     ports:       - \"6379:6379/udp\" ```  This maps the `6379` UDP port on the `redis` container to the `6379` UDP port on the Docker host. If you don't specify a specific UDP port, you can also map a randomly chosen UDP port using the `ports` keyword:  ```yaml services:   redis:     image: redis     ports:       - \"6379/udp\" ```  This maps a randomly chosen UDP port on the `redis` container to the `6379` UDP port on the Docker host.", "4c748329-91c9-4a2e-8c38-d4c7c8ec1ac8": "Starring a repository on Github is a way to save a repository to your personal list of starred repositories for easy access and future reference. It's a simple way to show support for the repository and its maintainers, and it doesn't give you any direct control over the repository's code or content.  Forking a repository, on the other hand, is a way to create a copy of the repository that you can modify and use for your own purposes. When you fork a repository, you become the owner of the copy, and you can make changes, add new features, or create a new project based on the original repository's code. Forking is a more hands-on way to contribute to an open-source project or to learn from someone else's code.", "e1ce08b4-dad6-4901-aedb-a5b2f2da157c": "{% data variables.product.prodname_copilot_for_business %} takes several measures to ensure user data is protected. These measures include:  1. Pseudonymous identifiers: User Engagement Data is associated with pseudonymous identifiers, which are unique codes assigned to each user. This ensures that user data is not directly linked to their personal information.  2. Access controls: Access to User Engagement Data is restricted to authorized personnel only, and access is granted on a need-to-know basis.  3. Encryption: User Engagement Data is encrypted both in transit and at rest to prevent unauthorized access.  4. Data minimization: Only the necessary data is collected and processed, and any data that is no longer required is deleted.  5. User control: Users have the ability to control the use of their data through the settings in their {% data variables.product.prodname_copilot_for_business %} account. They can choose to opt-out of data collection and processing, or they can request that their data be deleted.  In summary, {% data variables.product.prodname_copilot_for_business %} takes a multi-layered approach to data protection, which includes pseudonymous identifiers, access controls, encryption, data minimization, and user control. This ensures that user data is protected and that users have the ability to control the use of their data.", "2cfe38f2-382d-42e4-9db1-161b982d1322": "Supported syntax for Freestyle pipelines in Jenkins and GitHub Actions:  - Docker template: Both Jenkins and GitHub Actions support the `docker template` syntax for creating Docker containers. In Jenkins, this is done using the `dockerTemplate` plugin, while in GitHub Actions, it's done using the `jobs..container` syntax.  - Build: Both Jenkins and GitHub Actions support the `build` syntax for building projects. In Jenkins, this is done using the `Build` option in the Freestyle project, while in GitHub Actions, it's done using the `jobs` syntax.  - Build environment: Both Jenkins and GitHub Actions support the `build environment` syntax for setting up environment variables. In Jenkins, this is done using the `Environment variables` option in the Freestyle project, while in GitHub Actions, it's done using the `env` syntax.  - Build triggers: Both Jenkins and GitHub Actions support the `build triggers` syntax for specifying when a build should be triggered. In Jenkins, this is done using the `Build Triggers` option in the Freestyle project, while in GitHub Actions, it's done using the `on` syntax.  Partially supported syntax for Freestyle pipelines in Jenkins and GitHub Actions:  - Agent: Both Jenkins and GitHub Actions support the `agent` syntax for specifying the machine on which the build should run. However, in Jenkins, this is done using the `Agent` option in the Freestyle project, while in GitHub Actions, it's done using the `runners` syntax.  2. Supported syntax for Jenkinsfile pipelines in GitHub Actions and Jenkins:  - Docker: Both Jenkins and GitHub Actions support the `docker` syntax for creating Docker containers. In Jenkins, this is done using the `Dockerfile` plugin, while in GitHub Actions, it's done using the `jobs..container` syntax.  - Stage: Both Jenkins and GitHub Actions support the `stage` syntax for creating stages in a pipeline. In Jenkins, this is done using the `Stage` option", "809ccca5-0c49-43ea-8b1c-f3f15f3847a0": "According to the context information provided, the Company's privacy policy, available at https://docs.github.com/privacy, may impact the provision of data in case of termination of service. The Company agrees to provide user-generated content that is publicly visible through the Sites created by the Government within 30 days of termination, upon request, and in a commonly used file or database format as the Company deems appropriate. However, the Company will not provide data if doing so would violate its privacy policy. Therefore, it is necessary to review the Company's privacy policy to understand the specific circumstances under which data may not be provided.", "35231dab-1d03-4947-8979-f64ba84729ea": "When connecting to a {% data variables.product.prodname_dotcom %} repository from Git, you can authenticate using either HTTPS or SSH. The main difference between the two methods is the way that you establish a secure connection to {% data variables.product.prodname_dotcom %}.  With HTTPS, you connect to a URL that begins with `https://`, and Git handles the authentication process automatically. This method is recommended because it allows you to cache your {% data variables.product.prodname_ %} credentials, which can improve the performance of subsequent Git operations.  With SSH, you connect to a URL that begins with `git@`, and you are prompted to enter your {% data variables.product.prodname_ %} username and password or an SSH key. This method requires you to manually manage your {% data variables.product.prodname_ %} credentials, which can be less convenient than the HTTPS method.  In summary, the main difference between cloning with HTTPS and using SSH to authenticate with {% data variables.product.prodname_dotcom %} from Git is the way that you establish a secure connection to {% data variables.product.prodname_dotcom %} and manage your {% data variables.product.prodname_ %} credentials.", "a9d2fdf1-554e-4aaf-8c24-ac4d4f8a8141": "To install or request a GitHub app for your organization's GitHub account if you are using SAML, you need to follow these steps:  1. Go to `https://github.com/orgs/ORGANIZATION-NAME/sso` to start an active SAML session for your organization. Replace `ORGANIZATION-NAME` with the name of your organization. 2. Try to install or request the GitHub app again. This should work now as you have started an active SAML session. For more information, refer to the links provided in the context information: \"AUTOTITLE,\" \"AUTOTITLE,\" and \"AUTOTITLE.\"  If you are still facing issues, you may want to contact GitHub support for further assistance.", "89ee5909-a217-45a6-a82a-6ee7ab8f80de": "Some common mistakes to avoid when writing technical documentation to ensure clarity and avoid ambiguity include nominalization, ambiguous modal auxiliary verbs, prepositional phrases, vague nouns and pronouns, and excessive use of inline links. Nominalization occurs when a verb is turned into a noun, which can make the writing less clear and more complex. Instead, use active verbs. Ambiguous modal auxiliary verbs, such as \"may\" and \"might,\" should be avoided in favor of clearer language. Prepositional phrases, such as \"after trying many times\" or \"according to the repository log,\" can be replaced with more direct language. Vague nouns and pronouns, such as \"maintainers and contributors,\" should be replaced with specific nouns to eliminate ambiguity. Finally, inline links should be used sparingly and clearly introduced following a style guide.", "72090a72-8e47-4739-bb76-119258da7562": "The process of converting a personal account into an organization in the context provided is as follows:  1. Review the warning and click \"Turn USERNAME into an organization\". 2. Choose an organization owner, either the secondary personal account created earlier or another trusted user. 3. Select the organization's subscription and enter billing information if prompted. 4. Click \"Create Organization\". 5. Sign in to the new personal account created earlier and use the context switcher to access the new organization.  Note: This process is specific to the context provided, which mentions \"AUTOTITLE\" in the further reading section. However, without further context, it's unclear what \"AUTOTITLE\" refers to, so it's not possible to provide a complete answer.", "13cc0525-f439-449f-b484-70a33cb23dfc": "To submit feedback for the {% data variables.product.prodname_mobile %} app on {% data variables.product.prodname_github_community %}, you should follow these steps:  1. Go to the {% data variables.product.prodname_github_community %} website. 2. Log in to your GitHub account. 3. Search for \"GitHub Mobile\" or \"GitHub for Mobile\" in the community forum. 4. Click on the relevant topic or create a new one to submit your feedback. 5. Describe your feedback or feature request in detail, including any screenshots or error messages that may help illustrate your point. 6. Submit your feedback and wait for a response from the GitHub Mobile team.  Alternatively, you can also provide feedback directly within the app by tapping the gear icon in the bottom right corner, selecting \"Settings\", and then tapping \"Feedback\". This will open a pre-populated email message that you can use to send your feedback to the GitHub Mobile team.", "feddc37c-b2f0-46b2-b2ba-109c4a97f175": "The purpose of the command line option \"--sarif-category=\" in the context information provided is to specify a category for the analysis to include in the SARIF output, which can be used to distinguish multiple analyses performed on the same commit and repository, but on different languages or different parts of the code. This value should differ between each of the analyses, which tells Code Scanning that the analyses supplement rather than supersede each other. The value will appear as the \".automationId\" property in SARIF v1, the \".automationLogicalId\" property in SARIF v2, and the \".automationDetails.id\" property in SARIF v2.1.0.", "1e043521-fe69-43f7-b6d8-5b7572617cb7": "To configure tool-specific properties in the \"customizations\" section of the \"devcontainer.json\" file for VS Code, you can follow these steps:  1. Uncomment the \"customizations\" property in the \"devcontainer.json\" file.  2. Add the \"vscode\" object under the \"customizations\" property.  3. Add the IDs of the extensions you want installed when the container is created under the \"extensions\" array in the \"vscode\" object.  Here's an example of how the \"customizations\" section of the \"devcontainer.json\" file might look:  ```jsonc // Configure tool-specific properties. \"customizations\": {   // Configure properties specific to VS Code.   \"vscode\": {     // Add the IDs of extensions you want installed when the container is created.     \"extensions\": [       \"streetsidesoftware.code-spell-checker\",       \"vscjava.vscode-java-pack\"     ]   } } ```  In this example, the \"Code Spell Checker\" and \"Extension Pack for Java\" extensions will be installed when the container is created.", "0813e12a-4a7a-4178-a1df-3aecfa8d94c2": "A user account in GitHub Enterprise is considered dormant if the user hasn't performed any of the following activities in the past 90 days:  - Logged in to GitHub Enterprise - Created, edited, or commented on a pull request - Created, edited, or commented on an issue - Created, edited, or deleted a repository - Created, edited, or deleted a wiki page - Created, edited, or deleted a discussion - Created, edited, or deleted a comment - Created, edited, or deleted a release - Created, edited, or deleted a page - Created, edited, or deleted a gist - Created, edited, or deleted a team - Created, edited, or deleted a team membership - Created, edited, or deleted a team repository - Created, edited, or deleted a team repository access - Created, edited, or deleted a team repository collaboration - Created, edited, or deleted a team repository permission - Created, edited, or deleted a team repository topic - Created, edited, or deleted a team repository visibility - Created, edited, or deleted a team repository webhook - Created, edited, or deleted a team repository secret - Created, edited, or deleted a team repository branch protection - Created, edited, or deleted a team repository pull request review - Created, edited, or deleted a team repository pull request review request - Created, edited, or deleted a team repository pull request review comment - Created, edited, or deleted a team repository pull request review comment thread - Created, edited, or deleted a team repository pull request review comment thread comment - Created, edited, or deleted a team repository pull request review comment thread comment thread - Created, edited, or deleted a team repository pull request review comment thread comment thread comment thread comment - Created, edited, or deleted a team repository pull request review comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread comment thread", "a8189f1d-eeb4-476c-a0f3-583d4f21c534": "In IBE, selecting refers to choosing an option from a list or menu, such as a dropdown menu or a list of items. Clicking, on the other hand, refers to interacting with a specific element, such as a button or a checkbox, to perform an action. While both actions involve using a mouse or a touchpad, selecting is used to make a choice, while clicking is used to initiate an action.", "dc1779a6-7ba9-49ce-8dde-dbcad976a850": "According to the provided context, if Mada adds ten paid seats on June 4th, her organization will be immediately charged a prorated amount for the ten additional seats for the time between June 4th and June 14th. The seats will be available to use immediately after the additional charge is made.", "05a1c6a4-e9fa-4a0c-96a6-a00407b13c43": "Based on the context information provided, it is not entirely clear what the \"AUTOTITLE\" feature is or what its purpose is. However, based on the similarity in name to the \"AutoTitle\" feature in Microsoft Word, it can be inferred that \"AUTOTITLE\" is a feature that automatically generates titles for videos or other media files. This feature could be useful for streamlining the video editing process, as it would save time and effort that would otherwise be spent manually creating titles for each video. However, without further context or documentation, it is impossible to provide a definitive answer.  If the context information did not provide any further details about \"AUTOTITLE,\" then it would be necessary to say that without more information, it is unclear what the purpose of this feature is.", "b789d0fe-4c2b-452f-a5e6-1e79dcd20e22": "To disconnect an IdP group from a team on {% data variables.product.product_name %}, follow these steps:  1. Go to your organization settings. 2. Click on the team you want to disconnect the IdP group from. 3. Click on the \"Settings\" tab. 4. {% ifversion ghec %}Under \"Identity Provider Groups\", to the right of the IdP group you want to disconnect, click {% octicon \"x\" aria-label=\"Remove group\" %}.{%- elsif ghae %}Under \"Identity Provider Group\", to the right of the IdP group you want to disconnect, click {% octicon \"x\" aria-label=\"Remove group\" %}.     !Unselect a connected IdP group from the GitHub team. {%- endif %} 5. Click \"Save changes\".  Note: The steps may vary slightly depending on whether you're using GitHub Enterprise Cloud (GHEC) or GitHub Advanced Security (GHAS).", "e4883c71-26d6-4aff-b2c2-309523392ae5": "To provide visual cues in the procedural sections of a tutorial, you can use code blocks and screenshots heavily. This helps reassure the reader that they are performing the correct actions and makes it easier to follow along. Additionally, you can use clear and concise language, avoiding overly technical or complex terminology, and provide real examples to help illustrate the steps. By doing so, you can make the tutorial more accessible and user-friendly for readers with varying levels of experience.", "d93fc45e-174a-4f3d-9f24-6f56e11c1d64": "The process for privately collaborating to fix a vulnerability in a temporary private fork involves adding collaborators to a security advisory and creating a private fork of the affected software. Collaborators can then work together to develop a fix for the vulnerability without making it publicly available. This is necessary for several reasons. First, it allows developers to work on a fix without the pressure of a public deadline. Second, it prevents malicious actors from exploiting the vulnerability while a fix is being developed. Third, it allows developers to test the fix in a controlled environment before releasing it to the public. Once the fix is complete, it can be merged back into the main codebase and a new security advisory can be published to notify the community of the fix.", "545cbf30-5922-4ce4-9289-7234a23393d4": "Query packs in {% data variables.product.prodname_codeql %} analysis are collections of pre-compiled queries that can be executed on a {% data variables.product.prodname_codeql %} database. Their purpose is to provide a set of queries that can be used to analyze code for specific issues or patterns. When a query pack is published, it includes all the transitive dependencies and pre-compiled representations of each query, ensuring consistent and efficient execution of the queries in the pack. Query packs are designed to be run and can be downloaded and executed on a user's codebase to customize their {% data variables.product.prodname_codeql %} analysis.", "34d209e5-2d12-495e-8f46-2589d4f94d89": "If you allow the 7 day grace period to expire without enabling 2FA on {% data variables.product.prodname_dotcom_the_website %}, you will not be able to access {% data variables.product.prodname_dotcom_the_website %} until you enable 2FA. If you attempt to access {% data variables.product.prodname_dotcom_the_website %}, you will be prompted to enable 2FA.", "2d2290c4-57f2-489e-a9dc-93461790eefe": "The `devcontainer.json` file in the context provided is used to configure the environment and tools for a development container in a GitHub repository. It allows developers to specify aspects of the codespace's environment, such as installed extensions, libraries, and runtimes, and provides a consistent development environment across different machines and operating systems. The file is written in JSONC format and can be edited using a text editor or {% data variables.product.prodname_vscode_shortname %}. If the repository already contains a `devcontainer.json` file, it will be the default selection during codespace creation. If not, the default dev container configuration will be selected. The file can also be used to add new tools, libraries, or runtimes by adding them to the `features` object in the `devcontainer.json` file.", "17391fd1-d55b-4cb7-a080-81defade249d": "To follow the guide for publishing Docker images to Docker Hub or {% data variables.product.prodname_registry %} using GitHub Actions, you should have a basic understanding of workflow configuration options and how to create a workflow file. You should also have a complete definition for a Docker image stored in a GitHub repository, including a _Dockerfile_ and any other necessary files for performing a Docker build. Additionally, you will need a Docker Hub account and a Docker Hub repository created to push images to Docker Hub. If you are using {% data variables.product.prodname_registry %}, you will need to have an organization or enterprise with access to {% data variables.product.prodname_registry %}. If you are using a third-party Docker registry, you can use the example in the \"Publishing images to {% data variables.product.prodname_registry %}\" section as a template. {% ifversion fpt or ghec %} You should also be familiar with annotation keys, which are used to specify the Docker registry and credentials for pushing images. {% endif %}", "4d7e34f5-11b8-4fbd-9ee2-f6aacd566aef": "To test the Redis service container using Node.js and the `redis` npm module, you can follow the steps provided in the context information. First, create a new file called `_client.js` in your GitHub Actions workflow directory. Then, replace the contents of the file with the script provided in the context information. This script creates a new Redis client, adds some placeholder data, and retrieves the data. You can modify the script to include any Redis operations needed by your workflow. Finally, run the script using the following command:  ``` node _client.js ```  This will test your Redis service container using Node.js and the `redis` npm module.", "7ec194b3-287b-4283-b68f-b98070a68843": "According to the provided context, GitHub reserves the right to modify or discontinue the Service at any time without prior notice. This means that GitHub has the discretion to make changes to the Service, including its features, functionality, or availability, at any point in time. Customers should be aware that these modifications or discontinuations may affect their use of the Service, and they may not have any recourse or compensation for any resulting inconvenience or loss. Customers should also note that the terms and conditions governing their use of the Service, including these GitHub Supplemental Terms, may also be subject to modification or discontinuation by GitHub at any time. Therefore, customers are advised to regularly review the terms and conditions governing their use of the Service to stay informed of any changes that may impact their use of the Service.", "d4f96a21-c03a-4d12-9540-cdd58cc49179": "Organization owners can choose to further limit access to certain settings and actions across the organization. For more information on options for specific settings, you can refer to the documentation provided by GitHub, which may vary depending on the version of GitHub being used (e.g., {% data variables.product.prodname_ghe_cloud %}, GitHub Enterprise Server (GHES), or GitHub.com (GHAE)). Some examples of settings and actions that can be limited include repository settings, organization settings, and billing information. The specific options available may also depend on the type of setting or action being considered. For example, access to billing information may be restricted to organization owners and billing managers, while access to repository settings may be restricted to repository administrators. It's important for organization owners to carefully consider which settings and actions should be limited and to communicate these decisions clearly to other members of the organization to ensure that everyone has the access they need to contribute effectively.", "72cce2e8-a278-4238-bf5d-3044cb35aca3": "To handle errors when using Octokit.rb to interact with the {% data variables.product.company_short %} REST API, we can use a try/catch block to catch errors. If we want to catch all errors, we can use the `Octokit::Error` class to catch any error that Octokit.rb throws. Here's an example:  ```ruby begin   # Your code here rescue Octokit::Error => error   if error.response     puts \"Error! Status: #{error.response.status}. Message: #{error.response.data.message}\"   end   puts error end ```  In this example, we're using the `response` property of the error object to check if the error is a response from the server. If it is, we're printing the status code and the error message. If it's not, we're printing the error object itself.  To handle intended error codes, we can use specific exception classes to handle specific error codes. For example, the `GET /user/starred/{owner}/{repo}` endpoint returns a `404` if the repository is not starred. We can use the `NotFound` exception class to handle this error code:  ```ruby begin   response = octokit.request(\"GET /user/starred/{owner}/{repo}\", owner: \"github\", repo: \"docs\")   puts \"The repository is starred by me\" rescue Octokit::NotFound => error   puts \"The repository is not starred by me\" rescue Octokit::Error => error   puts \"An error\" end ```  In this example, we're using the `response` property of the error object to check if the error is a response from the server. If it is, we're handling the `NotFound` exception class to print a specific message. If it's not, we're handling the `Error` exception class to print a generic error message.  Pagination is a technique used to retrieve a specific number of items at a time from a large dataset. In the context of interacting with the {% data variables.product.company_short %} REST API using Octokit", "1c620467-b4f6-421c-a7fd-4d94b30f06c4": "To ensure that your locally signed commits and tags are cryptographically verifiable against a public key in a trusted root certificate using S/MIME in Git, follow these steps:  1. Tell Git about your signing key by adding the following lines to your Git configuration file (usually located at ~/.gitconfig):     ```    [user]    signingsign=smime    smimeconfig=/path/to/your/smime.cfg    ```     Replace `/path/to/your/smime.cfg` with the path to your S/MIME configuration file. This file should contain the following lines:     ```    [smime]    provider=gnupg    ```  2. Sign commits by running the following command after committing your changes:     ```    git commit --amend -S    ```     This will open your default text editor and prompt you to enter a passphrase for your signing key. After entering the passphrase, Git will sign the commit and amend it with the signature.  3. Sign tags by running the following command after creating a tag:     ```    git tag -s <tag_name> -m \"<tag_message>\"    ```     Replace `<tag_name>` and `<tag_message>` with the name and message of your tag, respectively. After entering the passphrase for your signing key, Git will sign the tag.  You don't need to upload your public key to GitHub, as Git will automatically verify the signatures against your public key. If you want to verify signatures for commits and tags made by others, you can add their public keys to your keyring using your email client or a command-line tool like `gpg`.  For more information on S/MIME and Git, see the Git documentation on S/MIME signing and verification.", "851e267c-20fd-42cb-a2ca-30a26d95215f": "For enterprises using {% data variables.product.prodname_emus %} or SAML SSO, usernames are automatically created for each member when they sign in through the external authentication system for the first time. In the case of {% data variables.product.prodname_emus %}, the username is normalized from the identifier provided by the SAML IdP and then appended with an underscore and short code specific to the enterprise. For SAML SSO, the username is created when the user signs in through the IdP for the first time.  Username conflicts can occur if multiple identifiers are normalized into the same username, as only the first user account is created. To resolve username conflicts, the normalized usernames should be made unique and within the 39-character limit by making a change in the SAML IdP. Additionally, for {% data variables.enterprise.prodname_emu_enterprises %}, the short code used for the enterprise members' usernames should be unique to avoid conflicts with other enterprises' short codes.  In the case of {% data variables.enterprise.prodname_emu_enterprises %}, the setup user who configures SAML SSO will have a username in the format of \"@S\" followed by the short code chosen during enterprise creation.", "d5d914b8-d3fc-4ec2-a3ed-60d31a68daf1": "The purpose of the \"AUTOTITLE\" directive in the given context is to automatically generate a title for a document or file. However, without further context, it is unclear what software or system is being used to generate this title. It could be a feature of a specific word processing program, a command in a programming language, or a setting in a content management system. Without this information, it is impossible to provide a more specific answer.  2. In the given context, the command \"git rm README.md\" is used to remove a file called \"README.md\" from the current repository. This command is part of the Git version control system, which allows multiple users to collaborate on a project by tracking changes to files and merging updates from different contributors. The \"git rm\" command removes the specified file from the repository, but does not delete it from the local machine. If the file is also deleted from the local machine, it can be added back using the \"git add\" command. After removing the file, it can be committed and merged with other branches using the \"git commit\" and \"git merge\" commands, respectively.  Answer:  In the given context, the command \"git rm README.md\" is used to remove a file called \"README.md\" from the current Git repository. This command is part of the Git version control system, which allows multiple users to collaborate on a project by tracking changes to files and merging updates from different contributors. By removing the file from the repository, it will no longer be included in future commits or merges, and other users will not be able to see or access the file unless they also remove it from their local copies. However, the file will still be present on the local machine, and can be added back to the repository using the \"git add\" command if needed. After removing the file, it can be committed and merged with other branches using the \"git commit\" and \"git merge\" commands, respectively.", "be7e7246-57d4-4187-8f0a-6136fc53ac60": "To view the total number of licenses, consumed licenses, and subscription expiration date for your enterprise account on {% data variables.product.prodname_dotcom_the_website %}, follow these steps:  1. Access your enterprise account on {% data variables.product.prodname_dotcom_the_website %}. 2. Go to the settings tab. 3. Click on the license tab. 4. Under \"User licenses\", you will see the total number of licenses, the number of consumed licenses, and the subscription expiration date.  Note: If you are using {% data variables.product.prodname_ghe_cloud %}, you can also view this information by looking at the \"AUTOTITLE\" section in the documentation. If you are using {% data variables.product.prodname_ghes %}, you can also view this information by clicking on the \"View details\" or \"Export license usage\" button, depending on whether you are using {% ifversion ghec %}CSV{% elsif ghes %}JSON{% endif %} reports. If you are using {% ifversion ghec %}CSV{% elsif ghes %}JSON{% endif %} reports, you can also view usage details for other features by clicking on the \"Billing\" tab in the left sidebar.", "0fdf56eb-f59c-4a39-935a-1788ed5f0eab": "Before generating a CodeQL database for a branch or pull request, you should:  1. Install and set up the CodeQL CLI. 2. Check out the branch or pull request you want to analyze. 3. Set up the environment for the codebase, ensuring that any dependencies are available. 4. Find the build command, if any, for the codebase.  The necessary information for this process includes the location of the codebase, the branch or pull request you want to analyze, and any necessary build commands or configuration files.", "12e94897-a796-4a8e-892c-75af32aa103b": "Without further context, it is unclear what \"AUTOTITLE\" refers to. If \"AUTOTITLE\" is referring to a specific document feature, it could potentially differ from other document features in terms of functionality based on the specific software or document type being used. More information is needed to accurately answer this question.", "15a851a9-a0ea-48b8-97b8-14cc1ce9c412": "To upgrade a sponsorship, you need to navigate to the sponsored account's sponsorship dashboard and review the available tiers. Then, select the desired tier and update your sponsorship. The change will become effective immediately, as per the prorated sponsorship policy.", "41c48a1e-5f07-46f9-85ca-7271c94f0c6a": "To convert a Travis CI pipeline to a GitHub Actions workflow using the `migrate` command, you can run the following command in your terminal, replacing the placeholders with the appropriate values:  ```shell gh actions-importer migrate travis-ci --target-url https://github.com/your-org/your-repo --output-dir tmp/migrate --travis-ci-repository your-travis-ci-repository ```  The output of this command includes the URL to the pull request that adds the converted workflow to your repository. An example of a successful output is:  ```shell $ gh actions-importer migrate travis-ci --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --travis-ci-repository my-travis-ci-repository [2022-08-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20220916-014033.log' [2022-08-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1' ```  You can also configure environment variables to connect to your Travis CI instance, as described in the \"Using environment variables\" section. These variables include `GITHUB_ACCESS_TOKEN`, `GITHUB_INSTANCE_URL`, and `GITHUB_TOKEN`.", "e6ec65a9-dde6-446c-843b-828a17b18915": "{% data variables.product.prodname_actions_importer %} uses the following environment variables to connect to a GitLab instance:  - `GITHUB_ACCESS_TOKEN`: The {% data variables.product.pat_v1 %} used to create pull requests with a converted workflow (requires the `workflow` scope). - `GITHUB_INSTANCE_URL`: The URL to the target {% data variables.product.prodname_dotcom %} instance (for example, `https://github.com`). - `GITLAB_ACCESS_TOKEN`: The GitLab {% data variables.product.pat_generic %} used to view GitLab resources. - `GITLAB_INSTANCE_URL`: The URL of the GitLab instance. - `NAMESPACE`: The namespaces or groups that contain the GitLab pipelines.  2. The `--source-file-path` argument can be used with the `forecast`, `dry-run`, or `migrate` subcommands to specify a source file path instead of fetching pipeline contents from source control by default. This argument is optional.", "6ceb34c8-4e46-427f-b598-a78eaf11bff6": "For {% data variables.product.prodname_docs %} purposes, version numbers for third-party actions in code examples should always be pinned to a full length commit SHA instead of the version number or branch. This is important because it ensures that the specific version of the action being used is clearly defined and can be easily reproduced, which is crucial for documentation purposes where accuracy and consistency are essential. By using a specific SHA, it also helps to prevent issues caused by version incompatibilities or updates that may break the action's functionality.", "bfc65e45-8c00-448d-ac46-012f89f2ff56": "To get more Actions minutes if you need more than allocated in the GitHub Team plan, you can increase your spending limit for GitHub Actions. This option is available for teams that require additional minutes for private repositories, as most small teams use fewer than 3,000 Actions minutes/month. For more information, see the GitHub documentation on increasing your spending limit for GitHub Actions.", "cdd0f12a-e92a-49a2-ae6d-a6cad1edfead": "To approve workflow runs for a pull request from a public fork, you can follow the steps provided in the context information. This involves accessing the repository settings, selecting the workflow, and approving the specific workflow run associated with the pull request. If the workflow run is awaiting approval for more than 30 days, it will be automatically deleted. This is a consequence of the workflow approval requirement, which is configurable at the repository, organization, or enterprise level. The purpose of this requirement is to ensure that workflows are reviewed and approved by appropriate team members before they are executed, helping to maintain code quality and prevent unintended consequences.", "a6fe8ce1-0854-427f-a8a1-2d7eeddb6f34": "To set up HTTPS cloning with OAuth tokens for deployments, you first need to create an OAuth token in your GitHub account settings. This can be done by navigating to your account settings, selecting \"Developer settings,\" and then \"OAuth apps.\" From there, you can create a new OAuth app and generate a token with the necessary scopes for your deployment needs.  To use the token for deployments, you can add it to your deployment script or configuration file. For example, in a bash script, you might use the following command to clone the repository using HTTPS with the OAuth token:  ```bash git clone --config core.editor='nano -w' https://oauth:<token>@github.com/<username>/<repository>.git ```  In this example, `<token>` is the OAuth token you generated, `<username>` is your GitHub username, and `<repository>` is the name of the repository you want to deploy. The `--config` flag is used to set the default editor to nano, which is optional.  To manage the tokens, you can revoke them at any time by navigating to the OAuth app settings and selecting \"Revoke\" next to the token you want to remove. This will render the token useless and prevent unauthorized access to your repository.  Overall, HTTPS cloning with OAuth tokens provides a secure and convenient way to deploy repositories without the need for SSH keys, as long as you manage the tokens properly and ensure that they have the necessary scopes for your deployment needs.", "12ec71e5-e23e-4eb3-9f3a-654c32d08da4": "The context information provided indicates that `ghp_` is the prefix for {% data variables.product.pat_v1_caps %}, which is a personal access token (PAT) for GitHub.com. On the other hand, `github_pat_` is the prefix for {% data variables.product.pat_v2_caps %}, which is also a PAT, but specifically for GitHub.com's new token system (PAT v2). Therefore, the main difference in token type indicated by these prefixes is the version of the PAT system used by GitHub.com.", "b76af094-269a-470f-8771-c4d8f7eb2189": "In the context of {% data variables.product.prodname_docs %}, a category is a broad and high-level topic that encompasses multiple related tasks or workflows. It serves as a container for map topics, which are more specific and task-based sections within the category. Map topics introduce a section of a category and group articles around more specific workflows or subjects that are part of the category\u2019s larger task. In summary, a category is a larger and more general topic, while a map topic is a more specific and task-based section within a category.", "6cc99450-a5f5-465f-bf55-ee1d5ddd372c": "To remove outside collaborators from all repositories in an organization on GitHub, follow these steps:  1. Go to your organization's settings by clicking on your profile, then clicking on the organization you want to manage. 2. Click on \"People\" in the left-hand menu. 3. Click on the \"Outside collaborators\" tab. 4. Select the outside collaborator or outside collaborators you'd like to remove from the organization. 5. Above the list of outside collaborators, select the \"X collaborators selected...\" dropdown menu, and click \"Remove from all repositories.\" 6. Review the outside collaborator or outside collaborators who will be removed from the organization, then click \"Remove outside collaborators.\"  Note: This will completely remove the selected outside collaborators from all repositories in the organization. If you only want to remove an outside collaborator from certain repositories, you can remove their access to specific repositories one at a time by following the steps outlined in the second answer.", "91e08134-ede5-41cf-9164-6bebeaaa5817": "{% data variables.product.prodname_codeql %} scans files for compiled languages like C/C++, C#, Go, and Java by analyzing the compiled versions of those files. This is because {% data variables.product.prodname_codeql %} is a static analysis tool that works by analyzing the source code without executing it. However, for compiled languages, {% data variables.product.prodname_codeql %} only scans files that are built during the analysis. This means that if some of the source code isn't compiled correctly, the number of lines of code scanned during analysis will be lower than expected. Factors that can lead to a lower number of lines of code being scanned during analysis include:  1. The {% data variables.product.prodname_codeql %} `autobuild` feature uses heuristics to build the code in a repository, but sometimes this approach results in an incomplete analysis of a repository, especially when multiple `build.sh` commands exist in a single repository.  2. Some compilers do not work with {% data variables.product.prodname_codeql %} and can cause issues while analyzing the code. For example, most vendor-specific C compilers will not be recognized by {% data variables.product.prodname_codeql %}. C code will need to be compiled with a recognized compiler (such as GCC, Clang, or MSVC) in order to be analyzed.  To ensure that {% data variables.product.prodname_codeql %} knows exactly how to compile all of the source files you want to scan, you can replace the `autobuild` step with the same build commands you would use in production. Alternatively, you can inspect the copy of the source files included with the {% data variables.product.prodname_codeql %} database to understand why some source files haven't been analyzed.", "8c44cf9d-c3da-4be2-b438-1c90c3702441": "Actions on {% data variables.product.product_name %} are individual tasks that can be combined to create jobs and customize workflows. They are used to automate repetitive tasks, such as checking out code, uploading artifacts, downloading artifacts, labeling issues, and setting up environments. By using and customizing actions shared by the {% data variables.product.prodname_dotcom %} community or creating your own actions, you can streamline your development process and reduce manual intervention. This can save time, improve consistency, and increase efficiency. Additionally, you can restrict access to actions to those stored on {% data variables.location.product_location %} or allow access to all actions from {% data variables.product.prodname_dotcom_the_website %}, depending on your organization's preferences and security requirements.", "87725ad0-dab4-42e0-9156-348273a4791c": "To view your past billing activity for your enterprise account on GitHub, follow these steps:  1. Go to your enterprise organization on GitHub.com. 2. Click on your profile icon in the top right corner of the page and select \"Settings\" from the drop-down menu. 3. Click on the \"Billing\" tab. 4. Click on the \"Past invoices\" tab to see a summary of your past billing activity.  Alternatively, you can follow the link labeled \"Download current invoice\" on the billing summary page to view your current invoice, which will also include a summary of your past billing activity.  I hope that helps! Let me know if you have any further questions.", "aae003b7-5f6f-45f1-bc06-bf78305bfee8": "To commit your changes with a comment during a merge conflict in a Git repository, you can use the following command in the command line:  ```shell git commit -m \"Resolved merge conflict by [keeping/deleting] the removed file and incorporating both suggestions.\" ```  Replace \"[keeping/deleting]\" with the action you chose to take with the removed file. This will create a new commit with your changes and the comment you provided. You can then merge the branches on the command line or push your changes to your remote repository on GitHub and merge your changes in a pull request.", "ab69fdb7-7fd6-46e1-a3a5-4da861741864": "To track sponsorships that originate from your blog using GitHub's sponsored program, you can use metadata tracking. Here's how to set it up:  1. Go to your sponsored account on GitHub. 2. Click on the \"Sponsors\" tab in the top navigation bar. 3. Click on the \"Settings\" button in the top right corner of the page. 4. Scroll down to the \"Metadata\" section and click on \"Add metadata\". 5. Enter \"metadata_campaign\" as the key and \"myblog\" as the value. 6. Click \"Save changes\".  Now, when someone sponsors your account through your blog, the \"campaign\" key will be included in the metadata column of your transaction export.  To view your sponsorship data, follow these steps:  1. Go to your sponsored account on GitHub. 2. Click on the \"Sponsors\" tab in the top navigation bar. 3. Click on the \"Your sponsors\" tab to see a list of your current sponsors. 4. To filter your sponsors by tier, use the \"Filter\" drop-down menu and select \"Active tiers\" or \"Retired tiers\".  To export your sponsorship transactions by month, follow these steps:  1. Go to your sponsored account on GitHub. 2. Click on the \"Sponsors\" tab in the top navigation bar. 3. Click on the \"Your sponsors\" tab to see a list of your current sponsors. 4. In the top right corner of the page, click on the \"Download\" icon (it looks like a downward-facing arrow with a line underneath it). 5. Select the month you want to export data for. 6. Choose the format you want to export the data in (CSV or JSON). 7. Click \"Start export\".  You can export up to 10 sets of data per hour for any of your sponsored accounts. The available time frames for export are the current month, the previous month, and the next month (if it's not yet the end of the month).", "591a7763-0a8e-46bf-893c-17a15b8d0975": "Release notes for changes should be written in the present tense, avoid passive language when possible, clarify actors and impact, and include relevant links to GitHub Docs if useful. They should also include descriptions of the new and old behaviors, and should be written for the implied audience. Known issues should also be included, with descriptions of the issue, details of the behavior, and next steps for the audience. Error messages should be formatted in the release notes for known issues.", "515547d9-5081-4539-9d17-458a26b3a275": "In order to finalize a CodeQL database that was created with the command 'codeql database init' and subsequently seeded with analysis data using the command 'codeql database trace-command', you need to run the command 'codeql database finalize' on that database. This step is necessary before the new database can be queried. The command 'codeql database finalize' takes a mandatory argument '<database>', which is the path to the CodeQL database under construction. If the option '--db-cluster' is given, this is not a database itself, but a directory that contains databases under construction, and all of those databases will be processed together. Additionally, the command accepts the option '--additional-dbs=<database>:<database>...' to include data from additional CodeQL databases under construction in the finalized database being created. This is an advanced option that may not have the desired effect in all cases. For more information, please refer to the documentation of 'codeql database import'. The command also accepts the option '--no-cleanup' to suppress all database cleanup after finalization, which is useful for debugging purposes. The option '--no-pre-finalize' skips any pre-finalize script specified by the active CodeQL extractor. The option '--[no-]skip-empty' outputs a warning instead of failing if a database is empty because no source code was seen during the build, and the empty database will be left unfinalized. The option '--dbscheme=<file>' overrides the auto-dbscheme.", "a115991d-72dd-451c-bd95-ffeae18aa668": "The purpose of the `publish` job in the provided YAML configuration is to publish the artifacts generated during the `build` job after it successfully completes and all the rules for the environment called `production` pass. This ensures that only stable and tested builds are published to the production environment.", "a114e46f-1e59-4637-be39-ab64049717f5": "The {% data variables.product.prodname_community_exchange %} community moderates all repository submissions. To delete a submitted repository from the gallery, go to the \"Submissions\" tab on the GitHub Community Exchange home page, locate the repository submission you want to remove, and click the {% octicon \"trash\" aria-label=\"The trash icon\" %} to the right of it.", "59bfbee6-ed7e-49b8-b351-316de5d11256": "In a {% data variables.product.prodname_github_app %} listing, the following information is required:  - App name - App description - App website URL - App contact email address - App privacy policy URL - App terms of service URL  The following fields are optional:  - Company URL - Status URL - Documentation URL - Logo and feature card (custom image and background color)", "a1677d31-b621-45d1-9210-5cb8719f0299": "If you are using a custom {% data variables.product.prodname_actions %} workflow for publishing, you should configure your workflow to run on the `pull_request` trigger to receive emails about build errors in your pull request. It is recommended to skip any deploy steps if the workflow was triggered by the `pull_request` event to allow you to see any build errors without deploying the changes from your pull request to your site. This is because deploying changes from a pull request with build errors can cause issues on your live site, and it's better to catch and fix errors during the build process rather than after deployment.", "0a524e6f-e992-47b7-943f-f676b7f5aef5": "{% data variables.product.prodname_codeql %} packs are collections of queries, libraries, query suites, and metadata used to customize and enhance {% data variables.product.prodname_codeql %} analysis. They can be downloaded and run on a codebase to identify security vulnerabilities, performance issues, and other code quality concerns. Query packs contain pre-compiled queries that can be evaluated on a {% data variables.product.prodname_codeql %} database, while library packs provide libraries for use by query packs or other library packs. Model packs, currently in beta for Java analysis, are used to expand {% data variables.product.prodname_code_scanning %} analysis to recognize libraries and frameworks that are not supported by default. By using {% data variables.product.prodname_codeql %} packs, developers can improve the accuracy and efficiency of their {% data variables.product.prodname_codeql %} analysis.", "51e8f09f-95ec-4079-97e9-7e1dc6aab406": "According to the note provided in the context, if you want to revert to using a local copy of the `actions/checkout` action after using it from GitHub's {% data variables.product.prodname_dotcom_the_website %} for the first time, you first need to remove the namespace from retirement. For more information, see the provided link for \"AUTOTITLE.\"", "1b53cb3f-7c93-4e3a-bd87-ba0830a62efa": "To prevent a pull request from being merged if your repository requires specific checks to pass first, you can't merge the pull request if you added the skip instruction to the commit message. You'll need to push a new commit to the pull request without the skip instruction in the commit message to allow the pull request to be merged. This is because skip instructions only apply to the `push` and `pull_request` events, and won't stop a workflow that's triggered `on: pull_request_target` from running. If you want to disable a workflow from running, you can do so by following the instructions provided in the context information. However, if the context doesn't provide enough information to give an answer, then you weren't given enough information to give an answer.", "154b9457-6860-4718-ba50-086d7a087abf": "To build integrations with the {% ifversion fpt or ghec %}{% data variables.product.prodname_dotcom %}{% else %}{% data variables.product.product_name %}{% endif %} API, such as {% data variables.product.prodname_github_apps %} or {% data variables.product.prodname_oauth_apps %}, for use in organizations in your enterprise, you need to have knowledge of the {% ifversion fpt or ghec %}{% data variables.product.prodname_dotcom %}{% else %}{% data variables.product.product_name %}{% endif %} API and how to build integrations using it. This may involve learning the API's endpoints, request and response formats, authentication methods, and any other relevant information. You may also need to have development skills in the programming language or framework you plan to use to build the integration. Additionally, you may need to have access to an organization in your enterprise to test and deploy the integration.", "4ab34e52-ce79-401b-932e-fd0fbddaf7f2": "GitHub enforces security measures for Markdown rendering in steps by enforcing ion between steps to prevent potentially malformed Markdown from a single step from breaking Markdown rendering for subsequent steps. Additionally, if more than 1MiB of content is added for a step, the upload for the step will fail, and an error annotation will be created. However, upload failures for job summaries do not affect the overall status of a step or a job.", "00f28eb8-5fd2-41e3-abf7-62732ff04251": "To check the availability of {% data variables.product.prodname_codespaces %} services, you can visit the GitHub status page at githubstatus.com. This page provides real-time information about the status of GitHub services, including {% data variables.product.prodname_codespaces %}. If there are any issues with the {% data variables.product.prodname_codespaces %} service, they will be listed on this page.", "3f91714e-5951-43b2-bf0f-dd6670369b27": "The shortcut to automatically update the URL to the permalink version of a file on GitHub is to press <kbd>Y</kbd>. To use it to copy a permanent link to the specific version of a file, follow these steps:  1. Go to the file you want to permalink on GitHub. 2. Make sure the version you want to permalink is displayed (i.e., the version at the head of the branch). 3. Press <kbd>Y</kbd> to update the URL to the permalink version. 4. Copy the URL from the address bar of your browser. 5. Share the URL with others, knowing that they will see the exact version of the file that you saw.  Note that you can also use any identifier that can be resolved to a commit in the URL, such as a branch name, specific commit SHA, or tag.", "db511f5e-88e1-4a6a-bee7-8c22fda947b1": "To ensure that people using a template with automatically added projects have write permissions for those projects, you should consider enabling your project's auto-add workflow. This feature automatically adds items to a project when they are created using a specific template. However, it's essential to note that the person opening the issue must have write permissions for the specified projects. If you don't expect people using this template to have write access, you should avoid using this feature and instead manually add projects to the issues created using the template. This approach ensures that only authorized individuals can modify the issues in the specified projects.", "8e9a8e48-dbfd-49ae-81a5-9956e44e176e": "The `upload-artifact` and `download-artifact` actions in the context provided are used to share data between jobs in a workflow. In this example, the `upload-artifact` action is used in job_1 to save the result of a math calculation to a text file and upload it as an artifact with the name `homework`. The `download-artifact` action is used in job_2 to download the `homework` artifact uploaded in job_1, perform a math calculation using the downloaded result, and then upload the updated result as an artifact with the same name, overwriting the previous one. This allows job_3 to download the updated `homework` artifact and display the final result of the math operation.", "42b4c6bd-ca66-4a17-b070-f64e20d48715": "To delete a custom field in the settings of your {% data variables.projects.project_v2 %}, you should click \"Delete field\". This can be found by clicking the name of the custom field you want to delete in the settings of your project.", "8a2fddd1-b366-47fb-a5bc-1208c8dac238": "To download the log files or artifacts from a workflow run on GitHub, follow these steps:  1. Go to your repository on GitHub. 2. Click on the Actions tab. 3. Select the workflow run that contains the logs or artifacts you want to download. 4. Click on the job that contains the logs or artifacts you want to download. 5. Click on the ellipsis (...) next to the job and select Download logs or Download artifacts, depending on what you want to download. 6. Follow the prompts to download the logs or artifacts.  Note: If you want to download all logs for a workflow run, you can also click on the ellipsis (...) next to the workflow run and select Download logs. This will download a compressed file containing all logs for that workflow run.", "dc400c07-63cc-4cfb-a869-dc2b0c649283": "The `updateVariable` function determines whether a variable already exists by checking the `variableExists` parameter, which is passed to the function. If `variableExists` is `true`, the function updates the existing variable using the `PATCH` method. If `variableExists` is `false`, the function creates a new variable using the `POST` method. The `variableExists` parameter is likely determined elsewhere in the script or passed in from an external source.", "73898cce-8bc1-4e75-a680-1bb0d6181cbe": "To rename an enterprise runner group, follow these steps:  1. Go to your enterprise's settings by clicking your profile picture in the top right corner of GitHub.com, then click Settings.  2. In the left-hand menu, click Runner groups.  3. Click the name of the runner group you want to rename.  4. Click Edit.  5. Change the name of the runner group in the \"Name\" field.  6. Click Save changes.  7. The runner group is now renamed.    Changing the name of an organization runner group  To rename an organization runner group, follow these steps:  1. Go to your organization's settings by clicking the organization's name in the top left corner of GitHub.com, then click Settings.  2. In the left-hand menu, click Runner groups.  3. Click the name of the runner group you want to rename.  4. Click Edit.  5. Change the name of the runner group in the \"Name\" field.  6. Click Save changes.  7. The runner group is now renamed.    Changing the name of a runner group  To rename a runner group, follow these steps:  1. Go to your organization's settings by clicking the organization's name in the top left corner of GitHub.com, then click Settings.  2. In the left-hand menu, click Runner groups.  3. Click the name of the runner group you want to rename.  4. Click Edit.  5. Change the name of the runner group in the \"Name\" field.  6. Click Save changes.  7. The runner group is now renamed.    Changing the name of a runner group  To rename a runner group, follow these steps:  1. Go to your organization's settings by clicking the organization's name in the top left corner of GitHub.com, then click Settings.  2. In the left-hand menu, click Runner groups.  3. Click the name of the runner group you want to rename.  4. Click Edit.  5. Change the name of the", "5afbe60d-54cd-4112-864b-f708fa5d1e6d": "To learn how to make the most of GitHub Sponsors, you can refer to the provided guides. These guides cover various aspects of using GitHub Sponsors, such as finding contributors to sponsor, attributing sponsorships to your organization, managing your sponsorship, managing your payouts from GitHub Sponsors, setting up GitHub Sponsors for your personal account, setting up GitHub Sponsors for your organization, configuring webhooks for events in your sponsored account, and more. By following these guides, you can maximize the benefits of GitHub Sponsors for your open source contributions or your organization's support of open source projects.", "9be37685-58eb-4d36-9ceb-8d764ee8c0c9": "The `echo` workflow command can be used to enable command echoing at a more granular level in GitHub Actions by setting the `action_echo` output parameter to `enabled` or `disabled` using the `set-output` command, and then using the `echo` command to print the desired output. Here's an example:     ```yaml    jobs:      my-job:        runs-on: ubuntu-latest        steps:          - name: Set command echoing            run: |              echo '::set-output name=action_echo::disabled'              echo '::echo::on'              echo '::set-output name=action_echo::enabled'              echo '::echo::off'              echo '::set-output name=action_echo::disabled'    ```     In this example, command echoing is first disabled, then enabled for a specific command, and then disabled again.  2. Environment variables can be created for sharing with pre and post actions in GitHub Actions using either the `save-state` command or the `set-output` and `env` workflow commands. Here's an example using the `set-output` and `env` commands:     ```yaml    jobs:      my-job:        runs-on: ubuntu-latest        steps:          - name: Set environment variable            run: |              echo 'MY_VAR=my_value' >> $GITHUB_ENV              echo '::set-output name=my_var::${MY_VAR}'          - name: Use environment variable            run: |              echo \"MY_VAR is: ${{ github.env.MY_VAR }}\"    ```     In this example, an environment variable `MY_VAR` is set using the `echo` command and appended to the `$GITHUB_ENV` file. The `set-output` command is used to pass the value of the environment variable to the next job or step. The value of the environment variable is then accessed using the `github.env` syntax in the second step.     Alternatively, you can use the `save-state` command to create environment variables", "b9f1166f-2ebe-4303-9265-b270a42f196a": "To use {% data variables.product.prodname_actions %} to build and deploy a .NET project to Azure App Service, you need to complete the following prerequisites before creating the workflow:  1. Create an Azure App Service plan. 2. Create a web app with a .NET runtime. 3. Optionally, configure a deployment environment.  Once you've completed these prerequisites, you can create a workflow using the provided example workflow, replacing the placeholders with your own values. Remember to set `AZURE_WEBAPP_NAME` in the workflow `env` key to the name of the web app you created, and adjust `AZURE_WEBAPP_PACKAGE_PATH` and `DOTNET_VERSION` as needed.  Note that {% data reusables.actions.about-oidc-short-overview %} and \"AUTOTITLE.\" are mentioned in the context information, but their significance is not explained. If you're not familiar with these terms, you may want to look them up for more information.", "5c111e40-eff8-4aa3-b8b9-9a62fb40b284": "The `services` keyword in the `jobs` section of the workflow file allows for the use of service containers in GitHub Actions by defining the service containers that should be started and made available to the runner job. This means that the runner job can access the resources provided by the service containers, such as databases or message queues, without the need to set up and manage these resources separately. The service containers are started before the runner job is executed, and they are stopped after the runner job completes. This makes it easier to manage and scale resources in a containerized environment, as the service containers can be easily replaced or scaled as needed. Additionally, the `ports` keyword can be used to map ports between the service container and the host, allowing for external access to resources provided by the service container.", "546f80ab-3af6-4660-93ba-26298fe34703": "Short titles for categories, map topics, and articles should be 2-3 words long, use the base form of verbs instead of gerunds, omit product and feature names if it is clear what product or feature they relate to, not introduce new words that are not in the full title, and be parallel to short titles for similar content. They should also adhere to character limits, with categories having less than 27 characters, map topics having less than 30 characters, and articles having less than 31 characters and ideally between 20 and 25 characters. This formatting provides contextual understanding while still being concise enough to fit in the sidebar navigation.", "44e549c9-90f2-4680-8cb5-75ec53288534": "The context information provided does not explicitly state the current pull request number. Therefore, we cannot determine the value of `$TRAVIS_PULL_REQUEST` in this context.", "e2527c05-cc52-4173-bd93-914a2bd8415e": "The purpose of configuring a custom dev container in a GitHub Codespaces project is to provide a fully reproducible environment for all users in the repository, ensuring that everyone has access to the necessary tools and scripts required for development. This can be achieved by configuring a dev container with all the required tools and scripts specific to the project. To do this for a PHP project, you can follow the steps provided in the context information. First, open the project in a codespace, then add a dev container configuration using the command palette. Select the PHP version you want to use, and install any additional tools required for your project. Finally, overwrite the devcontainer.json file with your custom configuration. This will ensure that all users in the repository have access to the same environment, making it easier to collaborate and debug issues.", "99b88e7d-0886-477d-8123-e8e14fbd0e04": "The category of events that contains all activities related to custom repository roles in GitHub is \"role\".", "1aa848c2-ebaa-46ca-acf7-b8349894ceee": "The difference between writing query help in the \".qhelp\" format and including it alongside the corresponding query is the format in which the query help is written. While query help can be included alongside the corresponding query for consistency with standard {% data variables.product.prodname_codeql %} queries, it can also be written in the \".qhelp\" format. Query help written in \".qhelp\" files cannot be included in SARIF files or processed by code scanning and must be converted to markdown before running the analysis. This is because SARIF files and code scanning are used for specific purposes, such as generating static analysis results, and query help written in \".qhelp\" format is not compatible with these formats. Therefore, it must be converted to markdown before running the analysis to ensure compatibility with these tools.", "e3a91b44-1261-4697-be94-c2b077138b06": "To find other projects to contribute to, one can explore popular repositories on platforms like GitHub, GitLab, or Bitbucket. They can also search for projects related to their interests or skills using keywords or tags. When deciding whether to merge a pull request submitted by someone else, the project owner should consider factors such as the relevance and usefulness of the changes, the quality and style of the code, the potential impact on the project's functionality or performance, and the level of testing and documentation provided by the contributor. It's also important to ensure that the changes align with the project's goals and values, and that they don't introduce any security or compatibility issues. Ultimately, the decision to merge a pull request is a judgment call based on a combination of technical, social, and practical considerations.", "30e25d1a-baa5-4aaa-8290-34606f8a7bec": "To determine the optimal inventory level for a retail store, several factors should be considered, including:  1. Demand: The historical sales data of the product should be analyzed to determine the average demand per unit time. This will help in estimating the future demand and the required inventory level.  2. Lead time: The time taken to receive the product from the supplier after placing an order should be considered. This will help in determining the reorder point and the safety stock required.  3. Holding cost: The cost incurred for storing the inventory should be calculated. This includes the cost of rent, insurance, and depreciation.  4. Shortage cost: The cost incurred due to stockouts should be calculated. This includes the lost sales, additional shipping costs, and the cost of expedited orders.  5. Service level: The desired level of service should be determined. This is the probability that the required product will be available in stock when needed.  6. Safety stock: The additional inventory held to mitigate the risk of stockouts should be calculated. This will depend on the demand variability and the lead time.  7. Reorder point: The inventory level at which a new order should be placed should be determined. This will depend on the lead time, the demand variability, and the safety stock.  By considering these factors, the optimal inventory level can be determined using inventory management techniques such as Economic Order Quantity (EOQ) or Just-in-Time (JIT) inventory management.", "d6af4194-6815-4de9-af12-d548ff321304": "To migrate CircleCI pipelines to GitHub Actions using the GitHub Actions Importer tool, you need to follow these steps:  1. Ensure that you have a CircleCI account or organization with projects and pipelines that you want to convert to GitHub Actions workflows. 2. Create a CircleCI personal API token for your account or organization. 3. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %} with the \"workflow\" scope. 4. Install the GitHub Actions Importer CLI extension. 5. Configure the required credentials and options for the GitHub Actions Importer using the `configure` command. 6. Use the `dry-run` command to preview the migration without actually converting the pipelines. 7. Use the `migrate` command to convert the pipelines.  Some limitations of this process include the fact that automatic caching in between jobs of different workflows is not supported, and the `audit` command is only supported when using an organization account. Additionally, certain CircleCI constructs, such as contexts, project-level environment variables, unknown job properties, and unknown orbs, must be migrated manually.", "8be461de-1bf8-4c69-a986-cb8aea5fadb0": "To ensure that a codespace has access to a specific package in a repository, you need to grant access to the repository where the codespace is being launched. This can be done by following these steps:  1. Go to the package settings page for the package you want to share with codespaces in a repository. 2. Click on \"Manage Codespaces access\" under the \"Access\" section. 3. Click \"Add repository\" and search for the repository you want to add. 4. Repeat step 3 for any additional repositories you would like to allow access. 5. If the codespaces for a repository no longer need access to the package, you can remove access by clicking the trash icon next to the repository.  Note that the specified repository does not need to be the repository where the source code for the package is kept. You can give codespaces in multiple repositories access to a package.  If you want to change the visibility of your package, you can do so by clicking \"Change visibility\" at the bottom of the package settings page and selecting either \"Public\" or \"Private\". However, once you make a package public, you cannot make it private again.", "4d78ba33-4db3-4ac6-aa72-7872bde1cff4": "To register your {% data variables.product.prodname_github_app %} to make authenticated requests to {% data variables.product.company_short %}'s APIs, follow these steps:  1. Go to your {% data variables.product.prodname_github_app %} settings page. 2. Click on the \"Registrations\" tab. 3. Click on the \"Register a new application\" button. 4. Enter a name for your app and a description (optional). 5. Choose the type of app you're creating (e.g., desktop, server, or mobile). 6. Enter the homepage URL and the authorization callback URL for your app. 7. Choose the scopes that your app needs to access GitHub data. 8. Click on the \"Register application\" button. 9. You'll be redirected to a page with your app's client ID and client secret. Keep these secure and use them to authenticate your requests to GitHub's APIs.  For more detailed instructions, refer to the official documentation on registering a {% data variables.product.prodname_github_app %}: https://docs.github.com/en/developers/apps/creating-an-app.", "6544f971-a3f9-4393-8f20-ac958fbaa476": "To cancel your {% data variables.product.prodname_copilot_business_short %} subscription for your organization account and disable {% data variables.product.prodname_copilot %} for your organization, you need to remove all assigned {% data variables.product.prodname_copilot %} seats, as outlined in the steps provided in the context information. This process is necessary to discontinue the use of {% data variables.product.prodname_copilot %} for your organization and avoid further subscription fees.", "bbce6ca1-86b7-4956-99d3-1335b26c21ed": "Yes, based on the context provided, we can use the {% data variables.product.prodname_cli %} to create a weekly team sync issue with a specific title, assignees, labels, and body. Here's an example of how to do it:  1. First, make sure you have the {% data variables.product.prodname_cli %} installed on your local machine. You can download it from the GitHub website.  2. Navigate to the repository where you want to create the weekly team sync issue.  3. Run the following command to create a new workflow file:     ```    gh workflow create Weekly-Team-Sync.yml    ```  4. Open the Weekly-Team-Sync.yml file in your preferred text editor.  5. Copy and paste the following YAML contents into the file:     ```yaml    name: Weekly Team Sync    on:      schedule:        - cron: 20 07 * * 1     jobs:      create_issue:        name: Create team sync issue        runs-on: ubuntu-latest        permissions:          issues: write        steps:          - name: Create team sync issue            run: |              if [[ $CLOSE_PREVIOUS == true ]]; then                previous_issue_number=$(gh issue list \\                  --label \"$LABELS\" \\                  --json number \\                  --jq '.[0].number')                if [[ -n $previous_issue_number ]]; then                  gh issue close \"$previous_issue_number\"                  gh issue unpin \"$previous_issue_number\"                fi              fi              new_issue_url=$(gh issue create \\                --title \"$TITLE\" \\                --assignee \"$ASSIGNEES\" \\                --label \"$LABELS\" \\                --body \"$BODY\")              if [[ $PINNED == true ]]; then                gh issue pin \"$new_issue_url\"              fi            env:              GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_", "40b6c874-fac5-426a-bcdd-442598b4d24f": "To find beginner-friendly issues in a specific repository on GitHub, you can visit the repository's `github.com///contribute` page. This page lists beginner-friendly issues that are labeled as \"good first issues\" or have a similar label. You can also use the repository's search function to search for issues with the label \"good first issue\" or a similar label. Additionally, you can sort the issues list by the \"assigned\" or \"open\" labels to find issues that are currently being worked on by other contributors. This can help you find issues that are more likely to be beginner-friendly, as they may have more guidance or support from the project maintainers.", "a09cddb9-42d9-4d7a-be42-8b87924b5012": "To retrieve a JWT from GitHub's OIDC provider using `curl` in your GitHub Actions, you can use the following steps:  1. Define the token settings using `actions/github-script` with the `core` toolkit in your deployment job. For example:  ```yaml jobs:   job:     runs-on: ubuntu-latest     steps:     - uses: {% data reusables.actions.action-github-script %}       id: script       timeout-minutes: 10       with:         debug: true         script: |           const token = process.env['ACTIONS_RUNTIME_TOKEN']           const runtimeUrl = process.env['ACTIONS_ID_TOKEN_REQUEST_URL']           core.setOutput('TOKEN', token.trim())           core.setOutput('IDTOKENURL', runtimeUrl.trim()) ```  2. Retrieve the token and the URL to request the JWT using `curl` in your GitHub Action. For example:  ```yaml     - run: |         IDTOKEN=$(curl -H \"Authorization: bearer {% r  Replace `{% r %}` with the value of the `TOKEN` output from the previous step.  3. Request the JWT from GitHub's OIDC provider using the URL and the token. For example:  ```yaml     - run: |         JWT=$(curl -s -X POST -H \"Authorization: Bearer {% r %}\" -H \"Content-Type: application/x-www-form-urlencoded\" --data-urlencode \"grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer\" --data-urlencode \"assertion={% r %}\" {% r %})  Replace `{% r %}` with the value of the `IDTOKENURL` output from the previous step, and replace `{% r %}` with the value of the `IDTOKEN` output from the previous step.  4. Use the JWT in your deployment process. For example:  ```yaml     - run: |", "9257b5a9-11e2-4175-bed2-3b1c3f89cad9": "If you want to publish your package to multiple registries, you should configure the `publishConfig` fields in your `package.json` file instead of specifying the registry-url using the `setup-node` action. This is because when you specify the registry-url using the `setup-node` action, it limits you to publishing the package to only that registry. By configuring the `publishConfig` fields in your `package.json` file, you can specify different registry URLs for different registries, allowing you to publish your package to multiple registries.", "909e51d6-61b3-4478-aec6-8053f2ee3b3c": "The recommended approach for minimizing the number of organizations in an enterprise setting is to create as few organizations as possible, as most businesses are best served by a single organization. This is important because it encourages innersource practice, allows discussions to involve a wider audience, and reduces administrative overhead. Additionally, a smaller number of organizations makes it easier to enforce business rules and regulatory compliance through policies.", "bbeb8c38-b761-4ca9-9e39-e225d8e33172": "The note labeled as \"Note:\" is a reminder for users who are utilizing Student Codespaces, a feature provided by GitHub Education, to be aware of the usage limits and restrictions. This note advises users to refer to the documentation labeled as \"AUTOTITLE.\", \"AUTOTITLE.\", and \"AUTOTITLE.\" for further information regarding Student Codespaces. The specific details of the usage limits and restrictions are not provided in the context information, but it is suggested to consult the documentation for more details.", "ee5f2e87-d435-40f1-a95b-2720491f2d5c": "Release notes for changes should include relevant links to GitHub Docs by providing links to the relevant GitHub Docs pages that provide more information about the change. This can help users understand the context and impact of the change, as well as provide additional resources for further learning. The links should be included in the release notes in a clear and concise manner, preferably in the body of the note rather than in a separate section or appendix. This can help ensure that users are aware of the links and can easily access the relevant documentation.", "f9c7bc08-aaa8-426a-9f56-a29231ccd863": "To contribute to the GitHub Advisory Database as a member of the curation team, you can follow these steps:  1. Fork the github/advisory-database repository. 2. Create a new branch for the advisory you want to contribute to. 3. Edit the advisory file, following the guidelines provided in the repository. 4. Add a comment explaining your changes and the reason for them. 5. Submit a pull request to merge your changes into the main repository. 6. Wait for the review and approval of the curation team.  Alternatively, you can also open a pull request directly on an advisory file in the github/advisory-database repository, following the contribution guidelines provided in the repository.  If the advisory originated from a GitHub repository, you can also tag the original publisher for optional commentary.  If you have GitHub Connect enabled for your organization, you can also view and suggest improvements to advisories directly on the GitHub Advisory Database website by following the instructions provided in the context information.", "bf052510-a25f-4091-a419-cf1287dbffb2": "Service accounts can be exempted from IP controls in your IdP's CAP to avoid IP controls in CAP for enterprises using OIDC SSO with {% data variables.product.prodname_actions %}. This is recommended as actions that use a personal access token (PAT) may be blocked by your IdP's CAP. By creating service accounts and exempting them from IP controls, you can ensure that actions using PATs are not affected by your IdP's CAP.", "728855df-932a-46d8-9c8f-52b2a9356adc": "The context information provided doesn't give any information about the maximum storage capacity for a {% data variables.product.prodname_github_codespaces %} container. Therefore, we cannot answer this question.", "4f404cff-16b7-4293-b8f8-dee1f030d516": "The error message \"Body[i]: `options` must be unique\" indicates that the choices defined in the `options` array for a dropdown or checkbox input type in the specified body block (represented by `i`) must not have duplicate values. To fix this error, ensure that each choice in the `options` array is unique. For example:  ```yaml body: - type: dropdown   attributes:     label: Favorite color     options:       - Red       - Green       - Blue ```  In this example, the `options` array contains unique choices for the dropdown input type.    Body[i]: `options` must not include the reserved word, none  The error message \"Body[i]: `options` must not include the reserved word, none\" indicates that the `options` array for a dropdown input type in the specified body block (represented by `i`) should not include the reserved word \"none\". This is because \"none\" is used to indicate non-choice when a dropdown is not required. To fix this error, remove \"none\" as an option from the `options` array. For example:  ```yaml body: - type: dropdown   attributes:     label: Favorite type of pie     options:       - Steak & Ale       - Chicken & Leek       - Apple       - Cherry ```  In this example, the `options` array does not include the reserved word \"none\".    Body[i]: `options` must not include booleans. Please wrap values such as 'yes', and 'true' in quotes  The error message \"Body[i]: `options` must not include booleans. Please wrap values such as 'yes', and 'true' in quotes\" indicates that the `options` array for a dropdown input type in the specified body block (represented by `i`) should not include boolean values (such as \"true\" or \"false\") without being wrapped in quotes. To fix this error, wrap boolean values in quotes when adding them to the `options` array. For example:  ```yaml body: - type: dropdown   attributes:     label: Do you prefer", "29326553-91e3-42fa-927f-502a7bbaca3e": "Branch protection rules can be configured to require review from code owners while still allowing for auto assignment of reviewers by specifying both the team and the individual reviewers as required reviewers. This ensures that the team is still requested for review, but the auto assignment feature will still assign individual reviewers from the team based on the chosen routing algorithm. The team request will be removed once individual reviewers have completed their reviews, as long as the branch protection rule is not configured to require review from code owners. If the branch protection rule does require review from code owners, then the team request cannot be removed and will remain in addition to the individual requests until all required reviews have been completed.", "2f052ca1-663a-4749-be02-25f00d542d48": "According to the context information provided, hanging build detection options are not transformed during the migration process from Bamboo to GitHub Actions because there is no equivalent in GitHub Actions. The closest option in GitHub Actions is `timeout-minutes` on a job, which can be used to set the maximum number of minutes to let a job run. This can be used as a workaround to simulate hanging build detection, but it is not a direct replacement.", "a44fa687-f644-414f-8957-9d3c7f2931b9": "Without further context, it is unclear how \"AUTOTITLE\" is utilized in the provided context. Please provide additional information or clarify the context to assist in answering this question.", "e7fbe36d-ba2f-43ea-9d0f-9f97628f6718": "To read or write gists on behalf of a user using the GitHub REST API, you need the gist OAuth scope and a token. Authentication is required to read or write gists for a specific user, except for public gists which can be read anonymously. If you're using the GitHub REST API for GitHub.com or GitHub Enterprise Server (GHES), you can create gists for anonymous users without a token for public gists. However, for reading or writing gists on behalf of a user, you need to obtain an OAuth token with the gist scope. For more information on OAuth authentication and scopes, refer to the GitHub documentation on OAuth apps and authorization.", "617ea4d2-0de6-4f24-9de6-8b1a93f6117c": "The name of the branch that should be pushed to the origin remote when configuring publishing source for GitHub Pages is BRANCH, as specified in step 1 of the provided context information.", "68aa2634-f5e6-4c1f-87aa-82d6d356c764": "To view and revoke authorized credentials for a user in GitHub Enterprise using SAML, follow these steps:  1. Go to your GitHub Enterprise organization. 2. Click on your profile picture in the top right corner and select \"Settings\" from the drop-down menu. 3. Click on \"Accounts\" in the left-hand menu. 4. Click on the \"Enterprise\" tab. 5. Click on the organization for which you want to view and revoke authorized credentials. 6. Click on the \"People\" tab. 7. Find the user for whom you want to view and revoke authorized credentials. 8. Click on the user's name to open their profile. 9. Click on the \"SAML\" tab. 10. Click on \"View authorized credentials\" to see a list of all the SAML identities linked to the user's account. 11. To revoke an authorized credential, click on the \"Revoke\" button next to the identity you want to revoke. 12. Confirm the revocation by clicking \"Revoke\" in the confirmation dialog box.  Note: The steps may vary slightly depending on the version of GitHub Enterprise you are using.", "1c08c128-60ff-4d8b-8407-f0ed10f287bd": "To determine if a step in a GitHub Actions workflow has failed, you can use the `failure()` function in the `if` condition of the step. This function returns `true` if the previous step in the workflow failed, and `false` otherwise.  Here's an example of how to use the `failure()` function:  ```yaml steps:   build:     name: Build the project     run: npm run build   test:     name: Run tests     if: failure()     run: npm run test:ci ```  In this example, the `test` step will only run if the previous `build` step failed.  You can also add conditions to the `failure()` step using object filters. For example, you can check if a specific step failed by using the `steps.*.conclusion` filter. Here's an example:  ```yaml steps:   demo:     name: Run a demo     run: npm run demo   check-demo:     name: Check demo result     if: failure() && steps.demo.conclusion == 'failure'     run: echo \"The demo failed!\" ```  In this example, the `check-demo` step will only run if the `demo` step failed and its conclusion is 'failure'.  Overall, the `failure()` function and object filters provide a powerful way to control the flow of your GitHub Actions workflow based on the success or failure of previous steps.", "06a58bc9-c9d5-4789-ba91-98e2af7fc75d": "The rules for naming secrets in {% data variables.product.prodname_actions %} are:  1. Secret names must be unique within their level (organization, repository, or environment). 2. Secret names must contain only alphanumeric characters, dots, and hyphens. 3. Secret names must start and end with an alphanumeric character or a dot. 4. Secret names must not contain two consecutive dots.  If an organization, repository, and environment all have a secret with the same name, the environment-level secret takes precedence. This means that if a secret with the same name exists at multiple levels, the value of the secret at the environment level will be used instead of the values at the organization or repository level. This helps ensure that the most specific and up-to-date value is used in workflows.", "a5b99997-2369-4a28-9dd5-22bb38d3a0af": "Storing sensitive information such as SSH keys, API access tokens, etc. On self-hosted runners in {% data variables.product.prodname_dotcom %} poses several risks. Firstly, if the runner is compromised, the attacker gains access to all the sensitive information stored on it. Secondly, if multiple jobs run on the same runner, there is a risk of secret leakages as secrets can be seen by other jobs running on the same runner. To mitigate these risks, it is recommended to minimize the amount of sensitive information stored on the runner and always be mindful that any user capable of invoking workflows has access to this environment. Additionally, using just-in-time (JIT) runners, which perform at most one job before being automatically removed, can further improve runner registration security. However, it's essential to ensure that the JIT runner uses a clean environment to avoid exposing information from the environment. Finally, the placement of the runner in the {% data variables.product.prodname_dotcom %} hierarchy determines who will be able to manage the runner. If a centralized team owns the runners, then it's recommended to add them at the enterprise or organization level to ensure consistent management and security policies.", "5acbcacd-3e6e-46fd-a36c-2dd45297b86d": "Docker Hub and {% data variables.product.prodname_registry %} are both Docker registries where you can store and manage Docker images. The main difference between the two is that Docker Hub is a third-party registry, while {% data variables.product.prodname_registry %} is a registry provided by GitHub. This means that with Docker Hub, you'll need to create an account and sign in to push images, while with {% data variables.product.prodname_registry %}, you can push images directly from your GitHub repository without the need for additional authentication. Additionally, {% data variables.product.prodname_registry %} integrates more closely with GitHub workflows and provides additional features such as image scanning and access control.", "bab81ec2-c2d3-445a-946b-c8bb377a95d3": "Based on the context information provided, \"employee\" refers to an individual whose personal information, whether supplied by GitHub or generated by the Developer, could be used to identify them alone or in combination with other information. \"Processing\" refers to any operation or set of operations performed on GitHub Protected Data, whether manual or automatic, including but not limited to collection, recording, organization, storage, adaptation, alteration, retrieval, consultation, use, disclosure by transmission, dissemination, or otherwise making available, blocking, erasure, or destruction. \"Protected Data\" refers to any confidential information transferred by GitHub to the Developer about a GitHub End User, including Personal Information, metadata, Usage Data, or other data or information associated with GitHub End Users.", "06f2334e-ca08-4ff7-a79e-37c52db0369e": "Workflow runs triggered by {% data variables.product.prodname_dependabot %} from `push`, `pull_request`, `pull_request_review`, or `pull_request_review_comment` events are treated as if they were opened from a repository fork, resulting in a read-only `GITHUB_TOKEN` and limited access to secrets. This can cause workflows that attempt to write to the repository to fail.", "c0a6429e-75b9-438a-9c59-df604c8a0f91": "The ID of the current pipeline can be obtained using the `CI_PIPELINE_ID` context variable, which is set to the format of `{repository}/{workflow}` for the current pipeline.", "08448fbe-3624-400b-aa2e-da4b1a32f9b1": "In SVN, an external is a reference to a separate repository that is included in the current repository. It is similar to a Git submodule, which is a way to include an existing Git repository as a subdirectory of another Git repository. However, while SVN externals are automatically updated when changes are made in the referenced repository, Git submodules must be explicitly updated. This is because Git allows for more flexible branching and merging, which can result in changes to previous commits. As a result, Git submodules do not automatically update to reflect these changes.", "641cddf5-42b5-4c02-b750-d9a9fda6ee39": "To schedule maintenance for different times or dates using the {% data variables.product.prodname_enterprise_api %} for {% data variables.location.product_location %}, you can refer to the documentation for \"AUTOTITLE\" in the REST API documentation. This will provide you with detailed instructions on how to use the API to schedule maintenance for specific times or dates.", "28910461-3f41-4f77-befd-00886d50a2a1": "The SHA of the commit that triggered the continuous integration process can be obtained using the variable `$TRAVIS_COMMIT` in the context information provided. Therefore, the answer is: {% raw %}`${{ github.sha }}`{% endraw %}", "dec89447-1ad6-43a6-ae8b-fb4027d20dee": "An alternative to using SSH URLs for cloning a repository is to use a Subversion client. However, as per the context information provided, it is recommended to use Git instead of Subversion, as Git is a distributed version control system, while Subversion is a centralized version control system. Therefore, it is suggested to migrate to Git instead of using Subversion. If you still want to use Subversion, you can refer to the GitHub documentation for Subversion users to learn how to use Git with your existing Subversion workflows. However, it is recommended to switch to Git as soon as possible, as Git provides many more features and benefits than Subversion.  If you're still interested in using Subversion, you'll need to have a Subversion client installed on your computer, and you'll need to have the necessary permissions to access the repository using Subversion. You'll also need to have a Subversion username and password, which you can obtain by contacting the repository owner or administrator. Once you have these requirements, you can use your Subversion client to clone the repository using the Subversion URL provided by the repository owner or administrator.    Cloning with GitHub CLI  {% data reusables.cli.about-cli %}  {% data reusables.cli.installation %}  {% data reusables.cli.authentication %}  {% data reusables.cli.usage %}  {% data reusables.cli.examples %}  {% data reusables.cli.troubleshooting %}  {% data reusables.cli.further-reading %}    Cloning with SSH URLs  {% data reusables.command-line.url-formats %}  {% data reusables.command-line.ssh %}  {% data reusables.command-line.ssh-keygen %}  {% data reusables.command-line.add-ssh-key %}  {% data reusables.command-line.ssh-authentication %}  {% data reusables.command-line.ssh-agent %}  {% data", "7295f43f-a75f-472f-a959-383cf1e98894": "According to the context provided, if a user chooses not to provide or allow certain types of automatic data collection, GitHub may not be able to provide certain services or features fully functional. This decision may result in the unavailability of those services or features. However, the specific consequences will depend on the nature of the data being requested and the service or feature in question. GitHub encourages users to carefully consider the potential impact of withholding certain information before making a decision.", "f233f234-8f8f-4c69-be8d-9ad9364e6ae5": "To use environment variables with a container while running jobs in a container using GitHub Actions, you can define the variables in your workflow file using the `env` keyword, and then reference them in your container's Dockerfile or in your container's command using the `--env` flag. Here's an example:  ```yaml name: My Workflow  jobs:   my-job:     runs-on: ubuntu-latest     container:       image: my-image:latest       options: >-         --env MY_VARIABLE=my-value     steps:       - name: Set up GitHub CLI         uses: github/setup-github-cli@v2.3.1       - name: Checkout         uses: actions/checkout@v2.3.1       - name: Run my command         shell: bash         run: |           echo $MY_VARIABLE ```  In this example, we're defining an environment variable `MY_VARIABLE` with a value of `my-value` using the `options` field of the container block. We're then referencing this variable in our command using the `$` syntax. This will pass the variable to the container as an environment variable, which can be accessed inside the container using the same syntax.", "7d2665e0-d2fc-43d4-a80d-20937988c5eb": "The \"discussions\" feature within a repository allows users to create and participate in discussions related to the repository's content. By using the \"discussions\" GraphQL query, users can list the discussions within a repository, and they can narrow down the list by specifying certain parameters.  Here are the parameters that can be specified:  1. `after` and `before`: These parameters are used to paginate through the list of discussions. `after` specifies the cursor of the last discussion to be included in the list, and `before` specifies the cursor of the first discussion to be excluded from the list.  2. `first` and `last`: These parameters are used to limit the number of discussions returned. `first` specifies the maximum number of discussions to be returned from the beginning of the list, and `last` specifies the maximum number of discussions to be returned from the end of the list.  3. `categoryId`: This parameter is used to filter the discussions by category. If a category ID is specified, only discussions within that category will be returned.  4. `answered`: This parameter is used to filter the discussions by whether they have been answered or not. If `answered` is set to `true`, only discussions with an accepted answer will be returned. If `answered` is set to `false`, only discussions without an accepted answer will be returned. If `answered` is not specified, both answered and unanswered discussions will be returned.  5. `orderBy`: This parameter is used to sort the discussions by a specific field and direction. The available fields are `CREATED_AT` and `UPDATED_AT`, and the available directions are `ASC` (ascending) and `DESC` (descending). The default sorting is by `UPDATED_AT` in descending order.  By using these parameters, users can efficiently search for specific discussions within a repository based on their preferences.", "b3459a72-3116-4b89-8d01-47cbb99119b7": "To disable a workflow from running in GitHub, you can follow these steps:  1. Go to your repository in GitHub. 2. Click on \"Settings\" in the top right corner of the page. 3. Click on \"Workflows\" in the left-hand menu. 4. Find the workflow you want to disable and click on its name to open it. 5. Click on the \"Disable this workflow\" button in the top right corner of the page. 6. Confirm that you want to disable the workflow by clicking on \"Disable workflow\" in the pop-up window.  Alternatively, you can also disable a workflow by editing its YAML file and adding the `name: <workflow-name>` line at the beginning of the file, followed by `disabled: true`. Save the changes and commit the file. This will disable the workflow and prevent it from running automatically.  Note that disabling a workflow will prevent it from running automatically, but you can still manually trigger it by clicking on the \"Run workflow\" button in the Actions tab of your repository.", "11f84c36-a220-419d-bc60-758525a572e7": "To replace a specific column ID with a different one in order to unlabel issues and pull requests in a GitHub repository using a workflow, follow these steps:  1. Navigate to your {% data variables.projects.projects_v1_board %} in your GitHub repository. 2. Click on the column that you want to replace the ID of. 3. Click on the three dots (kebab menu) next to the column name. 4. Click on \"Copy column link\". 5. Note down the column ID at the end of the copied link. For example, the column ID for \"https://github.com/octocat/octo-repo/projects/1#column-24687531\" is \"24687531\". 6. Open your workflow file in your GitHub repository. 7. Replace the specific column ID in the context information with the new column ID that you want to use. For example, replace \"12345678\" with \"24687531\". 8. If you want to act on more than one column, separate the conditions with \"||\". For example, \"if github.event.project_card.column_id == '12345678' || github.event.project_card.column_id == '87654321'\" will act whenever a project card is added to column \"12345678\" or column \"87654321\".  The purpose of removing a label from issues or pull requests that are moved to a specific column in a GitHub repository using a workflow is to automate the process of removing a label when an issue or pull request is moved to a specific column. This can be useful for streamlining workflows and reducing manual effort.  To change the label name in the \"github.rest.issues.removeLabel()\" function to remove a specific label, follow these steps:  1. Open your workflow file in your GitHub repository. 2. Locate the \"github.rest.issues.removeLabel()\" function. 3. Change the value for \"name\" to the name of the label that you want to remove from issues or", "bea11e2c-b4dd-45f2-aca2-d34ff5d979f0": "Based on the context provided, you should check the terminal window where your local server is running for any output or errors related to the webhook delivery. If your code is written to log the delivery to a file or database, you should also check those sources for confirmation that the delivery was processed successfully.  For example, if you're using Node.js and Express to create a simple webhook receiver, you might see output similar to the following in your terminal window:  ``` Received webhook: {   \"action\": \"opened\",   \"issue\": {     \"number\": 123,     \"title\": \"My issue title\",     \"url\": \"https://github.com/myorg/myrepo/issues/123\"   },   \"repository\": {     \"full_name\": \"myorg/myrepo\",     \"owner\": {       \"login\": \"myorg\"     },     \"name\": \"myrepo\",     \"url\": \"https://api.github.com/repos/myorg/myrepo\"   },   \"sender\": {     \"login\": \"myuser\"   },   \"organization\": {     \"login\": \"myorg\"   },   \"event\": {     \"id\": 123456789,     \"type\": \"IssuesEvent\",     \"actor\": {       \"id\": 123456789,       \"login\": \"myuser\"     },     \"repo\": {       \"id\": 123456789,       \"name\": \"myrepo\",       \"full_name\": \"myorg/myrepo\"     },     \"payload\": {       \"action\": \"opened\",       \"issue\": {         \"number\": 123,         \"title\": \"My issue title\",         \"url\": \"https://github.com/myorg/myrepo/issues/123\"       },       \"pull_request\": null,       \"comment\": null,       \"gist\": null,       \"public\": false,       \"pages\": null,", "b9612eb9-5eb6-4e26-8c4e-7f69ee07dff5": "Without further context, it is impossible to determine the specific software application being developed or the nature of its requirements. Some key considerations that may need to be taken into account while developing a software application include:  1. Functionality and user experience: The software application should meet the needs of its intended users and provide a user-friendly interface.  2. Performance and scalability: The software application should be able to handle a large number of users and transactions, and should be optimized for performance.  3. Security and privacy: The software application should implement appropriate security measures to protect user data and prevent unauthorized access.  4. Maintainability and testability: The software application should be easy to maintain and test, with clear documentation and a modular design.  5. Compatibility and integration: The software application should be compatible with other systems and applications, and should be able to integrate with third-party services as needed.  6. Regulatory and legal compliance: The software application should adhere to relevant regulatory and legal requirements, such as data protection laws and industry standards.  7. Cost and resource constraints: The software application should be developed within budgetary and resource constraints, and should provide a reasonable return on investment.  8. Future-proofing and adaptability: The software application should be designed with an eye towards future developments and trends, and should be adaptable to changing requirements and technologies.  These are just a few of the many factors that may need to be considered while developing a software application. The specific requirements will depend on the nature of the application and the needs of its users.", "ee074a33-611c-41d6-8736-7497a07b5bdb": "Starter workflows in the context of {% data variables.product.prodname_actions %} are pre-configured templates that help users create their own workflows for a repository. They differ from starting from a blank workflow file as some of the work has already been done for the user, making it easier and faster to set up a workflow. Starter workflows are recommended by {% data variables.product.product_name %} based on the language and framework used in the repository, and users can also create their own starter workflows to share with their organization.", "da77b2d7-abc2-4208-8ddf-dffdfa5cdc13": "Based on the given context, it is unclear how the concept of \"AUTOTITLE\" differs from \"AUTOTITLE\". Both terms are mentioned without any further explanation or context provided. Without prior knowledge or additional information, it is impossible to determine any differences between the two concepts. It is recommended to provide more context or clarify the meaning of these terms to better understand how they differ, if at all.", "c0dd0743-8678-4350-8c92-784ba95e7d5c": "The ports required for {% data variables.product.prodname_actions %} to connect to {% data variables.location.product_location %} are 443 for HTTPS and 80 for HTTP (required if TLS is not configured). On the other hand, for {% data variables.product.prodname_github_connect %}, the connection between {% data variables.product.product_name %} and {% data variables.product.prodname_dotcom_the_website %} uses HTTPS over ports 443 or 80, and TLS is required. Therefore, while both services require HTTPS, the specific ports required for {% data variables.product.prodname_actions %} and {% data variables.product.prodname_github_connect %} are different.", "6f71f06a-a4d7-406c-a0e0-cf24b16dd9f7": "Uptime refers to the percentage of total possible minutes that a specific GitHub service is available in a given calendar quarter. The calculation for Uptime varies for different Service Features, as outlined in the Uptime Calculation table provided. Downtime is defined as a period of time where either the error rate exceeds 5% in a given minute for any Service Feature or the service is unavailable as determined by a combination of GitHub's internal and external monitoring systems. If GitHub fails to meet the committed Uptime for a Service Feature, Service Credits may be awarded to the customer based on the Service Credits Calculation outlined in the table.", "39ab23ac-2e6e-41db-bf2c-b19002a3062d": "To ensure that an {% data variables.product.prodname_oauth_app %} user access token has the necessary scopes and permissions to use a specific endpoint, you should:  1. Check the documentation for the specific endpoint to determine which scopes are required. 2. Ensure that the user who authorized the token has the necessary permissions to use the endpoint. For example, if the endpoint can only be used by organization owners, make sure that the user who authorized the token is an owner of the affected organization. 3. Verify that the {% data variables.product.prodname_github_app %} has access to any repositories that the endpoint will affect. 4. Confirm that the user has access to any repositories that the endpoint will affect. 5. Make sure that the user has approved any updated permissions for your {% data variables.product.prodname_github_app %}. 6. Check that the organization has not blocked OAuth app access, if you are using an endpoint that will affect resources owned by an organization. 7. Verify that the token has not been expired or revoked.  By following these steps, you can ensure that the {% data variables.product.prodname_oauth_app %} user access token has the necessary scopes and permissions to use the specific endpoint.", "f7b2bf82-1cd3-473b-99f3-fe61f3c8c326": "{% data variables.product.product_name %} maintains three sets of default {% data variables.product.prodname_secret_scanning %} patterns:  1. Partner patterns: These patterns are used to detect potential secrets in all public repositories and public npm packages. They are specifically designed to detect secrets issued by specific service providers, and alerts are sent to the relevant service provider whenever a secret is detected in a commit.  2. User alert patterns: These patterns are used to detect potential secrets in public repositories with {% data variables.secret-scanning.user_alerts %} enabled. This feature is available for owners of public repositories and organizations using {% data variables.product.prodname_GHE_cloud %} with {% data variables.product.prodname_GH_advanced_security %}.  3. Push protection patterns: These patterns are used to detect potential secrets in repositories with {% data variables.product.prodname_secret_scanning %} as a push protection enabled.  In summary, partner patterns are used in all public repositories and public npm packages, user alert patterns are used in public repositories with {% data variables.secret-scanning.user_alerts %} enabled, and push protection patterns are used in repositories with {% data variables.product.prodname_secret_scanning %} as a push protection enabled.", "b193e5bd-88f6-4d9b-9654-f40046ffd44a": "According to the provided context information, GitHub does not knowingly collect information from or direct any of its content specifically to children under 13. If GitHub learns or has reason to suspect that a user is under the age of 13, it will unfortunately have to close their GitHub.com account. This is in accordance with GitHub's Terms of Service regarding account termination.", "5258f008-bc58-43d4-a9c2-3b6ddd1b6442": "{% data variables.product.prodname_code_scanning %} can be integrated into a development workflow using {% data variables.product.prodname_actions %}, which allows for the automation of tasks within a GitHub repository. This integration provides several benefits, including:  1. Early detection of vulnerabilities and errors: {% data variables.product.prodname_code_scanning %} can be configured to scan code automatically as part of the continuous integration (CI) process, allowing developers to catch and address issues early in the development cycle.  2. Improved code quality: By identifying and addressing vulnerabilities and errors early on, developers can improve the overall quality of their code, reducing the risk of security breaches and other issues down the line.  3. Increased efficiency: By automating the code scanning process, developers can save time and resources that would otherwise be spent manually reviewing code for vulnerabilities and errors.  4. Enhanced collaboration: By integrating {% data variables.product.prodname_code_scanning %} into the development workflow, developers can collaborate more effectively by sharing insights and findings related to code quality and security.  Overall, integrating {% data variables.product.prodname_code_scanning %} into a development workflow using {% data variables.product.prodname_actions %} can help to improve code quality, reduce risk, and increase efficiency, ultimately leading to better software and a more productive development team.", "53bf955e-0856-48ff-b0ad-bc13f671af25": "To determine the number of private Rubygems packages using the context information provided, we can refer to the `CN` and `CO` variables. `CN` represents the total number of published Rubygems packages (private, public, and internal), while `CO` represents the number of private Rubygems packages. By subtracting `CP` (number of public Rubygems packages) from `CN`, we can find the number of private and internal Rubygems packages. Therefore, the number of private Rubygems packages is `CO`:  Number of private Rubygems packages = `CO`  To find `CO`, we can use the following formula:  `CO` = `CN` - `CP`  So, the formula to calculate the number of private Rubygems packages using the context information provided is:  Number of private Rubygems packages = `CN` - `CP`  However, we need the values of `CN` and `CP` to calculate `CO`. These variables are provided in the context information, so we can use them to calculate the number of private Rubygems packages.  If the context information doesn't provide the values of `CN` and `CP`, then we wouldn't be able to calculate the number of private Rubygems packages using the provided context information.", "ac454edd-0863-40f2-82fb-ef07c27b77da": "To create a new Jekyll site, open your terminal or command prompt and navigate to the directory where you want to create the new site. Then, run the following command:  ```shell $ jekyll new --skip-bundle . ```  This command will create a new Jekyll site in the current directory with the name \"jekyll new\" and skip the bundle installation process.  After creating the new site, open the Gemfile that Jekyll created. To add the `github-pages` gem, find the line that starts with `gem \"jekyll\"` and add a \"#\" at the beginning of the line to comment it out. Then, add the following line below it:  ```shell gem \"github-pages\", \"~> GITHUB-PAGES-VERSION\", group: :jekyll_plugins ```  Replace `GITHUB-PAGES-VERSION` with the latest supported version of the `github-pages` gem. You can find this version here: \"Dependency versions.\"  The `github-pages` gem is necessary for deploying to GitHub Pages because it provides the necessary functionality to generate the static site and deploy it to GitHub Pages. Without this gem, you would have to manually generate the site and deploy it to GitHub Pages using other tools or scripts. By using the `github-pages` gem, you can automate this process and make it easier to deploy your site to GitHub Pages.", "97f2ccbe-1d5d-40ca-b934-e3c61af8b055": "If you use an external directory or identity provider (IdP) to centralize access to multiple web applications, you can manage and create accounts for your enterprise members on {% data variables.location.product_location %} using your IdP. This is called external authentication. To do this, you need to configure external authentication for {% data variables.location.product_location %} by following the instructions provided in the articles \"AUTOTITLE\", \"AUTOTITLE\", and \"AUTOTITLE\". If you choose to use external authentication, you can also configure fallback authentication for people who don't have an account on your external authentication provider. For more information, see \"AUTOTITLE.\"  If you use authentication through {% data variables.product.product_name %}, you can create accounts for your enterprise members by following the instructions provided in \"AUTOTITLE.\" However, if you want to provision user accounts on {% data variables.product.product_name %}, you need to configure provisioning on your IdP using System for Cross-domain Identity Management (SCIM) if you're using {% data variables.product.product_name %} on GitHub Enterprise Cloud (GHEC). For more information, see \"AUTOTITLE.\"  If you're using {% data variables.product.product_name %} on GitHub Enterprise Server (GHES) or GitHub Advanced Security (GHAS), you can manage and create accounts for your enterprise members using built-in authentication or external authentication, as described in the context information provided.  If you're using {% data variables.product.product_name %} on GitHub Enterprise Cloud (GHEC), you can't use built-in authentication or external authentication. Instead, you need to use SAML SSO for authentication, and you must configure SAML SSO with a SAML identity provider (IdP) during initialization. For more information, see \"AUTOTITLE.\"  In summary, the availability of external authentication and provisioning options depends on the specific version of {% data variables.product.product_name %} you're using.", "a0b004ab-dbd7-47fb-9b24-43553cadff97": "To automatically regain access to your GitHub account using a recovery code, follow these steps:  1. Go to the GitHub login page and enter your username and password. 2. Click \"Having problems?\" and select \"Use a recovery code\" or \"Request a reset\" (if you're using a GitHub Enterprise Cloud (GHEC) account). 3. Type in one of your recovery codes and click \"Verify\".  If you've saved your recovery codes in a password manager or on your computer's downloads folder, the default filename for recovery codes is \"github-recovery-codes.txt\".  Note: Remember that you should keep your recovery codes secure and not share them with anyone. If you lose your recovery codes, you can generate new ones by going to your account settings and selecting \"Security\" > \"Two-factor authentication\".  I hope that helps! Let me know if you have any other questions.", "e062bb85-74be-4cfc-90c8-e9ccbccbb1ce": "The number of Docker images created in {% data variables.product.prodname_registry %} for {% data variables.location.product_location %} yesterday can be found in the context information provided. The metric we need is BE, which represents the number of Docker images created daily. By checking the context information, we can see that BE is included in the list of metrics provided. Therefore, we can answer the question.  However, we don't know the exact date of yesterday, as the context information doesn't specify it. If we had this information, we could retrieve the value of BE for that specific date and provide an exact answer. If we don't have this information, we can't provide an exact answer, but we know that the metric we need is included in the context information, and we can retrieve the value of BE for any given date.  In summary, we can answer the question, but the accuracy of the answer depends on whether we have the necessary date information. If we don't have this information, we can't provide an exact answer, but we know that the metric we need is included in the context information, and we can retrieve the value of BE for any given date.", "2523ccf9-2e11-4d1c-8907-243b4679aa98": "If your GitHub account is locked due to two-factor authentication (2FA) and you have lost your 2FA credentials and are unable to recover them, you can unlink your email address from the locked account and then link it to a new or existing account. This will allow you to access your account again. To do this, follow these steps:  1. Go to the GitHub login page (https://github.com/login) and enter your username and password. 2. Click \"Unable to verify with your security key?\" to prompt two-factor authentication. 3. Click \"Use a recovery code or request a reset\" to bypass the 2FA process. 4. Click \"Locked out?\" and then click \"Recover your account or unlink an email address\" in the modal that appears. 5. Click \"Send one-time password\" to send an email containing a one-time password to each email address associated with your account. 6. Type the one-time password from your email into the \"One-time password\" text field and click \"Verify email address.\" 7. Follow the instructions to unlink your email address from the locked account. 8. Link your unlinked email address to a new or existing account.  Note that unlinking your email address will not transfer any other associated information, such as repositories, permissions, or profile, to your new account. If you have a backup email address, be aware that unlinking it will not connect your commit history to your new account.  If you prefer not to unlink your email address, you can also try to recover your 2FA credentials or contact GitHub support for further assistance.", "db788bff-9d81-4313-814e-ad3b906ec4e8": "To enable \"Unified contributions\" on the GitHub Connect page, you need to follow these steps:  1. Go to the GitHub Connect page (as described in the context information). 2. If you have an Enterprise account, make sure you're signed in and have access to it. 3. Locate the \"Unified contributions\" option and click the \"Enable\" button to the right of it. 4. If you're using GitHub Enterprise Server (GHES), you'll be redirected to the GHES site to complete the process. 5. Sign in to the GHES site and follow any further instructions provided.  Note: If you're not sure whether you have an Enterprise account or how to access it, you may need to consult your organization's IT or GitHub administrator for assistance.", "281d4a56-7eb5-410d-b8f5-2f7765622d50": "To make a pull request smaller and more focused, aim to fulfill a single purpose with each pull request. This can help make the review process smoother and faster by:  1. Making it easier and faster to review and merge the pull request. 2. Leaving less room to introduce bugs. 3. Providing a clearer history of changes.  By keeping pull requests small and focused, reviewers can quickly understand what the pull request does and provide more targeted feedback. This can also help prevent merge conflicts and make it easier to test and validate the changes. Overall, smaller and more focused pull requests can lead to a more efficient and effective review process.", "b90fba54-2131-4acc-a3eb-1f3afc7e631e": "To configure the IP address for {% data variables.product.prodname_ghe_server %} using the virtual machine console, you can follow these steps:  1. Open the VM console for your {% data variables.product.prodname_ghe_server %} instance. 2. Choose to configure the `IPv4` or `IPv6` protocol, depending on your network setup. 3. Configure options for the protocol you chose, such as the IP address, subnet mask, gateway, and DNS servers. 4. Save your changes and exit the console.  If your network supports both IPv4 and IPv6, we recommend using IPv4 for {% data variables.product.prodname_ghe_server %}, as it is more widely supported and better documented. However, if you prefer to use IPv6, you can follow the same steps to configure it.  Note that we do not support adding additional network adapters to {% data variables.product.prodname_ghe_server %}.", "23fd599e-ebcb-42eb-be62-f566fbb83d79": "Based on the error messages provided, it is clear that the 'input' argument in the 'addComment' mutation is expected to be of type 'AddCommentInput!'. The 'AddCommentInput' input object has an argument called 'clientMutationId' which is expected to be of type 'String'.", "fd066fc9-b23a-4bcb-a8ab-c2cb189354f9": "If a vendor or prospective vendor sends a gift or invitation that is \"of value\" to a customer or prospective customer, it should be declined or run by Legal for approval before accepting, as mentioned in the context information. The value of what's considered \"of value\" can vary, and what's acceptable can differ based on cultural norms, so it's always best to err on the side of caution and seek guidance from Legal.", "bfa3dfe4-a9e8-427e-b60e-54ab7841f9b6": "To resolve merge conflicts caused by competing line changes in Git, you must choose which changes to incorporate from the different branches in a new commit. This is done by following these steps:  1. Navigate into the local Git repository that has the merge conflict. 2. Generate a list of the files affected by the merge conflict. 3. Open the file with merge conflicts in your favorite text editor. 4. Decide which changes to keep based on the context of the changes and the desired outcome of the merge. 5. Modify the file to incorporate the chosen changes. 6. Add or stage the modified file. 7. Commit the changes with a clear commit message that explains the resolution of the merge conflict. 8. Push the changes to the remote repository and merge the branches.  In summary, resolving merge conflicts caused by competing line changes in Git requires making a decision about which changes to keep, modifying the file to incorporate those changes, and committing the changes with a clear commit message.", "1a8447a6-192c-4708-bb41-12c05dd01e3a": "Without further context, it's unclear what \"l queries\" refer to in the context provided. It could potentially be a reference to a specific feature or functionality within Visual Studio Code, but without more information, it's impossible to provide an example query. If additional context is provided, I can provide a more specific answer.", "0a76d75f-c8bb-4f8c-86e2-83da6afadc24": "An organization owner can manage the costs associated with {% data variables.product.prodname_github_codespaces %} usage by setting a spending limit for {% data variables.product.prodname_github_codespaces %}, which applies to the total compute and storage cost for {% data variables.product.prodname_github_codespaces %}. They can also check the compute and storage usage for {% data variables.product.prodname_github_codespaces %} for the current billing month. Additionally, they can choose to disable or limit billing for {% data variables.product.prodname_github_codespaces %} for their organization. Prebuilds for {% data variables.product.prodname_github_codespaces %} may also incur billable costs for {% data variables.product.prodname_actions %}, and a spending limit can be set for {% data variables.product.prodname_actions %} as well. Storage of generated prebuilds is charged at the same rate as codespaces and is included in the {% data variables.product.prodname_github_codespaces %} spending limit.", "608aab44-0230-4750-8a92-707727a3c1ac": "Both GitLab CI/CD and {% data variables.product.prodname_actions %} allow you to run workflows at a specific interval. In GitLab CI/CD, pipeline schedules are configured with the UI, while in {% data variables.product.prodname_actions %} you can trigger a workflow on a scheduled interval with the \"on\" key.    How can variables and secrets be set in GitLab CI/CD and {% data variables.product.prodname_actions %}?  Answer:  GitLab CI/CD and {% data variables.product.prodname_actions %} support setting variables in the pipeline or workflow configuration file, and creating secrets using the GitLab or {% data variables.product.product_name %} UI.    How can caching be implemented in GitLab CI/CD and {% data variables.product.prodname_actions %}?  Answer:  GitLab CI/CD and {% data variables.product.prodname_actions %} provide a method in the configuration file to manually cache workflow files. The syntax for caching is different for each system.    How can artifacts be uploaded in GitLab CI/CD and {% data variables.product.prodname_actions %}?  Answer:  Both GitLab CI/CD and {% data variables.product.prodname_actions %} can upload files and directories created by a job as artifacts. The syntax for uploading artifacts is different for each system.", "436f5452-d4bc-48f1-9e18-3d7cadf4ddf3": "The available authentication methods for {% data variables.product.product_name %} are built-in authentication and external authentication. To configure external authentication with an external directory or identity provider, you can follow the instructions provided in the context information. Specifically, for {% data variables.product.product_name %}, you can refer to the articles mentioned in the context information, which include \"AUTOTITLE\", \"AUTOTITLE\", and \"AUTOTITLE\". If you choose to use external authentication, you can also configure fallback authentication for people who don't have an account on your external authentication provider. Additionally, for {% data variables.product.product_name %} (GitHub Enterprise Cloud), you must configure provisioning on your IdP using System for Cross-domain Identity Management (SCIM). For more information, you can refer to the article mentioned in the context information, \"AUTOTITLE\".", "ffa9154b-462a-42ff-a9ea-cbaf16e39b5d": "To skip workflow runs triggered by the `push` and `pull_request` events using commit messages in GitHub, you can add any of the following strings to the commit message: `[skip ci]`, `[ci skip]`, `[no ci]`, `[skip actions]`, or `[actions skip]`. Alternatively, you can add a `skip-checks` trailer to your commit message, with the format `skip-checks:true` or `skip-checks: true`. These instructions only apply to the workflow run(s) that would be triggered by the commit that contains the skip instructions, and won't stop a workflow that's triggered by other events, such as `on: pull_request_target`. Skip instructions won't prevent a pull request from being merged if your repository requires specific checks to pass first, and you'll need to push a new commit without the skip instruction to allow the pull request to be merged.", "23505011-ab01-47a8-9f4e-f5d14f2b3d27": "To place a legal hold on a user or organization in your enterprise to prevent the permanent removal of their repositories, follow these steps:  1. Log in to your enterprise site as an administrator. 2. Access the settings for the user or organization you want to place a legal hold on. 3. Click the \"Admin\" tab at the top of the page. 4. Click \"Legal hold\" and then click \"Place legal hold\".  This will ensure that repositories owned by the user or organization cannot be permanently removed from your enterprise, and they will be available for restore indefinitely.", "5c8e9ca6-a187-40a0-80d4-9fe81ffc7b26": "In the context of GitHub Package Registry, a private package is visible only to the owner of the package, while a public package is visible to anyone. To modify the access settings for a package, you can access the package settings and select \"Change visibility\" at the bottom of the page. From there, you can choose to make the package public or private. Before making a package public, you should consider the potential consequences, as once a package is made public, it cannot be made private again. This means that anyone with the package's URL can access and use it, and it will be searchable in the Package Registry. Additionally, anyone with access to the repository where the package is hosted will also have access to the package. Therefore, you should ensure that the package's code and any associated data are appropriate for public consumption.", "f645c279-fe2f-48c3-b0d0-f662eb2d70f4": "To download your recovery codes after enabling two-factor authentication, follow these steps:  1. Go to your account settings by clicking on your profile picture in the upper right-hand corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. Click on the \"Security\" tab in the left-hand menu. 3. Scroll down to the \"Two-factor authentication\" section and click on \"Set up two-factor authentication\" if you haven't already done so. 4. Follow the prompts to set up two-factor authentication using your preferred authentication method. 5. Once two-factor authentication is enabled, click on \"Two-factor authentication\" in the \"Security\" tab to manage your settings. 6. Click on \"Show recovery codes\" to view your recovery codes. 7. To save your recovery codes, click on \"Download\" to save them to your device, \"Print\" to print a hard copy, or \"Copy\" to copy them for storage in a password manager.  Remember to keep your recovery codes secure and do not share them with anyone. If you lose access to your account, you can use your recovery codes to regain access.", "3acd3b36-35f0-4fb0-b896-f94cf145d3ec": "The contexts available in a GitHub Actions workflow include `github`, `jobs`, `steps`, `runner`, `strategy`, and `matrix`. The `github` context contains information about the workflow run and the event that triggered the run, including the repository, event name, and event payload. This context is available in all steps of a workflow.", "10ee27f4-6e3d-4289-bf7c-d05893e45db9": "The concurrency limit for low's prebuild configuration is one workflow run at a time for a given prebuild configuration, unless changes were made that affect the dev container configuration for the associated repository. A new workflow run will be queued instead of waiting for the current one to complete if there are pushes that change the dev container configuration on the branch, in which case the service will immediately run the prebuild creation workflow, updating the prebuild accordingly if it succeeds.", "7e1fa856-d244-4274-8505-7912f1537c5e": "Yes, based on the information provided, the limitations of liability outlined in Section N of the agreement include indirect damages being excluded, a cap on total cumulative liability, and exclusions for certain types of liability. Specifically, indirect damages are not recoverable, the total cumulative liability of either party is limited to the Fees paid by the customer in the previous 12 months or $5,000 USD for free products, and $500 USD for Beta Previews. Certain types of liability, such as breach of confidentiality obligations, are excluded from these limitations.", "a716a2b0-2115-4f3e-842d-00debe64d020": "Targeting an action using a tag, branch, or SHA allows you to specify a specific version of the action to use in your workflow. Here's a breakdown of the differences:  1. Targeting an action using a tag:     When you target an action using a tag, you're specifying a specific version of the action that has been released by the maintainer. This approach is useful when you want to use a stable version of the action and ensure that you're not using a development version that may contain bugs or issues. For example, let's say you're using an action to automate a specific task in your workflow. You can target a specific version of the action using a tag, like this:     ```yaml    steps:    - uses: actions/javascript-action@v1.0.1    ```     In this example, we're targeting the action `actions/javascript-action` with the tag `v1.0.1`. This ensures that we're using the exact same version of the action that was released with that tag.  2. Targeting an action using a branch:     When you target an action using a branch, you're specifying the branch where the action's code is located. This approach is useful when you want to use the latest version of the action, including any updates or bug fixes that have been pushed to the branch. However, this approach can be risky because if the branch contains breaking changes, it may cause issues in your workflow. For example, let's say you're using an action to automate a specific task in your workflow. You can target the action's branch, like this:     ```yaml    steps:    - uses: actions/javascript-action@main    ```     In this example, we're targeting the action `actions/javascript-action` with the branch `main`. This ensures that we're using the latest version of the action that's currently on the branch.  3. Targeting an action using a SHA:     When you target an action using a SHA, you're specifying the exact commit where the action's code is located. This approach is the most reliable for versioning because it ensures that you're using", "7fe6395e-92e0-443f-910b-e21e06da07fa": "The dependency graph in an application refers to the interconnected relationships between the various packages or libraries that the application uses. It is a directed acyclic graph that shows how each package depends on others, both directly and indirectly. When a developer tries to update a dependency, it can have a ripple effect on the entire graph, as other packages may also need to be updated to maintain compatibility.  For example, let's say we have an application that uses the following dependencies:  ``` my-app | --> A (1.0.0) --> B (2.0.0)        |        --> C (3.0.0) ```  If we try to update `A` to a newer version, it may break the compatibility with `B`, which in turn may break the compatibility with `C`. This is because `B` may have been specifically designed to work with a certain version of `A`, and updating `A` could cause `B` to malfunction.  To avoid this issue, it's best to keep all dependencies up-to-date, as this reduces the likelihood of compatibility issues arising. However, in some cases, it may not be possible to update a single dependency without breaking the entire graph. In these situations, developers may need to consider alternative solutions, such as forking the dependency or creating a patch to fix the issue.  In summary, the dependency graph is a critical concept to understand when managing dependencies in an application, as it can have a significant impact on the process of updating them. By staying aware of the graph and its potential implications, developers can make more informed decisions about how to manage their dependencies and minimize the risk of compatibility issues.", "146af2d4-ec5e-4524-bf21-b7fbd17209c4": "To handle webhook deliveries on your server, you can use any programming language that you can run on your server, such as Ruby, Python, or JavaScript.  To initialize your server and handle HTTP requests, you need to:  1. Create a new file with the extension `.js` or `.jsx` (if you're using React) and save it in a directory that you can access from your web server.  2. Import the necessary modules, such as `http` and `url`.  3. Define a function that will handle incoming requests. This function should take the `request` and `response` objects as arguments.  4. Parse the request body to extract the event data.  5. Take the desired action in response to the event.  6. Send a response to the client.  Here's an example using JavaScript:  ```javascript copy annotate const http = require('http'); const url = require('url');  const server = http.createServer((req, res) => {   const parsedUrl = url.parse(req.url, true);    if (parsedUrl.pathname === '/webhook') {     if (req.method === 'POST') {       const body = '';       req.on('data', chunk => {         body += chunk.toString();       });       req.on('end', () => {         const event = JSON.parse(body);         console.log(`Received event: ${JSON.stringify(event)}`);         // Take action based on event type         switch (event.name) {           case 'issues':             // Handle issues event             break;           case 'ping':             // Handle ping event             break;           default:             console.log(`Unsupported event: ${event.name}`);             break;         }         res.statusCode = 200;         res.setHeader('Content-Type', 'application/json');         res.end(JSON.stringify({ message: 'Event received' }));       });     } else {       console.log(`Invalid request method: ${req.method}`);       res.statusCode = 405;", "6e7ee173-891c-4f23-80c1-2a1284919978": "According to the context provided, a user with the operator role in the {% data variables.enterprise.management_console %} can add SSH keys to grant administrative access to the instance via SSH in the {% data variables.enterprise.management_console %}. However, the exact steps to add SSH keys are not provided in the context. It's recommended to refer to the official documentation or contact the support team for detailed instructions on how to add SSH keys as an operator in the {% data variables.enterprise.management_console %}.", "405dec29-53d7-4b44-b869-fb6a41bb21cb": "Once a {% data variables.product.prodname_ghe_cloud %} admin enables a {% data variables.product.prodname_copilot_business_short %} subscription in your organization, you can assign {% data variables.product.prodname_copilot %} seats to individuals and teams in your organization by following the steps provided in \"AUTOTITLE.\" This process involves granting access to {% data variables.product.prodname_copilot %} for specific users or teams within your organization.", "c61ba380-4340-425e-a191-cfe70d9684aa": "Accessibility settings on {% ifversion fpt or ghec or ghes %}{% data variables.location.product_location %}{% elsif ghae %}{% data variables.product.product_name %}{% endif %} can be customized to fit individual needs by adjusting the user interface through the accessibility settings. These settings allow users to choose whether to use all keyboard shortcuts, manage the display of animated images, and control the appearance of links. Customization of keyboard shortcuts is essential for people who navigate using voice control, but can be useful to anyone when a shortcut conflicts with another application's shortcut. The option to show link underlines is currently in beta and subject to change. These settings can be accessed through the user settings menu.", "22b25770-e1d8-48db-b043-595ce9f045c5": "The default behavior of a fork in GitHub Desktop is to contribute changes to their upstream repositories. This means that any changes made in the fork will be proposed as a pull request to the original upstream repository. This behavior can be modified at any time. If the context doesn't provide enough information to determine whether the default behavior applies, then it's not possible to give an answer.", "6ee77042-a48c-42d6-9ce1-0ca356135d76": "To ensure that {% data variables.product.prodname_dependabot %} can successfully update all dependency references for your project, including those hosted in private package registries, you should:  1. Make sure that all of the referenced dependencies are hosted at accessible locations.  2. If you're using {% data variables.product.prodname_dependabot %} for version updates, configure grouped version updates by creating groups per package ecosystem. Make sure that you're not accidentally creating empty groups by setting a `dependency-type` in the `allow` key for the overall job.  3. If you're using {% data variables.product.prodname_dependabot %} for security updates, make sure that your private package registries are accessible to {% data variables.product.prodname_dependabot %}. You can do this by granting {% data variables.product.prodname_dependabot %} access to your private package registries.  4. If you're using {% data variables.product.prodname_dependabot %} for both version and security updates, make sure that you're following the guidelines for both types of updates.  5. If you're still experiencing issues, you can check the logs for your {% data variables.product.prodname_dependabot %} manifests to diagnose the problem. For more information, see \"Investigating errors with {% data variables.product.prodname_dependabot_version_updates %}\" above.  By following these steps, you can ensure that {% data variables.product.prodname_dependabot %} can successfully update all dependency references for your project, including those hosted in private package registries.", "a888b122-eeab-40b1-89dd-411d1df985e2": "The purpose of setting the `exportToken` option to `true` in the Vault action is to export the issued Vault access token as an environment variable named `VAULT_TOKEN`. This allows you to access the token in subsequent steps of your GitHub Actions workflow. By default, the token is not exported, and you would need to retrieve it from the Vault server using the `VAULT_ADDR` and `VAULT_TOKEN` environment variables. However, exporting the token can simplify your workflow and reduce the number of API calls required. To access the token, you can reference the `VAULT_TOKEN` environment variable in your workflow steps, such as in a `curl` command or in a script that interacts with the Vault server.", "a18ab160-d8ec-452d-96fb-c62b39a3e217": "To ensure that large files are properly stored and accessed using Git LFS, you should follow these steps:  1. Install Git LFS on your local machine and add the Git LFS command to your Git configuration.  2. Add the large files to your Git repository and commit them.  3. Run `git lfs track` to tell Git LFS which files to manage.  4. Run `git lfs pull` to download the large files from the remote repository.  5. Run `git lfs push` to upload the large files to the remote repository.  6. Use Git LFS commands to manage the large files, such as `git lfs checkout` to check out a specific version of a large file, `git lfs add` to add a new large file to the repository, and `git lfs rm` to remove a large file from the repository.  7. Use Git LFS filters to exclude large files from Git commits and pushes, and to optimize performance by only downloading the large files you need.  8. Use Git LFS hooks to automate tasks such as compressing large files before pushing them to the remote repository, and to ensure that large files are properly formatted and compatible with the intended file format.  To migrate an existing repository with large files to Git LFS, follow these steps:  1. Install Git LFS on your local machine and add the Git LFS command to your Git configuration.  2. Run `git lfs track` to tell Git LFS which files to manage.  3. Run `git lfs pull` to download the large files from the remote repository.  4. Run `git lfs push` to upload the large files to the remote repository.  5. Commit the changes to your repository.  6. Push the changes to the remote repository.  7. Update the remote repository to use Git LFS by running `git lfs install` on the remote repository.  8. Update the Git LFS configuration on all other remote repositories that share the same repository.  To optimize performance and minimize storage requirements, follow these best practices:  1. Use Git LFS filters to exclude large files from Git commits and pushes, and to optimize performance by only downloading the", "973e4a3d-2c40-47ee-af57-d573f0954890": "In the Yarn ecosystem, private registries can be configured for individual scopes by defining them in the `.yarnrc` file using the format `\"@myscope:registry\" \"https://private_registry_url\"`. This allows for specific scopes to access packages from private registries instead of the default public registry. This configuration can be added to the `.yarnrc` file in the root directory of the project.", "14963f28-b1ac-48ec-a43e-e8265c270393": "According to the context information provided, it is appropriate to create a standalone troubleshooting article when the known issue applies to a specific set of articles or conceptual grouping of features, or if a product or feature has multiple known issues that should be grouped together. However, if the issue applies to a specific article, it should be documented in that article. The decision to create a standalone article should be made based on the specific circumstances and the best way to provide clear and concise information to users.", "887a6447-2fca-4c52-969c-3614693389b5": "To use {% data variables.product.prodname_copilot %}, you must first install the {% data variables.product.prodname_copilot %} extension for {% data variables.product.prodname_vscode %}. Follow these steps:  1. In the {% data variables.product.prodname_vscode %} Marketplace, go to the {% data variables.product.prodname_copilot %} extension page and click **Install**. 2. A popup will appear, asking to open {% data variables.product.prodname_vscode %}. Click **Open {% data variables.product.prodname_vscode %}**. 3. In the \"Extension: {% data variables.product.prodname_copilot %}\" tab in {% data variables.product.prodname_vscode %}, click **Install**. 4. If you have not previously authorized {% data variables.product.prodname_vscode %} in your {% data variables.product.prodname_dotcom %} account, you will be prompted to sign in to {% data variables.product.prodname_dotcom %} in {% data variables.product.prodname_vscode %}. 5. In your browser, {% data variables.product.prodname_dotcom %} will request the necessary permissions for {% data variables.product.prodname_copilot %}. To approve these permissions, click **Authorize {% data variables.product.prodname_vscode %}**.  Note: If you have previously authorized {% data variables.product.prodname_vscode %} in your {% data variables.product.prodname_dotcom %} account, {% data variables.product.prodname_copilot %} will be automatically authorized.", "0203ede3-4eaf-4025-882a-f40d0713996b": "Developers familiar with {% data variables.product.prodname_dotcom_the_website %} can onboard and contribute seamlessly using familiar features and workflows to an instance of {% data variables.product.product_name %}. This is possible because {% data variables.product.product_name %} provides the same Git version control, productivity and collaboration tools, and integrations as {% data variables.product.prodname_dotcom_the_website %}. Therefore, team members can easily adapt to the self-hosted platform and continue working in a familiar environment.", "bcc20916-a6e7-4343-ba92-6be06a074081": "In the context provided, a {% data variables.product.prodname_codeql %} pack for custom queries contains a collection of custom queries that can be used to analyze code, while a {% data variables.product.prodname_codeql %} pack for custom tests contains a collection of custom tests that can be used to validate the correctness of code. Both packs can be installed using the `codeql pack install` command, but they serve different purposes in the {% data variables.product.prodname_codeql %} ecosystem.", "1afa4197-0802-402d-95a8-0f0fc3492a1c": "The limitation on prioritizing issues in a milestone on GitHub is that there can be no more than 500 open issues in the milestone. This limitation is reached when there are more than 500 open issues in the milestone.", "2f90498f-573f-43e4-b11a-74265b3b387d": "The purpose of configuring the deployment repository and authentication for it in the _.m2/settings.xml_ file during package publishing using Maven is to provide Maven with the necessary credentials and configuration to deploy the package to a specific repository, such as the Maven Central Repository. This allows for the secure and efficient distribution of the package to other developers and projects that depend on it. The settings file can also be used to configure other Maven settings, such as proxy servers and plugin repositories.", "2735b0c5-1ecc-492f-99f9-3294febd6207": "The security measure that can be taken to prevent sensitive values being leaked from workflow run logs in {% data variables.product.prodname_dotcom %} is to delete workflow run logs. While {% data variables.product.prodname_dotcom %} automatically redacts secrets printed to workflow logs, runners can only delete secrets they have access to. This means a secret will only be redacted if it was used within a job. As a security measure, deleting workflow run logs can prevent sensitive values being leaked.", "26994960-71c6-498d-a85b-24eaf437d1b4": "GitHub collects personal data from third parties in a few different ways. Firstly, if you sign up for training or to receive information about GitHub from one of their vendors, partners, or affiliates, they may collect personal data about you and share it with GitHub. Secondly, GitHub may receive information from service providers who process data on their behalf, such as their payment processor who processes payment and billing information in connection with their Service. Additionally, if you post content on their Service, make it publicly available, or contribute to a public repository, that information may be collected by GitHub and made available to their user community and the general public. Finally, GitHub may obtain information from publicly available sources, such as GitHub repositories. The types of information that may be received in this way include contact information, payment information, and information about your preferences or characteristics.", "af72e473-2c64-4c04-9ab2-3b3f9d385676": "Yes, you can automatically allow {% data variables.product.prodname_secret_scanning %} to check the validity of a secret by sending it to relevant partners by selecting the relevant checkbox under \"{% data variables.product.prodname_secret_scanning_caps %}\" when enabling {% data variables.product.prodname_secret_scanning %} for your enterprise organization. This feature can also be enabled for a single repository or organization. For more information, see \"Allowing validity checks for partner patterns in a repository,\" and \"Allowing validity checks for partner patterns in an organization.\" Additionally, you can use the REST API to enable validity checks for partner patterns for your enterprise. Note that this feature is currently in beta.", "6cdea548-b46f-4766-94a5-a4c00810d225": "To configure Gradle to publish packages to both the Maven Central Repository and GitHub Packages, follow these steps:  1. Add the Maven Publishing plugin to your _build.gradle_ file:     ```groovy    plugins {      ...      id 'maven-publish'    }    ```  2. Configure the publishing section in your _build.gradle_ file:     ```groovy    publishing {      ...       repositories {        maven {          name = \"OSSRH\"          url = \"https://oss.sonatype.org/service/local/staging/deploy/maven2/\"          credentials {            username = System.getenv(\"SONTYPE_USERNAME\")            password = System.getenv(\"SONTYPE_PASSWORD\")          }        }         maven {          name = \"GitHubPackages\"          url = \"https://maven.pkg.githubusercontent.com/[OWNER]/[REPOSITORY]/maven-metadata.xml\"          credentials {            username = System.getenv(\"GITHUB_TOKEN\")          }        }      }    }    ```     In the example above, replace `[OWNER]` and `[REPOSITORY]` with your GitHub organization and repository names, respectively.     The `OSSRH` repository is used to publish to the Maven Central Repository, and the `GitHubPackages` repository is used to publish to GitHub Packages.     For the `OSSRH` repository, you'll need to provide the `SONTYPE_USERNAME` and `SONTYPE_PASSWORD` environment variables, which can be obtained from the Sonatype website.     For the `GitHubPackages` repository, you'll need to provide the `GITHUB_TOKEN` environment variable, which can be obtained from your GitHub repository settings.  3. Run the `publish` task in your Gradle build:     ```groovy    gradle publish    ```     This will publish your package to both the Maven Central Rep", "5241a869-c76d-4bc9-bc63-5a2d8b93877b": "The warning provided in the context information is that once you delete an artifact, it cannot be restored. This means that after deleting an artifact, it will be permanently deleted and cannot be undone. Therefore, you should be cautious while deleting an artifact and ensure that you are deleting the correct artifact and that you do not need it anymore. It is always recommended to double-check and confirm before deleting an artifact to avoid any data loss.", "9b19503c-b1a4-4730-8fdd-46f4b862d117": "The statement \"About adding languages to an existing default setup configuration\" in the given context indicates that if a repository's code changes to include a supported language by {% data variables.product.prodname_codeql %}, {% data variables.product.prodname_dotcom %} will automatically update the {% data variables.product.prodname_code_scanning %} configuration to include the new language. This ensures that the repository maintains {% data variables.product.prodname_code_scanning %} coverage without requiring any manual configuration changes.", "9edf2692-7cfb-4dc7-a51f-0fe8e3e7f478": "If you configured provisioning using SCIM, to grant the user enterprise ownership in {% data variables.product.product_name %}, you need to follow the steps outlined in the context information to configure user provisioning with SCIM for your instance. This will automatically grant the user enterprise ownership in {% data variables.product.product_name %}.  If you only use SAML, to grant the user enterprise ownership in {% data variables.product.product_name %}, you need to follow the steps outlined in the context information to configure SAML SSO for {% data variables.location.product_location %}. After that, you can assign the enterprise owner role to the user in Azure AD.  If you were not given enough information to determine whether you use SAML or SCIM, you should consult your organization's IT administrator or the documentation for your specific setup to determine the appropriate steps to make a person an enterprise owner for {% data variables.product.product_name %}.", "b68af1d9-2df1-4525-bad4-5faad9708e75": "Based on the context information provided, repository writers and maintainers can manage secret scanning alerts in their repositories by:  - Listing, dismissing, and deleting {% data variables.product.prodname_code_scanning %} alerts (a) - Viewing and dismissing {% data variables.secret-scanning.alerts %} in a repository (b) - Resolving, revoking, or re-opening {% data variables.secret-scanning.alerts %} (c)  Note that the availability of these features may vary depending on the specific GitHub plan being used (GitHub Enterprise Server, GitHub Advanced Security, or GitHub.com).  Regarding managing {% data variables.product.prodname_code_scanning %} alerts, the context only mentions listing, dismissing, and deleting these alerts (a). It doesn't provide information about any other features for managing {% data variables.product.prodname_code_scanning %} alerts.  If you're unsure about the availability of these features for your specific GitHub plan, it's best to consult the official documentation or contact GitHub support for clarification.", "7aa2820c-c0c3-4733-8e1a-a6ed9f73f60a": "The \"setup-node\" action in a GitHub workflow helps in accessing the \"npm\" command by installing the specified version of the \"node\" software package on the runner. In the provided example, the \"setup-node\" action is being used to install the \"node\" software package version 16.13.x, which is specified in the \"with\" section of the action configuration. This allows the user to run \"npm\" commands in subsequent steps of the workflow. The \"cache\" option is also being used to cache the \"npm\" command, which can improve the speed of the workflow by avoiding unnecessary downloads of dependencies.", "d140632f-d5e9-4a42-bc76-8d61017083c0": "According to the provided context, GitHub Insights and Learning Lab for Enterprise Server are tools that can be used by the customer to process their employees' and/or users' data. However, it is left to the customer's discretion whether to use these tools, and if they do, they are solely responsible for conducting such processing in compliance with applicable law. Therefore, the main difference between these tools seems to be their specific functionality, as the context doesn't provide further details about their features. The customer's use of these tools in accordance with applicable law likely involves ensuring that they obtain the necessary consent from their employees and users for the processing of their data, as well as implementing appropriate security measures to protect the data.", "8769a7fb-d68a-4755-b24a-ed8a6bbcd43b": "Based on the context information provided, can you explain the purpose of the \"Bug report\" form mentioned in the text?  2. In the \"Bug report\" form, what type of information is required to be provided by the user in the \"Name\" field?  3. How can you distinguish between the fields in the resulting issue if they have the same label attribute?  4. What is the significance of supplying an \"id\" attribute to any clashing top-level elements in the \"Bug report\" form?  5. What is the error \"body[i]: required key type is missing\" and how can it be fixed in the context of the \"Bug report\" form?  6. What is the error \"body[i]: `x` is not a valid input type\" and how can it be fixed in the context of the \"Bug report\" form?  7. How can you ensure that the \"Bug report\" form is filled out correctly to avoid errors related to missing or invalid input types?  8. Can you provide an example of a valid \"Bug report\" form submission using the context information provided?  9. How can you customize the \"Bug report\" form to suit the specific needs of your organization or project?  10. What are some best practices for creating effective \"Bug report\" forms that are easy to use and provide useful information to developers?", "e2b06eab-d48d-42a0-8c09-c8a50da5e7d6": "To upgrade an app for your enterprise on GitHub Marketplace, you need to have a GitHub Enterprise account. First, access your enterprise by following the steps provided in the context information. Then, go to the \"Billing\" tab in your enterprise account settings. In the \"Marketplace apps\" tab, find the app you want to upgrade and select \"More\" next to the organization where you want to upgrade the app. Click \"Change plan\" and select the \"Edit your plan\" dropdown to edit the plan for the selected account. Choose the new plan and quantity, and then click \"Issue plan changes\" to complete the upgrade process.", "7f956bc9-7dad-4789-be06-d69ab2ba7850": "Enterprise owners can enforce policies to control security settings for organizations owned by their enterprise on {% data variables.product.product_name %}. By default, organization owners can manage security settings. To require two-factor authentication for all organizations owned by an enterprise, the enterprise owner must first enable two-factor authentication for their own account. After that, they can enforce the policy for all members, billing managers, and outside collaborators in those organizations. However, it's important to note that members, outside collaborators, and billing managers who do not use two-factor authentication will be removed from the organization and lose access to its repositories. They can reinstate their access privileges and settings if they enable two-factor authentication for their account within three months of their removal from the organization. Additionally, any organization owner, member, billing manager, or outside collaborator who disables two-factor authentication for their account after the policy is enforced will automatically be removed from the organization. If the enterprise owner is the sole owner of the enterprise, they won't be able to disable two-factor authentication for their user account.", "73a415ef-fe88-46a8-95bc-477afa869a4d": "To create an assignment using {% data variables.product.prodname_classroom %}, follow these steps:  1. Log in to your {% data variables.product.prodname_classroom %} account and navigate to the classroom where you want to create the assignment.  2. Click on the \"Assignments\" tab on the left-hand side of the page.  3. Click on the \"New assignment\" button in the top right-hand corner of the page.  4. Enter a title and description for the assignment.  5. Choose the type of assignment you want to create, such as a coding challenge or a research project.  6. Select the repository where you want the assignment to be located.  7. Choose whether you want the assignment to be public or private.  8. Set a deadline for the assignment.  9. Invite your students to join the assignment.  10. Click \"Create assignment\" to save your changes.  11. Your students will receive an email notification about the new assignment.  12. You can monitor their progress and provide feedback on their work through the {% data variables.product.prodname_classroom %} platform.  13. Once the deadline has passed, you can review the submissions and provide final grades.  14. To create additional assignments, repeat the process.  15. For more information and resources, refer to the {% data variables.product.prodname_classroom %} documentation and support resources.", "837d6726-dea8-445f-bc9b-c92ef9f953ca": "The REST API can be utilized to manage assignees on issues and pull requests by allowing users to view, add, and remove assignees through API requests. This feature is shared between issues and pull requests, as mentioned in the context information provided. By using the REST API, users can automate assignee management tasks, such as assigning issues to team members or removing assignees from pull requests that have been merged. This can save time and improve efficiency in managing GitHub projects.", "f8b7e689-ab72-4dec-8430-76de1b4255a3": "To revoke authorized credentials for a user in GitHub Enterprise using SAML, follow these steps:  a. Log in to your organization's GitHub Enterprise account. b. Click on your organization's name in the top left corner of the page. c. Select \"Settings\" from the drop-down menu. d. Click on the \"Security\" tab. e. Click on the \"SAML\" tab. f. Find the user whose authorized credentials you want to revoke. g. Click on the user's name to open their profile. h. Click on the \"Revoke Credentials\" button. i. Confirm that you want to revoke the user's authorized credentials by clicking \"Revoke\" in the confirmation dialog box.  Answer:  2. To view authorized credentials for a user in GitHub Enterprise using SAML, follow these steps:  a. Log in to your organization's GitHub Enterprise account. b. Click on your organization's name in the top left corner of the page. c. Select \"Settings\" from the drop-down menu. d. Click on the \"Security\" tab. e. Click on the \"SAML\" tab. f. Find the user whose authorized credentials you want to view. g. Click on the user's name to open their profile. h. Click on the \"View Authorized Credentials\" button. i. Review the list of authorized credentials for the user.  Answer:  3. To link an identity to a SAML identity in GitHub Enterprise, follow these steps:  a. Log in to your organization's GitHub Enterprise account. b. Click on your organization's name in the top left corner of the page. c. Select \"Settings\" from the drop-down menu. d. Click on the \"Security\" tab. e. Click on the \"SAML\" tab. f. Find the user whose identity you want to link. g. Click on the user's name to open their profile. h. Click on the \"Link Identity\" button. i. Enter the necessary information to link the identity, such as the identity provider's metadata URL or the user's email address", "8e3c0e9c-f324-47b2-a0b3-6c5d7fce08ff": "To use AWS Lambda with Amazon S3 to trigger serverless workflows, follow these steps:  1. Log in to the AWS Management Console and navigate to the Lambda service. 2. Click the \"Create Function\" button to create a new Lambda function. 3. Choose \"Author from scratch\" and give your function a name and a description. 4. Choose the \"Python 3.8\" runtime and click \"Create function\". 5. In the function editor, paste the following code:  ```python import json import boto3  s3 = boto3.client('s3')  def lambda_handler(event, context):     bucket = event['Records'][0]['s3']['bucket']['name']     key = event['Records'][0]['s3']['object']['key']     print(\"Event received from S3:\")     print(\"Event record:\")     print(json.dumps(event, indent=4))     print(\"Bucket:\", bucket)     print(\"Object:\", key)     return \"Success\" ```  6. Save the function and test it by clicking the \"Test\" button. 7. In the S3 service, create a new bucket and upload a file to it. 8. Go back to the Lambda function and click the \"Monitor\" tab to view the function logs. 9. You should see the function output showing the details of the S3 event.  Congratulations, you've successfully triggered a Lambda function using Amazon S3!  Note: This tutorial assumes that you have basic knowledge of AWS services and the AWS Management Console. If you're new to AWS, you may want to check out the AWS Getting Started Guide for more information.", "23fd3a97-7ff2-4416-845f-5ecfaf7973c4": "To enable or disable a security or analysis feature for all repositories in an enterprise, you can follow these steps:  1. Go to your enterprise settings by clicking your profile picture in the top right corner of GitHub.com, then clicking Settings from the drop-down menu.  2. In the left-hand menu, click Developer settings.  3. In the \"Configure security and analysis features\" section, you'll see a list of features with \"Enable all\" and \"Disable all\" buttons to the right of each setting.  4. To enable or disable all features at once, click \"Enable all\" or \"Disable all\" to the right of the feature you want to manage.  5. To confirm the change, click the \"Enable/Disable all\" or \"Enable/Disable for eligible repositories\" button in the dialog that is displayed.  6. Optionally, you can select the checkbox below the feature to enable or disable it automatically when new repositories are added.  Answer: To allow {% data variables.product.prodname_secret_scanning %} to check the validity of a secret by sending it to the relevant partner, you can follow these steps:  1. Go to your enterprise settings by clicking your profile picture in the top right corner of GitHub.com, then clicking Settings from the drop-down menu.  2. In the left-hand menu, click Developer settings.  3. In the \"Configure security and analysis features\" section, scroll down to \"{% data variables.product.prodname_secret_scanning_caps %}\" and select the relevant checkbox to enable validity checks for partner patterns.  4. You can also enable validity checks for partner patterns for a single repository or organization by following the instructions in \"Allowing validity checks for partner patterns in a repository\" and \"Allowing validity checks for partner patterns in an organization.\"  5. For more information on using the REST API to enable validity checks for partner patterns for your enterprise, see \"AUTOTITLE\" in the REST API documentation.  6. Note that this feature is currently in beta, and you may need to contact GitHub support to enable it for your enterprise.  Answer: To disable all security and analysis features for", "c57f2700-cf35-41ff-8cd4-4002d0ab44d4": "When you visit GitHub in a browser as a user with SAML SSO enabled, the following steps occur:  1. You navigate to GitHub (https://github.com) in your web browser. 2. GitHub detects that you have SAML SSO enabled and redirects you to your organization's SAML IdP to authenticate. 3. You enter your credentials on the SAML IdP and click \"Sign In\" or \"Login\". 4. The SAML IdP validates your credentials and sends a response to GitHub. 5. GitHub receives the response from the SAML IdP and verifies its authenticity. 6. If the response is valid, GitHub grants you access to the organization's resources. 7. You are redirected back to GitHub and can access the organization's resources as usual.  Note that the exact steps may vary depending on the specific SAML IdP being used.", "0cba2b1e-3417-44f4-9d31-bb4e8925d8be": "The migration log for an organization migration on GitHub can be accessed by navigating to the repository named `gei-migration-results` in the destination organization. This repository is created at the beginning of the migration process but is only updated with the migration logs after the migration finishes. The `README.md` file in the root of the repository includes information about any warnings or errors related to the migration of organization-level data, as well as the number of successfully migrated repositories and the number of repositories that failed to migrate. The `/success` and `/failure` directories contain one file for each successfully migrated repository or repository that failed to migrate, respectively, following the naming convention `REPO_NAME.md`.", "1cb89c14-706e-4f1c-be45-9732e9165bea": "The purpose of submitting a W-9 or W-8 tax form to {% data variables.product.prodname_dotcom %} as a maintainer is to provide the necessary tax information required by the U.S. Internal Revenue Service (IRS) for reporting income paid to the maintainer. The W-9 form is required for U.S. taxpayers, while the W-8 BEN or W-8 BEN-E form is required for non-U.S. taxpayers. These forms help {% data variables.product.prodname_dotcom %} use the correct Taxpayer Identification Number (TIN) to report income paid to the maintainer to the IRS in a 1099 form. Failure to submit these forms may result in delays in publishing the maintainer's {% data variables.product.prodname_sponsors %} profile.", "5028696a-f5ec-4cb4-992d-6047e504322c": "To upload a custom background image for a feature card in the {% data variables.product.prodname_marketplace %}, follow these guidelines:  1. Log in to your {% data variables.product.prodname_marketplace %} account. 2. Go to your app's landing page by clicking your app's name from the {% data variables.product.prodname_marketplace %} homepage or category page. 3. Click the \"Edit app\" button in the top right corner of the page. 4. Scroll down to the \"Feature cards\" section. 5. Click the \"Add feature card\" button. 6. Enter a title for the feature card in the \"Title\" field. 7. Upload a background image for the feature card by clicking the \"Choose file\" button and selecting the image from your computer. The background image should be 965 pixels x 482 pixels (width x height). 8. Choose a text color for your app's name in the \"Text color\" field. Make sure the text color is legible against the background image. 9. Click the \"Save\" button to save the feature card.  By following these steps, you can upload a custom background image for your feature card in the {% data variables.product.prodname_marketplace %} and give it a visual identity that helps it stand out against the dark background of the homepage.", "4fa28f91-b783-45b9-bd3b-c8cc8c50c563": "Academics can make their research material more easily referenced in academic literature using Zenodo by creating persistent identifiers, also known as Digital Object Identifiers (DOIs), for their repositories. This can be done by archiving their repository on {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %} using the data archiving tool Zenodo and issuing a DOI for the archive. Zenodo can only access public repositories, so academics should ensure that their repository is public before archiving it. Additionally, including a license in the repository will help readers understand how they can reuse the work. Zenodo issues a new DOI each time a new {% data variables.product.product_name %} release is created, making it easy to reference the research material in academic literature.", "aca17ea2-5fe2-452b-9d0d-754e238720d5": "The syntax for defining Docker images differs between GitLab CI/CD and {% data variables.product.prodname_actions %} in the location of the key used to define the Docker image. In GitLab CI/CD, the `image` key is used to define the Docker image, while in {% data variables.product.prodname_actions %}, the `container` key is used to define the Docker image.", "de924af4-ce76-49d0-9f22-ce1182cc5ff5": "If you see a \"503 service unavailable\" error while working on a codespace in {% data variables.product.prodname_vscode %} or in your browser window, you should first check if a Start button is shown in {% data variables.product.prodname_vscode %} or in your browser window. If a Start button is shown, click Start to reconnect to the codespace. If a Start button is not shown, you should reset your codespace by reloading the window. From the Command Palette in {% data variables.product.prodname_vscode %}, click Developer: Reload Window.", "9cbc83e2-1e87-41a6-b7b4-04e2fe78b256": "You need to provide the email address for the user who will set up your enterprise and a short code that will be used as the suffix for your enterprise members' usernames when creating a new {% data variables.enterprise.prodname_emu_enterprise %} with the GitHub Sales team. For more information, see \"Usernames and profile information.\" (Refer to the context information provided.)", "ee22a540-5096-41c8-af18-dc563d02811f": "To retry or cancel expired invitations in GitHub Enterprise, follow these steps:  1. Go to your organization's settings by clicking on your profile picture in the top right corner of the page, then selecting \"Organizations\" from the drop-down menu, and clicking on the organization you want to manage. 2. Click on the \"People\" tab. 3. Click on \"Failed invitations\" under the \"People\" section. 4. To retry or cancel a single invitation, select the {% octicon \"kebab-horizontal\" aria-label=\"Show actions\" %} dropdown menu to the right of the invitation you want to cancel or retry, and click \"Retry invitation\" or \"Cancel invitation\". 5. To confirm, click \"Yes, retry\" or \"Yes, cancel\". 6. To retry or cancel multiple invitations at once, select the checkboxes next to the invitations you want to retry or cancel, and select \"Retry invitations\" or \"Cancel invitations\" from the drop-down menu at the top of the list.  Note that invitations expire after 7 days, and failed invitations to outside collaborators can also be found in this view.", "345a88e1-358d-4286-a45a-3c846782f2e9": "To manage your organization membership on GitHub, you can follow these steps:  1. Go to {% data reusables.profile.access_org %} to access your organization settings. 2. From there, select {% data reusables.user-settings.access_org %} to view your organization memberships. 3. Locate your username in the list of members and select the visibility dropdown menu next to it. 4. Choose either \"Public\" or \"Private\" to change the visibility of your organization membership.  Alternatively, you can also access your organization settings by going to the organization's repository and selecting the \"Settings\" tab. From there, select \"Members\" to view and manage your organization memberships.", "8c0aab61-edc3-4d06-8f50-691987b2b09f": "The \"Resource not accessible by integration\" error message that is displayed when using a {% data variables.product.prodname_github_app %}{% ifversion pat-v2 %} or {% data variables.product.pat_v2 %}{% endif %} with insufficient permissions indicates that the token associated with the app or personal access token (PAT) does not have the necessary permissions to access the requested resource. This error message is significant because it highlights the importance of ensuring that the app or PAT has the appropriate permissions to perform the desired actions. Without the necessary permissions, the app or PAT will be unable to access the requested resource, which can result in errors or unexpected behavior. To address this issue, you should review the permissions required for the specific endpoint and ensure that the app or PAT has those permissions. This can be done by modifying the permissions granted to the app or PAT, or by using a different app or PAT with the necessary permissions.", "ac45761c-1251-4557-a7c7-d5cd2b1182f5": "To authenticate a request and pass body parameters in the {% data variables.product.prodname_dotcom %} REST API, you need to include an access token in the authorization header of your request. This access token should be treated as a sensitive credential, as warned in the provided context information. For more information on authentication and authorization, refer to the \"AUTOTITLE.\" documentation provided by {% data variables.product.prodname_dotcom %}.", "5c164c11-dad4-45ba-abe8-c719f799c294": "Yes, based on the context provided, I can explain the difference between pre-migration and post-migration steps during the migration process and provide examples of each.  Pre-migration steps are actions that need to be taken before the actual migration process begins. These steps are essential to ensure a smooth migration and minimize disruption to your users. Some examples of pre-migration steps include:  1. Notifying your users: Let your users know ahead of time about the upcoming migration and its timeline. This will help them prepare for the change and minimize any surprises.  2. Setting up user accounts: Create user accounts in {% data variables.product.prodname_dotcom %} for your team. This will ensure that your users can access their repositories and continue working after the migration.  3. Updating local repositories: Send instructions to your users for updating their local repositories to point to your new system. This will ensure that their local copies of the repositories are up-to-date and can be easily synced with the new system.  Post-migration steps are actions that need to be taken after the migration process is complete. These steps are essential to ensure that your users can continue working and that your data is properly migrated. Some examples of post-migration steps include:  1. Letting your users know: Let your users know that the migration is finished. This will help them understand that the migration is complete and that they can continue working.  2. Linking activity: Link activity to users in your migration destination. This will ensure that all the activity in your repositories is properly attributed to the correct users.  3. Decommissioning your migration origin: Once the migration is complete, you can decommission your migration origin. This will ensure that you're not paying for two systems and that all your data is properly migrated to your new system.  In summary, pre-migration steps are essential to prepare for the migration, while post-migration steps are essential to ensure that your users can continue working and that your data is properly migrated. By following these steps, you can minimize disruption to your users and ensure a smooth migration process.", "4f6f4cc4-0ea6-4966-b696-2bf4d82d270a": "To obtain the SHA1 value of the current GitHub commit using CircleCI environment variables, you can use the `CIRCLE_SHA1` variable provided by CircleCI. This variable contains the SHA1 hash of the commit that triggered the CircleCI workflow. Therefore, you can access this variable in your CircleCI configuration file or in your scripts to retrieve the SHA1 value of the current GitHub commit.", "156a2dcb-0955-4505-8f5e-e1dafb9ea121": "The `audit` command in {% data variables.product.prodname_actions %} can be used to analyze pipelines in a Jenkins server by fetching all of the projects defined in the server, converting each pipeline to its equivalent {% data variables.product.prodname_actions %} workflow, and generating a report that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}. This command provides a high-level view of all pipelines in the server and helps in determining the feasibility of migrating to {% data variables.product.prodname_actions %} using {% data variables.product.prodname_actions_importer %}.", "d6ab178a-3360-41a9-8784-9bcc36c7634c": "Organizations using GitHub Enterprise Cloud (GHEC) with GitHub Advanced Security can additionally enable these features for private and internal repositories by following the instructions provided in the {% data variables.product.prodname_ghe_cloud %} documentation.", "4b1a4c7a-8c24-4882-9dd5-47792925b905": "{% data variables.product.prodname_github_apps %} have additional security features compared to {% data variables.product.prodname_oauth_apps %}. Specifically, {% data variables.product.prodname_github_apps %} tokens have a shorter validity period, which reduces the potential damage if the token is leaked. Additionally, {% data variables.product.prodname_github_apps %} do not consume a seat on {% data variables.product.product_name %} and remain installed even when the person who initially installed the app leaves the organization, which provides greater flexibility and continuity for integrations.", "2edf9d32-8da2-4dc6-a650-a86a8c857cfd": "Yes, based on the provided context, Section 4.2 of the agreement outlines that updates for Listing will be automatically requested, downloaded, and installed without prior notice to the user. GitHub makes no guarantees regarding the timing of these updates, and updates to Listing are subject to the same terms and conditions as the Listing itself, including GitHub's review and takedown policies outlined in Section 4.2.", "e2e694ca-dd0d-4e68-80da-2761f9d595f5": "The Source imports REST API, which allows programmatic import of repositories accessible over the internet to GitHub, is being deprecated by GitHub on April 12, 2024. This decision has been made because GitHub is introducing new features and capabilities that will be better served by newer APIs and technologies. As an alternative, users can consider using repository templates, which are available for source repositories hosted on GitHub. If the source repository is not hosted on GitHub, users may need to explore other options, such as using forks or third-party tools, depending on the specific use case. However, if the imported repository needs to be owned by the same user or organization as the source repository, or if the same source repository needs to be imported multiple times, then forking is not a viable alternative. In such cases, users will need to make changes to their code or application to accommodate the deprecation of the Source imports REST API.", "db48c46a-cff3-466a-a6de-71c95fad5c20": "To test an action in a workflow, you can follow these steps:  1. Create a new repository or use an existing one. 2. Create a new action by following the instructions in \"AUTOTITLE\". 3. Add a tag to your repository with a descriptive message, using the command `git tag -a -m \"Description of this release\" v1`. 4. Push the tag to the remote repository using the command `git push --follow-tags`. 5. Copy the workflow code provided in the context information into a `.github/workflows/main.yml` file in your repository. 6. Replace `actions/hello-world-composite-action@v1` with the repository and tag you created. 7. Replace `who-to-greet` with your name or any other input you want to pass to the action. 8. Save the file and commit the changes. 9. Go to the Actions tab in your repository and select the latest workflow run. 10. Verify that the output includes the expected results from your action.  In summary, you need to replace the action's repository and tag, as well as any input values, in the provided workflow code to make it work for your own repository.", "7fc3591c-4271-4fb0-92a7-4c5b513a9b48": "The value of the variable `$TRAVIS_PULL_REQUEST_SLUG` in the current context is not provided, as the context information only includes variables related to the Travis CI environment and GitHub repository information, but not the specific details of the current build or pull request. If the build is being triggered by a pull request, then `$TRAVIS_PULL_REQUEST_SLUG` would contain the name of the pull request, in the format of `owner/repo-name#pull-request-number`. However, if the build is being triggered by other means, such as a push to a branch or a manual trigger, then `$TRAVIS_PULL_REQUEST_SLUG` would be empty.", "26a9b32b-299b-47d2-98f3-0c0df0d02af7": "To disable team synchronization in GitHub, follow these steps:  1. Go to your organization settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. Click on \"Security\" in the left-hand menu. 3. Scroll down to the \"Team synchronization\" section. 4. Click \"Disable team synchronization\".  This will disable team synchronization for your organization, preventing any new members from being automatically added based on group membership in your identity provider.", "32514248-6257-4ed9-af43-2991b46bd0e8": "According to the context information provided, installing or updating operating system packages manually on {% data variables.product.product_name %} can cause unexpected behavior due to the customized Linux operating system with only necessary applications and services. This is because these customizations may be overwritten by the installation or update of operating system packages, leading to unintended consequences. For more information, you can refer to the \"AUTOTITLE\" documentation provided.", "e66a489a-07d2-4ad1-90e1-828f9afb4bff": "The role of Legal in approving expenses exceeding $150, as mentioned in the context information, is to provide written approval for any expenses exceeding this amount. This is to ensure compliance with the company's policies and avoid any potential violations of anti-bribery laws. It's important to always check with Legal before incurring any expenses, especially when dealing with customers or prospective customers, to avoid any misunderstandings or unintended consequences.", "25e77355-768c-4809-ae82-a8d4ea0a7f4d": "After completing a migration to {% data variables.product.product_name %}, storage pressure on your instance will increase due to the duplication of image files in the Docker registry and the {% data variables.product.prodname_container_registry %}. A future release of {% data variables.product.product_name %} will remove the duplicated files when all migrations are complete.", "6e599ece-d270-480c-876c-d9d9776c46b9": "Collaborators can be added to a temporary private fork associated with a security advisory by an admin with permissions to the security advisory. This action requires admin permissions to the security advisory. Collaborators who are added will have access to the temporary private fork and can collaborate on the security advisory.", "60bb4f70-8705-4d39-a91c-170dc8c2134d": "An organization owner can limit spending on {% data variables.product.prodname_github_codespaces %} and {% data variables.product.prodname_actions %} by setting spending limits for both services. The spending limit for {% data variables.product.prodname_github_codespaces %} applies to the total compute and storage cost for the service, while the spending limit for {% data variables.product.prodname_actions %} applies to the total cost for running workflows in the organization's account. Additionally, the organization owner can choose to disable or limit billing for {% data variables.product.prodname_codespaces %} by going to the organization settings and selecting the appropriate option. However, disabling or limiting billing for {% data variables.product.prodname_codespaces %} will also disable or limit the use of {% data variables.product.prodname_actions %}, as prebuilds for {% data variables.product.prodname_github_codespaces %} are created and updated using {% data variables.product.prodname_actions %}.", "aa296a23-894d-4057-83ae-f7e329cab312": "To add a workflow status badge to your repository, you can follow the steps outlined in the \"Adding a workflow status badge\" section provided in the context information. This badge displays the status of your workflows, such as whether they are passing, failing, or pending, directly on your repository's homepage. This provides a quick and easy way to monitor the health of your workflows and identify any issues that may need attention. It also helps to communicate the status of your workflows to other members of your team or organization.", "1e9c391b-8357-4f10-9423-5247d6989645": "Repository administrators can enforce required commit signing on a branch to block all commits that are not signed and verified by following these steps:  1. Generate a GPG key for each developer who will be pushing commits to the branch. 2. Add the GPG key to the developer's Git configuration using the `gpg --list-keys` command to display the key ID, and then running `git config --global user.signingkey <key-id>`. 3. Enable commit signing for the branch by running `git update-ref -f refs/heads/<branch-name> --add --set-bit=2` in the repository. 4. Push the changes to the branch to activate the commit signing requirement.  Developers will now be required to sign their commits using their GPG key before pushing them to the branch. If a commit is not signed or the signature cannot be verified, it will be rejected by the repository.  Note: This feature is available for GitHub Enterprise Server version 2.21 and later, and GitHub.com. For more information, see \"AUTOTITLE.\"", "bedf29c1-96b9-4b5d-b430-b67a79431e1a": "In CircleCI, the `matrix` concept allows for the creation of multiple test runs with different configurations, such as different operating systems, browsers, or database versions, by defining a matrix of parameters. This helps to ensure that the software works correctly in a variety of environments.  The `steps` concept in CircleCI refers to a series of commands or scripts that are executed in a specific order during the build process. These steps can include tasks such as checking out the code, installing dependencies, running tests, and deploying the software.  In {% data variables.product.prodname_actions %}, both the `matrix` and `steps` concepts are supported, but with some limitations. The `matrix` concept is supported through the use of the `jobs.strategy.matrix` syntax, which allows for the creation of multiple jobs with different configurations. However, the number of jobs that can be created in a matrix is limited to 1000.  The `steps` concept in {% data variables.product.prodname_actions %} is supported through the use of the `jobs.steps` syntax, which allows for the creation of a series of commands or scripts to be executed in a specific order during the build process. These steps can include tasks such as checking out the code, installing dependencies, running tests, and deploying the software.  Overall, while both the `matrix` and `steps` concepts are supported in {% data variables.product.prodname_actions %}, there are some limitations that should be considered when designing workflows.", "01c3c1eb-b9b6-4f7c-96c4-cc542d9bfbb0": "To disable {% data variables.product.prodname_actions %} for your repository, follow these steps:  1. Go to your repository in GitHub. 2. Click on the settings icon in the top right corner of the page. 3. Select \"Settings\" from the drop-down menu. 4. Click on \"Actions\" in the left-hand menu. 5. Under \"Actions permissions,\" select \"Disable {% data variables.product.prodname_actions %} for this repository.\" 6. Click \"Save\" to confirm the action.  Disabling {% data variables.product.prodname_actions %} for your repository will prevent the use of automated workflows and actions in your repository. This means that you will not be able to use features such as continuous integration, automated testing, or deployment. Additionally, you will not be able to use pre-defined workflows or reusable workflows that are created by other users or organizations. If you have existing workflows or actions in your repository, they will be disabled and will no longer run automatically.  Note that if your organization has an overriding policy or is managed by an enterprise that has overriding policy, you might not be able to manage these settings. In this case, you should contact your organization's administrator for assistance.", "026b9323-4736-413f-970f-e9831de2c30c": "Custom fields in {% data variables.product.prodname_projects_v2 %} are additional fields that can be added to issues, pull requests, and draft issues to track metadata specific to a team's needs and processes. These fields can be used to add various types of data, such as dates, numbers, text, and single select options. For example, a custom date field can be used to track target ship dates, a custom number field can be used to track task complexity, a custom single select field can be used to track priority levels, and a custom text field can be used to add quick notes. These custom fields can be used to build a richer view of item attributes and provide more flexibility in tracking and managing work.", "dab636d6-5a80-4854-9cce-e4552af8476d": "Without further context, it is unclear what software or programming language is being referred to in this text material. Therefore, it is impossible to determine the specific outcome that can be achieved by utilizing the \"AUTOTITLE\" command. However, in some programming languages such as Microsoft Excel, the \"AUTOTITLE\" command is used to automatically display a title at the top of a worksheet. For example, in Excel, the syntax for using the \"AUTOTITLE\" command is:  `AUTOTITLE [row_num] [col_num] [title] [font_size] [font_name] [color_index]`  This command can be used to display a title at a specific location on the worksheet, with customizable font size, font name, and color index. The row and column numbers specify the location of the title on the worksheet. If the row and column numbers are omitted, the title will be displayed at the top left corner of the worksheet by default. Here's an example:  `AUTOTITLE 1 0 \"My Worksheet\" 12 \"Arial\" 0`  This command will display the title \"My Worksheet\" in size 12, using the Arial font, and with black text color (color index 0) at the top left corner of the worksheet (row 1, column 0).", "9a117e00-f4c0-4b4e-a53f-ce8a6ca2b314": "To target specific repositories in your organization using repository properties, follow these steps:  1. In your GitHub organization, go to the repository where you want to create a ruleset. 2. Click on the settings tab. 3. Click on the Code scanning tab. 4. Click on the Rulesets tab. 5. Click on the New ruleset button. 6. In the \"Create a new ruleset\" modal dialog, enter a name and description for your ruleset. 7. In the \"Targeting criteria\" section, select {% octicon \"goal\" aria-hidden=\"true\" %} **Target: REPOSITORIES**. 8. Click on the **Dynamic list by property** option. 9. In the modal dialog that appears, select a property from the dropdown menu, then select a value for the property. 10. Click on the **Add target** button. 11. Repeat steps 9 and 10 for any additional properties and values you want to target. 12. Click on the Save ruleset button to create your ruleset.  Note: Repository properties are in public beta and subject to change. For more information, see \"AUTOTITLE.\"", "2cde9547-4a1e-4d02-b91d-6d5b3362ea3f": "It's appropriate to lock a discussion on GitHub when the entire conversation is not constructive or violates the community's code of conduct or GitHub's Community Guidelines. It's also appropriate to lock a discussion to prevent further comments on an announcement or a resolved issue. When a discussion is locked, people with write access to the repository or source repository for organization discussions can still comment on it, but regular users cannot. This helps to prevent further unnecessary comments and keeps the discussion organized and focused. However, if you want to allow emoji reactions while the discussion is locked, you can select that option while locking the discussion.", "6b06a8a2-3ca7-4ed0-86c6-b23abbcbd343": "To push changes from your local repository to {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %} using Git (assuming the default branch is named \"main\"), follow these steps:  1. Make sure you have Git installed on your local machine. 2. Open your terminal or command prompt. 3. Change the current working directory to your local project. 4. Run the following command to add the URL for the remote repository where your local repository will be pushed:     ```shell    git remote add origin REMOTE-URL    ```     Replace `REMOTE-URL` with the repository's full URL on {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %}.  5. Run the following command to verify that you set the remote URL correctly:     ```shell    git remote -v    ```  6. Run the following command to push the changes in your local repository to {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %}:     ```shell    git push origin main    ```     If your default branch is not named \"main,\" replace \"main\" with the name of your default branch.  7. Wait for Git to push your changes to {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %}.  8. You can now view your changes on {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %} by navigating to your repository's page.  Note: If you encounter any errors during the push process, make sure you have the correct remote URL and that you are logged in to {% ifversion ghae %}{% data variables.product.product_name %}{% else", "99ac099e-a31f-43a3-9c5e-57a4c24f01f6": "The purpose of creating a new branch before making changes to a project in {% data variables.product.product_name %} is to follow GitHub Flow and ensure that changes are isolated in their own branch, making it easier to contribute to the same project again in the future and preventing conflicts with other contributors' changes. This allows for better collaboration and organization of work within the project.", "9bc1e951-2600-4d99-b814-d6d2d1c60ad3": "Alerts that are shown as annotations in the {% data variables.product.prodname_code_scanning_caps %} results check are new alerts that have been caused by changes made in the pull request. These alerts are shown as annotations directly on the lines of code that caused them, making it easy to see exactly where the issue is and what needs to be fixed. To access all the alerts for the analyzed branch, including both new and existing alerts, you can click the \"View all branch alerts\" link in the {% data variables.product.prodname_code_scanning_caps %} results check. This will open the full alert view, where you can filter and sort the alerts by various criteria to help you quickly identify and address any issues.", "2eadcd67-1d3e-480d-ad05-c8b72ed2a0ea": "The environment variable `BITBUCKET_COMMIT` in this context represents the unique identifier for the specific changeset or revision in the Git repository being built. It provides information about the specific version of the codebase that is being built and tested, allowing for more targeted and precise testing and debugging. This variable can be used to retrieve specific details about the changes made in the commit, such as the author, date, and message, as well as to compare the current state of the codebase with previous versions.", "ac1160a9-e7bc-491f-8043-dbe240c9e6f4": "According to the context information provided, an \"open\" {% data variables.secret-scanning.alerts %} refers to a detected secret that has not been resolved or addressed yet, while a \"closed\" {% data variables.secret-scanning.alerts %} refers to a detected secret that has been resolved or addressed. The context information doesn't provide further details on what actions can be taken to resolve or address a detected secret, but it does provide options to filter and sort the alerts based on their resolution status.", "48158619-746b-4df5-9fb4-55335765faed": "The self review checklist provided for contributing to the {% data variables.product.prodname_docs %} team includes steps such as confirming adherence to the content design plan, reviewing changes for technical accuracy and grammar, checking versioning, and ensuring the pull request follows guidelines for translation. The checklist is designed to help contributors ensure their changes are accurate, user-friendly, and ready for review by the {% data variables.product.prodname_docs %} team.", "cce93114-04c0-4907-ab44-4ca7dec3369b": "The process of contributing changes to a repository through pull requests involves forking the repository, making changes in your own copy, and then creating a pull request to merge your changes back into the original repository. The project owner can then review the changes, provide feedback, and either merge the pull request or reject it.  To make a pull request more valuable to the project owner, it's important to provide a clear and detailed description of the changes being made. This should include a title that accurately reflects the nature of the changes, as well as a detailed explanation of why the changes are being made and how they will benefit the project. It's also helpful to include any relevant background information or context, as well as any potential drawbacks or trade-offs that should be considered. By providing a thorough and well-written description, you can help the project owner make an informed decision about whether to merge your changes.", "71499613-ce17-4f8d-8c8b-7c7ec5a0e535": "To enable {% data variables.product.prodname_GH_advanced_security %} on your repositories, you need to have a {% data variables.product.prodname_GH_advanced_security %} (GHAS) license. If you have a {% data variables.product.prodname_GH_advanced_security %} license, you can follow these steps to enable it:  1. Go to your repository settings. 2. Click on the \"Security\" tab. 3. Click on \"Advanced security\" to expand the section. 4. Toggle the switch to enable {% data variables.product.prodname_GH_advanced_security %}.  The benefits of enabling {% data variables.product.prodname_GH_advanced_security %} include:  1. Dependency graph: This feature helps you visualize the dependencies between the packages in your project, making it easier to identify and manage security vulnerabilities. 2. Secret scanning: This feature automatically scans your codebase for secrets, such as API keys and passwords, and alerts you if any are found. 3. Code scanning: This feature automatically scans your codebase for security vulnerabilities and alerts you if any are found. 4. Code quality: This feature provides insights into the quality of your code, helping you identify and address issues before they become problems. 5. License management: This feature helps you manage the licenses for the packages in your project, ensuring that you are using them in compliance with their terms.  Overall, enabling {% data variables.product.prodname_GH_advanced_security %} provides a comprehensive set of tools and features to help you improve the security and quality of your codebase.", "ee861817-0570-4552-90cf-2a56b30cb681": "The identifier for the current pipeline is `CI_PIPELINE_ID`.", "7f039545-301f-4a12-a079-5db4c92b0a0e": "a) GitHub and Bamboo  Explanation:  The context information provided specifies that the {% data variables.product.pat_generic %} is being created for both GitHub and Bamboo. This indicates that the user is integrating GitHub and Bamboo for their CI/CD pipeline. Therefore, the `configure` command is being used to configure the {% data variables.product.pat_generic %} for both GitHub and Bamboo. This leads us to conclude that the user is integrating GitHub and Bamboo for their CI/CD pipeline.  If the context information didn't provide enough information to determine which CI/CD tools are being integrated, then we wouldn't be able to answer the question accurately. In such a case, we would say that we don't have enough information to give an answer.", "4de03c98-7c51-415b-b21d-e9c36245c8d0": "Unfortunately, based on the provided context, it seems that it's not possible to move a discussion to a different category in a repository or organization on GitHub. The context only mentions discussing how to move a discussion to a different category, but it's specifically stated that it's not possible to do so in the Polls category. If you need to move a discussion to a different category, you may want to consider archiving the discussion and creating a new one in the desired category. Alternatively, you could contact GitHub support to request this feature be added.", "2fe9acc6-19dc-4141-9f5e-a8b975b1fd91": "To download metrics from your organization on GitHub, follow these steps:  1. Access your organization by following the instructions provided in the context information. 2. Click on your organization's name in the top left corner of the page. 3. Click on \"Settings\" in the left-hand menu. 4. Click on \"GitHub Connect\" in the left-hand menu. 5. Click on the \"Download CSV\" button to download the metrics.  Alternatively, you can follow these steps if you have access to the enterprise account:  1. Access the enterprise account by following the instructions provided in the context information. 2. In the left-hand menu, click on \"GitHub Connect\". 3. Click on the \"Download CSV\" button to download the metrics.  Note that the specific steps may vary slightly depending on the version of GitHub you are using.", "09d86a6b-b9a3-4f5b-b643-143936986618": "To create automatically generated release notes for a new release in {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to your repository in {% data variables.product.prodname_dotcom %}. 2. Click on \"Releases\" in the left-hand menu. 3. Click on \"Create a new release\" to create a new release. 4. Enter the release title and tag version. 5. Click on \"Generate release notes\" (or \"Auto-generate release notes\" if you haven't created a release before) to automatically generate the release notes. 6. Review the generated notes to ensure they include all the necessary information. 7. Click \"Publish release\" to publish the release and its notes.  If you want to customize the automatically generated release notes, you can create a `release.yml` file in the `.github` directory of your repository. This file allows you to exclude certain labels and authors from appearing in the release notes, as well as create new categories and list the pull request labels to be included in each of them.  For more information on creating and customizing automatically generated release notes, refer to the {% data variables.product.prodname_dotcom %} documentation on this topic.", "f48da6ef-5a5f-4e32-be9b-2dba76deb699": "The benefits of using private vulnerability reporting include protecting sensitive information, reducing the risk of public disclosure, and enabling a coordinated response to the vulnerability. Notification preferences for this feature can be configured by going to the organization settings, selecting \"Code security and analysis\", and adjusting the notification settings for private vulnerability reporting. This can include specifying who receives notifications, the type of notifications received, and the frequency of notifications. It's important to ensure that the appropriate individuals are notified in a timely manner to address the vulnerability.", "15c2b39f-9c59-4338-991a-6455e7a2a08a": "The difference between downloading a source code archive from a release and downloading a source code archive from a tag on GitHub is that a release is a specific version of the software that has been formally announced, while a tag is a label that can be applied to any commit in the repository. When downloading a source code archive from a release, you are downloading the exact version of the software that was released, including any associated assets or binaries. When downloading a source code archive from a tag, you are downloading the code as it existed at the time the tag was created, but without any associated assets or binaries.  For example, let's say we have a repository called `my-repo`. If we create a release for version 1.0.0 and tag the commit with `v1.0.0`, we can download the source code archive for the release by going to the Releases page for the repository and clicking the \"Source code (zip)\" or \"Source code (tar.gz)\" link for the release. This will download a zip or tar.gz file containing the code, assets, and binaries for version 1.0.0.  Alternatively, we can download the source code archive for the tag by going to the Tags page for the repository and clicking the \"zip\" or \"tar.gz\" link for the tag. This will download a zip or tar.gz file containing only the code for the commit associated with the tag, without any associated assets or binaries.  In summary, when downloading a source code archive from a release, you are downloading a specific version of the software with all associated assets and binaries, while when downloading a source code archive from a tag, you are downloading only the code for the commit associated with the tag.", "e634361c-3640-4dc0-aaa4-98c2194237ce": "To update all files on the current branch to reflect the new configuration after marking files as binary in \".gitattributes\", follow these step-by-step instructions:  1. Remove all files from the Git index and reset the current branch to match the HEAD commit using the following commands:     ```shell copy    git rm -rf --cached .    git reset --hard HEAD    ```  2. Check the status of your repository to confirm that all files have been rewritten and normalized according to the new configuration:     ```shell copy    git status    ```  3. Optionally, commit any outstanding changes in your repository by running the following command:     ```shell copy    git commit -m \"Normalize all the line endings\"    ```  These steps will ensure that all files on the current branch are updated to reflect the new configuration after marking them as binary in \".gitattributes\".", "98579f64-88b4-4174-93fc-680851e9147f": "To set up requested actions using the REST API, as described in the context information, you can follow these general steps:  1. Authenticate your request using your personal access token or an OAuth app access token. 2. Send a POST request to the `/repos/{owner}/{repo}/check-suites/{check-suite}/requests` endpoint with the requested action details in the request body. 3. The requested action will be added to the check suite and executed when the associated check runs.  For a detailed example of how to set up requested actions using the REST API, you can refer to the \"AUTOTITLE\" link provided in the context information.", "99ff39b1-31d2-4735-a5c1-728b999d1693": "The syntax for defining Docker images differs between GitLab CI/CD and {% data variables.product.prodname_actions %} in the location of the key used to define the Docker image. In GitLab CI/CD, the `image` key is used to define the Docker image, while in {% data variables.product.prodname_actions %}, the `container` key is used to define the Docker image.", "55c6c7c6-6ffc-4eea-b4ee-3383b3341d13": "A security researcher can report a vulnerability privately to an organization using the private vulnerability reporting feature by accessing the organization's repository on GitHub, navigating to the Security section, selecting the Private vulnerability reporting option, and filling out the provided form with details about the vulnerability. This will submit the report directly to the organization, who will then review and respond to the report through the platform's notification system.", "e2365cfb-f698-47ec-8ab8-5a9ec5ac6464": "Based on the information provided, Section 6.5 seems to indicate that GitHub has proposed a solution to address a specific issue related to data protection, but the customer has not accepted it. The fact that GitHub has not been able to provide an alternative solution within ninety days suggests that the proposed solution may not be feasible or practical, and the customer may be considering other options. This could potentially lead to a breach of the data protection agreement, as failure to address the issue in a timely manner is considered a material breach under Section 7.3. It's also possible that the customer is exploring alternative service providers, as the inability to resolve the issue within the specified timeframe may have impacted the customer's confidence in GitHub's ability to meet its data protection obligations. Without further context, it's difficult to determine the exact significance of this information.", "43d65ad6-024a-4b37-8d3a-1aaa52156f74": "To configure the severity level of {% data variables.product.prodname_code_scanning %} alerts to prevent pull request check failures, you can override the default behavior in your repository settings. This allows you to specify the level of severities and security severities that will cause a pull request check failure. For more information, refer to the documentation provided by GitHub.", "bde37ef8-222b-44b7-8832-cdd50631a7d6": "To configure authentication for {% data variables.product.product_name %} using your SAML IdP, follow these steps:  1. Log in to your {% data variables.product.product_name %} account and navigate to the organization settings. 2. Click on the \"Security\" tab and select \"Set up single sign-on (SSO)\" from the drop-down menu. 3. Click \"Configure\" to the right of \"Set up your identity provider\". 4. Under \"Sign on URL\", copy and paste the URL for your SAML IdP. This URL is typically found in the configuration settings of your IdP. 5. Under \"Issuer\", copy and paste the issuer URL for your SAML IdP. This URL is also found in the configuration settings of your IdP. 6. Under \"Public certificate\", copy and paste the public certificate for your SAML IdP. This certificate is also found in the configuration settings of your IdP. 7. Click \"Test SAML configuration\" to ensure that the information you've entered is correct. 8. Click \"Save\" to complete the configuration process.  Available policies for your enterprise on {% data variables.product.product_name %} include:  1. Default Repository Permissions: This policy determines the default permissions level for repositories in your enterprise. You can select from three options: \"Restricted\", \"Team-managed\", or \"Everyone\". If a person has multiple avenues of access to an organization, either individually, through a team, or as an organization member, the highest permission level overrides any lower permission levels. 2. Repository Creation: This policy determines whether members can create repositories within your enterprise. You can select \"Allow\" or \"No policy\". If you select \"No policy\", organizations within your enterprise can set their own repository creation policies. 3. Repository Forking: This policy determines whether members can fork private and internal repositories within your enterprise. You can select \"Allow\" or \"No policy\". If you select \"No policy\", organizations within your enterprise can set their own repository forking policies. 4. Repository Invitations: This policy determines whether members or organizations can invite collaborators to repositories within your enterprise. You can select \"Allow\"", "0a83f7bf-f26e-4ac9-8cc9-cafc892e6169": "To create a custom label for a repository runner, follow these steps:  1. Go to the repository where you want to create the custom label. 2. Click on the \"Actions\" tab in the left-hand menu. 3. Click on \"Self-hosted runners\" in the \"Runner groups\" section. 4. Click on the runner for which you want to create the custom label. 5. Click on the \"Labels\" tab. 6. Click on the \"New label\" button. 7. Enter a name for the custom label in the \"Label name\" field. 8. (Optional) Enter a color for the custom label by clicking on the color picker. 9. Click on the \"Create label\" button.  The custom label will now be available for use with this runner.  Note: Labels are case-insensitive.", "a2e3398d-5370-4519-b9d2-8284f15137f2": "In the given context, if {% data variables.product.prodname_code_scanning %} fails with a new configuration that includes a {% ifversion code-scanning-default-setup-recommended-languages %}{% ifversion code-scanning-default-setup-go %}Go, {% endif %}JavaScript/TypeScript, Python, or Ruby,{% else %}a {% data variables.product.prodname_codeql %}-supported language,{% endif %} {% data variables.product.prodname_dotcom %} will automatically resume the previous configuration to ensure that the repository does not lose {% data variables.product.prodname_code_scanning %} coverage.", "8505c042-a481-4803-bb08-e9e19c21f7e0": "The Security Overview feature includes risk and coverage information about GitHub's built-in security features, such as {% data variables.product.prodname_dependabot %}, {% data variables.product.prodname_code_scanning %}, and {% data variables.product.prodname_secret_scanning %}. For enterprises that use GitHub Advanced Security, the feature also includes information about other advanced security features. The feature provides interactive views with filters that allow you to investigate security issues and take action to remediate them. Additionally, for enterprise-owned organizations that use GitHub Advanced Security, an organization-level dashboard of insights from security features is provided.", "ae960cb4-83ac-409c-8cd0-120b17a46a1b": "To review the details about the commits from the upstream repository before syncing a fork branch using the GitHub CLI, you can use the `gh repo view -R UPSTREAM_REPOSITORY -b UPSTREAM_BRANCH` command. This will display a list of commits from the specified branch in the upstream repository. You can then decide whether to proceed with syncing your fork branch or not. Here's an example command:  ```shell gh repo view -R owner/upstream-repository -b main ```  Replace `owner/upstream-repository` with the name of the upstream repository and `main` with the name of the branch you want to review.", "bc97e92f-1615-4e6f-863f-57e935edd9ec": "To change the cache size limit in rofile.org's settings, you need to follow these steps:  1. Go to your rofile.org account and log in. 2. Click on your organization's name in the top left corner of the page. 3. Select \"Settings\" from the drop-down menu. 4. In the left-hand menu, click on \"General\". 5. Scroll down to the \"Actions\" section and click on \"Change cache size limit\". 6. Enter the desired cache size limit in bytes. 7. Click \"Save changes\" to apply the new limit.  Note that this feature may not be available for all rofile.org accounts, as it is mentioned in the context information that this action is only available for certain users. If you are unsure whether you have access to this feature, you can check with rofile.org's support team.", "04406350-bbf3-4532-b35c-990e6e36010e": "To stop a codespace using the {% data variables.product.prodname_cli %}, you can use the following command:  ```shell gh codespace stop -c CODESPACE-NAME ```  Replace `CODESPACE-NAME` with the unique name of the codespace you want to stop. This command will stop the selected codespace and release its resources. For more information, you can refer to the {% data variables.product.prodname_cli %} online help for \"gh codespace stop.\"", "0a2ef44c-3e5d-443f-9339-58b96f33a53e": "The `--registries-auth-stdin` option is used to authenticate to GitHub Enterprise Server Container registries. It allows the user to provide the authentication credentials through standard input instead of using the `GITHUB_TOKEN` environment variable. This option overrides the `GITHUB_TOKEN` environment variable. This is useful when working with GitHub Enterprise Server Container registries, as it provides a more secure way to manage authentication credentials. To use this option, the user needs to provide the authentication credentials through standard input when running the command. This can be done by redirecting the input from a file containing the credentials or by piping the credentials from another command.", "8ecbb533-7430-426a-8e50-7b0632c0ee08": "To ensure that your domain settings are properly configured before booting up {% data variables.location.product_location %} for the first time, you should follow these steps:  1. Access the Enterprise Site Administration settings by following the instructions provided in the context information. 2. Click on the Management Console option. 3. Click on the Hostname menu item. 4. Click on the Test domain settings button to test your appliance's DNS and SSL settings. 5. If the test is successful, click on the Save settings button to save your changes. 6. If the test fails, review your domain settings and make the necessary changes before attempting the test again.  By following these steps, you can ensure that your domain settings are properly configured and avoid any issues when booting up {% data variables.location.product_location %} for the first time.", "20fca698-5913-44e0-9b54-63c0d65d1663": "The Subversion properties currently not supported by {% data variables.product.product_name %} are `svn:externals` and `svn:global-ignores`, as well as any custom properties. This is because {% data variables.product.product_name %} doesn't currently support these properties. No reason is provided in the context information as to why these properties are not supported.", "611d869a-78bb-4cf4-beb2-4cf8bf70fb56": "The SARIF format is a standardized format for representing the output of static analysis tools. It allows for the interchange of analysis results between different tools and platforms, making it easier to integrate and compare results.  To use SARIF to represent the output of a {% data variables.product.prodname_codeql %} database analysis using a {% data variables.product.prodname_code_scanning %} query suite, you can use the `--format=sarifv2.1.0` flag in the {% data variables.product.prodname_codeql %} CLI. This will generate a SARIF file that can be uploaded to {% data variables.product.prodname_dotcom %} or integrated into your own code-review or debugging infrastructure.  To integrate SARIF into your own infrastructure, you can use a SARIF viewer plugin for your IDE. This plugin will highlight alerts in the correct location in your source code, making it easier to review and address issues. For example, the Visual Studio Code extension \"SARIF Viewer\" provides a SARIF viewer plugin for Visual Studio Code.  Here's an example of how to use the SARIF format with the {% data variables.product.prodname_codeql %} CLI:  ```shell codeql database analyze codeql/cpp-queries:codeql-suites/cpp-code-scanning.qls --format=sarifv2.1.0 --output=cpp-results.sarif --download ```  This command downloads the `codeql/cpp-queries` {% data variables.product.prodname_codeql %} query pack, runs the analysis, and generates a SARIF file in the `cpp-results.sarif` format. The `--download` flag downloads the results to your local machine, but you can omit this flag if you prefer to upload the results directly to {% data variables.product.prodname_dotcom %}.  Once you have the SARIF file, you can open it in a SARIF viewer plugin for your IDE, or upload it to {% data variables.product.prodname_dotcom %} for further analysis and collaboration", "d1d24326-d9ed-4232-a7e2-cfe9e8c899b9": "When specifying the access required to take an action, the role at the same level as the action should be referred to, as outlined in the context information. This means that if the action is at the repository level, the role at the repository level should be referred to, such as \"admin access to a repository\" for configuring protected branches. If the action is at the account level, such as managing billing, the role at the account level should be referred to, such as \"billing manager for an enterprise account.\" The context should provide enough information to determine the appropriate level for the action and the corresponding role. If the context doesn't provide enough information, it may not be possible to determine the required access level and role.", "e89d1f34-3ea5-4dfa-b81d-9a96a9ee7983": "To enable default setup for eligible repositories in your organization through security overview, follow these steps:  1. Go to your organization's security overview page. 2. Click the \"Coverage\" tab. 3. In the search bar, enter \"code-scanning-default-se\". 4. Find the repositories that are eligible for default setup and have not yet enabled it. 5. Click the checkbox next to each repository you want to enable default setup for. 6. Click the \"Bulk actions\" dropdown menu and select \"Enable default setup\". 7. Confirm the action by clicking \"Enable default setup\" in the confirmation dialog box.  Note: If you want to recommend the \"Extended\" query suite throughout your organization when enabling default setup, select \"Recommend the extended query suite for repositories enabling default setup\" when enabling default setup for eligible repositories.  Alternatively, you can also enable default setup for eligible repositories through the organization settings page. For more information, see \"AUTOTITLE.\"    Eligible repositories for {% data variables.product.prodname_codeql %} default setup at scale  {% data reusables.organizations.navigate-to-org %} {% data reusables.organizations.settings %} {% data reusables.organizations.security %} {% data reusables.security.code-scanning %} 1. In the \"Default setup\" section, click \"Configure\". 2. Review the list of eligible repositories and their status. 3. To enable default setup for all eligible repositories, click \"Enable for eligible repositories\". 4. To enable default setup for specific repositories, click \"Configure for selected repositories\". 5. In the \"Configure for selected repositories\" dialog box, select the checkboxes next to the repositories you want to enable default setup for. 6. Click \"Enable default setup\" to confirm the action.  Note: If you want to recommend the \"Extended\" query suite throughout your organization when enabling default setup, select \"Recommend the extended query suite for repositories enabling default setup\" when enabling default setup for specific repositories", "30c1ba54-3201-4c1a-a344-81d9fbad2875": "The purpose of the Get feeds endpoint in {% data variables.product.prodname_dotcom %} is to list all the feeds available to the authenticated user, allowing them to access timeline resources in either JSON or Atom format by sending a request to a specific feed URL. This feature enables users to stay updated on various GitHub resources, such as security advisories, in a convenient and efficient manner.", "7ff6083b-0df6-448c-b13a-aed0f94e4787": "The context information provided does not indicate whether the team member can open or close issues. Therefore, it is unclear whether they have the authority to perform these actions.  Answer:  The context information provided does not indicate whether the team member can view GitHub Actions workflow runs. Therefore, it is unclear whether they have the authority to perform this action.  Answer:  Based on the context information provided, it is unclear whether the team member can approve or request changes to pull requests with required reviews. The information provided only indicates that they can submit reviews on pull requests and apply suggested changes to pull requests. However, it does not explicitly state whether they have the authority to approve or request changes with required reviews.  Answer:  Based on the context information provided, it is unclear whether the team member can reopen closed issues. The information provided only indicates that they can close issues themselves and have issues assigned to them. However, it does not explicitly state whether they have the authority to reopen closed issues.  Answer:  Based on the context information provided, it is unclear whether the team member can view GitHub Actions workflow runs. The information provided only indicates that they can view published releases. However, it does not explicitly state whether they have the authority to view GitHub Actions workflow runs.  Answer:  Based on the context information provided, it is unclear whether the team member can close issues themselves. The information provided only indicates that they can close issues themselves and have issues assigned to them. However, it does not explicitly state whether they have the authority to close issues themselves.  Answer:  Based on the context information provided, it is unclear whether the team member can submit reviews on pull requests. The information provided only indicates that they can send pull requests from forks and apply suggested changes to pull requests. However, it does not explicitly state whether they have the authority to submit reviews on pull requests.  Answer:  Based on the context information provided, it is unclear whether the team member can assign issues to themselves or others. The information provided only indicates that they can have issues assigned to them. However, it does not explicitly state whether they have the authority to assign issues to themselves or others.  Answer:  Based on the context information provided, it is unclear", "08762833-a434-4406-8488-ebcb7f0bf5f1": "To display the status of workflow runs triggered by the `push` event, you can modify the URL for a workflow status badge by adding `?event=push` to the end of the URL. This will show the status of the build for the current state of that branch when the workflow is triggered by a push event. For example, you can use the following URL to display the status badge for a workflow with the file path `.github/workflows/main.yml` and the status of workflow runs triggered by the `push` event:  ```text {% ifversion fpt or ghec %}https://github.com{% else %}HOSTNAME{% endif %}/OWNER/REPOSITORY/actions/workflows/WORKFLOW-FILE/badge.svg?event=push ```  Replace `OWNER`, `REPOSITORY`, and `WORKFLOW-FILE` with the appropriate values for your repository.", "9638b085-11ce-428f-b80e-191174ddd0ff": "{% data variables.contact.premium_support %} may close a ticket if the ticket is outside the scope of support or if multiple attempts to contact you have gone unanswered.", "314b891b-8272-46af-9411-69c4fbfe9f4d": "To authenticate on the command line using Git Credential Manager after enabling two-factor authentication on {% data variables.product.product_name %}, you need to follow these steps:  1. Install Git Credential Manager for your operating system from the official GitHub website. 2. Open Git Bash or your preferred Git command-line tool. 3. Configure Git to use Git Credential Manager as your credential helper by running the following command:     ```    git config --global credential.helper \"credential-manager\"    ```  4. Enter your GitHub username and password when prompted by Git Credential Manager. 5. After entering your credentials, Git Credential Manager will store them securely and automatically fill them in when you push, pull, or clone repositories.  Note: If you're using Git over HTTPS, you'll still need to enter your GitHub username and password when prompted by Git Credential Manager. However, if you're using Git over SSH, you won't need to enter your password again after the initial setup.  For more information about Git Credential Manager, see the official GitHub documentation.", "816d9c2e-5cf0-47c4-b794-13d8dfbcf93d": "To retry or cancel failed invitations to outside collaborators in GitHub Enterprise, you can follow these steps:  1. Go to your organization's settings by clicking on your profile picture in the top right corner of the page, then selecting \"Organizations\" from the drop-down menu, and clicking on the organization you want to manage. 2. Click on the \"People\" tab. 3. Click on \"Failed invitations\" under the \"People\" tab. 4. To retry or cancel a single invitation, select the {% octicon \"kebab-horizontal\" aria-label=\"Show actions\" %} dropdown menu to the right of the invitation you want to cancel or retry, and click \"Retry invitation\" or \"Cancel invitation\". 5. To confirm, click \"Yes, retry\" or \"Yes, cancel\". 6. To retry or cancel multiple invitations at once, select the checkboxes next to the invitations you want to retry or cancel, and select \"Retry invitations\" or \"Cancel invitations\" from the drop-down menu at the top of the list.  Note that invitations expire after 7 days, and failed invitations to outside collaborators can also be found in this view.", "b2b41eff-676e-4a97-a468-affb1a296208": "To customize the keyboard shortcuts for opening the command palette in both search mode and command mode in {% data variables.product.prodname_command_palette %}, you can go to the Accessibility section of your account settings. From there, you can customize the keyboard shortcuts for opening the command palette in both search mode and command mode. The default keyboard shortcuts used to open the command palette may conflict with your default OS and browser keyboard shortcuts, so customizing these shortcuts can help avoid conflicts.", "05b01a74-20ad-4d43-943a-7ac32a7baddc": "The provided workflow takes the following steps to publish a package to GitHub Packages:  1. Checks out a copy of the project's repository. 2. Sets up the Java JDK and automatically configures the Maven _settings.xml_ file to add authentication for the `github` Maven repository using the `GITHUB_TOKEN` environment variable. 3. Publishes the package using the `mvn --batch-mode deploy` command.  To modify this workflow to publish to a different registry, you need to replace the `publish` job with a new job that sets up the required environment for the new registry and publishes the package using the appropriate Maven command. You also need to add a distribution management repository for the new registry in your _pom.xml_ file with an `id` that you can use in the new job. For example, if you want to publish to the Maven Central Repository through the OSSRH hosting project, you can add a distribution management repository with an `id` set to `ossrh` and modify the workflow to use the `setup-java` action for the OSSRH hosting project. Here's an example:  ```yaml copy name: Publish package to the Maven Central Repository and GitHub Packages on:   release:     types: [created] jobs:   publish-ossrh:     runs-on: ubuntu-latest     permissions:       contents: read       packages: write     steps:       - uses: {% data reusables.actions.action-checkout %}       - uses: {% data reusables.actions.action-setup-java %}         with:           java-version: '11'           distribution: 'temurin'       - name: Publish package to the Maven Central Repository         run: mvn --batch-mode deploy -DaltDeploymentRepository=ossrh::default::https://oss.sonatype.org/content/repositories/releases/         env:           GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}           OSSHUB_USERNAME: {%", "0602d828-31aa-43b7-be37-a7e9c45145a3": "To launch {% data variables.product.prodname_ghe_server %} on Google Cloud Platform, you need:  1. A software license for {% data reusables.enterprise_installation.software-license %} 2. A Google Cloud Platform account capable of launching Google Compute Engine (GCE) virtual machine (VM) instances 3. Familiarity with the gcloud compute command-line tool for initial setup (optional, but recommended)  Note: The context doesn't provide information about the hardware requirements or the specific machine type recommended for {% data variables.product.prodname_ghe_server %}. It's also unclear whether the context assumes prior knowledge of configuring the firewall for the network associated with the {% data variables.product.prodname_ghe_server %} VM. If more context is needed to answer these questions, it's best to ask for clarification.", "853d9ef2-b0d0-4fb0-8808-2d74718a5661": "According to the provided context, the pre-release software will have limited support and development compared to the commercially released version. The pre-release software will only be supported until either the commercial release of the software, discontinuation of support or development by GitHub, or termination of the technical preview by either party. This means that users may not be able to access data used in the software once it stops running. In contrast, it is not specified how long the commercially released version will be supported, but it can be assumed that it will have ongoing support and development.", "fef7e779-d317-4826-8623-e3261980f420": "To change which workflows can access an enterprise runner group, follow these steps:  1. Go to your enterprise's Actions settings. 2. Click Runner groups in the left-hand menu. 3. Find the runner group you want to change and click the three dots to the right of its name. 4. Click Edit. 5. In the Edit runner group dialog, scroll down to the \"Workflows\" section. 6. Select the checkboxes next to the workflows you want to grant access to. 7. Click Save.  Alternatively, you can also change which workflows can access an enterprise runner group from the runner group's settings page. To do this, go to your enterprise's Actions settings, click Runner groups in the left-hand menu, find the runner group you want to change, and click its name to go to its settings page. From there, scroll down to the \"Workflows\" section and follow steps 6 and 7 from the previous answer.  Note that you can also restrict access to a runner group to specific workflows by following the same steps, but instead of selecting all workflows, select only the workflows you want to grant access to.  I hope that helps! Let me know if you have any other questions.", "b2931886-4e88-47b8-892f-1c2979de08cf": "The `--sarif-category` option is optional for single database analysis and required to define the language when analyzing multiple databases for a single commit in a repository. Its role is to include a category in the SARIF results file for this analysis, which is used to distinguish multiple analyses for the same tool and commit but performed on different languages or different parts of the codebase. This helps to differentiate and organize the results from multiple analyses for the same commit, making it easier to understand and interpret the results.", "e76d9ab5-1650-494a-95ff-4bb68d46f1af": "The location of the user's home directory in this context is `$HOME`, which is expanded to `${{ github.workspace }}` in this context. This suggests that the user's repository is being built on a GitHub-hosted runner, and `$HOME` is being used as a shorthand for the directory where the user's code is being built and tested.", "a048731d-0955-4852-bcba-c6b0cfd02627": "Without further context, it is unclear what \"AUTOTITLE\" refers to. If \"AUTOTITLE\" is referring to a specific document feature, it could potentially differ from other document features in terms of functionality based on the specific software or document type being used. More information is needed to accurately answer this question.", "78c51905-9544-4782-93e8-d635bd50b21a": "Importing a ruleset refers to uploading a pre-existing ruleset file to a repository on GitHub, while creating a new ruleset refers to defining a set of rules from scratch using GitHub's built-in ruleset editor. Importing a ruleset allows you to use a ruleset that has already been created and tested, while creating a new ruleset allows you to customize rules to fit the specific needs of your repository.", "97726eb2-2d6a-42d1-a4c7-9d03fb53038a": "The `main` function should be updated to call the `whoami` function when the `whoami` command is given, like this:  ```ruby def main   case ARGV[0]   when \"help\"     help   when \"login\"     login   when \"whoami\"     whoami   else     puts \"Unknown command #{ARGV[0]}\"   end end ```  The `help` function should include the `whoami` command, like this:  ```ruby def help   puts \"usage: app_cli \"   puts \"  help                   Display this help message\"   puts \"  login                   Log in to GitHub\"   puts \"  whoami                  Display your GitHub username\" end ```    Testing:  1. Run the script with the `help` command:     ```    $ ruby app_cli help    usage: app_cli help    usage: app_cli login    usage: app_cli whoami    ```  1. Run the script with the `login` command:     ```    $ ruby app_cli login    Enter your GitHub username: your_username    Enter your GitHub password:    ```  1. Run the script with the `whoami` command:     ```    $ ruby app_cli whoami    You are your_username    ```  1. Run the script with an unknown command:     ```    $ ruby app_cli unknown_command    Unknown command unknown_command    ```  1. Run the script with an expired or revoked token:     ```    $ ruby app_cli whoami    You are not authorized. Run the `login` command.    ```  1. Run the script with an invalid command:     ```    $ ruby app_cli invalid_command    Unknown command invalid_command    ```  1. Run the script with no command:     ```    $ ruby app_cli    usage: app_cli help    usage: app_cli login    usage", "642a3631-a14c-484b-bc87-517d7bf7ff5b": "The `--config-file-path` argument in {% data variables.product.prodname_actions_importer %} can be used to specify which repository a converted reusable workflow should be migrated to. This is achieved by including the repository slug and path to the reusable workflow YAML file in the configuration file, with each repository slug being unique. During the migration process, {% data variables.product.prodname_actions_importer %} uses the specified configuration file as the source of reusable workflows, and migrates them to the repository specified in the configuration file. This allows for the conversion and migration of multiple reusable workflows from different repositories in a single command.", "4a1c81e7-f0f8-4f49-9829-eee9e11d20c1": "To disable auto assignment for a specific team in the context provided, you need to access the organization, go to the specific team, open its settings, and deselect the \"Enable auto assignment\" option. After that, click \"Save changes\" to apply the changes.", "d1786e50-2f63-45c5-b486-6c0a6b62d8f6": "Unfortunately, the provided context information doesn't provide enough details to implement security hardening for self-hosted runners in a specific environment. However, \"AUTOTITLE\" likely refers to GitHub's documentation on securing self-hosted runners, which includes recommendations such as:  - Limiting access to the runner machine to authorized users only - Ensuring that the runner machine is up-to-date with the latest security patches and software updates - Restricting the use of privileged accounts and minimizing the use of root privileges - Implementing network segmentation and firewall rules to isolate the runner machine from other networks and systems - Regularly monitoring the runner machine for suspicious activity and logging all access and events - Encrypting sensitive data, such as encryption keys and credentials, both at rest and in transit - Implementing multi-factor authentication (MFA) for all user accounts with access to the runner machine - Regularly backing up critical data and configurations to ensure recovery in case of a security incident or hardware failure  It's recommended to review the full documentation for more detailed guidance on securing self-hosted runners in your specific environment.", "cb16bab9-94ec-49cf-b94b-e29d02afbe15": "Composite actions allow you to combine multiple workflow steps into a single action, making it easier to manage and simplify workflow steps. This feature is particularly useful when you have a series of commands that need to be executed in a specific order or require certain dependencies. By bundling these commands into a composite action, you can avoid the need to create multiple separate actions and simplify the workflow. This also offers benefits such as improved readability and maintainability of the workflow, as well as reduced latency and resource usage compared to executing multiple commands separately. Additionally, composite actions can help to reduce the overall number of actions required in a workflow, making it more efficient and easier to manage.", "9717a7a7-f753-4653-b467-10acca716c76": "The `$(Agent.ReleaseDirectory)` variable in the context information represents the directory where the artifacts generated during the build process are stored. This directory is typically created during the release process and contains the final build output.  On the other hand, the `$(Agent.RootDirectory)` variable represents the root directory of the Git repository being built. This is the directory where the source code is located and where the build process begins.  In summary, the `$(Agent.ReleaseDirectory)` variable points to a specific directory within the Git repository, while the `$(Agent.RootDirectory)` variable points to the entire Git repository.", "1e0d48b0-4a63-467e-8dd8-1c5ce2e1e92d": "To join {% data variables.product.prodname_sponsors %} as a sponsored organization, the following information is required: contact information, payout choice, acceptance of legal terms, and completion of the sponsored organization profile, which includes a short bio, introduction, team information, featured work, and opt-in to being featured. Additionally, sponsored organizations must create sponsorship tiers with a price, welcome message, and save as a draft before reviewing and publishing. Bank information is also required for payouts, either to a bank account in a supported region or via a fiscal host.", "0c7d95dd-954d-475b-b702-3a7b3364d3e2": "To edit an existing assignment in GitHub Classroom, follow these steps:  1. Log in to your GitHub Classroom account. 2. Click on the class you want to edit the assignment for. 3. In the Assignments tab, click the pencil icon next to the assignment you want to edit. 4. You can edit the assignment title, default repository prefix, deadline, and status. 5. To edit the assignment title, click in the text field and delete the existing text, then type the new title. 6. To edit the default repository prefix, click the pencil icon next to the prefix, then type the new prefix. 7. To change the deadline, click in the text field and use the date picker to select a new date. 8. To change the status, select the Assignment status dropdown menu and choose Active or Inactive.  Note that you cannot change the assignment type (individual or group) or the online integrated development environment (IDE) after assignment creation.", "29d2b4de-0951-4423-b118-ed3279b7e104": "According to the context information provided, if a team cannot sync with an IdP group on GitHub, it will display an error. For more information, you can refer to the section \"Viewing IdP groups, group membership, and connected teams\" in the documentation. However, the context doesn't provide any specific steps to take if a team cannot sync with an IdP group. It only mentions that the team will display an error. Without further information, it's unclear what specific error message is being displayed or what steps can be taken to resolve the issue. If you're experiencing this issue, I would recommend contacting GitHub support or checking the GitHub documentation for more information on how to troubleshoot this issue.", "a266525a-8520-450e-bd08-93d540131227": "To extend {% data variables.product.prodname_codeql %} coverage for your dependencies that are not recognized by the standard libraries included with {% data variables.product.prodname_codeql %}, you need to define the models of your additional dependencies in a {% data variables.product.prodname_codeql %} model pack in your repository. For more information about {% data variables.product.prodname_codeql %} model packs, you can refer to the documentation for {% data variables.product.prodname_codeql %}.", "6297c7f3-31b7-4212-9e75-bb6848fa0ac1": "The script handles failed deliveries by keeping track of deliveries with the same GUID value and checking if any of them have been successfully delivered within a certain time frame. If no deliveries for a particular GUID have been successfully delivered within the time frame, the script gets the delivery ID of one of the deliveries with that GUID and redelivers it. This prevents duplicate redeliveries for a failed delivery and ensures that only one redelivery is attempted for a particular GUID. The script also prevents redelivery of failed deliveries that have already been successfully redelivered. The redelivery process is carried out by calling the `redeliverWebhook` function, which takes the delivery ID, organization name, hook ID, and Octokit instance as arguments. The script also updates a configuration variable to store the time that the script started, which is used to determine whether the script needs to check and redeliver webhooks during subsequent runs. If there is an error during the check and redelivery process, the script logs the error and throws it to register the workflow run as a failure.", "baac31d0-f21c-4a49-a9ee-9d6abfdaaf9d": "If {% data variables.product.prodname_dependabot %} job logs are available for viewing, you can access them by following these steps:  1. Go to the repository where the dependency update is blocked. 2. Click on the {% data variables.product.prodname_dependabot %} tab in the left-hand menu. 3. Find the manifest file for the dependency that is causing the error. 4. Click on the \"Last checked TIME ago\" link next to the manifest file. 5. Click on the \"View logs\" button to the right of the log entry that corresponds to the error.  If job logs are not available, the context indicates that an error icon will be displayed on the manifest file. However, no further information is provided on how to access the error details. In this case, it's unclear how to view the error details, and more context is needed to provide an answer.", "9c34db2c-03e8-45ad-8c55-effcb9833cf6": "The {% data variables.product.prodname_codeql_cli %} is a command-line tool that allows software developers and security researchers to secure their code using {% data variables.product.prodname_codeql %} analysis. It can be used to run {% data variables.product.prodname_codeql %} analyses using queries provided by {% data variables.product.prodname_dotcom %} engineers and the open source community, generate code scanning alerts, create {% data variables.product.prodname_codeql %} databases, and develop and test custom {% data variables.product.prodname_codeql %} queries. This tool can analyze dynamic languages like JavaScript and Python, compiled languages like C/C++, C#, Go, and Java, and codebases written in a mixture of languages. It can also be used to run {% data variables.product.prodname_codeql %} analysis on code being processed in a third-party continuous integration (CI) system for external CI systems.", "27066e06-5ca2-4fbd-940e-0df3b49a56a6": "To improve performance in {% data variables.product.prodname_codeql %} by defining which queries to run, you can:  1. Use a build command that compiles only one variant to reduce the size of the results file. 2. Specify your chosen build command either by calling the `database create` command with the `--command` option or by defining the build command in a YAML configuration file for code scanning and calling the file using the `--codescanning-config` option. 3. Check the workflow file for the `queries` keyword in {% data variables.product.prodname_codeql %} advanced setup for {% data variables.product.prodname_code_scanning %}. If it is not present, then only the default query suite is run. If it is present, it defines which queries to run. Try commenting out this line of the workflow file to run only the default query suite. 4. Check the database analysis command for any paths that specify queries, query suites, or query packs in {% data variables.product.prodname_codeql_cli %}. If none are present, then only the default query suite is run. If any are present, they define which queries to run. You can try removing them from the call to run only the default query suite. 5. If the results are dominated by the results for a single rule, exclude that rule from analysis by adding its ID to the `exclude` list in the YAML configuration file for code scanning.  Note: The specific steps may vary depending on the version and configuration of {% data variables.product.prodname_codeql %} and {% data variables.product.prodname_codeql_cli %}. It's always recommended to consult the official documentation for the latest information.", "8056da18-6192-4c52-bf4d-2e2dd57a3ea4": "To set up a workflow using the `actions/stale` action to automatically label and close issues based on inactivity, follow these steps:  1. Create a new workflow file in your repository's `.github/workflows` directory, or edit an existing one.  2. Copy and paste the following YAML code into the file:  ```yaml name: Stale Issues  on:   schedule:     - cron: \"0 13 * * *\"  jobs:   stale:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Set up Java 11         uses: actions/setup-java@v1         with:           java-version: 11        - name: Install actions/stale         uses: actions/setup-node@v2         with:           node-version: '14'        - name: Run actions/stale         uses: fitzgen/actions-stale@v1         with:           days-before-issue-stale: 30           stale-issue-label: stale           stale-issue-message: \"This issue has been inactive for more than 30 days. Please update or close it.\"           close-issue-message: \"This issue has been inactive for more than 30 days and has been automatically closed.\"           operations-per-run: 30 ```  3. Save the file and commit your changes.  4. Go to your repository's Settings > Workflows and click the Stale Issues workflow to run it manually.  5. Wait for the workflow to complete and check your repository's issues to see if any have been labeled or closed.  6. The workflow will now run automatically every day at 1:30 PM UTC, as specified in the `schedule` section.  7. To learn more about additional features of the `actions/stale` action, such as closing inactive pull requests or ignoring certain labels or milestones, see the `actions/stale", "d6f2a451-c521-498f-9e55-648a30c0e8e9": "The `--model-packs` option can be used in a `codeql database analyze` command to incorporate dependency information from a specific model pack into the analysis by providing the path to the model pack as an argument. For example:  ```shell $ codeql database analyze /codeql-dbs/my-company --format=sarif-latest \\   --model-packs my-repo/my-java-model-pack \\   --output=/temp/my-company.sarif codeql/java-queries ```  In this example, the `--model-packs` option is used to specify the path to the model pack, `my-repo/my-java-model-pack`, which will be used to check for vulnerabilities in code that calls dependencies.  2. The purpose of adding precompiled representations of queries to published query packs is to make the queries faster to execute during analysis. When a pack is published, precompiled representations of each query are added to files with a `.qlx` suffix next to the `.ql` source for each query. During analysis, {% data variables.product.prodname_codeql %} loads these files instead of compiling the QL source for the query, which results in faster query execution. This is particularly useful for large-scale analyses with many queries, as it can significantly reduce the time required to complete the analysis.", "8c6435d1-7bfb-478c-9980-38272af2913e": "To obtain the `ownerId` for the destination organization in the context of migrating data from a GitHub Enterprise Cloud (GHEC) instance to {% data variables.product.prodname_dotcom %} using the {% data variables.enterprise.management_console %} of {% data variables.location.product_location_enterprise %}, follow these steps:  1. Log in to the {% data variables.enterprise.management_console %} of your {% data variables.location.product_location_enterprise %}. 2. Click on the organization for which you want to obtain the `ownerId`. 3. Click on the Settings tab. 4. Scroll down to the Danger zone section. 5. Click on the Export organization content link. 6. In the Export organization content dialog box, you will see the `ownerId` displayed in the URL. The `ownerId` is the long string of numbers and letters that follows `https://{% data variables.location.product_location_enterprise %}/{% data variables.enterprise.management_console %}/organizations/{` and ends with `}/export`. 7. Copy the `ownerId` and use it in the migration process.  If you don't have access to the {% data variables.enterprise.management_console %} of your {% data variables.location.product_location_enterprise %}, you may need to contact your organization's administrator to obtain the `ownerId`.", "2d04b2fe-21b7-4c1a-9ed5-546c2239d558": "GitHub does not allow harassment, bullying, or abuse of any kind on its platform. This includes targeted personal attacks, following users around the platform in a way that causes intimidation, making sexual advances or comments, disingenuously participating in conversation to instigate conflict, and creating alternative accounts to evade moderation action. While not all unwelcome conduct is considered harassment, users are encouraged to be mindful in how they engage with others and the platform to avoid disrupting the experience of other users.", "c3f190c0-c363-4bc5-9d50-f5cb53d3e8ac": "Yes, based on the context information provided, the steps involved in deleting files using Git are:  1. Review the files you will delete. 2. Stage the changes using `git add` command. 3. Commit the changes using `git commit` command with a commit message. 4. Choose the email address to use for the commit using `git config --global user.email` command. 5. Choose the branch to delete the files from using `git branch -D` command. 6. Propose the file change using `git pull` command to ensure that there are no conflicts.  Note: The steps 4 and 5 are optional and can be skipped if you don't want to change the email address or branch.", "ac571eaa-fc11-4387-95b9-6746b0ed829c": "The file extension used to render wiki content on GitHub is \".md\" or \".markdown\", as GitHub uses the open-source Markup language for wiki content rendering.", "86548dca-a19b-4dc9-8d31-6c956463e180": "To update the remote fork from its parent using the `gh repo sync` subcommand, you can follow these steps:  1. Open your preferred terminal or command prompt. 2. Navigate to the directory where your fork is cloned. 3. Run the following command, replacing `owner` with the owner of the upstream repository and `cli-fork` with the name of your fork:     ```shell    gh repo sync owner/cli-fork -b BRANCH_NAME    ```     Replace `BRANCH_NAME` with the name of the branch you want to sync.  4. If the changes from the upstream repository cause conflicts, you can set the `--force` flag to overwrite the destination branch.     ```shell    gh repo sync owner/cli-fork -b BRANCH_NAME --force    ```     Use this command with caution, as it will overwrite any changes you've made to the destination branch.  5. Follow the prompts to complete the sync process.     If conflicts are detected, you'll be prompted to create a pull request to resolve the conflicts.     If the sync is successful, you'll see a message confirming that the branch has been updated.     ```shell    Branch 'BRANCH_NAME' successfully updated.    ```     Replace `BRANCH_NAME` with the name of the branch you synced.  6. Push the changes to the remote fork to complete the sync process.     ```shell    git push origin BRANCH_NAME    ```     Replace `BRANCH_NAME` with the name of the branch you synced.  7. Verify that the changes have been synced by checking the remote fork on {% data variables.product.product_name %}.     ![Screenshot of the main page of a fork with the \"Sync fork\" dropdown menu highlighted](https://docs.github.com/assets/22952317/15844188-f5f5f5f5f5f5f5f5f5f5f5f5f5f5f5f", "2e9fa59d-1e4c-4834-af94-4e394325dc66": "To obtain the ID of your app and generate a private key for it, you need to follow these steps:  1. Go to the settings page for your GitHub app. You can find this by navigating to your app's page in your GitHub organization or repository, clicking on the app's name, and then clicking on the \"Settings\" tab.  2. On the settings page, you should see the app's ID displayed. This will be a long string of numbers and letters. Copy this ID and save it somewhere safe.  3. To generate a private key for your app, you'll need to use a tool like OpenSSL. Here's an example command you can use on a Unix-based system:  ``` openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout app.key -out app.crt ```  This will generate a self-signed certificate and a private key in the `app.key` and `app.crt` files, respectively.  4. Copy the contents of the `app.key` file and save it somewhere safe. You'll need this key to authenticate your app with GitHub's API.  5. That's it! You now have the ID of your app and a private key that you can use to authenticate your app with GitHub's API using the `octokit` library.", "fa787e72-a106-46b6-957e-31c2aacb95a7": "The \"--expect-discarded-cache\" option makes decisions about which predicates to evaluate and what to write to the disk cache based on the assumption that the cache will be discarded after the queries have been executed. This means that the option influences the decision-making process for evaluating predicates and writing to the disk cache by prioritizing efficiency over disk space usage, as the cache will be discarded anyway. This can potentially lead to faster query execution times, as the system can make more aggressive decisions about which predicates to evaluate and what to write to the cache, without worrying about the long-term impact on disk space usage.", "bafe14e4-ef27-4fe0-95a3-bb07d23aeb28": "The \"Code scanning results / CodeQL\" check that appears next to the pull request merge button indicates that {% data variables.product.prodname_code_scanning %} (specifically, CodeQL) has analyzed the code changes in the pull request and has identified potential security vulnerabilities or other issues. This check ensures that developers review and address any alerts or issues before merging the pull request into the main branch. The severity of the issues is displayed, with high severity issues being marked as \"1 new alert including 1 high severity security vulnerability...\". The check can be configured to fail the pull request if certain severity levels or security vulnerabilities are present, as described in the context information.", "080c6778-39a8-405f-bc11-c813a80d88fb": "The recommended way to interact with the {% data variables.product.company_short %} REST API using Ruby is by using the Octokit.rb SDK, which is maintained by {% data variables.product.company_short %}. This SDK implements best practices and makes it easier to interact with the REST API via Ruby. It works with all modern browsers, Node.rb, and Deno. To use Octokit.rb, you need to install and import the `octokit` gem. The guide assumes that you are familiar with Ruby and the {% data variables.product.company_short %} REST API. If you want to use the {% data variables.product.company_short %} REST API for personal use, you can create a {% data variables.product.pat_generic %}. The guide provides recommendations for storing your authentication credentials securely, such as storing them as a secret in {% data variables.product.prodname_actions %} or in a {% data variables.product.prodname_codespaces %} secret. If these options are not possible, consider using another CLI service to store your credentials securely.", "f00d95bf-ac61-45b2-8ebb-da9e264b08d1": "The name of the section where I can choose the branch for which I want to set up prebuilds is \"Configuration\" in the \"Prebuild configuration\" section of the \"Codespaces\" settings page.", "4917cc90-a549-4687-827c-67391806862d": "Enabling {% data variables.product.prodname_secret_scanning %} for all repositories at the organization level will consume all your licenses, even if no one is using code scanning, which may result in exceeding the license limit and being unable to use {% data variables.product.prodname_GH_advanced_security %} on newly created repositories if the number of active developers is likely to increase in the coming months. Additionally, enabling {% data variables.product.prodname_secret_scanning %} on a large organization may result in a high number of secrets found, which could raise alarms and require a response plan.", "235473ab-a304-4a2e-9fb2-aa757482826c": "The purpose of the \"get_diff_files\" step in the provided context information is to create a file containing the list of files changed in the pull request, using the output from a previous step. This step runs a shell command that uses the output from the \"get_diff_files\" step to create a file called \"get_diff_files.txt\" that contains the list of changed files. This step is likely used in subsequent steps to perform actions based on the list of changed files, such as running tests or generating documentation.", "61f4b866-3981-4c0c-9a56-41ecb7cee18c": "In a CE project, \"AUTOTITLE\" can be utilized to enhance existing issues by automatically generating a title for the issue based on its content. This feature can save time and effort for the project team, as they no longer have to manually create a title for each issue.  For example, in a software development project, \"AUTOTITLE\" can be used to generate a title for a bug report based on the error message. This can help the development team quickly understand the nature of the issue and prioritize it accordingly.  In a construction project, \"AUTOTITLE\" can be used to generate a title for a task based on its location and type. This can help the project team easily identify the task and ensure that it is completed in a timely manner.  To compare and contrast the use of \"AUTOTITLE\" in two different CE projects, let's consider a software development project and a marketing campaign project.  In the software development project, \"AUTOTITLE\" was highly effective in generating accurate and descriptive titles for the issues. This helped the development team quickly identify and prioritize the issues, resulting in a faster resolution time.  In the marketing campaign project, \"AUTOTITLE\" was less effective due to the variability in the nature of the tasks. Some tasks were more straightforward, such as \"Create Facebook Ad,\" while others were more complex, such as \"Develop Content Strategy.\" As a result, \"AUTOTITLE\" was not able to accurately generate a title for all tasks, and the project team still had to manually create titles for some tasks.  Overall, \"AUTOTITLE\" is a useful feature in a CE project, but its effectiveness depends on the nature of the tasks and issues. In projects with more straightforward tasks, \"AUTOTITLE\" can be highly effective in generating accurate and descriptive titles. In projects with more complex tasks, \"AUTOTITLE\" may still be useful, but the project team may need to manually create titles for some tasks.", "11e802b6-a3b4-4156-b6e1-1fdef98ca2d2": "To authorize a GitHub App or SSH key for use with a SAML SSO organization, you will need to follow these steps:  1. Ensure that you have an active SAML session. This means that you have successfully logged in to your organization's SAML identity provider and have been redirected back to GitHub.  2. Go to your organization's settings and select \"Developer settings\" from the left-hand menu.  3. Select \"GitHub Apps\" or \"SSH and GPG keys\" depending on whether you want to authorize a GitHub App or an SSH key.  4. Click the \"New GitHub App\" or \"New key\" button to create a new app or key.  5. Follow the prompts to fill in the necessary information, such as the app's name, description, and the permissions it will have.  6. When prompted, select the SAML SSO organization you want to authorize the app or key for.  7. Complete the authorization process by following the on-screen instructions. This may involve granting the app or key access to specific repositories or teams within your organization.  8. Once the app or key has been authorized, you can use it to access protected content in your organization's repositories using SAML SSO.  Note that if you are using a GitHub App, you may need to configure it further to work with your organization's specific setup. For more information, see the GitHub App documentation.", "b9e19194-6baa-4ed7-9924-0ee95c18923f": "To configure Git Large File Storage to use a third-party server, you need to follow these steps:  1. First, you need to ensure that Git LFS is installed and configured on your local machine. You can check this by running the command `git lfs env` in your terminal. This will display the version of Git LFS, Git, and the endpoint URL for the current remote.  2. Next, you need to add a new remote to your repository that points to the URL of the third-party server. You can do this by running the command `git remote add NEW-REMOTE https://NEW-REMOTE-HOSTNAME/path/to/repo` in your terminal. Replace `NEW-REMOTE-HOSTNAME` with the URL of the third-party server, and `path/to/repo` with the path to your repository on the third-party server.  3. After adding the new remote, you need to fetch all objects from the old remote using the command `git lfs fetch origin --all`. This will scan for all objects ever referenced and fetch them from the old remote.  4. Finally, you can push all objects to the new remote using the command `git lfs push NEW-REMOTE --all`. This will scan for all objects ever referenced and push them to the new remote.  During the configuration process, you will need to provide the URL of the third-party server as the endpoint URL for the new remote. This URL should be in the format `https://THIRD-PARTY-SERVER-HOSTNAME/path/to/repo/info/lfs` and should be provided with authentication credentials, if required.  Note that the context information provided does not specify the exact URL format for the third-party server endpoint URL, so you may need to consult the documentation for the specific third-party server you are using for more information.", "cac2c0ca-8eda-4609-bf02-fc4da1a2af59": "{% data variables.product.prodname_actions %} is a feature in GitHub that allows developers to automate tasks in their workflows. It can be used to view workflows for a repository using the REST API. Workflows are a series of steps that are executed automatically when a specific event occurs, such as a push to a branch or the creation of a pull request. By using the REST API to view workflows, developers can monitor the status of their automation tasks and troubleshoot any issues that may arise. For more information on {% data variables.product.prodname_actions %} and workflows, you can refer to the official documentation provided by GitHub.", "b75ea55a-aa3c-4801-ab25-ad370b6d7a93": "Policies for {% data variables.product.prodname_copilot %} can be enforced at both the enterprise and organization levels. Enterprise owners can choose to grant or revoke access to {% data variables.product.prodname_copilot %} for all, none, or a selection of organizations within the enterprise. If a restrictive policy is chosen at the enterprise level, organization owners will not be able to select a more permissive policy at the organization level. In cases where no policy is selected at the enterprise level, and multiple organizations within the enterprise have chosen different policies, the most restrictive policy will always take precedence. This means that if an organization owner chooses a more permissive policy at the organization level, but a restrictive policy is enforced at the enterprise level, the restrictive policy will override the organization's policy. It's important for enterprise owners to carefully consider the policies they choose, as they can significantly impact the use of {% data variables.product.prodname_copilot %} within their organization.", "2a6040ed-0645-453e-9428-2d34269f10df": "To access the procedural content on managing a feature or setting in the software, you can follow these steps:  1. Open the software and log in to your account. 2. Navigate to the settings or preferences menu. 3. Look for the section related to the feature or setting you want to manage. 4. Click on the section to expand it and reveal the available options. 5. Follow the instructions provided in the procedural content to manage the feature or setting.  If you're having trouble finding the procedural content, you can refer to the software's help documentation or contact customer support for assistance.", "a171e9a6-7b8c-4678-9ace-d1ceb18a7b75": "To use your token to access repositories from the command line, you should select the \"repo\" scope. This will grant your token access to repositories in [PRODUCT NAME]. If you want your token to have access to other resources as well, you can select additional scopes as needed. However, if you don't select any scopes, your token will only be able to access public information.", "3cd6b03a-4489-4e9a-84ee-5831c5367e8f": "The purpose of using a predefined dev container configuration in {% data variables.product.prodname_codespaces %} is to quickly get started with a configuration that already has the appropriate container options, {% data variables.product.prodname_vscode %} settings, and {% data variables.product.prodname_vscode %} extensions that should be installed for particular project types. This can provide common setups and help users save time and effort in configuring their development environment. It's a great idea if you need some additional extensibility or if you're working on a project that uses specific languages or tools that are included in the predefined configuration.", "59ce40a7-d28d-408f-a1e8-3dbb04df9785": "To incrementally increase the number of progress messages printed using the CLI tool provided, you can use the `-v` or `--verbose` option. This option can be used multiple times to increase the verbosity level.  To incrementally decrease the number of progress messages printed, you can use the `-q` or `--quiet` option. This option can also be used multiple times to decrease the verbosity level.  If you want to explicitly set the verbosity level to a specific level, you can use the `--verbosity` option with one of the following values: `errors`, `warnings`, `progress`, `progress+`, `progress++`, or `progress+++`. This option overrides the `-v` and `-q` options.  Note that the `--verbosity` option is an advanced feature and should be used with caution, as it can generate a large number of messages, especially for complex operations.", "a7a389ec-b27d-4336-ab9a-7840f8f1516a": "To review the status of an approval for a workflow run, you can send a GET request to `/repos/OWNER/REPOSITORY_ID/actions/runs/RUN_ID/approvals`. This will return a list of approvals for the specified workflow run, including their status (either `APPROVED` or `PENDING`). You can then use this information to determine whether the deployment is approved and can be published.", "16311a81-8155-4969-88d3-6eca5724e133": "Without further context, it is unclear how \"AUTOTITLE\" is used in a specific scenario within the context of FPT or GHEC. Please provide more information or context to assist in answering this question.", "25b8eb42-66e6-46a8-8723-dd685d1f18ba": "Log forwarding is a feature that allows you to send log messages from your GitHub Enterprise Server or GitHub Enterprise Cloud to an external log management system for centralized monitoring and analysis. This is achieved by configuring your server to forward log messages to a remote syslog server using the syslog protocol. The communication between the server and the remote syslog server is encrypted using SSL/TLS (Secure Sockets Layer/Transport Layer Security) to ensure the confidentiality and integrity of the log messages. The remote syslog server can be any log collection system that supports syslog-style log streams, such as Logstash or Splunk. By enabling log forwarding, you can centralize your log management, simplify log analysis, and improve your overall log management efficiency.", "5e595566-f2c9-4541-afed-3f2b1137d5a6": "The {% data variables.product.prodname_cli %} can be used to automate the process of creating issues on a regular basis by creating a workflow file that utilizes the scheduling feature of GitHub Actions. This tutorial demonstrates how to create such a workflow to generate a weekly team sync issue. Customization options include specifying the issue title, assignees, labels, and body, as well as the option to close and unpin the previous week's issue. The workflow can be customized further by adjusting the cron schedule and the steps within the workflow. The tutorial provided in the context information serves as a starting point for creating a customized workflow that meets your specific needs.", "5ab9e7aa-bf12-443f-9614-ecb875ce0f6a": "To set custom environment variables for all codespaces created for a specific repository, you can use either of the following methods:  1. Edit the `devcontainer.json` configuration file:     a. Open the repository in your preferred code editor.    b. Navigate to the `.devcontainer` directory.    c. Open the `devcontainer.json` file.    d. Add the following JSON object to the file:       ```json      {        \"remoteEnv\": {          \"VARNAME\": \"value\"        }      }      ```     e. Save the changes and commit them to the repository.     This method is useful when you want to set environment variables for all codespaces created for the repository. However, it's essential to note that this method is suitable for non-sensitive values as the variables will be committed to the repository as plaintext.  2. Use secrets:     a. Go to your repository's settings page.    b. Click on the \"Secrets\" tab.    c. Click on the \"New repository secret\" button.    d. Enter the name of the secret (e.g., VARNAME) and the value of the variable.    e. Click on the \"Add secret\" button.     b. Open the repository in your preferred code editor.    c. Navigate to the `.devcontainer` directory.    d. Open the `devcontainer.json` file.    e. Add the following JSON object to the file:       ```json      {        \"remoteEnv\": {          \"VARNAME\": \"${{ secrets.VARNAME }}\"        }      }      ```     f. Save the changes and commit them to the repository.     This method is useful when you want to set environment variables for all codespaces created for the repository, and the variables contain sensitive information. The variables will be stored as secrets, and they won't be committed to the repository as plaintext.  The difference between setting a custom environment variable for a single codespace and setting it for all codespaces created for a specific repository is that the former is specific to a single codespace, while the latter applies to all codespaces created for the repository.", "f4cfde45-eee6-422b-955c-6c3dda0b4be3": "The context provided does not include information about the machine sizes for {% data variables.actions.hosted_runner %}s. Therefore, it is not possible to provide an answer to this question.", "5620d9de-3a4f-409c-a542-d5acb7c17d2d": "The dev container configuration file (`devcontainer.json`) is used to specify the dev container image for a repository. It can be found in the root directory of the repository. The `image` property in this file is used to specify the Docker image that will be used to create the development container for the codespace. If this property is not specified, the default dev container image is used. As an organization owner, you can also restrict which dev container images can be used for codespaces created within your organization by adding a policy. If the specified image does not match one of the allowed images, a message will be displayed when someone tries to create a codespace for the repository.", "5695bce1-66a1-4cdb-b896-a655d70ecc90": "To decrypt an encrypted file using a shell script in GitHub, you can follow these steps:  1. Create a new secret in your GitHub repository that contains the passphrase used to encrypt the file. 2. Copy the encrypted file to a path in your repository and commit it. 3. Create a shell script in your repository to decrypt the secret file. 4. Ensure the shell script is executable before checking it in to your repository. 5. In your GitHub Actions workflow, use a step to call the shell script and decrypt the secret. Reference the shell script using the `run` command relative to the root of your repository.  The shell script should contain the following commands:  ```shell #!/bin/sh  # Decrypt the file mkdir $HOME/secrets # --batch to prevent interactive command # --yes to assume \"yes\" for questions gpg --quiet --batch --yes --decrypt --passphrase=\"$LARGE_SECRET_PASSPHRASE\" \\ --output $HOME/secrets/my_secret.json my_secret.json.gpg ```  Replace `$LARGE_SECRET_PASSPHRASE` with the name of the secret containing the passphrase, and `my_secret.json.gpg` and `my_secret.json` with the names of the encrypted and decrypted files, respectively.  Note that you'll need to use the `actions/checkout` action to have a copy of your repository in the environment that your workflow runs in.", "6542875f-1c40-4540-a80f-2367ccf48383": "If LDAP Sync is not enabled, you must manually suspend the {% data variables.product.prodname_ghe_server %} account after you remove the LDAP credentials. This information is provided in the context, but it's important to note that if LDAP Sync is not enabled, the {% data variables.product.prodname_ghe_server %} account will not be automatically suspended after the next synchronization run. Therefore, it's necessary to manually suspend the account after removing the LDAP credentials.", "61569376-d9fb-46d2-b20b-43b4ba152388": "To configure your workflow's YAML to use the `{% data reusables.actions.action-checkout %}` action from the `actions` organization on GitHub, follow these steps:  1. Go to the repository where you want to use the `checkout` action. 2. Click on the repository name at the top of the page to go to the repository summary page. 3. Click on \"Settings\" in the left-hand menu. 4. Click on \"Actions\" in the left-hand menu. 5. Click on \"New workflow\" to create a new workflow or select an existing workflow to edit. 6. Add the following line to the YAML file:     ```yaml    name: My workflow    on:      push:        branches:          - main    jobs:      my-job:        runs-on: ubuntu-latest        steps:          - name: Checkout repository            uses: {% data reusables.actions.action-checkout %}            with:              repository: actions/checkout              ref: v2    ```     In this example, the workflow is named \"My workflow\" and is triggered on the `main` branch when a push event occurs. The job \"my-job\" runs on an Ubuntu runner and uses the `checkout` action from the `actions` organization with the `v2` version.  7. Save the YAML file and commit the changes to your repository. 8. Push the changes to your repository to trigger the workflow.  Each time the workflow runs, the `checkout` action will be used to checkout the specified version of the `actions/checkout` repository from {% data variables.product.prodname_dotcom_the_website %}.", "b08ba22d-7cd7-4c5a-990e-434e1965d49c": "The purpose of the following command in the provided context information: \"gh issue comment {% raw %}${{ env.NEW_REPORT_URL }}{% endraw %} --body \"\u2b05\ufe0f Previous report\"\" is to add a comment to the new issue with the URL specified in the environment variable NEW_REPORT_URL, and the body \"\u2b05\ufe0f Previous report\". This comment will provide a link to the previous report.", "54e84998-3a7f-4533-b3fa-38e9e12e2fba": "To run jobs on macOS {% data variables.actions.hosted_runner %}s provided by {% data variables.product.company_short %}, you should use the labels provided in the \"Available macOS {% data variables.actions.hosted_runner %}s\" table in the context information. The labels you should use in your workflow YAML files are \"macos-latest-large\", \"macos-12-large\", \"macos-13-large\" for the Large runner size, and \"macos-latest-xlarge\", \"macos-13-xlarge\" for the XLarge runner size. Note that the \"-latest\" label uses the macOS 12 runner image for the Large runner size, and the macOS 13 runner image for the XLarge runner size.", "34e5d327-7f0d-4839-addc-544985d687c7": "The path to the repository containing the source branch of an external pull request in this context is `{% raw %}${{ github.event.pull_request.head.repo.full_name }}`{% endraw %}.", "a361dffb-bcbf-42e8-90bc-ae7e9769e12e": "To utilize the `paginate` method to request multiple pages of data while making requests using Octokit in Ruby, you can follow these steps:  1. Call the `paginate` method on the endpoint method that returns paginated results. 2. Pass the necessary parameters as a hash to the endpoint method. 3. The `paginate` method will automatically fetch the next page of results until it reaches the last page and then return all of the results as an array.  Here's an example of how to use the `paginate` method to fetch all issues from the `github/docs` repository:  ```ruby issues = Octokit::Client.new.rest.issues.list_for_repo(owner: \"github\", repo: \"docs\", per_page: 100)  # Fetch all issues using pagination all_issues = Octokit::Client.new.rest.issues.get_all_for_repo(owner: \"github\", repo: \"docs\")  # Both methods return an array of issues puts issues.size puts all_issues.size ```  In this example, the `list_for_repo` method is called with the necessary parameters to fetch 100 issues at a time. The `paginate` method is not explicitly called, as it is automatically called by the `list_for_repo` method.  To fetch the same data using the `request` method, you can make a `GET` request to the `/repos/{owner}/{repo}/issues` endpoint with the necessary parameters:  ```ruby response = Octokit::Client.new.request(\"GET /repos/{owner}/{repo}/issues\", owner: \"github\", repo: \"docs\", per_page: 100)  # Parse the response to get an array of issues issues = JSON.parse(response.body)[\"issues\"]  # Both methods return an array of issues puts issues.size ```  In this example, the `request` method is called with the necessary parameters to make a `GET` request to the `/repos/{owner}/{repo}/issues` endpoint. The response is parsed to get an array of issues.  The main", "07fc25e0-e418-4737-9fad-23d2cb4897af": "Precise code navigation and search-based code navigation are two strategies used by code editors to help developers navigate through code. Precise code navigation, also known as _point-based code navigation_, resolves definitions and references based on the set of classes, functions, and imported definitions that are visible at any given point in the code. This strategy ensures that the results are accurate and avoids inappropriate results such as comments. For example, if you type \"print\" in a Python file, precise code navigation will show you the definition of the built-in function \"print\" instead of other functions or variables with the same name.  On the other hand, search-based code navigation, also known as _global code navigation_, searches all definitions and references across a repository to find entities with a given name. This strategy can be useful when you're not sure where a particular function or variable is defined, but it may return too many results, making it harder to find the right one.  Cross-repo code navigation is a feature in {% data variables.product.prodname_dotcom %} that allows developers to jump to the definition of functions or variables defined in dependencies imported by their project, as long as the dependency is a repository hosted by {% data variables.product.prodname_dotcom %}. This feature is only available for languages that are supported by precise code navigation and the dependency graph. For example, if you're working on a Python project that imports a function from a library hosted on {% data variables.product.prodname_dotcom %}, you can use cross-repo code navigation to jump directly to the definition of that function, even if it's defined in a different repository.  However, cross-repo code navigation has some limitations at this time. It only supports finding definitions, not find-all-references requests. This means that you can't use it to find all the places where a particular function or variable is used in your project and its dependencies. Additionally, cross-repo code navigation is only available for repositories with fewer than 100,000 files. If your project or its dependencies exceed this limit, you may need to use search-based code navigation instead.", "001d545d-d1c6-42a5-85c3-33721afe32b2": "To migrate your Bitbucket Pipelines to GitHub Actions using the {% data variables.product.prodname_actions_importer %} tool, follow these steps:  1. Ensure that you meet the prerequisites for using {% data variables.product.prodname_actions_importer %}.  2. Install the {% data variables.product.prodname_actions_importer %} CLI extension.  3. Run the following command to migrate your Bitbucket Pipelines to GitHub Actions:     ```    bitbucket-pipelines-to-github-actions <bitbucket-repo-url> <github-repo-url>    ```     Replace `<bitbucket-repo-url>` and `<github-repo-url>` with the URLs of your Bitbucket and GitHub repositories, respectively.  4. Review the generated GitHub Actions workflow file and make any necessary adjustments.  5. Commit and push the changes to your GitHub repository.  Some limitations of using {% data variables.product.prodname_actions_importer %} to migrate from Bitbucket Pipelines to GitHub Actions include:  - Private AWS ECR images are not supported. - The Bitbucket Pipelines option `size` is not supported. - Metrics detailing the queue time of jobs are not supported by the `forecast` command. - Bitbucket after-scripts are supported using {% data variables.product.prodname_actions %} `always()` in combination with checking the `steps..conclusion` of the previous step.  Manual tasks involved in this process include migrating secured repository, workspace, and deployment variables, as well as SSH keys. These tasks must be done manually and are not automated by {% data variables.product.prodname_actions_importer %}.", "8ff261fb-2c4a-4173-af0b-452ce03f4331": "Tags can be created, viewed, and deleted in {% data variables.product.prodname_desktop %} as follows:  Creating a tag: 1. Open {% data variables.product.prodname_desktop %} and navigate to the repository containing the commit you want to tag. 2. Click the \"History\" tab to view the repository's commit history. 3. Locate the commit you want to tag and click on it to view its details. 4. Click the \"Create Tag\" button in the top right corner of the commit details window. 5. Enter a name for the tag in the \"Tag name\" field. 6. (Optional) Enter a message for the tag in the \"Tag message\" field. 7. Click the \"Create\" button to create the tag.  Viewing tags: 1. Open {% data variables.product.prodname_desktop %} and navigate to the repository containing the tag you want to view. 2. Click the \"History\" tab to view the repository's commit history. 3. Locate the commit associated with the tag you want to view and click on it to view its details. 4. The tags associated with the commit will be displayed in the commit details window.  Deleting tags: 1. Open {% data variables.product.prodname_desktop %} and navigate to the repository containing the tag you want to delete. 2. Click the \"History\" tab to view the repository's commit history. 3. Locate the commit associated with the tag you want to delete and click on it to view its details. 4. Click the \"Delete Tag\" button in the top right corner of the commit details window. 5. Confirm the deletion by clicking the \"Delete\" button in the confirmation dialog.  Note: You can only delete tags associated with commits that have not yet been pushed.", "26e4aee7-8a37-49d9-b15b-3e4581844c42": "According to the provided context information, the limitations and restrictions on using GitHub Actions include:  1. Compliance with the Agreement, GitHub Acceptable Use Policies, and GitHub Actions service limitations set forth in the Actions documentation. 2. Avoiding the use of Actions for cryptomining, unauthorized access, commercial purposes, or activities that place a disproportionate burden on GitHub servers. 3. Restricting the use of GitHub-hosted runners to activities related to the production, testing, deployment, or publication of the software project associated with the repository where GitHub Actions are used. 4. Monitoring of Actions usage to prevent violations and abuse. 5. Consequences for misuse of GitHub Actions, such as termination of jobs, restrictions in Actions usage, disabling of repositories, or suspension or termination of GitHub accounts.  Therefore, it is essential to adhere to these limitations and restrictions to avoid any negative consequences and ensure the proper use of GitHub Actions.", "9c32d8d6-a8ad-4150-bffa-d7c1730eafb3": "The \"queue time\" metrics in Travis CI refer to the amount of time a job spends waiting for a runner to become available to execute it. This metric is significant because it can indicate bottlenecks in the pipeline, where there are more jobs waiting to run than there are available runners.  To determine the number of runners required for a pipeline based on the \"queue time\" metrics, you can analyze the historical data to identify the peak number of concurrent jobs and the corresponding queue time. This will help you determine the minimum number of runners required to ensure that the queue time is within an acceptable range.  For example, if the historical data shows that the pipeline has a peak of 50 concurrent jobs with an average queue time of 10 minutes, you may want to configure at least 50 runners to ensure that the queue time is minimized. However, you should also consider the cost of running these additional runners and the hardware requirements for the pipeline.  In summary, the \"queue time\" metrics can be used to optimize the pipeline by ensuring that there are enough runners to handle the peak number of concurrent jobs, while minimizing the queue time and associated costs.", "ae9bf38f-8c23-4402-84ed-a51e9c7bbaa8": "Autofix in {% data variables.product.prodname_codeql %} {% data variables.product.prodname_code_scanning %} helps developers fix security alerts in pull requests by providing targeted recommendations generated automatically by large language models using data from the codebase, the pull request, and from {% data variables.product.prodname_codeql %} analysis. These potential fixes are relevant to the existing source code and translate the description and location of an alert into code changes that may fix the alert, lowering the barrier of entry for developers who may have little training in code security. Instead of starting with a search for information about the vulnerability, the developer starts with a code suggestion that demonstrates a potential solution for their codebase, making it easier for them to evaluate and implement the fix.", "ef9e727e-2c40-4e87-b772-1e41d7df1762": "To create a branch in a {% data variables.product.product_name %} repository, you need to follow these steps:  1. Go to your repository in {% data variables.product.product_name %}. 2. Click the \"Code\" tab to view the code for your repository. 3. Click the dropdown menu labeled \"main\" (which is the default branch name) located above the list of files. 4. Select \"New branch\" from the dropdown menu. 5. Type a name for your new branch in the \"Create a new branch\" dialog box. 6. Click \"Create branch\" to create your new branch.  The default branch name in {% data variables.product.product_name %} is \"main\". However, you can change the default branch name for your repository by going to the repository settings and selecting \"Branches\" from the left-hand menu. From there, you can rename the default branch and set up branch protection rules.", "76325614-24dc-424d-9ad8-d5bfc558e176": "To ensure that your repositories are properly archived using the {% data variables.product.prodname_archive %} feature, you should follow these steps:  1. Go to the repository you want to archive and click on the \"Settings\" tab. 2. Scroll down to the \"Danger zone\" section and click on \"Archive this repository\". 3. Confirm that you want to archive the repository by clicking on \"Archive repository\". 4. Once the repository is archived, you can access it by clicking on the \"Archived\" tab in the repository list.  The benefits of archiving a repository using {% data variables.product.prodname_archive %} include:  1. Freeing up space: Archiving a repository frees up space on your account, as archived repositories don't count towards your storage limit. 2. Preserving history: Archived repositories are still accessible and can be viewed and downloaded, so you can still access the code and history of the repository. 3. Organizing your account: Archiving a repository allows you to keep your account organized and focused on the repositories you're actively working on. 4. Preventing accidental deletion: Archiving a repository prevents it from being accidentally deleted, as archived repositories can't be deleted.  Overall, archiving a repository using {% data variables.product.prodname_archive %} is a simple and effective way to manage your GitHub account and ensure that your repositories are properly organized and preserved.", "39a68b66-ef45-41d0-a2e1-4bfc85038c18": "To view insights for rulesets in your organization's repository settings, follow these steps:  1. Access your organization's settings by clicking on your profile icon in the top right corner of GitHub and selecting \"Organizations\" from the dropdown menu. Then, select the organization for which you want to view ruleset insights. 2. In the left sidebar, click on \"Settings\" and then click on \"Code, planning, and automation\" in the left sidebar. 3. Click on \"Repository rule insights\" in the left sidebar. 4. Use the dropdown menus at the top of the page to filter the actions by ruleset, repository, actor, and time period. 5. You can now view insights for rulesets in your organization's repositories.  Note: These steps assume that you have already created and enabled rulesets in your organization's repositories. If you haven't done this yet, you may need to follow the instructions in the context information provided to you.", "479169b5-9a97-469d-b44f-fe53025211a2": "Auto-merge ensures that merge requirements are met for pull requests in a repository by automatically merging a pull request into the main branch once all merge requirements are satisfied. These merge requirements can be configured by the person who created the pull request or by the repository administrator. Some common merge requirements include passing all automated tests, obtaining required approvals, and meeting certain code quality standards. By enforcing these requirements, auto-merge helps to ensure that changes are thoroughly reviewed and tested before they are merged into the main branch, reducing the risk of introducing bugs or breaking changes.", "79e795d5-c876-421f-be4c-a3a5200bc2b5": "The purpose of the text \"f your workflow runs to see this workflow run periodically\" is to inform the user that this specific line of code is included in a workflow that is set to run automatically at regular intervals. This means that if the workflow is triggered, it will execute this line of code as part of its routine. The syntax \"f\" is a common shorthand for \"if\" in programming languages, but in this context, it is being used as a conditional statement to indicate that the following line of code should only be executed if the workflow is running periodically. This can be useful for tasks that need to be performed regularly, such as generating reports or sending notifications, without the need for manual intervention.", "c5604deb-a0a7-4738-b4b8-479c059c56b6": "To retrieve the octocat as ASCII art using cURL, you can use the following request:  ```shell curl --request GET \\ --url \"https://api.github.com/octocat\" \\ --header \"Accept: application/vnd.github+json\" \\ --header \"X-GitHub-Api-Version: 2022-11-28\" ```  This request uses the \"Get Octocat\" endpoint to retrieve the octocat as ASCII art. The `--request GET` option specifies that we want to make a GET request, `--url \"https://api.github.com/octocat\"` specifies the endpoint we want to access, and `--header \"Accept: application/vnd.github+json\"` and `--header \"X-GitHub-Api-Version: 2022-11-28\"` specify the required headers for the request.", "ba50cc36-6d76-4f5c-8a6e-a9ef826fb85e": "Yes, here's an example of how to use the `first` and `last` arguments to retrieve a specific number of discussions or comments:  To retrieve the first 10 discussions for a specific repository, you can use the following query:  ```graphql query {   repository(owner: \"octocat\", name: \"hello-world\") {     discussions(first: 10) {       nodes {         ...       }       pageInfo {         ...       }     }   } } ```  Similarly, to retrieve the last 10 comments for a specific discussion, you can use the following query:  ```graphql query {   discussion(number: 123) {     comments(last: 10) {       nodes {         ...       }       pageInfo {         ...       }     }   } } ```  In both examples, the `first` and `last` arguments are used to specify the number of elements to return. The `nodes` field returns a list of the requested elements, and the `pageInfo` field provides additional information about the pagination, such as the total number of elements and whether there are more elements available.  Note that the `first` and `last` arguments are optional, and you can omit them to retrieve all elements without pagination. However, using pagination can improve the performance of your queries, especially when dealing with large datasets.", "c92c7e2a-1b64-4cfb-8684-f6233864395e": "To change which organizations in an enterprise can access a runner group, follow these steps:  1. Go to your enterprise's Actions settings. 2. Click Runner groups in the left-hand menu. 3. Find the runner group you want to change access for and click the three dots to the right of its name. 4. Click Manage access. 5. In the Manage access dialog, select the organizations you want to grant access to. 6. Click Save.  Alternatively, you can also change access for multiple runner groups at once by selecting them and clicking the three dots to the right of their names, then clicking Manage access.  Note: This feature is only available for GitHub Enterprise Cloud (GHEC) and GitHub Enterprise Server (GHES) versions 3.0 and later.    Changing which repositories can access a runner group  To change which repositories in an organization can access a runner group, follow these steps:  1. Go to your organization's Actions settings. 2. Click Runner groups in the left-hand menu. 3. Find the runner group you want to change access for and click the three dots to the right of its name. 4. Click Manage access. 5. In the Manage access dialog, select the repositories you want to grant access to. 6. Click Save.  Alternatively, you can also change access for multiple runner groups at once by selecting them and clicking the three dots to the right of their names, then clicking Manage access.  Note: This feature is only available for GitHub Enterprise Cloud (GHEC) and GitHub Enterprise Server (GHES) versions 3.0 and later.    Changing which workflows can access a runner group  To change which workflows in an organization can access a runner group, follow these steps:  1. Go to your organization's Actions settings. 2. Click Runner groups in the left-hand menu. 3. Find the runner group you want to change access for and click the three dots to the right of its name. 4. Click Manage access. 5. In the Manage access dialog, select the workflows you want to", "4001e9c7-ea09-4049-8228-53d9418f6aed": "In GitHub Discussions and Issues, the \"Subscribe\" and \"unsubscribe\" commands allow users to opt in or out of notifications for additions or changes to the discussion or issue, respectively. When a user subscribes to a discussion or issue, they will receive email notifications and in-app notifications for any new comments or updates. If they unsubscribe, they will no longer receive these notifications. These commands can be invoked using the command palette, which is accessed by pressing Ctrl + Shift + P (Windows, Linux) or Command + Shift + P (Mac) in GitHub Desktop or by clicking the gear icon in the top right corner of the discussion or issue and selecting \"Command Palette\" in the web interface. In the command palette, users can search for \"Subscribe\" or \"unsubscribe\" and select the appropriate command for their desired action.", "588cf0ee-78cb-4809-a127-9933bdbcae1c": "GitHub apps can be granted three types of permissions: read-only, read and write, or no access. These permissions can be modified in the app settings by navigating to the app's settings and selecting the desired permission level from the dropdown menu under the sections \"Repository permissions,\" \"Organization permissions,\" and \"Account permissions.\" Adding a note to users when changing app permissions is optional, but it can be used to explain to users why the permissions for the app are being changed. This can help to prevent confusion or unexpected behavior when the new permissions take effect.", "812d4f15-834b-4f65-a026-6d35e7cb42ea": "Based on the context provided, a \"Subscription License\" refers to the specific number of licenses granted to the customer for the use of the software during the \"Subscription Term\". The \"Subscription Term\" refers to the duration for which the customer is licensed to use the software, as stated in the Order Form. In other words, a \"Subscription License\" is the actual license granted to the customer, while a \"Subscription Term\" is the timeframe during which the customer is authorized to use the software under that license.", "cf00cf1f-04fc-4c07-add3-4b5172239022": "The \"--max-resolve-depth\" option in the \"codeql dataset check\" command specifies the maximum depth to which IDs should be resolved to explain inconsistencies. This option affects the computation of precise locations for inconsistencies by limiting the number of IDs that are traced back to their original locations in the source code. By default, the depth is set to 3, but this can be adjusted using this option. A higher depth may result in more precise and detailed location information, but it may also increase the computation time and memory usage. The \"--precise-locations\" option can be used in conjunction with this option to expend extra effort to compute precise locations for inconsistencies, which may further increase the computation time.", "66f4b769-0380-4531-a1cc-60534d03c7db": "To download, install, and configure Git on your computer to use it on the command line, follow these steps:  1. Go to the official Git website (https://git-scm.com/downloads) and download the version of Git that is appropriate for your operating system.  2. Once the download is complete, run the installation program and follow the on-screen instructions to install Git.  3. After the installation is complete, open your preferred command prompt or terminal window.  4. To configure Git, run the following commands:     ```    git config --global user.name \"Your Name\"    git config --global user.email \"your_email@example.com\"    ```     Replace \"Your Name\" and \"your_email@example.com\" with your actual name and email address.  5. To test that Git is properly configured, run the following command:     ```    git config --list    ```     This will display a list of your Git configuration settings.  6. You're now ready to use Git on the command line! For more information on Git commands and usage, refer to the official Git documentation (https://git-scm.com/docs).", "e0b6096b-2855-4e35-95b6-016641288e3c": "To ensure that multiple versions of Java are supported when using the runner for analyzing repositories, you need to follow these steps:     a. First, download the JDK versions that you need from the official Oracle website or from a third-party vendor.     b. Extract the downloaded JDK versions to separate directories.     c. Create a `toolchains.xml` file in the root directory of your repository.     d. Open the `toolchains.xml` file and add the following configuration:     ```xml    <toolchains>      <toolchain>        <type>jvm</type>        <provides>          <version>11</version>          <vendor>Oracle</vendor>        </provides>        <configuration>          <jvm>            <vendor>Oracle</vendor>            <version>11</version>            <home>path/to/jdk-11</home>          </jvm>        </configuration>      </toolchain>      <toolchain>        <type>jvm</type>        <provides>          <version>8</version>          <vendor>Oracle</vendor>        </provides>        <configuration>          <jvm>            <vendor>Oracle</vendor>            <version>8</version>            <home>path/to/jdk-8</home>          </jvm>        </configuration>      </toolchain>    </toolchains>    ```     In this example, we have added two toolchains for Java 11 and Java 8. You can add more toolchains for other JDK versions as needed.  2. The following executables are likely required for a range of Java projects and should be present in the PATH variable:     a. `java`     b. `javac`  3. Besides the required Java executables, you should also install the following build system and utilities for Java projects:     a. `ant` (Apache Ant)     b. `gradle` (", "31524f48-c22d-46ef-ac58-5ef61be99d66": "The purpose of running the command \"./kustomize edit set image gcr.io/PROJECT_ID/IMAGE:TAG=gcr.io/$PROJECT_ID/$IMAGE:$GITHUB_SHA\" in the \"Deploy\" step of the workflow is to update the image tag used in the Kubernetes deployment manifest. The command uses the Kustomize tool to edit the deployment manifest and replace the image tag with a new tag that is based on the current GitHub commit SHA. This ensures that the deployed image is always the latest version that corresponds to the current GitHub commit.", "6f4d45aa-f687-4bc9-8ac4-d9e99732381b": "To specify the strategy for running jobs in parallel using GitHub Actions, you can use the `parallel` keyword in the `jobs` section of your workflow file. This allows you to define multiple jobs that will run in parallel instead of sequentially. You can also specify the maximum number of concurrent jobs using the `max-jobs` option within the `parallel` keyword. Here's an example:  ```yaml name: My Workflow  jobs:   build:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Build         run: npm run build    test:     runs-on: ubuntu-latest     needs: build     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Test         run: npm run test    deploy:     runs-on: ubuntu-latest     needs: [build, test]     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Deploy         run: npm run deploy  parallel:   build-test:     jobs: [build, test]     strategy:       fail-fast: false       matrix:         parallelism: 3  ```  In this example, we've defined three jobs: `build`, `test`, and `deploy`. The `build` and `test` jobs are dependent on the `build` job, and the `deploy` job is dependent on both the `build` and `test` jobs.  We've also defined a parallel job called `build-test` that runs both the `build` and `test` jobs in parallel. We've specified a matrix of parallelism with a value of 3, which means that up to three instances of the `build` and `test` jobs will run concurrently.  By default, the `fail-fast` option is set to `true`, which means that if any job in the parallel group fails, the entire parallel group will fail. If you want to allow all jobs in the", "4dd45bc7-e754-4d44-9681-17560e52691b": "To address build failures before pushing changes to GitHub Pages, we recommend testing your site locally and reviewing the build logs for any errors. You can view Jekyll build error messages locally by running the `jekyll build` command in your site's repository directory. This will generate a preview of your site in the `_site` directory, and any errors will be displayed in the console output. By addressing any build failures locally, you can ensure that your changes are ready to be pushed to GitHub Pages without causing any issues.  If you're using GitHub Actions to build and deploy your site, you can also view build error messages in the workflow run logs. To find potential build errors, you can check the workflow run for your GitHub Pages site by reviewing your repository's workflow runs. This will allow you to identify any issues with your workflow configuration or your site's content, and address them before pushing your changes to GitHub Pages.  If you're not using GitHub Actions to build and deploy your site, you can still view build error messages by running the `jekyll build` command locally. This will generate a preview of your site in the `_site` directory, and any errors will be displayed in the console output. By addressing any build failures locally, you can ensure that your changes are ready to be pushed to GitHub Pages without causing any issues.  In summary, to address build failures before pushing changes to GitHub Pages, we recommend testing your site locally, reviewing the build logs for any errors, and addressing any issues before pushing your changes to GitHub Pages. You can view Jekyll build error messages locally by running the `jekyll build` command, and you can also view build error messages in the workflow run logs if you're using GitHub Actions to build and deploy your site. By taking these steps, you can ensure that your site is ready to be published to GitHub Pages without causing any issues.    If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Question: How can I view Jekyll build error messages in my pull request on GitHub Pages?  Answer:  If you're publishing from a branch, when", "d772b03a-187b-4849-be54-37b4bde38754": "The purpose of the \"yarn npm publish\" command in the context provided is to publish the package to the npmjs registry. This command is executed in the \"build\" job of the GitHub Actions workflow, which is triggered when a release is created with the \"published\" type. The command is run in a Ubuntu environment with Node.js version 20.x installed, and the NODE_AUTH_TOKEN environment variable is set to the NPM token stored as a secret in GitHub. This allows the package to be published to the npmjs registry using the specified scope \"@octocat\".", "4971a92c-98de-4eb5-a90c-ec52ec9d0acf": "By default, when an unauthenticated user attempts to access a private resource in an enterprise, {% data variables.product.company_short %} displays a 404 error to hide the existence of private resources. This behavior can be changed by enabling Single Sign-On (SSO) redirect for unauthenticated users in the enterprise settings. This will redirect unauthenticated users to the organization's SSO provider for authentication, instead of displaying a 404 error. This can prevent confusion for developers who may mistakenly access a private resource without proper authentication. However, this feature is currently in beta release phase and may not be available for all enterprises.", "88a7d4c0-f443-4998-952d-e4e79c2dec84": "> Alt text: \"Screenshot of the 'Add collaborator' button in GitHub with a green outline.\"    Based on the passage above, Can you provide an example of how to describe a highlighted area in an image's alt text, as outlined in the guidelines?    Answer: \"Alt text: 'Screenshot of the 'Save' button in Microsoft Excel with a yellow border and a tooltip that says \"Save Changes\".'\"", "6dbbe0a8-c5ff-4452-a335-26bebce6c29d": "To add a new remote repository to your fork using the terminal, follow these steps:  1. Change directories to the location of the fork you cloned.     - To go to your home directory, type just `cd` with no other text.    - To list the files and folders in your current directory, type `ls`.    - To go into one of your listed directories, type `cd your_listed_directory`.    - To go up one directory, type `cd ..`.  2. Type `git remote -v` and press **Enter**. You will see the current configured remote repository for your fork.     ```shell    $ git remote -v    > origin  https://{% data variables.command_line.codeblock %}/YOUR_USERNAME/YOUR_FORK.git (fetch)    > origin  https://{% data variables.command_line.codeblock %}/YOUR_USERNAME/YOUR_FORK.git (push)    ```  3. Type `git remote add -t fetch --track upstream REMOTE_URL` and replace `REMOTE_URL` with the URL of the repository you want to add as a remote.     ```shell    git remote add -t fetch --track upstream https://{% data variables.command_line.codeblock %}/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git    ```  4. To verify the new remote repository you have specified for your fork, type `git remote -v` again. You should see the URL for your fork as `origin`, and the URL for the upstream repository as `upstream`.     ```shell    $ git remote -v    > origin    https://{% data variables.command_line.codeblock %}/YOUR_USERNAME/YOUR_FORK.git (fetch)    > origin    https://{% data variables.command_line.codeblock %}/YOUR_USERNAME/YOUR_FORK.git (push)    > upstream  https://{% data variables.command_line.codeblock %}/ORIGINAL_OWNER", "c9612c92-6e07-4b13-b1cc-d62d40ed9c43": "If you notice that a specific service, such as `mps_frontend` or `actions_backend`, is at or near its CPU or memory limit in the \"Nomad Jobs\" section of the monitor dashboard in the management console, you should increase the resource allocation for that service. To do this, follow these steps:  1. Log in to the administrative shell using SSH. 2. Run the following command to see what resources are available for allocation:     ```shell    nomad node status -self    ```  3. Find the \"Allocated Resources\" section in the output. 4. Check how much CPU and memory is allocated to the total of all services and how much is available. 5. Increase the resource allocation for the specific service that is at or near its limit. You can do this by running the following command, replacing `<service_name>` with the name of the service that needs more resources:     ```shell    nomad run <job_definition_file> --group <service_group_name> --datacenter <datacenter_name> --region <region_name> --node-label node-role=<node_role> --node-label os=<os_name> --node-label cpu=<cpu_number> --node-label memory=<memory_size> --node-label disk=<disk_size> --node-label availability-zone=<availability_zone> --node-label affinity=<affinity_group> --node-label constraints=<constraints> --node-label metadata=<metadata> --node-label tags=<tags> --node-label attributes=<attributes> --node-label labels=<labels> --node-label client=<client_name> --node-label server=<server_name> --node-label ip-address=<ip_address> --node-label hostname=<hostname> --node-label mac-address=<mac_address> --node-label dns-name=<dns_name> --node-label docker=<docker_version> --node-label kubernetes=<kubernetes_version> --node-label kubernetes-namespace=<kubernetes_namespace> --node-label kubernetes-", "4d609993-0c6b-43a2-83eb-31dc31101e8b": "To search for files within a specific repository using Code Search, you can use the `repo:` qualifier followed by the full repository name, including the owner. For example, to search for files containing the term \"function\" in the repository \"octocat/Hello-World\", you can use the following syntax:  ```text repo:octocat/Hello-World function ```  To narrow down your Code Search results to a specific language, you can use the `language:` qualifier followed by the language name. For example, to search for Ruby files containing the term \"class\" in the repository \"octocat/Hello-World\", you can use the following syntax:  ```text repo:octocat/Hello-World language:ruby class ```  Some supported language names include \"ruby\", \"python\", \"javascript\", \"java\", \"csharp\", and \"cpp\". You can find a complete list of supported language names in the \"languages.yaml\" file in the github-linguist/linguist repository.  If your preferred language is not on the list, you can open a pull request to add it. To do this, follow these steps:  1. Fork the github-linguist/linguist repository. 2. Clone your fork to your local machine. 3. Create a new branch for your changes. 4. Edit the \"languages.yaml\" file to add your preferred language name. 5. Commit your changes and push your branch to your fork. 6. Create a pull request to merge your changes into the main repository. 7. Wait for the pull request to be reviewed and merged by the maintainers. 8. Once your pull request is merged, your preferred language will be added to the list of supported language names in Code Search.", "2f8d4fdf-91a7-4f85-bc9f-bf75133ab803": "The context information provided states that in a clustering configuration for {% data variables.product.prodname_ghe_server %}, the DNS record must point to a load balancer. Therefore, in a clustering configuration, a load balancer is required for the DNS record.", "74ad8f27-0259-4f50-a027-03ed905f2cf0": "To implement required status checks and commit verifications to enforce compliance standards in {% data variables.location.product_location %}, you can use the compliance features provided by {% data variables.product.prodname_GH_advanced_security %}. These features allow you to define custom rules and policies that must be met before a pull request can be merged or a commit can be pushed. You can also set up automated status checks that run on each pull request, such as code scanning, security vulnerability scanning, and code quality checks. By enforcing these compliance standards, you can ensure that your organization's code meets your organization's security and compliance requirements. For more information, see \"AUTOTITLE.\" and \"AUTOTITLE.\"", "b88f1d98-8c8a-41d9-957b-96868b252ac2": "{% data variables.product.prodname_copilot %} can be utilized to generate a summary for a pull request on {% data variables.product.prodname_dotcom_the_website %} by selecting the \"Summary\" command in the header of the text field where the pull request description is being added or edited. This will prompt {% data variables.product.prodname_copilot %} to scan through the pull request and provide an overview of the changes made in prose, as well as a bulleted list of changes with the files that they impact. This summary can be used to help reviewers understand the changes made in the pull request or to quickly understand the changes in a pull request being reviewed. It is recommended to start with a blank description when using {% data variables.product.prodname_copilot %} to generate a summary, as it does not take into account any existing content in the pull request description.", "ec07a217-bec3-4e98-92bc-2fc3fbc1e216": "During the configuration process for the {% data variables.product.prodname_actions_importer %} CLI command, we are prompted to enter the values of two access tokens: a GitHub personal access token (PAT) and a Travis CI API access token.  The GitHub PAT is used to authenticate the {% data variables.product.prodname_actions_importer %} CLI command with GitHub. It allows the CLI command to access the user's repositories and perform actions on their behalf. This token is required to create and manage GitHub Actions workflows, as well as to import and manage Travis CI configurations.  The Travis CI API access token is used to authenticate the {% data variables.product.prodname_actions_importer %} CLI command with the Travis CI API. It allows the CLI command to access the user's Travis CI configurations and perform actions on their behalf. This token is required to import and manage Travis CI configurations, as well as to create and manage GitHub Actions workflows that use Travis CI as a runner.  In summary, both access tokens are necessary to provide the {% data variables.product.prodname_actions_importer %} CLI command with the necessary authentication to import and manage GitHub Actions workflows and Travis CI configurations.", "567f1489-a282-434f-aaa3-74db790a3994": "To push changes with merge conflicts to a branch on GitHub using the command line, you need to resolve the merge conflict locally on your computer and then commit the changes. After committing the changes, you can push the changes to your branch on GitHub. Here are the steps:  1. Open your local clone of the repository in your preferred text editor or command line interface.  2. Locate the file(s) with merge conflicts. These files will have a \"CONFLICT\" message in the status output when you run `git status`.  3. Open the file(s) and review the changes made by both branches. Use your preferred text editor to manually merge the changes and resolve the conflict.  4. Save and close the file(s).  5. Stage the changes using `git add` and commit the changes using `git commit -m \"Resolved merge conflict\"`.  6. Push the changes to your branch on GitHub using `git push origin BRANCH-NAME`.  If you prefer to use GitHub Desktop, you can follow these steps:  1. Open GitHub Desktop and connect to your repository.  2. Locate the file(s) with merge conflicts. These files will have a red exclamation mark next to them.  3. Open the file(s) and review the changes made by both branches. Use your preferred text editor to manually merge the changes and resolve the conflict.  4. Save and close the file(s).  5. Stage the changes using the checkbox next to the file(s) and commit the changes using the \"Commit to\" dropdown menu.  6. Push the changes to your branch on GitHub using the \"Push\" button.  After pushing the changes, the merge conflict will be resolved and the changes will be reflected on GitHub.", "6393f6ea-c6c9-4b9b-856f-dadd03d5d803": "To access the URL of the repository using the provided context information, you can use the variable `CI_PROJECT_URL` which is set to `{% raw %}${{ github.server_url }}/${{ github.repository }}`{% endraw %}. This variable contains the URL of the repository on the GitHub server where the action is running.", "6f199c30-f720-45e9-bf5d-ec3e39d77ecf": "The URL for the project containing the target branch of an external pull request in this context can be obtained using the `CI_MERGE_REQUEST_TARGET_PROJECT_URL` variable, which is calculated as follows:  ``` CI_MERGE_REQUEST_TARGET_PROJECT_URL = ${{ github.server_url }}/${{ github.event.pull_request.base.repo.full_name }} ```  This variable combines the URL of the GitHub server (`github.server_url`) with the full name of the repository containing the target branch (`github.event.pull_request.base.repo.full_name`).  Therefore, the URL for the project containing the target branch of an external pull request in this context can be obtained by accessing the `CI_MERGE_REQUEST_TARGET_PROJECT_URL` variable.", "6da6c21e-5a0f-41ca-b4e1-537f618f88fe": "The maximum file size you can upload to a repository on {% data variables.product.product_name %} using the browser is {% data variables.large_files.max_github_browser_size %}. To upload larger files, up to {% data variables.large_files.max_github_size %} each, you can use the command line. For even larger files, you must use {% data variables.large_files.product_name_long %}.", "ea9229a4-cd03-484e-87d0-7fcc64931a81": "If you receive a \"404 Not Found\" response for an existing resource, as described in the context information, you should first check your authentication. This could include ensuring that your personal access token (PAT) has the necessary scopes, or that you are properly authenticated using OAuth. If your authentication is correct, you may want to double-check the URL or resource ID that you are requesting to ensure that it is correct. If you continue to receive a \"404 Not Found\" response for an existing resource, you may want to contact {% data variables.product.company_short %} support for further assistance.", "c98b77af-4589-4938-98f9-f6c1b8e8f140": "To create a Travis CI API access token, you need to follow these steps:  1. Log in to your Travis CI account. 2. Click on your profile picture in the top right corner of the page and select \"Account settings\" from the drop-down menu. 3. Click on the \"API keys\" tab. 4. Click on the \"Generate new token\" button. 5. Enter a name for the token (e.g., \"GitHub Actions Importer\") and select the \"Read and write\" permission level. 6. Click on the \"Generate token\" button. 7. Copy the generated token and save it in a safe location for later use.  During the configuration process for the {% data variables.product.prodname_actions_importer %} CLI command, you need to provide the following information:  - For \"Which CI providers are you configuring?\", select \"Travis CI\". - For \"{% data variables.product.pat_generic_caps %} for GitHub\", enter the value of the {% data variables.product.pat_v1 %} that you created earlier. - For \"Base url of the GitHub instance\", enter the URL for your {% data variables.product.product_name %} instance (e.g., \"https://github.com\"). - For \"{% data variables.product.pat_generic_caps %} for Travis CI\", enter the value for the Travis CI API access token that you created earlier. - For \"Base url of the Travis CI instance\", enter the URL of your Travis CI instance (e.g., \"https://travis-ci.com\"). - For \"Travis CI organization name\", enter the name of your Travis CI organization.  After providing this information, the {% data variables.product.prodname_actions_importer %} CLI command will create a configuration file that you can use to import your Travis CI workflows into GitHub Actions.", "97f5f04f-ee34-4775-aaf1-1fb9f7843a04": "To use the GitHub Actions Importer tool for migrating from Jenkins to GitHub Actions, you need to configure two sets of credentials: a GitHub personal access token (PAT) with the \"workflow\" scope and a Jenkins API token.  To create a GitHub PAT, follow these steps: 1. Go to your GitHub profile settings. 2. Click on \"Developer settings\" in the left-hand menu. 3. Click on \"Personal access tokens\" in the \"Settings\" section. 4. Click on the \"Generate new token\" button. 5. Enter a name for the token and select the \"workflow\" scope. 6. Click on the \"Generate token\" button. 7. Copy the generated token and save it in a safe location for later use.  To create a Jenkins API token, follow these steps: 1. Log in to your Jenkins account. 2. Click on your username in the top right corner of the page. 3. Click on \"Configure\" in the drop-down menu. 4. Click on the \"Security\" tab in the left-hand menu. 5. Scroll down to the \"API\" section. 6. Click on the \"Add API token\" button. 7. Enter a name for the token and click on the \"OK\" button. 8. Copy the generated token and save it in a safe location for later use.  After creating both tokens, you can configure them using the `gh actions-importer configure` command in your terminal. The command will prompt you to enter the credentials, and you should provide the Jenkins API token when prompted for the \"Jenkins API token\" and the GitHub PAT when prompted for the \"GitHub PAT\". Save these tokens in a secure location, as you will need them to migrate your Jenkins pipelines to GitHub Actions using the GitHub Actions Importer tool.", "eac413c6-cfda-40da-b0cc-e8fc2086bfdb": "If you're not already signed in to the GitHub Connect page, you'll need to sign in with your GitHub account credentials. Once you're signed in, to the right of \"Unified contributions\", click \"Enable\". After clicking \"Enable\", you may be redirected to the GitHub Enterprise Server site to check your current terms of service. If you're not already a user of GitHub Enterprise Server, you'll need to request access from your organization's administrator to receive further instructions.", "fb55b98f-9c0a-47a4-8e19-f3ad7d05ea54": "According to the context provided, you can store your GitHub token as a GitHub Codespaces secret and use it in your scripts running in a GitHub Codespaces environment. To do this, follow these steps:  1. Go to your GitHub repository settings. 2. Click on \"Secrets\" in the left-hand menu. 3. Click on \"New repository secret\" and enter a name for your secret (e.g., \"GITHUB_TOKEN\"). 4. Paste your GitHub token into the value field. 5. Click \"Add secret\" to save your secret.  In your GitHub Codespaces environment, you can access your secrets by running the following command in your terminal:  ``` echo $GITHUB_TOKEN ```  This will print your GitHub token to the console, which you can then use in your scripts.  Best practices for storing GitHub tokens securely include:  1. Use a unique token for each application or service that requires access to your GitHub account. 2. Store your tokens securely, either in a password manager or in a secure environment like GitHub Codespaces. 3. Avoid storing your tokens in plain text or in publicly accessible locations. 4. Rotate your tokens regularly to minimize the risk of unauthorized access. 5. Use two-factor authentication (2FA) to add an extra layer of security to your GitHub account. 6. Limit the scope of your tokens to only the resources and permissions that are necessary for your application or service. 7. Monitor your GitHub activity regularly to detect any suspicious or unauthorized access. 8. Use HTTPS and SSL/TLS encryption to protect your tokens during transmission. 9. Avoid sharing your tokens with others, even if they are trusted. 10. Use a strong password or passphrase for your GitHub account to prevent unauthorized access.  By following these best practices, you can help ensure the security and confidentiality of your GitHub tokens and protect your GitHub account from unauthorized access.", "86c4b7c4-ea7c-4b96-9fb9-896ce73d0ce5": "To contact GitHub if you have questions or concerns about a new subprocessor they are using, you can reach out to them via {% data variables.contact.contact_privacy %}. This information is provided in the context given.", "35a930ae-e535-4e1f-a94e-d42c7a431d29": "The purpose of configuring an integrated development environment (IDE) for an assignment is to be able to run and test the code within the IDE without having to clone the assignment repository to your computer. This allows for a more streamlined and efficient development process, as you don't have to constantly switch between the IDE and the command line to run and test your code. It also eliminates the need to manage multiple repositories on your local machine, as everything is contained within the IDE. Overall, configuring an IDE for an assignment can save time and improve productivity.", "13efa1e1-0b41-496a-b047-c41d97eec636": "Without further context, it is unclear how the software utilizes context information in version `v2.15.2` to enhance functionality. Please provide more information about the software and its capabilities to better understand how context information is utilized in this version.", "d525ae38-00b2-4e04-8166-98da3d23b45e": "Based on the context information provided, it is not clear what software is being referred to as \"it\". Without prior knowledge, it is unclear how the recommended feature during the beta differs from the current version of the software. More context is needed to answer this question.", "4ec5c17a-3524-43f4-b19b-c150bdaa2624": "The recommended action to submit file coverage information to the {% data variables.code-scanning.tool_status_page %} while running a code scanning analysis using the {% data variables.product.prodname_codeql_cli %} is to use the \"--sarif-add-baseline-file-info\" flag. This flag is marked as recommended and is explained in more detail in the context information provided. It is recommended to use this flag to submit file coverage information to the {% data variables.code-scanning.tool_status_page %} for more information, you can refer to the \"AUTOTITLE.\" section mentioned in the context information.", "9efa7b14-be3c-433e-ac25-963e5188a8a0": "The name of the current job can be obtained using the variable `CI_JOB_NAME`. This variable holds the name of the current job being executed in the GitHub Actions workflow.", "c1caec39-513a-482c-917c-4c4610d4f64a": "To download the {% data variables.product.prodname_ghe_server %} image for OpenStack KVM (QCOW2), follow these steps:  1. Go to {% data variables.product.prodname_dotcom %} On-premises page. 2. Select \"Select your hypervisor\" dropdown menu and click \"OpenStack KVM (QCOW2)\". 3. Click \"Download for OpenStack KVM (QCOW2)\".  Note: If you don't have access to {% data variables.product.prodname_dotcom %} On-premises page, you may need to contact your system administrator for assistance.", "80b951f4-a7ac-4042-b305-a66eba8c40a7": "If the changes from the upstream repository cause conflicts when syncing a fork branch using the GitHub CLI, you can either resolve the conflicts and create a pull request to merge the changes, or you can use the `--force` flag to overwrite the destination branch. This should be done with caution, as overwriting the destination branch will discard any local changes that have not been committed or pushed. It's recommended to create a pull request to resolve conflicts and ensure that the changes are properly reviewed and tested before merging them into the destination branch.", "d5767ddd-a435-4e62-9d1a-6e6fbfa1b418": "The nodes that you intend to use for high availability replication should be standalone {% data variables.product.prodname_ghe_server %} instances. Don't initialize the replica nodes as a second cluster. (Note provided in the context information)", "87bb6b81-5ee1-4315-923e-4f44de2312ca": "To upload a support bundle directly using SSH, you need to have SSH access to {% data variables.location.product_location %} and ensure that outbound HTTPS connections over TCP port 443 are allowed from {% data variables.location.product_location %} to _enterprise-bundles.github.com_ and _esbtoolsproduction.blob.core.windows.net_.  Here are the steps to follow:  1. Connect to your {% data variables.location.product_location %} server via SSH. 2. Run the following command:  ```shell ssh -p122 admin@HOSTNAME -- 'ghe-support-bundle -u' ```  Replace `HOSTNAME` with the hostname or IP address of your {% data variables.location.product_location %} server.  This command will upload the support bundle to our server using the `ghe-support-bundle` tool. The `-u` flag is used to upload the bundle.  Note that this method is only recommended if you have SSH access to your {% data variables.location.product_location %} server and can ensure that outbound HTTPS connections are allowed. If you don't have SSH access, you can still upload the bundle using the {% data variables.enterprise.management_console %}.", "56d88f1d-5773-436b-a202-09622a50acaa": "To utilize the {% data variables.product.prodname_cli %} to create scheduled issues using {% data variables.product.prodname_marketplace %} actions, you can follow these steps:  1. First, make sure you have the {% data variables.product.prodname_cli %} installed on your local machine. You can download it from the official GitHub CLI documentation.  2. Next, search for {% data variables.product.prodname_marketplace %} actions related to scheduled issues. One such action is the \"Create scheduled issue\" action by GitHub. You can find it by going to the {% data variables.product.prodname_marketplace %} and searching for \"Create scheduled issue\".  3. Once you have found the action, click on it to view its documentation. Here, you will find information on how to use the action, as well as any required inputs and outputs.  4. To use the action, you will need to create a workflow file in your repository. This file will contain the steps for your workflow, including the use of the \"Create scheduled issue\" action.  5. In your workflow file, add the following steps:     ```yaml    name: Schedule Issues    on:      schedule:        - cron: \"0 9 * * *\"    jobs:      schedule-issues:        runs-on: ubuntu-latest        steps:          - name: Checkout            uses: actions/checkout@v2           - name: Create scheduled issue            uses: github/create-scheduled-issue@v1            with:              title: \"Weekly status update\"              body: \"Please provide a summary of the work completed this week, as well as any upcoming tasks.\"              labels: \"status-update\"              assignees: \"your-github-username\"              due-date: \"2022-01-07\"    ```     In this example, we are creating a workflow that runs every weekday at 9:00 AM UTC. The workflow creates a scheduled issue with a specific title, body, labels, assignee, and due date.  6. Save your work", "82551464-f169-4d01-a9d0-a4eeb2a470d6": "Only organization members with the _owner_ or _billing manager_ role can access or change billing settings for your organization on {% data variables.location.product_location %}.", "7192fd9a-9bd1-475e-8294-9fd0f9b21fc3": "Service containers are Docker containers that provide a simple and portable way to host services that are required for testing or operating an application. When running jobs in a container, {% data variables.product.prodname_dotcom %} connects service containers to the job using Docker's user-defined bridge networks, simplifying network access. The hostname of the service container is automatically mapped to the label name configured in the workflow, and all containers that are part of the same Docker network expose all ports to each other, with no ports exposed outside of the Docker network. This simplifies communication between the job and its service containers, as they can be accessed using their label names without the need for configuring ports.", "8cd88155-a300-45ba-ac6c-a95225cbbd1f": "To install the GitHub Actions Importer CLI extension, follow these steps:  1. Open your terminal or command prompt. 2. Ensure that you have the latest version of Node.js installed on your machine. You can download Node.js from the official website (https://nodejs.org/en/download/). 3. In your terminal or command prompt, run the following command to install the GitHub Actions Importer CLI extension:     ```    npm install -g github-actions-importer    ```     This command will download and install the GitHub Actions Importer CLI extension globally on your machine.     Note: If you encounter any errors during the installation process, make sure that you have the necessary permissions to install packages globally. You may need to run the command with administrator privileges on Windows or use `sudo` on Linux.     After the installation is complete, you can verify that the extension is installed by running the following command:     ```    ghai --version    ```     This command will display the version of the GitHub Actions Importer CLI extension that you have installed.", "6bf545e9-f148-465e-a9c6-7760bb07fc89": "To submit and push changes to a remote repository on GitHub using the command line interface, follow these steps:  1. Make changes to your local repository using your preferred text editor. 2. Stage the changes using `git add .` 3. Commit the changes using `git commit -m \"a short description of the change\"` 4. Push the changes to the remote repository using `git push`  The purpose of making a pull request in GitHub is to propose changes to the main project from a fork. After making changes to a fork of someone else's project, follow these steps to make a pull request:  1. Go to the repository on GitHub where the main project lives. 2. Click the \"New pull request\" button in the top right corner of the repository page. 3. Select the branch with your changes from the fork as the source branch. 4. Select the branch you want to merge your changes into as the destination branch. 5. Write a clear and concise title and description for the pull request. 6. Click the \"Create pull request\" button to submit the pull request. 7. Wait for the maintainer of the main project to review and merge your changes.  Note: The exact steps may vary slightly depending on the specific version of GitHub you are using.", "17b18d29-6fa4-499c-8017-106dd5d6cc78": "To unpin a discussion from a category in GitHub, follow these steps:  1. Go to the repository or organization where the discussion is located. 2. Click on the Discussions tab. 3. Find the discussion you want to unpin and click on it to open it. 4. In the right sidebar, click on the pin icon (looks like a pushpin) to unpin the discussion from the category. 5. Confirm that you want to unpin the discussion by clicking on \"Unpin from this category.\"  Note: This feature is only available for discussions created in a category. If the discussion was not created in a category, it cannot be pinned or unpinned.", "f507c645-1742-461a-8924-13f1cb2a8fba": "If the `--common-caches` option is not set explicitly, the default location for cached data on disk that persists between several runs of the CLI is a directory named `.codeql` in the user's home directory. This directory will be created if it doesn't already exist. (Available since `v2.15.2`.)", "f650f981-5afc-43e6-9724-71d63c887573": "An authenticator is a device or application that provides an additional layer of security during the login process by requiring a unique code or biometric verification in addition to a username and password. Passkeys are a new type of authentication method that replaces passwords with a single, unique key that is stored on a user's device and can be used to securely sign in to websites and services. Authenticators can be used to generate passkeys, as well as provide additional security measures beyond passkeys. Some authenticators allow passkeys to be used with nearby devices, while others create device-bound passkeys that cannot be backed up or moved to another authenticator. The context provided does not give enough information to determine which type of authenticator is being referred to, but it does suggest that passkeys can be used with authenticators to provide a more secure login experience.", "b1fe22b4-89b1-40b8-8bda-f1066870a247": "To view actions performed in your enterprise using the Audit log API, you can refer to the documentation provided in \"AUTOTITLE.\" This resource should provide detailed instructions on how to access and utilize the Audit log API to view audit log actions in your enterprise. If you're not familiar with the term \"AUTOTITLE,\" it's possible that the context provided doesn't include enough information to answer your question. In this case, you may need to consult additional resources or contact support for further assistance.", "dd9a92f3-5af9-4eee-a3b8-11ac7b959ed4": "To create a pre-receive hook script for {% data variables.product.prodname_ghe_server %}, follow these steps:  1. Create a new file named `pre-receive` in the `.git/hooks` directory of your repository.  2. Make the file executable with the following command:  ```shell chmod +x .git/hooks/pre-receive ```  3. Add the following content to the `pre-receive` file:  ```shell #!/bin/bash  # Check if the branch is protected if [[ \"$GIT_RECEIVE_REFNAME\" == \"refs/heads/protected-branch\" ]]; then   # Get the current branch head   current_head=$(git rev-parse --verify \"refs/heads/$GIT_RECEIVE_REFNAME\")    # Get the new branch head   new_head=$(git rev-parse --verify \"refs/heads/$GIT_RECEIVE_REFNAME\")    # Compare the current and new branch heads   if [[ \"$current_head\" != \"$new_head\" ]]; then     # Check if the new branch head is ahead of the current branch head     if [[ \"$(git rev-list --count \"$current_head\"... \"$new_head\")\" -gt 0 ]]; then       # Reject the push if the new branch head is ahead of the current branch head       echo \"Branch is ahead of its base branch. Please merge the changes before pushing.\"       exit 1     fi   fi fi  # Accept the push exit 0 ```  In this example, the script checks if the branch being pushed is a protected branch. If it is, the script gets the current and new branch heads, compares them, and checks if the new branch head is ahead of the current branch head. If it is, the script rejects the push with an error message. If it's not, the script accepts the push.  Consider the following input, output, exit status, and environment variables while creating your pre-receive hook script:  Input:  - The input for your pre-receive hook script is", "f75bd87d-67f0-4ca6-8cfe-9a4657e1a39f": "Without further context, it is unclear what \"AUTOTITLE\" refers to. If \"AUTOTITLE\" is referring to a specific document feature, it could potentially differ from other document features in terms of functionality based on the specific software or document type being used. More information is needed to accurately answer this question.", "2d51dcc9-9ba2-4604-8d1a-574db4e005d3": "According to the context information provided, there is a storage cost associated with each prebuild version that is retained. The exact amount of this cost is not specified, but it is mentioned that if prebuilds are generated in four regions and two versions are retained, the user will be charged for the storage of up to eight prebuilds. The factors that determine this cost are the number of prebuilds generated and the number of versions retained. The cost may also be affected by the size of the prebuilds and the duration for which they are retained. However, for more information on billing, the context suggests referring to a separate document titled \"AUTOTITLE.\" without providing the title. Without this additional information, it's not possible to provide a detailed answer on how the storage cost associated with prebuilds works and what factors determine this cost.", "49dd132b-308b-4ece-8a88-57d1e64f68ea": "The automatic dependency review in pull requests, provided by the {% data variables.dependency-review.action_name %}, helps prevent the introduction of vulnerable versions of dependencies into a repository by enforcing a dependency review on pull requests. This feature makes it easy for developers to see if a pull request will introduce a vulnerable version of a dependency to the repository, and when a vulnerability is detected, the {% data variables.dependency-review.action_name %} can block the pull request from merging. This ensures that only secure versions of dependencies are introduced into the repository, reducing the risk of vulnerabilities being introduced into the codebase. Additionally, the context provides information on how to enable this feature for different GitHub products, including GitHub Enterprise Server (GHES), GitHub Enterprise Cloud (GHEC), and GitHub.com (FPT). For more information on how to implement this feature, developers can refer to the \"AUTOTITLE\" documentation provided in the context.", "a5b1c57d-1240-422a-9c7e-2a9cc1134e4b": "The complexity of searching code can be overwhelming due to the vast amount of code available in repositories. To ensure efficient results, there are some restrictions in place. Firstly, only the default branch is indexed for code search, which reduces the amount of code that needs to be searched. Secondly, only files smaller than a certain size are searchable, which helps to prevent slow search times. Thirdly, only the first 500 KB of each file is searchable, which further reduces the amount of code that needs to be searched. Fourthly, only the most recently updated of the first 10,000 private and internal repositories are searchable, which helps to prevent overloading the search system with too many repositories. Fifthly, only repositories with fewer than 500,000 files are searchable, which helps to prevent slow search times due to the large number of files in some repositories. Sixthly, only repositories that have had activity or have been returned in search results in the last year are searchable, which helps to ensure that the search results are relevant and up-to-date. Finally, you must always include at least one search term when searching source code, and you can't use certain wildcard characters in your search query. These restrictions help to ensure that the search process is efficient and produces relevant results.", "dead185a-ebf7-47dc-81b6-7ea8cd615532": "Any commits signed by the corresponding private key will no longer show as verified.", "5d75527c-4326-4a34-8e10-c2f07cda3bd2": "If you have {% data variables.product.prodname_github_connect %} enabled for your organization or enterprise, you can access the security advisories in the GitHub Advisory Database by navigating to `https://HOSTNAME/advisories`, where `HOSTNAME` is the domain name of your organization or enterprise. If you don't have {% data variables.product.prodname_github_connect %} enabled, you can still view the advisories on the GitHub Advisory Database repository on GitHub.com. Alternatively, you can search for security advisories using the GitHub search function by going to `https://github.com/search?q=repo:github/advisory-database+is:issue+label:security-advisory` and replacing `github` with the organization or enterprise that published the advisory.", "6deb5edb-b030-4424-87a9-0c68639c7a15": "{% data variables.product.prodname_actions_importer %} provides several optional parameters to customize the migration process. These parameters include:  1. `--allowed-actions`: This option allows you to specify a list of actions that are allowed in converted workflows. Wildcards are supported, and any other actions not included in the list will be disallowed.  2. `--allow-verified-actions`: This option allows all actions from verified creators.  3. `--allow-github-created-actions`: This option allows actions published from the `github` or `actions` organizations, such as `github/super-linter` and `actions/checkout`.  4. `--credentials-file`: This parameter specifies the path to a file containing credentials for different servers that {% data variables.product.prodname_actions_importer %} can authenticate to. This is useful when build scripts are stored in multiple {% data variables.product.prodname_ghe_server %} instances.  Using these optional parameters, you can limit which actions are allowed in converted workflows and provide credentials for authentication. These parameters can be used to customize the migration process to suit your specific needs.", "876b46e8-8640-4959-92f4-fb19c9e1fa9a": "What is the purpose of copying a {% data variables.projects.projects_v1_board %} on GitHub's {% data variables.product.prodname_project_v1 %} feature?  Answer: Copying a {% data variables.projects.projects_v1_board %} on GitHub's {% data variables.product.prodname_project_v1 %} feature allows you to quickly create a new project board by reusing the title, description, and automation configuration of an existing board. This can help standardize your workflow by eliminating the manual process of creating new boards for similar workflows.  2. How do I copy a {% data variables.projects.projects_v1_board %} on GitHub's {% data variables.product.prodname_project_v1 %} feature?  Answer: To copy a {% data variables.projects.projects_v1_board %} on GitHub's {% data variables.product.prodname_project_v1 %} feature, follow these steps:  1. Go to the {% data variables.projects.projects_v1_board %} you want to copy. 2. Click the three dots (...) in the top right corner of the board. 3. Select \"Copy\" from the drop-down menu. 4. Choose the repository or organization where you want to copy the board. 5. Optionally, change the project board's name and visibility settings. 6. Click \"Copy board\" to complete the process.  Note: You must have read access to the board you want to copy, and write access to the repository or organization where you want to copy it.", "6f697c1f-0d84-4184-869a-6fe44af1482a": "According to the context information provided, if you want to add an expired or revoked GPG key to your GitHub account, you can do so by following the steps to add a public key to your account on GitHub. This is possible because GitHub allows you to add multiple public keys to your account, and commits signed by any of the corresponding private keys will still show as verified, even if the key is expired or revoked. However, GitHub will indicate that the signing key is expired or revoked for those commits.", "3b0acfd8-8a76-48ba-a003-8253c1477f8c": "To use grouped security updates for your repository or organization, you need to first enable the following features:  - **Dependency graph**. For more information, see \"AUTOTITLE.\" - **{% data variables.product.prodname_dependabot_alerts %}**. For more information, see \"AUTOTITLE.\" - **{% data variables.product.prodname_dependabot_security_updates %}**. For more information, see \"AUTOTITLE.\"", "8748bd41-18bc-4748-8d48-62237d10b3d0": "{% data variables.product.prodname_desktop %} notifies users about pull request check failures and pull request reviews through system notifications. When a pull request branch is updated and the checks fail, or when a teammate leaves a comment, approval, or requested changes on a pull request, {% data variables.product.prodname_desktop %} displays a system notification. Clicking the notification opens a dialog with more detailed information about the event, allowing users to quickly review and address the issue.  To enable system notifications for {% data variables.product.prodname_desktop %} on a Mac computer, follow these steps:  1. Open {% data variables.product.prodname_desktop %} and click on {% data variables.product.prodname_desktop %} in the menu bar. 2. Select \"Preferences\" from the drop-down menu. 3. In the \"Preferences\" window, select the \"Notifications\" pane. 4. Check the box next to \"Enable notifications\". 5. Click the \"Notification Settings\" link in the description field. 6. In the \"Application Notifications\" list, select {% data variables.product.prodname_desktop %}. 7. Click \"Allow Notifications\".  This will allow {% data variables.product.prodname_desktop %} to display system notifications on your Mac computer.", "2d6dccd7-d750-4412-bea3-a14278952db1": "The `qlpack.yml` file in the context provided serves as a configuration file for {% data variables.product.prodname_codeql %} packs. It specifies the name and dependencies of the pack, which allows {% data variables.product.prodname_codeql %} to automatically download and install any required dependencies when running queries in the pack. In the context provided, a `qlpack.yml` file is created for both the custom queries and tests, with the dependencies set to match the name of the pack containing the custom queries. This ensures that any updates to the custom queries or dependencies are automatically downloaded and installed when running queries in the pack.", "c2650d77-6dee-49f4-8440-7d67e0f20258": "There are four different operating systems listed in the context information: macOS, Ubuntu, and Windows (listed twice). The storage capacities for each operating system are as follows:  - macOS: 14 GB and 14 GB - Ubuntu: 16 GB, 32 GB, 64 GB, and 128 GB - Windows: 32 GB and 64 GB  Overall, there are a total of 12 different storage capacities listed across all operating systems.", "2ef4259b-3a8b-4119-811b-9bd7ad465252": "The \"demilestoned\" event in the context of GitHub issues and pull requests indicates that the issue or pull request has been removed from a milestone. This event is triggered when an issue or pull request is manually removed from a milestone or when the milestone is deleted. The event payload includes the `milestone` object, which contains the title of the milestone that the issue or pull request was removed from.  Example:  ```json {   \"type\": \"IssueEvent\",   \"actor\": {     \"id\": 1234,     \"login\": \"octocat\",     \"gravatar_id\": \"example\",     \"url\": \"https://api.github.com/users/octocat\",     \"html_url\": \"https://github.com/octocat\",     \"followers_url\": \"https://api.github.com/users/octocat/followers\",     \"following_url\": \"https://api.github.com/users/octocat/following{/other_user}\",     \"gists_url\": \"https://api.github.com/users/octocat/gists{/gist_id}\",     \"starred_url\": \"https://api.github.com/users/octocat/starred{/owner}{/repo}\",     \"subscriptions_url\": \"https://api.github.com/users/octocat/subscriptions\",     \"organizations_url\": \"https://api.github.com/users/octocat/orgs\",     \"repos_url\": \"https://api.github.com/users/octocat/repos\",     \"events_url\": \"https://api.github.com/users/octocat/events{/privacy}\",     \"received_events_url\": \"https://api.github.com/users/octocat/received_events\",     \"type\": \"User\",     \"site_admin\": false   },   \"repo\": {     \"id\": 1234,     \"name\": \"repo-name\",     \"full_name\": \"octocat/", "967c4b27-54f2-4b13-b5a5-d6e4f24d4aa8": "To control the amount of RAM used by the query evaluator during the upgrade process, you can use the `-M` or `--ram` option. This option allows you to set the total amount of RAM the query evaluator should be allowed to use. The value provided should be in megabytes (MB). This option is available for advanced users and can be passed when running the CLI command.", "fb62b405-dbae-46de-ae03-1113a0f7939b": "To determine the machine type that best fits the needs of your organization when launching {% data variables.product.prodname_ghe_server %} on Google Cloud Platform, you should review the minimum requirements for {% data variables.product.prodname_ghe_server %} as mentioned in the context information. Additionally, {% data variables.product.company_short %} recommends a general-purpose, high-memory machine for {% data variables.product.prodname_ghe_server %}. For more information, you can refer to the \"Machine types\" section in the Google Compute Engine documentation.", "fc7b333e-3001-45a5-a5d9-aa5c542bebb7": "The \"viewerCanUpdate\" field in the DiscussionComment type indicates whether the current viewer has permission to update or modify the comment. This field is significant because it determines whether the viewer has the necessary authorization to make changes to the comment, such as editing or deleting it. If the field returns true, the viewer can update the comment, and if it returns false, the viewer does not have the necessary permissions to make changes. This field helps to ensure the security and integrity of the comment and prevents unauthorized modifications.", "ceab8216-f269-4a63-922e-0c8ef035d305": "Additional enablement settings may include:  - Automatic enablement for a specific type of repository - Feature-specific settings, such as recommending the extended query suite for code scanning default setup throughout your organization, or automatic secret validation for secret scanning.  These settings can be found in the specific feature's section of the security and analysis settings for your organization.", "4046e479-d641-423f-b710-d29a65666631": "Expiring user access tokens and installation access tokens are both types of tokens used to authenticate and authorize access to resources on behalf of a user or an installation. The main difference between them is the scope of access they provide.  Installation access tokens are used to grant access to an entire organization or repository on behalf of an installation, such as a GitHub app or a bot. These tokens are generated when an installation is authorized to access resources and are valid for one hour by default.  User access tokens, on the other hand, are used to grant access to specific resources on behalf of a user. These tokens are generated when a user grants permission to an app or a bot to access their resources, and they can be used to access resources across multiple organizations and repositories. By default, user access tokens expire after eight hours, but this can be configured to a longer duration.  In both cases, it's recommended to revoke tokens as soon as they're no longer needed to prevent unauthorized access. Additionally, it's strongly encouraged to use tokens that expire to mitigate the risk of long-lived tokens being compromised. Installation access tokens expire after one hour, expiring user access tokens expire after eight hours, and refresh tokens expire after six months by default. However, these values can be customized based on the specific use case.", "80d7bce3-fbea-418f-be0c-892e7e7ac097": "Based on the context information provided, if Ravi switches from a yearly to monthly billing on December 10th, he will be charged for a month of service on November 5th, which is his next billing date after his account remains on the yearly subscription until the end of its current billing cycle on October 4th of the next year.", "f0fc5311-99a3-40a1-a9c5-1879b99a2565": "The purpose of the \"--search-path\" option in the provided context is to specify a list of directories to search for QL packs that cannot be found otherwise. This option is recommended to be set up once and for all in a per-user configuration file, as it allows for packs to be found in directories other than the default search path. However, on Windows, the path separator is \";\". If a pack name is found in multiple places through this list, it is an error. This option is not recommended to be overridden in a config file, as some internal actions will add this option on the fly, overriding any configured value.", "ed41709e-30f5-4728-945a-caa1bd2be6c8": "The guidelines for titling procedural content are to use gerunds as the article or header title, with active and specific verbs that accurately reflect the contained tasks. The titles should be general enough to encompass all of the content, and should be no longer than 80 characters, with a preference for 60 characters if possible. This ensures that the titles accurately reflect the contained tasks and are easily searchable and understandable for users.", "7c60daa4-2956-4de1-bb5d-16d932114c65": "Based on the limited context provided, it is unclear what \"AUTOTITLE\" refers to. Without further information, it is impossible to suggest a specific type of question related to this term. Please provide more context or clarify the meaning of \"AUTOTITLE\" for a more specific question to be asked.", "f2bb36c6-9352-4363-bdfe-0e522dac74ef": "The `childGroups` frontmatter property should be utilized in the homepage to highlight specific content areas by creating an array of mappings containing a `name` for the group, an optional `icon` for the group, and an array of `children`. This property allows for the grouping and organization of related content, making it easier for users to navigate and find what they need.  For example, in the frontmatter of the homepage, the `childGroups` property could be used as follows:  ```yaml --- title: GitHub Docs childGroups:   - name: Getting Started     icon: \"fas fa-rocket\"     children:       - path: /docs/getting-started-with-github       - path: /docs/github-learning-lab   - name: GitHub Pages     icon: \"fas fa-globe-americas\"     children:       - path: /docs/pages       - path: /docs/pages/getting-started-with-github-pages       - path: /docs/pages/customizing-your-github-pages-site --- ```  In this example, the `childGroups` property is used to group together content related to getting started with GitHub and GitHub Pages. Each group is given a name and an optional icon, which can be used to visually distinguish the groups. The `children` array for each group contains the paths to the corresponding pages in the docs site. By using `childGroups`, the homepage can provide a more organized and intuitive navigation experience for users.", "94d4c09c-ac40-41e7-8943-d29d1a322cda": "{% data variables.product.product_name %} automatically converts standard URLs to links by detecting the protocol (http, https, etc.) and creating a clickable link. For example, the URL \"https://github.com\" would be automatically converted to a blue clickable link.  To reference issues, pull requests, and labels in {% data variables.product.product_name %} Markdown, there are several ways to do so, and the resulting short links depend on the specific reference type. Here are some examples:  - Issue or pull request URL: The full URL of the issue or pull request is used as the reference, and the resulting short link is the issue or pull request number prefixed with \"#\" or \"GH-\". For example, \"https://github.com/jlord/sheetsee.js/issues/26\" would be automatically converted to \"#26\" or \"GH-26\". - `#` and issue or pull request number: Just the issue or pull request number is used, prefixed with \"#\". For example, \"#26\" would be automatically converted to a link to the corresponding issue or pull request. - `GH-` and issue or pull request number: The prefix \"GH-\" is used, followed by the issue or pull request number. For example, \"GH-26\" would be automatically converted to a link to the corresponding issue or pull request. - `Username/Repository#` and issue or pull request number: The username, repository name, and issue or pull request number are used, separated by \"/\" and \"#\". For example, \"jlord/sheetsee.js#26\" would be automatically converted to a link to the corresponding issue or pull request. - `Organization_name/Repository#` and issue or pull request number: The organization name, repository name, and issue or pull request number are used, separated by \"/\" and \"#\". For example, \"github-linguist/linguist#4039\" would be automatically converted to a link to the corresponding issue or pull request.  In {% ifversion fpt or ghec %}GitHub Enterprise Cloud{% endif %}, references to issues, pull requests, or discussions in a list will unfurl to show the title and state instead.", "b9cd1e06-0972-4056-9f31-979a75be3605": "To filter the list of repositories in the security overview to display only those that have a high level of risk, you can use the `risk:high` qualifier in the filter bar. This qualifier will display only the repositories that are currently at a high level of risk, as determined by the number and severity of alerts from security features. If this qualifier is not available in your view, it may be because you are not using a supported version of GitHub Enterprise or GitHub.com with the necessary security features enabled.", "847e32e1-271d-408d-b1aa-4903434405aa": "Based on the context information provided, GitHub will promptly notify the customer of any Security Incident involving Customer Data or Personal Data, investigate the incident, and provide detailed information about it. GitHub will also take reasonable steps to mitigate the effects and minimize any damage resulting from the Security Incident. Additionally, GitHub will make commercially reasonable efforts to assist the customer in fulfilling its obligations under applicable law or regulation to notify the relevant supervisory authority and data subjects about such Security Incident. These actions are required by the terms outlined in the context information.", "855fa5d0-c1bf-4bb6-bd82-e0b49d211e5c": "Actions that involve misrepresenting one's identity or association with another person or organization are considered impersonation by GitHub. This includes copying another user's avatar or personal profile information, posting content under another user's email address, using a deceptively similar username, organization name, or other namespace, accessing an account or organization with another user's token or credentials, or otherwise posing as another individual or organization. GitHub defines impersonation as a form of harassment because it can mislead or deceive others, leading to confusion, mistrust, and potential harm. This policy is in place to protect the integrity and trustworthiness of the GitHub community.", "0fc5bb85-1732-4b2a-98b6-232d83bdbcb9": "To ensure that your contribution guidelines are clear and easy to understand for new contributors, you can follow these best practices:  1. Use simple and concise language: Avoid using technical jargon or complex terms that may confuse new contributors. Use clear and simple language that is easy to understand.  2. Provide step-by-step instructions: Break down the contribution process into small and manageable steps. Provide clear instructions on how to create an issue, how to submit a pull request, and how to review and merge a pull request.  3. Use screenshots and examples: Use screenshots and examples to illustrate the contribution process. This will help new contributors understand the process better and avoid any confusion.  4. Provide links to external resources: If there are any external resources that can help new contributors, such as documentation or mailing lists, provide links to them. This will help new contributors find the information they need quickly and easily.  5. Be consistent: Use a consistent style and format throughout your contribution guidelines. This will help new contributors understand the guidelines better and avoid any confusion.  6. Test your guidelines: Before publishing your contribution guidelines, test them with new contributors. This will help you identify any issues or confusion and make necessary improvements.  By following these best practices, you can ensure that your contribution guidelines are clear, concise, and easy to understand for new contributors.", "574d05c6-16d9-4776-a0b3-9a3e40e7e1f5": "The process of closing a pull request on GitHub involves navigating to the pull request in the \"Pull Requests\" list, clicking \"Close pull request\" at the bottom of the pull request, and optionally deleting the branch. Someone might choose to close a pull request without merging it into the upstream branch if the changes proposed in the branch are no longer needed or if another solution has been proposed in another branch. This can be helpful in keeping the list of branches in the repository tidy and avoiding unnecessary merges. Additionally, closing a pull request with the wrong base branch can be addressed by changing the base branch instead of closing and reopening the pull request.", "9ccd9380-11f7-45f3-ba02-dd506ecc5bce": "The {% data variables.product.prodname_container_registry %} offers several benefits for storing and sharing Docker images, including:  1. Integrated with GitHub: The {% data variables.product.prodname_container_registry %} is directly integrated with GitHub, making it easy to manage and share Docker images as part of your GitHub workflow.  2. Secure access: The {% data variables.product.prodname_container_registry %} provides secure access to your Docker images through authentication and authorization mechanisms.  3. Scalability: The {% data variables.product.prodname_container_registry %} is designed to handle large-scale Docker image storage and distribution, with support for billions of images and petabytes of storage.  4. Collaboration: The {% data variables.product.prodname_container_registry %} supports collaboration and team workflows, with features like organization-level access control and fine-grained permissions.  5. Community: The {% data variables.product.prodname_container_registry %} is part of the larger GitHub community, providing access to a wide range of resources and support.  For more information, see \"AUTOTITLE.\"", "b068f746-fccd-45e0-a2f6-401fa1b845dc": "GitHub will Support a given Release of the Software for one (1) year from the original Release date, or six (6) months from the last Update of the Release, whichever is longer. After this period ends, GitHub will no longer provide Support for that Release. If the customer requires Support for earlier Releases of the Software, they must pay for that Support in accordance with the terms of a mutually agreed upon Order Form or SOW.", "923a5ce0-edf4-4ec1-920d-1b6146eea743": "The \"features\" section in the dev container configuration allows you to add pre-installed tools and utilities to your container. By adding a feature, you can enable certain functionalities without having to install them manually.  To enable JSHint linting in your codespace, you can add the \"ghcr.io/devcontainers-contrib/features/jshint:2\" feature to the \"features\" section of your dev container configuration. This feature provides a pre-installed version of JSHint, which is a popular JavaScript linting tool.  Here's an example of how to add this feature:  ```json {   \"name\": \"Node.js\",   \"image\": \"mcr.microsoft.com/devcontainers/javascript-node:0-18-bullseye\",   \"features\": {     \"ghcr.io/devcontainers-contrib/features/jshint:2\": {}   } } ```  In this example, the \"features\" section is added to the dev container configuration, and the \"ghcr.io/devcontainers-contrib/features/jshint:2\" feature is added as a key-value pair. The value is an empty object, which is optional and can be used to pass additional configuration options to the feature.  By adding this feature, JSHint will be automatically installed and configured in your codespace, without the need to install it manually or add it to your `package.json` file. This can save time and simplify the setup process for your codespace.", "df251d84-cc3c-43ba-bbd2-d20cfedd752e": "In the event of subprocessing, the processing activity is carried out in accordance with Clause 11 by a subprocessor providing at least the same level of protection for the personal data and the rights of data subject as the data importer under the Clauses, as outlined in the context information.", "2d8bf7ad-4921-4b0b-8563-daed1d52cef3": "Based on the given context, it is unclear how the concept of \"AUTOTITLE\" differs from \"AUTOTITLE\". Both terms are mentioned without any further explanation or context provided. Without prior knowledge or additional information, it is impossible to determine any differences between the two concepts. It is recommended to provide more context or clarify the meaning of these terms to better understand how they differ, if at all.", "dd45dfee-e374-4226-afdf-9b6f6dc417aa": "To downgrade an individual organization within an enterprise account to a free team plan, you need to remove the organization from the enterprise account. This will automatically downgrade the organization to a free team plan. For more information, refer to the provided links.", "c0585a65-3146-4148-a23d-2ef07bf2f055": "To view the usage information for {% data variables.product.prodname_copilot %} at the organization level, follow these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. Click on the \"Members\" tab. 3. Scroll down to the \"Seats\" section. 4. Here, you can see the total number of seats assigned to your organization, as well as the number of seats carried over from the previous billing cycle and the number of new seats added during the current cycle. 5. You can also see the number of seats that will be removed at the end of the current billing cycle.  This information provides a detailed breakdown of your organization's {% data variables.product.prodname_copilot %} usage, allowing you to track your usage and make informed decisions about your subscription.", "4922a1ae-8372-4fde-9236-3ea133b581e9": "To filter audited records using the ghe-migrator audit command in GitHub Enterprise Migrator, you can use the `-m` and `-s` flags. The `-m` flag is used to filter based on the record type, and the `-s` flag is used to filter based on the import state.  For example, to view every successfully imported organization and team, you can enter the following command:  ```shell $ ghe-migrator audit -m organization,team -s mapped,renamed -g MIGRATION-GUID ```  This command will display a list of audited records, where the `model_name` is either `organization` or `team`, and the `state` is either `mapped` or `renamed`.  To view every failed import, mapping, or renaming, you can enter the following command:  ```shell $ ghe-migrator audit -s failed_import,failed_map,failed_rename -g MIGRATION-GUID ```  After completing the import on GitHub Enterprise Server using GitHub Enterprise Migrator, you should wait around two weeks to ensure that everything is functioning as expected before unlocking the repositories and deleting them off the source.  To unlock repositories on GitHub Enterprise Server, you can follow these steps:  1. SSH into your GitHub Enterprise Server instance.  2. Run the following command to unlock the repositories:  ```shell $ curl -u admin:<YOUR_ADMIN_PASSWORD> -X POST -H \"Content-Type: application/json\" -d '{\"repositories\": [\"repo1\", \"repo2\", \"repo3\"]}' https://ghe.your-domain.com/api/v3/repositories/unlock ```  Replace `repo1`, `repo2`, and `repo3` with the names of the repositories you want to unlock.  3. After unlocking the repositories, you can delete them off the source using your preferred method.  Note: Before deleting your source data, we recommend contacting GitHub Support at {% data variables.contact.contact_ent_support %} if you have any", "ae54c77c-6303-4df8-96e0-2976257904e5": "Yes, based on the context information provided, I can explain how to use the `--config-file-path` argument with the `audit`, `dry-run`, and `migrate` subcommands in {% data variables.product.prodname_actions_importer %}. The `--config-file-path` argument allows you to specify a source file instead of fetching pipeline contents from source control by default. When used with the `audit`, `dry-run`, or `migrate` subcommands, {% data variables.product.prodname_actions_importer %} matches the repository slug to the job represented by the `--source-url` option to select the pipeline and uses the `config-file-path` to pull the specified source file. This feature is useful when you want to audit, perform a dry run, or migrate a specific pipeline or set of pipelines using a config file instead of fetching them from source control. The config file must be in a specific format, and each `repository_slug` value must be unique.", "8490bda2-f986-4c6c-b048-b1ce6c4188b6": "Without storing the last run time as a {% data variables.product.prodname_actions %} secret, the script would require API calls to access and update the configuration variable. Specifically, the script would need to make a GET request to retrieve the current configuration variable value and a PUT request to update it with the new last run time. The API endpoints for these requests would be `/repos/{owner}/{repo}/actions/secrets/{NAME}` for GET and `/repos/{owner}/{repo}/actions/secrets/{NAME}` for PUT, where `{NAME}` is the name of the configuration variable. The script would need to include the necessary authentication headers and parameters to make these requests successfully.", "f1068e00-586a-4fc7-885a-0c5551fe91ad": "When editing an assignment in GitHub Classroom, you should be aware that you cannot change the assignment type (either individual or group) or the online integrated development environment (IDE) after assignment creation. This means that if you accidentally created an assignment as a group assignment instead of an individual assignment, or if you want to change the IDE used for the assignment, you will need to create a new assignment instead. It's important to double-check these settings when creating a new assignment to avoid any confusion or inconvenience for your students.", "ea032e5f-5f91-4b29-acba-bfee763b40fa": "Unfortunately, the provided context does not offer enough information to write a 1000-word report on the latest trends in software development, as it only mentions three emerging technologies without providing any further details or context. To write a comprehensive report, it is necessary to have a clear understanding of the current state of these technologies, their potential impact on the industry, and the best practices for developers to stay up-to-date with these developments. Without this information, it is impossible to provide a detailed report that meets the required length and format.  In light of this, I recommend that you provide additional context or clarify your request to enable me to provide a more detailed and informative report. Alternatively, you may want to consider narrowing the scope of your request to focus on a specific aspect of these technologies, such as their implementation in cloud computing or their potential impact on cybersecurity. This would allow me to provide a more focused and detailed report that meets your requirements.  If you decide to provide additional context, please let me know as soon as possible, so I can adjust my approach and ensure that the report meets your expectations.  Thank you for your attention, and I look forward to working with you.  Best regards,  [Your Name]", "45558bb0-b4b5-4922-8926-ba526b882982": "Without further context, it's unclear what specific \"wonkiness\" is being referred to. However, based on the statement \"to do a little bit of wonkiness to make it work,\" it's possible that there's a configuration or setting that needs to be adjusted to prevent the confirmation dialog from appearing in the future. It's recommended to consult the documentation or support resources for the specific application or service being referred to for guidance on how to prevent the confirmation dialog from appearing. If you're still unable to find a solution, it may be necessary to contact the application or service provider for assistance.", "f1d095e7-63f4-4ad6-ab5f-335ee0788151": "If an organization owned by an enterprise with email restrictions enabled no longer has any verified or approved domains, either inherited from the enterprise that owns the organization or for the specific organization, email restrictions will be disabled for the organization. This means that users in that organization will be able to receive email notifications using any email address, regardless of whether it belongs to a verified or approved domain. The enterprise's email restrictions will still apply to other organizations owned by the enterprise. It is the responsibility of the enterprise administrator to inform users that email restrictions have been disabled for the affected organization.", "907ba5c8-d938-4528-acab-59975ee3e938": "The JavaScript code provided handles the `issues` and `ping` event types. For the `issues` event, it checks the `action` field and takes different actions based on its value. If the `action` is `opened`, it logs the title of the opened issue. If the `action` is `closed`, it logs the username of the person who closed the issue. If the `action` is not recognized, it logs an unhandled action message. For the `ping` event, it simply logs a message indicating that GitHub sent the ping event. If any other event type is received, it logs an unhandled event message.", "bca6d1a4-37d5-46ba-bff7-3af67576d8fe": "To configure a service container using the Docker Hub `redis` image in a workflow that runs a script to create a Redis client and populates it with data, you can follow these steps:  1. Create a new workflow file in your repository with the name `.github/workflows/redis-container-example.yml`.  2. Copy and paste the following YAML code into the new file:  ```yaml name: Redis container example on: push  jobs:   container-job:     runs-on: ubuntu-latest     container: node:10.18-jessie     services:       redis:         image: redis         options: >-           --health-cmd \"redis-cli ping\"           --health-interval 10s           --health-timeout 5s           --health-retries 5      steps:       - name: Check out repository code         uses: {% data reusables.actions.action-checkout %}        - name: Install dependencies         run: npm ci        - name: Connect to Redis         run: |           # Load the Redis client module           const redis = require('redis');            # Create a new Redis client           const client = redis.createClient({             host: 'redis',             port: 6379,           });            # Set some keys and values           client.set('key1', 'value1');           client.set('key2', 'value2');            # Get the values of the keys           client.get('key1', (err, value) => {             console.log(`Value of key1: ${value}`);           });           client.get('key2', (err, value) => {             console.log(`Value of key2: ${value}`);           });            # Close the Redis client           client.quit(); ```  3. Save the file and commit your changes.  4. Push your changes to your repository to trigger the workflow.  5", "beac8355-87a3-47e5-ba58-6f25fc883d42": "The pre-release software collects information about the user and their use of the software, which is then sent to GitHub. GitHub uses this collected data for analytics and measurement to understand how the pre-release software and related products are used. Specifically, the software collects data and usage information about events generated when interacting with it, which helps GitHub analyze and measure performance and functionality. This data collection and usage is described in section 4 of the text material.", "9f95ff9a-ae31-48bd-80c8-93f2fcaa3f71": "Enabling or disabling {% data variables.product.prodname_dependabot_security_updates %} for all qualifying repositories owned by your personal account or organization allows you to manage the feature at a higher level, while enabling or disabling {% data variables.product.prodname_dependabot_security_updates %} for an individual repository allows you to manage the feature on a per-repository basis. This can be useful if you want to apply the feature to some repositories but not others, or if you want to temporarily disable the feature for a specific repository. The context information provided does not specify whether the feature is currently enabled or disabled, so it is not possible to determine the difference between enabling and disabling the feature in this case.", "1a00293b-7921-4570-8842-fff81d2435dd": "The metrics overview for {% data variables.product.prodname_secret_scanning %} push protection provides organization owners and security managers with a summary of how many pushes containing secrets have been successfully blocked and how many times push protection was bypassed. This information helps them assess the overall performance of push protection and identify areas where action may be needed to prevent leaks of sensitive information. Additionally, the metrics provide more granular insights, such as the secret types that have been blocked or bypassed the most, the repositories with the most pushes blocked, the repositories that are bypassing push protection the most, and the percentage distribution of reasons that users give when they bypass the protection. By analyzing these metrics, organization owners and security managers can take proactive measures to improve push protection and reduce the risk of sensitive information leaks.", "a890ca99-8ebf-4d1c-b00c-d98edddf8bc2": "To view the subscription and usage for your enterprise account on {% data variables.product.prodname_dotcom_the_website %}, follow these steps:  1. Access your enterprise account on {% data variables.product.prodname_dotcom_the_website %}. 2. Go to the settings tab. 3. Click on the license tab. 4. Here, you can view your total licenses, consumed licenses, and subscription expiration date. 5. To view details for license usage or download a CSV file with license details, click \"View details\" or \"{% octicon \"download\" aria-hidden=\"true\" %} CSV report\" (depending on whether you're using {% ifversion ghec %}GitHub Enterprise Cloud{% elsif ghes %}GitHub Enterprise Server{% endif %}). 6. Optionally, to view usage details for other features, click \"Billing\" in the left sidebar (if you're using {% ifversion ghec %}GitHub Enterprise Cloud{% endif %}).  Note: If you're not sure whether you're using {% ifversion ghec %}GitHub Enterprise Cloud{% elsif ghes %}GitHub Enterprise Server{% endif %}, you can check the context information provided. If it mentions \"AUTOTITLE\" in the {% data variables.product.prodname_ghe_cloud %} documentation, you're using {% ifversion ghec %}GitHub Enterprise Cloud{% endif %}. Otherwise, you're using {% ifversion ghes %}GitHub Enterprise Server{% endif %}.", "8f728ccc-95b1-4e78-a6ca-17971a266b64": "{% ifversion fpt or ghec %}  The purpose of {% data variables.secret-scanning.user_alerts %} in {% data variables.product.prodname_dotcom %} is to notify users when their repositories contain secrets issued by a variety of service providers. These alerts are generated by {% data variables.secret-scanning.user_alerts %}, which is a feature of {% data variables.product.prodname_secret_scanning %}. When enabled, {% data variables.product.prodname_dotcom %} scans repositories for secrets and reports alerts to users through the Security tab on the repository. {% endif %}  If the context doesn't provide information about whether {% data variables.product.prodname_dotcom %} is being used as a Freemium Plan (FPT), GitHub Enterprise Cloud (GHEC), or GitHub Enterprise Server (GHES) or GitHub Advanced Security (GHAE), then it's not possible to determine whether user alerts are reported to users or not. However, based on the context provided, it's clear that user alerts are a feature of {% data variables.product.prodname_secret_scanning %} and are reported to users on {% data variables.product.prodname_dotcom %}.", "d3fff80b-c4c9-4a62-81c4-a90d6e8b4bf3": "According to the context information provided, the precedence order for configuration variables with the same name in an organization, repository, and environment is as follows: environment-level variables take precedence over repository-level variables, and organization-level variables take precedence over both repository- and environment-level variables. This means that if a variable with the same name exists at all three levels, the variable defined at the environment level will be used.  In terms of the availability of variables in reusable workflows, the context states that for reusable workflows, the variables from the caller workflow's repository are used. Variables from the repository that contains the called workflow are not made available to the caller workflow. This means that if a variable with the same name exists in both the caller workflow's repository and the repository that contains the called workflow, the variable defined in the caller workflow's repository will be used. If the variable does not exist in the caller workflow's repository, the variable defined in the repository that contains the called workflow will not be available to the caller workflow.", "824f44f9-c647-4ca0-8b1d-351103a6f43b": "According to the context provided, {% data variables.product.prodname_copilot %} shares context information with Microsoft when it is used. However, the specific nature of this context information is not explicitly stated in the context. For more information about the types of context information shared with Microsoft by {% data variables.product.prodname_copilot %}, it is recommended to refer to the \"The {% data variables.product.prodname_copilot %} Trust Center\" mentioned in the context.", "58b30295-5c8a-44b9-8778-ad9628718be9": "In order to obtain an access token with the necessary permissions to manage runners in an enterprise context, you must include the `manage_runners:enterprise` scope in your request.  2. If you need an access token with the ability to manage runners in an enterprise environment, you should request a token with the `manage_runners:enterprise` scope. This will grant you the necessary permissions to perform runner management tasks within your organization.", "37eae0f5-b459-456d-9fed-c34ba8b2ea00": "The `checkout` action in the context information is used to check out the repository and download it to the runner, allowing actions to be run against the code. This is necessary because actions cannot access the repository's code directly, and must be downloaded and extracted to a specific location on the runner. The `checkout` action is required for any workflow that will use the repository's code, and additional options can be provided using the `with` key. In this context, the `checkout` action is used to download the repository's code, and some options are provided to enable LFS (Large File Storage) and to prevent credential persistence.", "d1f02f7a-dbe6-4740-b9c0-d3811776cd74": "The context information provided briefly mentions using OpenID Connect to access cloud resources, but doesn't explain its significance. Without further context, it's unclear how using OpenID Connect specifically helps mitigate security risks or improves access to cloud resources. More information would be needed to fully understand the significance of this recommendation.", "f7eb2e79-c2d3-496b-8c13-3c99740f97c3": "A workflow trigger is a mechanism in GitHub Actions that initiates the execution of a workflow in response to a specific event, such as a push to a branch or the creation of a pull request. When an event occurs, GitHub searches for workflows with matching `on:` values in the repository's `.github/workflows` directory, and triggers a run for each matching workflow. The version of the workflow used in the run is determined by the commit SHA or Git ref associated with the triggering event. Workflow triggers can be used to automate tasks and workflows in response to various events in a GitHub repository.", "464533f7-a33c-4a6a-a9b4-dde1a4648997": "To authenticate to two GitHub Enterprise Server instances using the provided command-line tool, you can use the `--registries-auth` option and provide the authentication credentials for each instance. The provided context information shows an example of how to use this option with two GitHub Enterprise Server instances. This option overrides the `CODEQL_REGISTRIES_AUTH` environment variable. If you only need to authenticate to the github.com Container registry, you can instead use the simpler `--github-auth-stdin` option and pass a GitHub Apps token or personal access token via standard input. The `--github-auth-stdin` option overrides the `GITHUB_TOKEN` environment variable.", "9617eaeb-036d-4d92-9663-3ac9507de1e9": "To set up credentials for {% data variables.product.prodname_actions_importer %} with Bitbucket Pipelines and {% data variables.product.prodname_dotcom %}, you will need the following information:  1. A {% data variables.product.pat_v1 %} for {% data variables.product.prodname_dotcom %}. This token must have the `workflow` scope. You can create a new token in your {% data variables.product.prodname_dotcom %} settings.  2. A Workspace Access Token for Bitbucket Pipelines. This token must have the `read` scope for pipelines, projects, and repositories. You can create a new token in your Bitbucket instance settings.  When prompted by the `configure` command, enter the values for both tokens and the URLs for your {% data variables.product.prodname_dotcom %} and Bitbucket instances. The `configure` command will guide you through the process of entering this information.", "fb838d58-8f0e-470d-9bc8-58c05fa154a7": "A \"Page build warning\" message indicates that Jekyll encountered a potential issue during the build process, but the build still completed successfully. These warnings may be informational or suggest best practices to follow. For example, a warning might suggest that you add a missing front matter key to a page. To address these warnings, you should review the messages and make any necessary changes to your site's content or configuration.  A \"Page build failed\" message indicates that Jekyll was unable to complete the build process due to an error. These errors can be caused by a variety of factors, such as syntax errors in your site's configuration or content, missing dependencies, or incorrect file permissions. To address these errors, you should review the error message and take appropriate action to resolve the issue. This may involve updating your site's configuration, fixing syntax errors, or ensuring that files have the correct permissions.  In both cases, it's a good practice to regularly review your site's build logs to ensure that your site is building and deploying successfully. This can help you catch and address issues before they become major problems.  If you're using {% data variables.product.prodname_actions %} to build and deploy your site, you can view your build logs by navigating to your repository's Actions tab and selecting the appropriate workflow run. From there, you can view the build logs for each step in the workflow, including the Jekyll build step. This can help you identify the source of any build errors and take appropriate action to resolve them.  If you're not using {% data variables.product.prodname_actions %} to build and deploy your site, you can view your build logs by running the `jekyll build` command in your site's repository and reviewing the output. This can help you identify any issues with your site's configuration or content that may be causing build errors.  In either case, it's a good practice to regularly review your site's build logs to ensure that your site is building and deploying successfully. This can help you catch and address issues before they become major problems.    Troubleshooting Jekyll build errors  If you encounter a Jekyll build error, there are several steps you can take to troubleshoot the issue:  1. Review the", "790b91c5-b96b-4d85-b6a7-2abf685243ad": "In the context of Github, an internal gist is a type of gist that is associated with a user's account when they create it while signed in to Github. These gists are not publicly visible by default and cannot be found through search or discovery features unless the user shares the URL with others. Internal gists are suitable for sharing code snippets with specific individuals or collaborators within an organization.  On the other hand, a secret gist is also a private gist that is not publicly visible by default, but it is not associated with a user's account. Instead, it is created anonymously, and the user can choose to keep it secret or share it with specific individuals by providing them with the URL. Secret gists are not searchable unless the user is logged in and is the author of the gist. The main difference between internal and secret gists is that internal gists are associated with a user's account, while secret gists are anonymous.", "0bb92fa5-a18f-460d-9b7e-9bee4112c20f": "To assign yourself to a new issue in GitHub without leaving your tasklist, follow these steps:  1. Next to the new issue in your tasklist, click the {% octicon \"kebab-horizontal\" aria-label=\"tracking block item menu\" %} icon. 2. In the context menu, click \"Set assignees\". 3. In the list of people, select yourself.  This will allow you to quickly and easily assign yourself to the new issue without having to navigate away from your tasklist.", "9501f0e4-00c7-46f3-a5f2-4eff371be633": "To create screenshots for GitHub documentation, the following visual style should be followed:  1. PNG file format 2. Static images only (no GIFs) 3. 144 dpi 4. 750\u20131000 pixels wide for full-column images 5. File size of 250 KB or less 6. Descriptive file names 7. Accompanied by complete instructions in the procedural step 8. Full contrast with nothing obscured or reduced in opacity or color contrast 9. Alt text that describes the content of the image and the appearance of its highlighting, if any 10. Clear and crisp with text and UI elements as legible as possible 11. Show a UI element with just enough surrounding context to help people know where to find the element on their screen 12. Reduce negative space by resizing your browser window until optimal 13. Show interfaces in light theme wherever possible 14. If the software is available in dark mode only, it's fine to use dark mode 15. Replace your username and avatar with @octocat's username and avatar 16. Do not include a cursor 17. If the primary goal in showing a dropdown menu is to help the reader locate the menu itself, show the menu closed.", "4a6b12d5-55da-4d8d-ae1b-89c93afac239": "The role of $(System.HostType) in the provided context is to indicate the type of host on which the build is running. It provides information about the environment in which the build is executing, which can be useful for debugging and troubleshooting purposes. In this case, the value of $(System.HostType) is \"build\", which suggests that the build is being executed on a build server or agent. However, the specific host or agent is not specified in the provided context.", "53a412da-7cb8-41bf-824c-a8b45dc9ed45": "If your locally-hosted code is not tracked by any version control system (VCS) and you want to add it to GitHub using the command line, you should initialize a Git repository. To do this, open your terminal and navigate to the root directory of your project. Then, run the following command:  ```shell git init -b main ```  This will initialize the local directory as a Git repository with the default branch called `main`.  If your project is already tracked by Git, you can skip this step and move on to importing your Git repository with the command line.  If your project is tracked by a different version control system, such as Mercurial, Subversion, or Team Foundation Version Control, you must convert the repository to Git before you can add it to GitHub. This process is called \"cloning\" in Git terminology. You can find detailed instructions on how to clone a repository from these version control systems in the Git documentation.  In summary, the steps to convert a repository from a different version control system to Git are:  1. Create a new Git repository in the directory where your project is located. 2. Add all the files in your project to the Git repository using the `add` command. 3. Commit your changes to the Git repository using the `commit` command. 4. Push your changes to a remote Git repository using the `push` command. 5. Create a new repository on GitHub and add the remote Git repository as a remote repository using the `add remote` command. 6. Push your changes to the GitHub repository using the `push` command.  These steps will convert your repository from the original version control system to Git and allow you to add it to GitHub.", "8fe7ad2f-4067-4ea0-bb33-bfbc5b42f9d9": "Based on the context information provided, users should behave professionally in the GitHub community by following the guidelines listed in both sections. This includes taking personal responsibility for their posts, verifying the accuracy of AI-generated content, using their own authentic voice, and avoiding misleading other users into thinking they are receiving an official response from GitHub. Additionally, users should avoid creating multiple free accounts, violating the Terms of Service, and engaging in conduct that could be considered inappropriate in a professional setting.", "ed821bb1-e1f6-4a79-acd4-4a8f44c8f223": "Conditions can be used to control job execution in workflows by specifying certain criteria that must be met before a job is executed. This can help to prevent unnecessary work and save resources.  For example, let's say you have a workflow that includes a job to deploy a new version of your application. However, you only want to deploy the new version if a specific environment variable is set to a certain value.  To implement this, you can use a condition to check the value of the environment variable before executing the job. Here's an example:  ```yaml name: Deploy Application  on:   push:     branches:       - main  jobs:   deploy:     runs-on: ubuntu-latest      steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up JDK 11       uses: actions/setup-java@v2       with:         java-version: '11'         distribution: 'adopt'      - name: Install dependencies       run: npm install      - name: Build and package application       run: npm run build      - name: Deploy application       if: github.event.push.head_ref == 'refs/heads/main' && github.env.DEPLOY_ENVIRONMENT == 'staging'       uses: your-org/your-repo/.github/actions/deploy-to-staging@v1      - name: Deploy application       if: github.event.push.head_ref == 'refs/heads/main' && github.env.DEPLOY_ENVIRONMENT == 'production'       uses: your-org/your-repo/.github/actions/deploy-to-production@v1 ```  In this example, the `deploy` job is executed only if the push event is on the `main` branch (`github.event.push.head_ref == 'refs/heads/main'`) and the `DEPLOY_ENVIRONMENT` environment variable is set to either 'staging' or '", "6beb2a5e-e9db-440e-8030-689cd4c06879": "The environment variables used by the `client.js` script to create a new PostgreSQL client in this context are `POSTGRES_HOST` and `POSTGRES_PORT`. The value of `POSTGRES_HOST` is `postgres`, which is the label used to access the PostgreSQL service container. The value of `POSTGRES_PORT` is not explicitly stated in the context, but it is mentioned that the default PostgreSQL port is 5432, and it is also used to map the port on the service container to the host in the `services` section. Therefore, it can be inferred that the value of `POSTGRES_PORT` is 5432 in this context.", "36f3a1ff-8f2f-4f53-b8b0-796e132d93a7": "The recommended style for listing items in web development is to use a serial comma. This means adding a comma after the second-to-last item in a list, before the conjunction (such as \"and\" or \"or\"). This style is recommended because it makes the list more clear and less ambiguous, especially when the list contains long or complex items.  We should avoid referring to the {% data variables.product.company_short %} community as \"users\" because {% data variables.product.company_short %} is a company, and its users are developers, organizations, and businesses that use its products and services. Using the term \"users\" to refer to this community can be misleading and may not accurately reflect the diverse range of people who use {% data variables.product.company_short %}. Instead, we should use more specific and accurate terms to describe the different types of people who interact with {% data variables.product.company_short %}, such as developers, contributors, maintainers, and organizations.", "85bbf291-936b-403d-a8ed-e0d188fa9ad5": "To publish Docker images to a third-party Docker registry, you can use the example provided in the \"Note\" section as a template. However, you will need to replace the specifics of the third-party registry, such as the registry URL and authentication credentials, in the action configuration. It's also important to ensure that the Docker registry supports the use of Docker Hub authentication tokens, as this is the method used in the example. Consult the documentation for your specific registry for more information on how to authenticate and push images.", "6a79d9d2-65a0-4a77-92a1-c40b4dbe783d": "To create a private repository using the GitHub API, you can use the following URL parameters:  `https://{% data variables.product.product_url %}/repos/{% user %}/{% repository %}/{% branch %}/{% sha %}/{% asset_path %}?{% query_params %}`  To create a private repository, you can add the following query parameters to the URL:  - `visibility=private`  Here's an example URL:  `https://github.com/api/repos/{% user %}/{% repository %}/{% branch %}/{% sha %}/{% asset_path %}?visibility=private`  To create a public repository owned by a specific organization using the GitHub API, you can use the following URL parameters:  `https://{% data variables.product.product_url %}/repos/{% owner %}/{% repository %}/{% branch %}/{% sha %}/{% asset_path %}?{% query_params %}`  To create a public repository owned by an organization, you can add the following query parameters to the URL:  - `owner={% owner %}` - `visibility=public`  Here's an example URL:  `https://github.com/api/repos/avocado-corp/my-repo/{% branch %}/{% sha %}/{% asset_path %}?visibility=public`  Yes, it's possible to specify the owner as a personal user account instead of an organization. In this case, you can use the `@me` syntax to refer to your own user account. Here's an example URL:  `https://github.com/api/repos/{% user %}/{% repository %}/{% branch %}/{% sha %}/{% asset_path %}?visibility=public`  In this example, `{% user %}` would be replaced with your GitHub username.", "fc64ccd6-fa22-448c-84e6-76843ab3346a": "To prevent accidental commits in Git while working on a project, you can follow these specific techniques and commands:  1. Use a visual program like GitHub Desktop or gitk to commit changes. Visual programs make it easier to see exactly which files will be added, deleted, and modified with each commit.  2. Avoid using the catch-all commands `git add .` and `git commit -a` on the command line. Instead, use `git add filename` and `git rm filename` to individually stage files.  3. Use `git add --interactive` to individually review and stage changes within each file.  4. Use `git diff --cached` to review the changes that you have staged for commit. This is the exact diff that `git commit` will produce as long as you don't use the `-a` flag.  Regarding your second question, the following commands are used to dereference and garbage collect objects in Git:  1. `git for-each-ref --format=\"delete %(refname)\" refs/original | git update-ref --stdin`: This command deletes all references to the `refs/original` namespace, which is used to store backup references. The `--format` option is used to format the output of `git for-each-ref`, and the `delete` command is used to delete the references. The output of `git for-each-ref` is piped to `git update-ref`, which updates the references.  2. `git reflog expire --expire=now --all`: This command removes all references that are older than the current time. The `--expire` option is used to set the expiration time, and the `--all` option is used to expire all references.  3. `git gc --prune=now`: This command runs a garbage collection on the Git repository. The `--prune` option is used to remove unreachable objects, and the `--prune=now` option is used to prune objects immediately.  These commands contribute to dereferencing and garbage collecting objects in Git by removing unused references and objects, which can help to free up disk space and improve Git's performance.", "a3958ff1-f00c-4553-a52c-6757298037f3": "The name of the repository being built in this context, as represented by the environment variable `BITBUCKET_REPO_FULL_NAME`, is {% raw %}`${{ github.repository_owner }}`{% endraw %}/{% raw %}`${{ github.repository }}`{% endraw %}.", "163ba60e-1cbd-48a5-9db5-2af1c9d6cb11": "After fixing the number of results reported by queries, you can update the configuration to expand the analysis to cover more code or run more queries by following these steps:  1. For {% data variables.product.prodname_codeql %} analysis, you can add more queries to the analysis by modifying the `queries` section in the `codeql.yml` file. For more information, see \"Adding queries to the analysis.\"  2. For {% data variables.product.prodname_codeql %} analysis, you can modify the `paths` and `paths-ignore` sections in the `codeql.yml` file to analyze more code. For more information, see \"AUTOTITLE.\"  3. For {% data variables.product.prodname_codeql %} analysis, you can modify the `dataflow-paths` section in the `codeql.yml` file to analyze more dataflow paths. For more information, see \"AUTOTITLE.\"  4. For {% data variables.product.prodname_codeql %} analysis, you can modify the `exclude-paths` section in the `codeql.yml` file to exclude less important code from analysis. For more information, see \"AUTOTITLE.\"  5. For {% data variables.product.prodname_codeql %} analysis, you can modify the `exclude-queries` section in the `codeql.yml` file to exclude less important queries from analysis. For more information, see \"AUTOTITLE.\"  6. For {% data variables.product.prodname_codeql %} analysis, you can modify the `exclude-dataflow-paths` section in the `codeql.yml` file to exclude less important dataflow paths from analysis. For more information, see \"AUTOTITLE.\"  7. For {% data variables.product.prodname_codeql %} analysis, you can modify the `exclude-dataflow-paths-from-output` section in the `codeql.yml` file to exclude less important dataflow paths from the output. For more information, see \"AUTOTITLE.\"  8. For {% data variables.product.prodname_codeql %} analysis, you can modify", "a6f213f6-81c0-48c7-b0bb-f5d14341f6c0": "The `image` key in GitLab CI/CD syntax for databases and service containers specifies the container image to be used for the job. This allows for the use of pre-built images with specific dependencies or tools, making it easier to set up and run the job. For example:  ```yaml container-job:   variables:     POSTGRES_PASSWORD: postgres     POSTGRES_HOST: postgres     POSTGRES_PORT: 5432   image: node:10.18-jessie   services:     - postgres   script:     # Performs a clean installation of all dependencies     # in the `package.json` file     - npm ci     # Runs a script that creates a PostgreSQL client,     # populates the client with data, and retrieves data     - node client.js   tags:     - docker ```  In this example, the `image` key is used to specify the `node:10.18-jessie` image, which contains Node.js version 10.18 and the Jessie distribution. This allows for the use of Node.js and its dependencies without having to install them separately.  The `container` key in {% data variables.product.prodname_actions %} syntax for databases and service containers serves a similar purpose. It specifies the container image to be used for the job, allowing for the use of pre-built images with specific dependencies or tools. For example:  ```yaml jobs:   container-job:     runs-on: ubuntu-latest     container: node:10.18-jessie      services:       postgres:         image: postgres         env:           POSTGRES_PASSWORD: postgres      steps:       - name: Check out repository code         uses: {% data reusables.actions.action-checkout %}        # Performs a clean installation of all dependencies       # in the `package.json` file       - name: Install dependencies         run: npm ci        - name: Connect", "aa8d2a74-fd57-4b38-8172-e994cbd2471d": "To call core commands in {% data variables.product.prodname_cli %} programmatically without prompting for user input, you can supply necessary information explicitly via arguments. For example, to create an issue programmatically, you can use the `gh issue create` command with the `--title` and `--body` arguments to specify the title and body of the issue.  To fetch data programmatically, many core commands support the `--json` flag. This flag returns the output in JSON format, which can be easily parsed and manipulated in scripts. For example, to list all the repositories in a specific organization, you can use the `gh repo list` command with the `--json` flag to return the output in JSON format.  Here's an example of how to fetch all the repositories in a specific organization programmatically:  ```shell gh repo list --json=repository --filter=organization:my-org --jq '.[] | {name: .name, url: .html_url}' ```  In this example, we're using the `jq` command to filter and transform the JSON output. The `--filter` flag is used to filter the output based on specific criteria, and the `--jq` flag is used to apply a `jq` script to the output.  By supplying necessary information explicitly via arguments and using flags like `--json` and `--jq`, you can avoid prompting for user input and automate tasks using {% data variables.product.prodname_cli %}.", "7819fcd4-1e6c-4139-8bdc-7fed04229df8": "The recommended approach for configuring access to actions on {% data variables.product.prodname_dotcom_the_website %} is to enable automatic access to all actions from {% data variables.product.prodname_dotcom_the_website %} by using {% data variables.product.prodname_github_connect %} to integrate {% data variables.product.product_name %} with {% data variables.product.prodname_ghe_cloud %}. This approach provides automatic access to all actions without the need for manual downloads or syncing. However, if stricter control over which actions are allowed in your enterprise or if you do not want to allow outbound connections to {% data variables.product.prodname_dotcom_the_website %}, you can manually download and sync actions onto your enterprise instance using the provided instructions. The main difference between these approaches is the level of control and customization over which actions are allowed and how they are accessed.", "e6ad0e75-f68e-413d-94f7-765a1595160d": "Automatic caching in between jobs of different workflows is not supported. The `audit` command is only supported when using an organization account, but the `dry-run` and `migrate` commands can be used with an organization or user account. These are the limitations of migrating processes automatically from GitLab pipelines to GitHub Actions with GitHub Actions Importer. Manual migrations are required for masked project or group variable values and artifact reports.", "130ef336-44fb-41d9-812f-b159a2c232d2": "The recommended way to extract metadata, such as tags and labels, for a Docker image in GitHub Actions using the `metadata-action` is by using the `docker/metadata-action` provided by Docker in their official GitHub Actions repository. This action can be used to extract metadata, such as tags and labels, for a specific Docker image by passing the image's namespace and name as the `images` parameter. The metadata is then outputted as a JSON object that can be accessed in subsequent steps using the `steps.meta.outputs.TAGS` syntax. This action is recommended over manually extracting metadata as it provides a more efficient and standardized way of handling metadata in GitHub Actions.", "a0e577ae-ff29-4e43-92bc-bada9a438f0a": "To manage the `GITHUB_TOKEN` and ensure it has the minimum required permissions, you should follow these best practices:  1. Set the default permission for the `GITHUB_TOKEN` to read access only for repository contents. 2. Increase permissions for individual jobs within the workflow file as required. 3. Periodically review registered secrets to confirm they are still required and remove those that are no longer needed. 4. Rotate secrets periodically to reduce the window of time during which a compromised secret is valid.  Granting excessive permissions to the `GITHUB_TOKEN` can have serious consequences, as it can give attackers access to sensitive information and resources. For example, if the `GITHUB_TOKEN` has write access to repositories, an attacker can push malicious code, delete files, or create new repositories. Additionally, if the `GITHUB_TOKEN` has access to secrets, an attacker can steal sensitive information or impersonate other users. Therefore, it's essential to grant the minimum required permissions to the `GITHUB_TOKEN` and review permissions regularly to ensure they remain appropriate.", "3ac77f08-ef1f-4499-9552-296152f9758a": "To safeguard against data loss or service disruptions for developers using {% data variables.product.company_short %}, it is strongly recommended to establish a disaster recovery plan. This can be achieved by deploying and configuring a Linux or Unix host system with {% data variables.product.prodname_enterprise_backup_utilities %} to back up the instance's configuration and user data. Additionally, a passive replica instance can be configured to fail over in the event of a system or network failure. For more information, refer to the documentation on {% data variables.product.prodname_docs %} and the {% data variables.product.prodname_roadmap %} in the `github/roadmap` repository.", "ff85ed5b-9e10-4145-8cf5-672411137370": "The command to push a branch to the origin remote on GitHub using Git is:  ```shell git push -u origin BRANCH ```  Replace `BRANCH` with the name of the branch you're working on.  Note: If you're using GitHub Enterprise Cloud (GHEC) or GitHub Enterprise Server (GHES), you can replace `origin` with the name you used when adding the remote, which is typically `https://github.com/USER/REPOSITORY.git` for GHEC or `https://HOSTNAME/USER/REPOSITORY.git` for GHES, where `USER` is your GitHub username or organization name and `REPOSITORY` is the name of your repository. If you're not using GHEC or GHES, you can skip this step.", "b4a7e8c1-1274-46e0-b1c9-64719d073847": "To use the Octokit SDK to find and redeliver failed webhook deliveries, you can follow these steps:  1. Create a new instance of the Octokit class and pass your personal access token (PAT) as an argument.  ```javascript const octokit = new Octokit({   auth: process.env.TOKEN, }); ```  2. Define a function to check and redeliver failed webhook deliveries.  ```javascript async function checkAndRedeliverWebhooks() {   // Get the values    const organizationName = process.env.ORGANIZATION_NAME;   const hookId = process.env.HOOK_ID;   const lastRedeliveryVariableName = process.env.LAST_REDELIVERY_VARIABLE_NAME;   const hostname = process.env.HOSTNAME || \"github.com\";   const workflowRepoName = process.env.WORKFLOW_REPO_NAME;   const workflowRepoOwner = process.env.WORKFLOW_REPO_OWNER;    // Get the last redelivery time for the current repository    const lastRedelivery = await getLastRedeliveryTime(     workflowRepoOwner,     workflowRepoName,     lastRedeliveryVariableName   );    // Get the failed webhook deliveries for the specified organization and hook ID    const failedDeliveries = await getFailedWebhookDeliveries(     organizationName,     hookId,     lastRedelivery   );    // Redeliver the failed webhook deliveries    for (const delivery of failedDeliveries) {     await redeliverWebhookDelivery(delivery, hostname);   }    // Save the current time as the last redelivery time for the current repository    await saveLastRedeliveryTime(     workflowRepoOwner,     workflowRepoName,     lastRedeliveryVariableName,     new Date()   ); } ```  3. Define a function to get the last redelivery time for a repository.  ```javascript", "af40ecb2-ddae-4c3b-b4ec-3cab3bfbab8d": "To view and manage reported content on GitHub, you need to enable reported content for the repository. Then, you can access the \"Abuse reports\" list to view all reports and navigate directly to each reported comment on GitHub. You can track, triage, and respond to reports of disruptive content, mark reports as resolved or unresolved, and use moderation tools to manage reported content. Additionally, you can use third-party tools like GitHub Apps and services to automate moderation workflows and improve the overall moderation experience.", "9859af4c-c47a-41c8-8374-ad4f3bf4ae19": "The number of pull requests in a repository can significantly impact the timeline for migrating from Bitbucket Server to {% data variables.product.prodname_gHE_cloud %} using {% data variables.product.prodname_importer_proper_name %}. Each pull request needs to be migrated, and the more pull requests there are, the longer the migration process will take. This is because the migration tool needs to transfer all the necessary data, such as comments, reviews, and merge commits, for each pull request. Therefore, if a repository has a large number of pull requests, it may take longer to migrate than a repository with fewer pull requests. Organizations should consider the number of pull requests in their repositories when planning their migration timeline.", "4d43919d-3106-4195-9d0d-686f57493d19": "To launch a virtual machine on Azure for {% data variables.product.prodname_ghe_server %}, you need a software license, an Azure account capable of provisioning new machines, and the Azure command line interface (CLI) version 2.0 installed. Additionally, you should review the hardware considerations and determine the machine type that best fits your organization's needs. It's also recommended to review the minimum resource requirements for {% data variables.product.prodname_ghe_server %}.", "55c37369-baac-40ab-99b1-eddb59ff752b": "Without further context, it's unclear why the \"User info\" heading is highlighted in dark orange. It could be a visual cue to draw attention to the user's current status, indicating that they are currently active. However, without more information, it's impossible to say for certain.", "a880bb1f-6427-46cc-bb49-b15087de0e7c": "To ensure that they do not have to contact support personnel in order to delete their data, as mentioned in the context information, users should look for a self-service option within the R data platform. This option could be in the form of a user interface or a command within the software that allows users to delete their own data. The context information suggests that this feature should be readily available and easily accessible, without the need for external assistance. Users should check the platform's documentation or help resources to locate this feature and familiarize themselves with the process. If they are unable to find a self-service option, they may need to contact support personnel for assistance.", "7df251fa-3f11-419c-a0bd-db1186bf48a9": "According to the context information provided, passkeys stored on FIDO2 hardware security keys are \"device-bound,\" meaning they can only be used on a single authenticator. However, these passkeys have the advantage of being portable and can be attached to other devices in a variety of ways (USB, NFC or Bluetooth). This makes them a convenient option for users who want to securely sign in to multiple devices without the need for a separate passkey for each device.  The context also mentions that some passkey providers may offer device-bound passkeys as an option during passkey creation, while others may not offer the choice between device-bound and synced passkeys. This suggests that users should check with their passkey provider to see if they offer device-bound passkeys.  As for which devices and operating systems support passkeys, the context provides links to resources that can provide more information. The Passkeys.dev documentation and the CanIUse documentation both offer insights into device support for passkeys. Users can refer to these resources to determine whether their specific device and operating system support passkeys.  In summary, passkeys stored on FIDO2 hardware security keys offer the advantage of being portable and device-bound, making them a convenient option for users who want to securely sign in to multiple devices. Users should check with their passkey provider to see if they offer device-bound passkeys, and can refer to resources like Passkeys.dev and CanIUse for information on device support for passkeys.", "f6614733-26a8-4f27-8777-2bf200967663": "Custom actions can be created in {% data variables.product.prodname_dotcom %} by writing custom code that interacts with the repository using {% data variables.product.prodname_dotcom %}'s APIs or any publicly available third-party API. These actions can be written as Docker container, JavaScript, or composite actions. Docker container actions package the environment with the {% data variables.product.prodname_actions %} code, making them more consistent and reliable units of work for the consumer of the action. JavaScript and composite actions can run directly on a machine, and all actions require a metadata file to define inputs, outputs, and environment variables. To share actions, the repository must be public (or internal for enterprise-specific actions).", "0e51752e-b904-4a5b-8a30-7edc471b22cb": "Through the {% data variables.product.prodname_student_leader_program %} program training, students can gain leadership skills such as public speaking, technical writing, and software development. These skills can be used to build a strong technical community by allowing students to teach others, create new opportunities for their student community, and organize events and activities that promote learning and collaboration. By positioning their institution within a global community of student leaders, students can also foster connections with other student leaders, learn about best practices and resources, and contribute to a larger network of like-minded individuals. This can help to raise the profile of their institution and attract more students to their technical community.", "5daafc32-d5f5-4bf3-85ea-f6d8e42f2490": "To sign into {% data variables.product.prodname_classroom_with_url %} and reuse assignments from a classroom, you need to follow these steps:  1. Go to the URL provided for {% data variables.product.prodname_classroom_with_url %}. 2. Enter your GitHub username and password to log in. 3. Once you're signed in, you'll be able to access the classroom and reuse assignments as described in the context information.  If you don't have a GitHub account, you'll need to create one before you can sign in to {% data variables.product.prodname_classroom_with_url %}.", "1be681f3-2697-408e-81fe-a2a0da6793b7": "To view and manage pending changes to your organization's plan on GitHub, follow these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. Click on \"Billing\" in the left-hand menu. 3. Click on \"Review pending changes\" to view any pending changes to your organization's plan. 4. If you want to cancel a pending change, click on the \"Cancel\" button next to the change you want to cancel. 5. Confirm that you want to cancel the pending change by clicking on \"Cancel pending changes\" in the confirmation dialog box.  Note: These steps are based on the context information provided. If you're not sure how to access your organization's settings or the \"Billing\" section, you may need to consult additional resources or contact GitHub support for assistance.", "6b3a2ef8-f8b2-4ab0-97e6-e9fae882df05": "To generate a unit test for a specific code segment in {% data variables.product.prodname_vs %} using {% data variables.product.prodname_copilot_chat %}, you can follow these steps:  1. Open the file containing the code segment you want to generate a unit test for in {% data variables.product.prodname_vs %}. 2. Highlight the code segment you want to generate a unit test for. 3. Ask {% data variables.product.prodname_copilot_chat %} to generate a unit test for the selected code segment by typing a request like, \"Write a unit test for this code\".  Alternatively, you can also ask {% data variables.product.prodname_copilot_chat %} to generate a unit test for the entire file by asking, \"What unit tests should I write for this file?\".  {% data variables.product.prodname_copilot_chat %} will then provide you with a unit test that you can use to test the functionality of the code segment.  Note: {% data variables.product.prodname_copilot_chat %} is not a replacement for writing unit tests yourself, as it may not fully understand the context and requirements of your code. It's still important to write unit tests that cover all the necessary scenarios and edge cases.", "4263fcff-f811-420f-87e7-28f4d4652146": "According to the context information provided, the \"app manager\" role in an organization does not give a person the ability to install a {% data variables.product.prodname_github_app %} in the organization.", "24368c21-e366-489e-b1c9-e77beb03f882": "According to the context information provided, we recommend ensuring your snapshots are not kept in a subdirectory of the {% data variables.product.prodname_enterprise_backup_utilities %} installation directory to avoid inadvertently overwriting your data directory when upgrading {% data variables.product.prodname_enterprise_backup_utilities %} versions. It's essential to keep your snapshots in a location that supports symbolic and hard links, as mentioned in the context information. By following this recommendation, you can prevent any potential data loss or corruption during the upgrade process.", "3fa06efb-3f86-47b5-851c-c2995b9870ef": "To obtain the ID of the user who triggered the CI pipeline in the context provided, you can use the variable `GITLAB_USER_ID` which is set to the ID of the user who triggered the pipeline. This variable is provided by GitHub Actions and can be accessed using the YAML syntax for variable substitution. Therefore, you can use this variable in your CI pipeline script to perform actions based on the user who triggered the pipeline.", "dce0262d-8ce8-4a51-9efa-19aa6d3ff39b": "Based on the context provided, the topics related to the API in the context of {% data variables.product.product_name %} are \"API\" and \"meta\". To access these topics using the REST API, you can refer to the documentation provided in the context, which is the \"REST API\" section. This section should provide detailed information on how to make API requests, including endpoints and request parameters. You can also use the \"topics\" field in the context to navigate to specific sections of the documentation related to these topics. Additionally, the \"redirect_from\" field in the context may provide alternative URLs for accessing the documentation. If you need further assistance, please refer to the official documentation or contact the support team for {% data variables.product.product_name %}.", "f74bdd60-b6b3-452f-9bef-d44d4a875488": "Without the \"{% ifversion ghec %}\" statement, \"AUTOTITLE\" appears to be a command or instruction that needs to be followed. However, without further context, it's unclear what this command does or how to execute it.  With the \"{% ifversion ghec %}\" statement, \"AUTOTITLE\" seems to be a conditional statement that executes only in the context of GitHub Enterprise Cloud (GHEC). This suggests that \"AUTOTITLE\" is a feature or configuration option specific to GHEC, and its behavior may differ from that in other GitHub Enterprise Server (GHES) or GitHub.com environments.  In summary, the presence of the \"{% ifversion ghec %}\" statement adds context and clarifies the scope of the \"AUTOTITLE\" command. Without this statement, the meaning of \"AUTOTITLE\" is less clear.", "c29344bc-9949-4e85-9838-d9e59085a193": "To assign a custom label to a self-hosted runner at the repository level, follow these steps:  1. Go to the repository for which you want to assign the label. 2. Click on the \"Actions\" tab in the left-hand menu. 3. Click on \"Self-hosted runners\" in the \"Runner groups\" section. 4. Click on the runner for which you want to assign the label. 5. Scroll down to the \"Labels\" section and click on the \"Edit\" button. 6. Enter the name of the custom label you want to assign in the \"Add labels\" field. 7. Click on the \"Save\" button to save the changes.  The custom label will now be assigned to the selected self-hosted runner.", "bf657e4d-6d6d-48f0-9634-5f8d620edfc0": "Base64 encoding can be used to store small binary blobs as secrets in GitHub by encoding the binary file into a Base64 string using a command like `base64 -i file.bin -o file.base64` on MacOS or `base64 -w 0 file.bin > file.base64` on Linux. The Base64 string can then be stored as a secret in GitHub using the `gh secret set` command. To access the Base64 string from a runner, it can be piped to `base64 --decode` to decode it back into the original binary format. This is a useful technique for storing small binary blobs as secrets in GitHub, as it allows them to be easily accessed and used in workflows without having to store the actual binary file in a public repository. However, it should be noted that Base64 encoding is not a substitute for actual encryption, as it only converts binary to text and does not provide any additional security.", "32eadb99-f67f-47f1-a08c-6c901e6f3321": "The value of the variable `BITBUCKET_PROJECT_KEY` in this context is `{% raw %}${{ github.repository_owner }}`{% endraw %}. This is because the context provides a mapping for this variable, where it is replaced with the value of the `github.repository_owner` variable, which is the owner of the GitHub repository associated with this workflow. In the context of Bitbucket, this variable would be equivalent to the project key, which is a unique identifier for the project in Bitbucket.", "dd341023-cc02-4ec6-9078-49a0943653c0": "In the context of {% data variables.product.prodname_actions %}, secrets are encrypted variables that store sensitive information such as passwords, tokens, and keys. They are created at the organization, repository, or repository environment level in {% data variables.product.prodname_dotcom %}. To access a secret in a workflow, it must be explicitly included as an input or environment variable in the workflow file. Required reviewers can also control access to secrets stored at the environment level by enabling approval for access. It's important to follow naming conventions for secrets to avoid conflicts and ensure that {% data variables.product.prodname_dotcom %} redacts secrets in logs.", "eddccc63-f03c-427c-af34-1f455b2aa030": "To merge a pull request into a branch while squashing commits and deleting the local and remote branch using the \"gh pr merge\" command, you can use the following syntax:  ```shell gh pr merge <PR_NUMBER> --squash --body \"my squash commit\" --delete-branch ```  Replace `<PR_NUMBER>` with the number of the pull request you want to merge. The `--squash` flag squashes the commits into a single commit, and the `--body` flag allows you to provide a commit message for the squashed commit. The `--delete-branch` flag deletes both the local and remote branch after the merge is complete.  The \"AUTOTITLE\" directive is used to automatically generate a title for a document based on its contents. It's typically used in Markdown files to create a title that accurately reflects the content of the document.  To implement \"AUTOTITLE\" using the provided context, you can add the following line at the top of your Markdown file:  ```markdown --- AUTOTITLE: --- ```  Replace the text inside the quotes with a descriptive title for your document. When you save and preview the file, the title will be automatically generated based on the contents of the document.  For example, if your document is about \"How to use GitHub Pages for hosting a website\", you can use the following \"AUTOTITLE\" directive:  ```markdown --- AUTOTITLE: How to use GitHub Pages for hosting a website --- ```  This will generate a title that accurately reflects the content of your document, making it easier for readers to understand what your document is about.", "bfb2d65b-f598-4f33-9be9-aa1a8ec4f0b0": "To verify that a workflow run has completed successfully on GitHub Pages, you can follow these steps:  1. Go to your repository on GitHub. 2. Click on the \"Settings\" tab in the repository menu. 3. Click on \"Pages\" in the left-hand menu. 4. Check the \"Branch\" dropdown menu to ensure that the branch associated with the workflow is selected. 5. Scroll down to the \"Workflow runs\" section to view the status of the workflow run. 6. Click on the workflow run to view its details, including the status, duration, and logs. 7. If the workflow run is successful, you should see a green checkmark next to the run. If the run is still in progress, you will see a yellow circle with a white arrow. If the run failed, you will see a red circle with a white X. 8. To view the site generated by the workflow, click on the \"View site\" button in the \"Workflow runs\" section.  If you are using GitHub Actions, you can also view the status of the workflow run in the Actions tab of your repository. From there, you can click on the workflow run to view its details and logs.  If you are using a third-party CI/CD tool, you may need to consult the documentation for that tool to view the status of the workflow run.", "bd8a227b-e699-4215-a131-4983280bcb65": "The purpose of configuring a no-reply email address in the SMTP server settings is to use it in the From and To fields for all notification emails, and it can also be used to discard all incoming emails addressed to that email address. This is typically done to prevent unnecessary email replies and to ensure that the email address used in the notification emails is consistent and easily recognizable.", "355ce40c-fc9b-4432-9db0-140767f33dcf": "The `job` context in GitHub Actions workflows can be utilized to access information about the current job being executed. Specifically, the context provides access to the following information:  1. `id`: The unique identifier for the current job. 2. `name`: The name of the current job. 3. `steps`: An array containing the steps that have been executed in the current job. 4. `services`: An object containing information about the services that have been defined for the current job. 5. `started_at`: The timestamp when the current job started executing. 6. `completed_at`: The timestamp when the current job completed executing. 7. `status`: The current status of the current job, which can be `queued`, `in_progress`, `completed`, `cancelled`, or `failed`. 8. `outputs`: An object containing the outputs that have been defined for the current job. 9. `result`: The result of the current job, which can be `success`, `failure`, `cancelled`, or `skipped`.  This context can be used to perform various tasks, such as accessing the output of a previous step, accessing the environment variables defined for the current job, and accessing the ports that have been mapped for the current job.  Example usage of the `job` context:  1. Accessing the output of a previous step:  ```yaml name: My Workflow  jobs:   build:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: {% data reusables.actions.action-checkout %}       - name: Build         run: |           echo \"Building...\"           echo \"Build complete!\" >> $GITHUB_OUTPUT       - name: Test         uses: {% data reusables.actions.action-checkout %}         with:           repository: octocat/hello-world           ref: v1.2.3       - name: Access Output         run: |           echo \"Output of previous step: ${{ steps.Build.outputs.my_output }}\" ```", "6d9419ed-b6d2-4002-a2fc-505718e3fa19": "To ensure that your PowerShell tests are successful in a {% data variables.product.prodname_dotcom %}-hosted runner environment, you should follow these specific steps or commands:  1. Install any necessary dependencies using the `Install-Module` command in your PowerShell script. For example:     ```powershell    Install-Module -Name Azure.Storage.Blobs -Scope CurrentUser -Force    ```     This command installs the Azure.Storage.Blobs module, which is required for working with Azure Blob Storage.  2. Configure your tests to work in the {% data variables.product.prodname_dotcom %}-hosted runner environment. For example, you may need to specify the location of your test files or the path to your dependencies.     ```powershell    $testPath = \"$(Build.SourcesDirectory)/tests\"    $dependencyPath = \"$(Build.SourcesDirectory)/dependencies\"     Invoke-Pester -Path \"$testPath\\Unit.Tests.ps1\" -DependencyPath \"$dependencyPath\"    ```     This command executes the tests defined in the `Unit.Tests.ps1` file, located in the `tests` directory, and specifies the path to the dependencies, which are located in the `dependencies` directory.  The difference between PowerShell system modules and PowerShell add-on modules in a {% data variables.product.prodname_dotcom %}-hosted runner is that PowerShell system modules are installed by default and are located in a specific directory, while PowerShell add-on modules are installed separately and are located in a different directory.  To locate and access these modules in your PowerShell scripts, you can use the `Import-Module` command to import the necessary modules. For example:  ```powershell Import-Module -Name Azure.Storage.Blobs  $connectionString = \"DefaultEndpointsProtocol=https;AccountName=myaccount;AccountKey=mykey;EndpointSuffix=core.windows.net\" $containerName = \"mycontainer\" $blobName = \"myblob\"  $blob = Get-AzStorageBlob -Container $containerName -Blob $blobName -ConnectionString $connectionString  Write", "3707e0f9-b50f-4dcc-9ca2-a5cea5d3f8a3": "A webhook is a mechanism that allows software systems to notify other systems when a specific event occurs. It works by sending an HTTP request to a specified URL with data about the event. This allows the receiving system to take immediate action in response to the event, without the need for polling or manual intervention.  For example, a webhook could be used to automatically deploy code to a production server when a pull request is merged. The receiving server could receive the webhook delivery, extract the necessary information (such as the repository, branch, and commit SHA), and then deploy the code to the production environment.  On {% data variables.product.company_short %}, webhooks can be used to subscribe to a variety of events, such as push events (when code is pushed to a repository), pull request events (when a pull request is opened, closed, or merged), and organization events (when a new member is added to an organization).  When a webhook delivery is received by {% data variables.product.company_short %}, it contains information about the event, such as the repository, branch, and commit SHA. The receiving server can then extract this information and take appropriate action, such as deploying code, triggering a CI pipeline, or sending a notification.  For example, a server could receive a webhook delivery when a pull request is merged, extract the necessary information, and then trigger a CI pipeline on an external CI server (such as Jenkins or CircleCI) to build and test the code. This allows for a more streamlined and automated development process, without the need for manual intervention.", "18f63b34-bae8-4bda-8999-dfc0f5870992": "The name of the GitHub workflow being executed in this context can be determined using the `>` symbol, which represents the workflow being executed. In this context, the value of `>` is `$GITHUB_WORKFLOW` and `$GITHUB_WORKFLOW` is a variable that contains the name of the workflow being executed. Therefore, we can use this variable to determine the name of the GitHub workflow being executed in this context.    If the context didn't provide the value of `>` or `$GITHUB_WORKFLOW`, then we wouldn't be able to determine the name of the GitHub workflow being executed in this context, as this information is necessary to identify the workflow being executed. In this case, we would say that we weren't given enough information to give an answer.", "51bf7d82-6594-459b-ac71-4c92c66a02e0": "People with write, maintain, or admin permissions in a repository are automatically notified about new {% data variables.product.prodname_dependabot_alerts %} by default. However, the owner or admin of the repository can also choose to make {% data variables.product.prodname_dependabot_alerts %} visible to additional people or teams working on that repository, regardless of their permission level. This allows for broader communication and collaboration around security vulnerabilities and updates.", "7c5ad55c-ce65-43b2-8975-4a1bcc59e471": "In {% data variables.product.prodname_classroom %}, the default repository prefix for an individual assignment is the assignment title. The repository name for a student with a {% data variables.product.product_name %} username of @octocat would be `assignment-1-octocat` if the assignment is named \"assignment-1\".    Choosing a visibility for assignment repositories  In {% data variables.product.prodname_classroom %}, you can choose whether assignment repositories are private or public.  - Private: Only the student and the instructor can see the repository. - Public: The repository is visible to anyone with the URL.  To choose the visibility, click the radio button next to **Private** or **Public**.    Inviting students to an assignment  To invite students to an assignment, click the **Invite students** button in the {% octicon \"repo\" aria-hidden=\"true\" %} **Assignments** tab.    Next steps  Now that you've created an assignment, you can add instructions, add files, and add a grading rubric.    Further reading  For more information about {% data variables.product.prodname_classroom %}, see the {% data variables.product.prodname_classroom %} documentation.    How do I invite students to an assignment in {% data variables.product.prodname_classroom %}?", "6630ec2d-acf1-4a6e-be5c-e435e2fe6bd3": "The purpose of the idle timeout feature in codespaces is to automatically stop a codespace after a period of inactivity to conserve resources and prevent unnecessary costs. However, this feature only applies to the person who started the codespace and does not affect collaborative work done through Live Share. Collaborators can continue working in the codespace even if the person who started it becomes inactive due to the idle timeout. Therefore, it's important for the person who started the codespace to remain active or communicate with their collaborators to avoid any unintended interruption of the Live Share session.", "f7f0b481-2efa-45c8-a996-fddb223084dc": "To enable or disable security and analysis features for all existing repositories in {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to your account settings by clicking on your profile picture in the top right corner of the page and selecting \"Settings\" from the drop-down menu. 2. Click on \"Security and analysis\" in the left-hand menu. 3. Under \"Code security and analysis\", you can see a list of available features. To the right of each feature, you'll see a toggle switch. 4. To disable all security and analysis features for all your existing repositories, click on the \"Disable all\" button at the top of the page. 5. To enable all security and analysis features for all your existing repositories, click on the \"Enable all\" button at the top of the page. 6. Optionally, you can also choose to enable or disable these features by default for new repositories that you create. To do this, toggle the switch next to the feature you want to enable or disable. 7. Click \"Disable FEATURE\" or \"Enable FEATURE\" to confirm your choice.  Note that some security and analysis features are enabled by default. You can find more information about these features in the \"About managing security and analysis settings\" section.  To learn more about managing security and analysis settings for individual repositories, refer to the \"Managing security and analysis settings for a repository\" guide. To view the security log for all activity on your personal account, refer to the \"Viewing the security log\" guide.", "59bb3ec9-82dd-4ac5-9e8c-a066f3cdcf91": "The method of adding a member to an organization impacts the process of removing them. If a member was added to an organization manually, they must be removed manually by the organization owner or an administrator. Unassigning them from the {% data variables.product.prodname_emu_idp_application %} application on your IdP will suspend the user but not remove them from the organization. On the other hand, if a user became a member of an organization because they were added to IdP groups mapped to one or more teams in the organization, removing them from all of the mapped IdP groups associated with the organization will remove them from the organization.  For example, let's say that John Doe was added to an organization called \"Acme Corp\" manually by the organization owner. If John decides to leave the company, the organization owner or an administrator must manually remove John from the organization. However, if Jane Doe was added to \"Acme Corp\" because she was added to an IdP group mapped to a team in the organization, removing her from that IdP group will also remove her from the organization.  As for authentication methods, {% data variables.enterprise.prodname_managed_users %} must authenticate through your IdP. If your enterprise is configured for SAML authentication, a {% data variables.enterprise.prodname_managed_user %} can access your enterprise by visiting their IdP application portal. If your enterprise is configured for OIDC authentication, a {% data variables.enterprise.prodname_managed_user %} can access your enterprise by using the login page on {% data variables.product.prodname_dotcom_the_website %}. To initiate authentication through the organization or enterprise's page on {% data variables.location.product_location %}, the {% data variables.enterprise.prodname_managed_user %} simply needs to click the \"Sign in\" button on the organization or enterprise's page. This will redirect them to their IdP's login page, where they can enter their credentials to authenticate. After authentication, they will be redirected back to {% data variables.product.prodname_dotcom_the_website %} and will be able to access the organization or enterprise they are a member of.", "8ae42f30-f140-4e04-be66-b8826c225502": "To configure Azure Blob Storage account credentials in the {% data variables.product.prodname_cli %} for {% data variables.product.prodname_ghe_server %}, follow these steps:  1. Log in to your Azure account and navigate to your Azure Blob Storage account.  2. Click on the \"Access keys\" tab.  3. Copy the \"Account name\" and \"Primary access key\" values.  4. In your terminal, run the following command:     ```shell copy    gh config set azure.blob.account-name <ACCOUNT_NAME>    gh config set azure.blob.access-key <ACCESS_KEY>    ```     Replace `<ACCOUNT_NAME>` and `<ACCESS_KEY>` with the values you copied in step 3.  5. Verify that the credentials are correctly configured by running the following command:     ```shell copy    gh auth list    ```     You should see your Azure Blob Storage account listed under \"Authorized organizations\".  6. You can now generate a migration script using the `gh gei gene` command, as described in the previous answer.", "58adc0d0-9da1-4735-8ab5-dc48e2a853bb": "Yes, based on the context provided, we can identify that UI elements that can be interacted with in IBE include buttons, checkboxes, dynamic text, lists and list items, and location. An example of a UI element that can be interacted with in IBE is the \"Pull request\" button, which should be formatted in bold and described as \"click **Pull request**\" instead of \"press the Pull request button.\"", "2597d9bc-23a5-49f9-9342-f68305d1986d": "The `groupId` and `artifactId` fields in the _pom.xml_ file for Maven packages serve as unique identifiers for the package. The `groupId` identifies the organization or group that owns the package, while the `artifactId` identifies the specific package within that group. Both fields are used by Maven and package registries to link the package to its corresponding metadata and dependencies.", "c8c09e0b-7853-4ebc-9142-716b6659bea6": "To pin a discussion to a specific category in a repository or organization on GitHub, follow these steps:     a. Go to the repository or organization where the discussion is located.     b. Click on the Discussions tab.     c. Locate the discussion you want to pin.     d. Click on the discussion to open it.     e. In the right-hand sidebar, click on the three dots (...) next to the discussion title.     f. Select \"Pin discussion\" from the drop-down menu.     g. In the pop-up window, select the category you want to pin the discussion to.     h. Click on \"Pin to category\" to confirm.  Answer:  1. To unpin a discussion from a specific category in a repository or organization on GitHub, follow these steps:     a. Go to the repository or organization where the discussion is located.     b. Click on the Discussions tab.     c. Locate the discussion you want to unpin.     d. Click on the discussion to open it.     e. In the right-hand sidebar, click on the three dots (...) next to the discussion title.     f. Select \"Unpin discussion\" from the drop-down menu.     g. Read the warning message and click on \"Unpin discussion\" to confirm.     h. The discussion will no longer be pinned to the selected category.", "cbdb16b4-63c5-48b8-95b3-5e1ec2eb8881": "To enable runner diagnostic logging in a repository containing a workflow, you need to set the `ACTIONS_RUNNER_DEBUG` secret or variable to `true`. This will generate two additional log files, the runner process log and the worker process log, which provide additional information about how the runner is executing the job. The resulting logs can be downloaded from the log archive of the workflow run, which can be accessed by following the instructions provided in the context information.", "a964656b-a39c-4c0f-b635-3f7ce229b736": "No, there is no telephone number or facsimile number provided for the GitHub Security Incident Response Team (SIRT).", "412c39ba-2ec9-4615-a5e2-ed133b8f0022": "To test your webhook locally using smee.io and node.js, follow these steps:  1. Make sure that you have smee.io and node.js installed on your computer or codespace. 2. Create a new file in your preferred code editor and copy the code provided in the context information. 3. Save the file with a `.js` extension, such as `webhook-handler.js`. 4. In a separate terminal window, navigate to the directory where your code is saved. 5. Run the following command to start a local server on your computer or codespace:     ```shell copy    node webhook-handler.js    ```  6. Trigger your webhook, such as by creating a new issue in your repository. 7. Navigate to your webhook proxy URL on smee.io. You should see an event that corresponds to the event that you triggered. 8. In the terminal window where you ran `node webhook-handler.js`, you should see a message corresponding to the event that was sent. 9. In both terminal windows, enter Ctrl+C to stop your local server and stop listening for forwarded webhooks.  If you encounter any issues during the testing process, refer to the troubleshooting steps provided in the context information.", "0b359aa7-5ff1-4a8a-a07d-5bfbd565d3c1": "In the context of workflow jobs, the statement \"This check was skipped.\" indicates that a specific condition or requirement for executing a job was not met, causing the job to be skipped instead of being run. This could be due to a variety of reasons, such as a failed pre-requisite job, a missing environment variable, or a failed condition check. The skipped job will still be listed in the workflow run, but it will not have a status of \"completed\" or \"failed.\" Instead, it will have a status of \"skipped.\"", "b114026e-66a4-4a6b-b386-706c798c578c": "If the context is from a GitHub Enterprise Server (GHES) or GitHub Advanced Security (GHAS) organization, allowing access to components in a private repository from repositories in the same enterprise provides a wider scope for access compared to allowing access from repositories in the same organization. This is because a GitHub Enterprise Server (GHES) or GitHub Advanced Security (GHAS) organization can have multiple affiliated organizations, and allowing access from repositories in the same enterprise includes all affiliated organizations, while allowing access from repositories in the same organization only includes repositories within the specific organization. In contrast, if the context is from a GitHub.com (GHAE) organization, both options provide the same scope for access, as GitHub.com (GHAE) does not support affiliated organizations.", "2800c128-2433-4ef1-b8c4-2680cef02ae6": "Without further context, it is unclear what \"AUTOTITLE\" refers to. If \"AUTOTITLE\" is referring to a specific document feature, it could potentially differ from other document features in terms of functionality based on the specific software or document type being used. More information is needed to accurately answer this question.", "20c5389d-62b2-4b53-a5ed-3ab2b551a1f1": "To ensure that you are reviewing the correct files before deleting them using Git, you should follow these steps:  1. Use Git's status command to see which files have been modified, added, or deleted in your current branch. This will help you identify which files you want to delete.  2. Use Git's diff command to compare the current branch with the previous one or a specific commit to see the changes made to the files you want to delete. This will help you confirm that you want to delete those files.  3. Use Git's checkout command to switch to the branch where the files you want to delete are located. This will allow you to review the files in their current state.  4. Use Git's rm command to delete the files you want to remove. This will stage the changes for the next commit.  5. Use Git's add command to add the deleted files to the staging area. This will prepare them for the next commit.  6. Use Git's commit command to create a new commit with a descriptive message explaining why you are deleting the files. This will record the changes in your local repository.  7. Use Git's push command to send the changes to the remote repository. This will update the remote branch with the deleted files.  By following these steps, you can ensure that you are reviewing the correct files before deleting them using Git.", "ea7d3afe-df69-4810-9bc3-2085e3098ea7": "Based on the context provided, the text suggests that there are advanced formatting features available on GitHub that can help with effective communication. However, it doesn't specify what these features are. To learn more about these features, the text suggests reading \"AUTOTITLE,\" but it doesn't provide the actual title of the resource. Without further context, it's unclear what \"AUTOTITLE\" refers to. Therefore, it's not possible to provide an answer to this question.", "52a00b70-dc44-4ce3-b300-feb9d2f44b28": "To access the traffic graph for a specific repository on GitHub, follow these steps:  1. Go to the repository you want to view the traffic graph for. 2. Click on the repository name to open it. 3. In the left sidebar, click on \"Settings\". 4. In the \"Settings\" tab, click on \"Analytics\" in the left sidebar. 5. Click on \"Traffic\" to view the traffic graph.  Alternatively, you can also access the traffic graph by following these steps:  1. Go to your GitHub profile. 2. Click on the repository you want to view the traffic graph for. 3. Click on the three dots in the top right corner of the repository page. 4. Select \"Analytics\" from the drop-down menu. 5. Click on \"Traffic\" to view the traffic graph.  I hope this helps! Let me know if you have any further questions.", "6b03c22d-cd88-413b-b5b2-b2503f35e733": "After configuring high availability replication for nodes in a cluster, you need to configure a load balancer that will accept connections from users and direct traffic to the replica nodes in the event of a failure. This is required to ensure high availability and fault tolerance for your GitHub Enterprise Server installation. The specific configuration steps for the load balancer will depend on the load balancing software you are using, but generally, you will need to add the IP addresses or hostnames of the replica nodes to the load balancer's configuration and update the load balancer's health checks to verify that the replica nodes are available and responding. It's also important to test the failover process to ensure that it works as expected and that users can seamlessly switch to the replica nodes in the event of a failure.", "70237611-87bf-42ee-8b78-0b0b98f126b4": "The node `ghes-database-node-1` in the cluster has multiple roles and provides several services. It is identified as a Consul server and a MySQL primary, as well as a Redis server, based on the configuration in the `cluster.conf` file. As a Consul server, it helps in managing and coordinating services in the cluster. As a MySQL primary, it is responsible for handling write requests and synchronizing data with other MySQL nodes in the cluster. As a Redis server, it provides in-memory data storage and retrieval services.", "9d91c3e1-0377-4bb0-89de-059a66848966": "The purpose of selecting a primary category and a secondary category for your action in the \"Release Action\" section when publishing your action to the GitHub Marketplace is to help people find your action easily. The primary category is the main category that best describes your action, while the secondary category is an optional category that provides more specific information about your action. This categorization helps users to quickly find and select the right action for their needs, making it easier for them to use and integrate your action into their workflows. To select these categories, you can follow the steps provided in the context information and select the appropriate categories from the dropdown menus provided.", "a4ba607d-b5fb-4850-bcfe-4a71e50bc117": "To lock a repository on the Enterprise Site Admin Settings page, follow these steps:  1. Go to the Enterprise Site Admin Settings page by clicking on the \"Settings\" option in the top right corner of the GitHub interface and selecting \"Enterprise\" from the drop-down menu. 2. Search for the repository you want to lock using the \"Repository search\" feature. 3. Click on the repository to open its settings page. 4. Click on the \"Admin\" tab at the top of the page. 5. Scroll down to the \"Single Repository Lock\" section. 6. Click on the \"Lock\" button to lock the repository.  Note: The steps may vary slightly depending on the specific version of GitHub being used.", "a3ec5a2f-c405-48c2-95bb-e608c3a47ee2": "The \"--additional-packs\" option in CodeQL is used to search for packs in a list of directories before the ones in the default search path. This is useful when temporarily developing a new version of a pack that also appears in the default path. By adding the directory containing the new pack to this list, CodeQL will search for the pack in that directory first, allowing the developer to test the new version without affecting the existing one in the default path. It is not recommended to override this option in a config file, as some internal actions may add this option on the fly, overriding any configured value.", "7decbffe-9c70-40ef-ab39-d9b8a0e836d6": "Commit and tag verification in {% data variables.product.product_name %} can be achieved by signing them locally using GPG, SSH, or S/MIME, depending on the user's preference and organization's requirements. If the commit or tag has a verifiable signature, it will be marked as \"Verified\" or \"Partially verified\" in {% data variables.product.product_name %}. If the signature cannot be verified, it will be marked as \"Unverified\" in {% data variables.product.product_name %}. GPG signatures offer features such as expiration and revocation, while SSH signatures are simpler to generate and can be uploaded as an existing authentication key. S/MIME signatures are typically required in larger organizations. By default, vigilant mode is not enabled, but it can be enabled to provide additional verification statuses.", "196cfd44-7482-4ca0-808e-8d23c55529d3": "To associate a package with a repository in the {% data variables.product.prodname_registry %}, you can include a `repository` field in the `package.json` file with the URL of the repository you want to associate the package with. This is optional, but it allows for better organization and easier management of your packages and repositories. Some benefits of associating a package with a repository include:  1. Clearer documentation: By linking a package to a repository, you can provide clearer documentation and examples for how to use the package.  2. Easier collaboration: If multiple developers are working on a project that uses the package, associating the package with the repository makes it easier for them to collaborate and contribute to the package.  3. Better version control: By linking the package to the repository, you can manage the package's version control alongside the repository's version control, making it easier to keep track of changes and updates.  4. Improved security: By associating the package with the repository, you can ensure that the package is being used in the context of the repository, which can help improve security by making it easier to manage access and permissions.  Overall, associating a package with a repository in the {% data variables.product.prodname_registry %} can provide several benefits, and it's recommended to do so if possible.", "9f5b59b2-1cf3-48af-8c26-ca1c926a25f8": "To explicitly set the verbosity level to one of errors, warnings, progress, progress+, progress++, progress+++ using the CodeQL CLI, you can use the `--verbosity` option followed by the desired verbosity level. The available verbosity levels are errors, warnings, progress, progress+, progress++, and progress+++. This option overrides the `-v` and `-q` options.", "69bd47be-7dbc-4418-90b4-f1184e466d18": "Runners in the context of {% data variables.product.prodname_marketplace %} are used to execute workflows in automation tools like GitHub Actions. Each runner can run a single job at a time, and they are responsible for executing the steps defined in a workflow's YAML file. The context information provided mentions that runners can be hosted by the user or provided by {% data variables.product.company_short %} for certain plans. The purpose of runners is to enable the automation of tasks, such as building, testing, and deploying software, by providing a virtual machine to execute the workflow's steps.", "07168aa9-f97e-4c14-8aaa-aaef9edbf794": "OpenSSF Scorecards is an automated security tool that flags risky supply chain practices. By integrating the Scorecards action and starter workflow, organizations can follow best security practices and automatically run checks on repository changes to alert developers about risky supply chain practices using the built-in code scanning experience. The Scorecards project runs a number of checks, including script injection attacks, token permissions, and pinned actions, to ensure secure workflows. This helps organizations to identify and address potential security issues early in the development process, reducing the risk of compromised runners and securing their supply chain.", "95899542-e22d-40f8-a41b-d12c3d2ecdb2": "The --config-file-path argument in {% data variables.product.prodname_actions_importer %} can be used with the audit, dry-run, and migrate subcommands to specify a source file or configuration file instead of fetching pipeline contents from source control by default. This argument can also be used to specify the repository a converted reusable workflow or composite action should be migrated to. The format of the configuration file for auditing Azure DevOps instances is provided, and the --azure-devops-organization and --azure-devops-project options are used to select the pipeline for a dry run.", "239abfe3-dc32-4472-b694-06515c90bdac": "Without further context, it's unclear what \"AUTOTITLE\" is or what its purpose is. It's possible that \"AUTOTITLE\" is a command or function specific to the software being discussed, but without more information, it's impossible to say for certain. If you're asking this question in the context of a specific software or tool, please provide more details so that we can better understand the context and provide an accurate answer.", "869883c5-74da-45a0-af4e-adc91da6ef63": "A Subscription License on GitHub is the license assigned to each User to install, operate, access, and use the Service on behalf of the Customer. Customer may only assign one Subscription License per User. This means that each individual user who will be accessing and using the Service on behalf of the Customer is assigned a separate Subscription License. The number of Subscription Licenses that can be assigned to a single Customer is limited to the number of Users that the Customer has.", "5ef7c233-b891-4747-ac84-f349102a91b4": "The purpose of running the following two commands, `ghe-config actions-rate-limiting.enabled true` and `ghe-config actions-rate-limiting.queue-runs-per-minute RUNS-PER-MINUTE`, in order to enable and configure the rate limit is to set up a limit on the number of actions that can be executed within a certain time period, in this case, a limit of **RUNS-PER-MINUTE** actions per minute. The first command, `ghe-config actions-rate-limiting.enabled true`, enables the rate limiting feature, while the second command, `ghe-config actions-rate-limiting.queue-runs-per-minute RUNS-PER-MINUTE`, sets the maximum number of actions that can be queued for execution within a minute. This helps prevent excessive resource usage and ensures that the GitHub Actions service remains stable and responsive for all users.", "0a61b9fb-0c06-4729-abe8-1b44ec4e46e6": "To view the history of workflow runs in GitHub using the context information provided, you can follow these steps:  1. Go to your GitHub repository. 2. Click on the \"Actions\" tab in the left-hand menu. 3. Find the workflow that was triggered by adding the issue to the project column. 4. Click on the workflow to view its details. 5. Click on the \"Runs\" tab to see a list of all the runs for this workflow. 6. Click on a specific run to view its details, including the status, duration, and logs. 7. To view the history of all workflow runs, click on the \"History\" tab in the left-hand menu under the \"Actions\" tab. This will show you a list of all the workflows that have run in your repository, along with their status and details.  Alternatively, you can also use the GitHub CLI to view the history of workflow runs by running the following command:  ``` gh run list --repository <your-repository> --filter workflow-name ```  Replace `<your-repository>` with the name of your repository, and `workflow-name` with the name of the workflow you want to view. This will list all the runs for that workflow, along with their status and details.  I hope this helps! Let me know if you have any further questions.", "64b7d2ae-98f3-41d2-ab89-eb0e526a7e55": "To authenticate to the {% data variables.product.prodname_container_registry %} using a personal access token (PAT), follow these steps:  1. Go to your {% data variables.product.prodname_dotcom %} account settings. 2. Click Developer settings in the left-hand menu. 3. Click Personal access tokens in the left-hand menu. 4. Click the Generate new token button. 5. Enter a descriptive name for the token, such as \"Container Registry Token\". 6. Select the checkbox next to \"registry:*\" to grant the token access to the container registry. 7. Click the Generate token button. 8. Copy the generated token to your clipboard. 9. In your terminal or command prompt, run the following command to log in to the registry using the token:  ```shell docker login -u _token -p <YOUR_TOKEN> {% data variables.product.prodname_container_registry %} ```  Replace `<YOUR_TOKEN>` with the copied token.  10. You can now push and pull images using the logged-in credentials.  Note: If you're using {% data variables.product.prodname_ghe_server %}, your site administrator must first configure the container registry and enable subdomain isolation for your instance. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"", "fd797a52-9a5a-474d-8ff6-7c00166d99b0": "Cherry-picking a commit in Git refers to the process of selectively applying the changes introduced in a specific commit from one branch to another branch, without merging the entire history of the source branch. This feature allows developers to incorporate individual changes from one branch into another branch, without the need to merge the entire branch.  For instance, let's say we have two branches, the main branch (branch A) and a feature branch (branch B). The feature branch (branch B) has several commits, and we want to incorporate a specific change from commit C in branch B into the main branch (branch A).  First, we navigate to the main branch (branch A) and execute the following command:  ``` git cherry-pick <commit hash of commit C in branch B> ```  This command will create a new commit on the main branch (branch A) that contains the changes introduced in commit C from the feature branch (branch B). The new commit will have a different hash value than the original commit C in the feature branch (branch B).  Cherry-picking a commit is useful in managing code changes because it allows developers to selectively incorporate changes from one branch into another branch without the need to merge the entire branch. This feature is particularly helpful when we want to incorporate a specific change from a feature branch into the main branch without the need to merge the entire feature branch.  In summary, cherry-picking a commit in Git is a powerful feature that allows developers to selectively incorporate changes from one branch into another branch, without the need to merge the entire branch. This feature helps in managing code changes by providing developers with greater flexibility and control over the codebase.", "274528ca-3856-4308-b552-24eedd17d08b": "To configure Notepad++ as the default text editor for Git on a Windows operating system, follow these steps:  1. Download and install Notepad++ from https://notepad-plus-plus.org/.  2. Open the Git Bash terminal.  3. Type the following command to open the Git configuration file:     ```shell    git config --global --edit    ```  4. Add the following line to the configuration file, replacing `C:/Program Files (x86)/Notepad++/notepad++.exe` with the path to your Notepad++ installation:     ```shell    core.editor \"'C:/Program Files (x86)/Notepad++/notepad++.exe' -multiInst -notabbar -nosession -noPlugin\"    ```  5. Save and close the configuration file.  Now, when you use Git to edit files, it will automatically open Notepad++ with the specified settings.  To configure Sublime Text as the default text editor for Git on a Linux operating system, follow these steps:  1. Download and install Sublime Text from https://www.sublimetext.com/download/.  2. Open the terminal.  3. Type the following command to open the Git configuration file:     ```shell    git config --global --edit    ```  4. Add the following line to the configuration file:     ```shell    core.editor \"subl -n -w\"    ```  5. Save and close the configuration file.  Now, when you use Git to edit files, it will automatically open Sublime Text with the specified settings.  Note: The `-n` and `-w` flags in the Sublime Text command are used to open the file in a new window and prevent the creation of a new session, respectively. These flags can be omitted if you prefer to open files in the same window and session.", "f5d54ab8-a5fe-41c4-bf01-a9efa8ed0ab5": "The Privacy Policy outlines our commitment to protecting the personal data of our users in accordance with applicable data protection laws. It describes the types of personal data we collect, how we use it, and the measures we take to ensure its security and confidentiality. By adhering to this policy, we strive to ensure compliance with data protection laws and provide transparency to our users regarding our data handling practices.", "3c29f3e8-2e01-40f9-b51d-7f672b0bad02": "In addition to the {% data variables.enterprise.management_console %}, you can use the site admin dashboard and the administrative shell (SSH) to manage {% data variables.location.product_location %} beyond initial setup. This includes configuring applications and rate limits, viewing reports, and using command-line utilities. Some examples of these configurations are:  - Enabling or disabling specific GitHub features, such as Git Large File Storage or GitHub Pages - Setting up two-factor authentication for users - Configuring SSL/TLS encryption for HTTPS connections - Customizing email notifications sent to users - Setting up a backup and recovery strategy for your data - Configuring network settings, such as DNS servers and proxy servers  These configurations can be accessed through the site admin dashboard or by connecting to the administrative shell via SSH. For more information on these configurations, refer to the {% data variables.location.product_location %} documentation.", "f7de7089-7181-4cba-b7d7-c0bf39547ca5": "The {% data variables.contact.premium_support %} plan includes crisis prevention, technical advisory hours, and application upgrade assistance by request. To report an issue, {% data variables.contact.premium_support %} customers can use the {% data variables.contact.contact_landing_page_portal %} to submit a ticket in writing, in English. {% data variables.contact.premium_support %} is available 24 hours a day, 7 days per week. Prior to September 17, 2018, support was limited during holidays, but the holiday schedule is available at \"AUTOTITLE.\" The initial response time guaranteed by the Service Level Agreement is dependent on the priority level of the ticket, with urgent tickets receiving a response in 30 minutes and high priority tickets receiving a response in 4 hours. The priority level of a ticket is determined by the customer when submitting the ticket.", "de2cfb44-ecb8-4bac-806d-c12696ee3e4e": "To prepare our teams to use {% data variables.product.prodname_code_scanning %} effectively, we should provide them with training and resources on how to use the tool and interpret its results. This can include workshops, documentation, and one-on-one sessions with security experts.  During the rollout process, remediation should be a secondary focus. Our primary goal is to enable as many teams as possible to use {% data variables.product.prodname_code_scanning %} and gather data on the security posture of our repositories. Once we have a better understanding of the issues that are most common and impactful, we can develop a remediation plan that is tailored to our organization's specific needs.  In the meantime, we should encourage teams to prioritize fixing critical and high-severity issues, but we should avoid overwhelming them with too many alerts or requests for immediate action. By taking a measured and strategic approach to remediation, we can ensure that we are making progress without disrupting our development processes or causing unnecessary work for our teams.  To collect information about our repositories, we can use the {% data variables.product.prodname_code_scanning %} API to gather data on the languages used, the number of lines of code, and the presence of common security vulnerabilities. We can also use this data to identify patterns and trends that can help us prioritize our remediation efforts and allocate resources more effectively.  Overall, our goal is to use {% data variables.product.prodname_code_scanning %} as a tool for continuous improvement, rather than as a punitive measure. By empowering our teams to take ownership of their security posture and providing them with the resources they need to succeed, we can build a more secure and resilient organization over time.", "eb960f44-d053-4170-9471-cde9b7f4e2ad": "To configure SSO for your enterprise using {% data variables.enterprise.prodname_managed_users %} on {% data variables.product.prodname_dotcom_the_website %}, you should follow the guide provided for your chosen authentication method. The context information provided suggests that you can choose between OpenID Connect (OIDC) and Security Assertion Markup Language (SAML). If you're using GitHub as your Identity Provider (IdP), you can use OIDC, which includes support for Conditional Access Policies (CAP). However, if you require multiple enterprises with {% data variables.enterprise.prodname_managed_users %} provisioned from one tenant, you must use SAML for each enterprise after the first. If you're using another IdP, like Okta or PingFederate, you can use SAML to authenticate your members. Once authentication is configured, you can also configure SCIM provisioning to create {% data variables.enterprise.prodname_managed_users %} on {% data variables.product.prodname_dotcom_the_website %}. For more information on configuring SSO, SCIM provisioning, and managing organization memberships, you can refer to the guides provided in the context information.", "721f2f31-1729-4a86-accc-fefc7c6fdbd5": "A recovery code is a backup method to access an enterprise account in case of authentication configuration errors or issues with the identity provider (IdP). It can be used instead of single sign-on (SSO) when these issues prevent access to the account. To use a recovery code, it must have been previously downloaded and stored by the user. If the enterprise uses {% data variables.product.prodname_emus %}, the user must sign in as the setup user to use the recovery code.", "a6cf7409-bf1b-48c3-b563-581782eba105": "The Release and Indemnification clause in the Marketplace Terms requires the user to release GitHub from any claims, demands, and damages arising from disputes with Product Providers, and to indemnify, defend, and hold GitHub harmless from any claims, liabilities, and expenses related to the user's use of Marketplace, including violations of the Marketplace Terms. The user is responsible for controlling the defense and settlement of any claims, demands, suits, or proceedings, and must provide reasonable assistance to GitHub at their expense. This clause applies to disputes related to Marketplace, including the software and systems that make Marketplace available, unauthorized access, statements or conduct of third parties, user interactions, and any other matter relating to Marketplace. The clause also specifies that any disputes related to the Developer Products must be brought in a tribunal of competent jurisdiction located in or near San Francisco, California, and that GitHub may assign or delegate these Marketplace Terms at any time with or without the user's consent, while the user is not allowed to assign or delegate any rights or obligations under the Marketplace Terms.", "9af9d8db-44a8-4069-9f1c-eb53e8c4782e": "To modify the remote of a test repository and push to the `test.git` repo within the Docker container, follow these steps:  1. Clone the repository you want to test from the remote server (in this case, GitHub) using the following command:     ```shell    $ git clone git@github.com:octocat/Hello-World.git    ```  2. Change into the newly created directory:     ```shell    $ cd Hello-World    ```  3. Add a new remote called `test` that points to the `test.git` repository running inside the Docker container:     ```shell    $ git remote add test git@127.0.0.1:test.git    ```     Note that `127.0.0.1` is the IP address of the Docker host, and `test.git` is the name of the Git repository running inside the container.  4. Set the new remote as the default remote:     ```shell    $ git remote set-url --push test origin    ```  5. Push the changes to the new remote:     ```shell    $ GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -p 52311 -i ../id_ed25519\" git push -u test master    ```     Note that `GIT_SSH_COMMAND` is an environment variable that is used to specify the SSH command to use when pushing changes. In this case, we're specifying the IP address, port, and SSH key file to use when connecting to the Docker container.  6. The push will be rejected by the pre-receive hook running inside the Docker container, as shown in the output:     ```shell    > Warning: Permanently added '[127.0.0.1]:52311' (ECDSA) to the list of known hosts.    > Counting objects: 7, done.    > Delta compression using up to 4 threads.    > Compressing objects:", "8de048fb-e27c-4670-a05d-3f32fbcc25a3": "I'm sorry, but the provided context information is not sufficient to generate diverse questions for an upcoming quiz/examination. Please provide more context, such as the subject matter, learning objectives, and target audience, to help generate appropriate questions. Without this information, it's challenging to create questions that accurately assess the students' knowledge and skills.", "a70097bf-ed48-46b6-87bb-0731530a7981": "To narrow down your search for topics using search qualifiers such as \"is:featured\", \"is:curated\", \"repositories:n\", and \"created:YYYY-MM-DD\", you can use the following examples:  1. `is:featured`: This qualifier will narrow your search results to the topics with the most repositories on {% data variables.product.product_name %}. For example, **is:featured javascript** will match topics that are featured on https://github.com/topics/ and contain the word \"javascript.\"  2. `is:curated`: This qualifier will narrow your search results to topics that community members have added extra information to. For example, **is:curated javascript** will match topics that are curated and contain the word \"javascript.\"  3. `repositories:n`: This qualifier allows you to filter topics based on the number of repositories associated with them. For example, **repositories:>100 javascript** will match topics with more than 100 repositories that contain the word \"javascript.\"  4. `created:YYYY-MM-DD`: This qualifier allows you to filter topics based on when they were created. For example, **created:2021-01-01..2021-01-31 javascript** will match topics that contain the word \"javascript\" and were created between January 1, 2021, and January 31, 2021.  Note that you can also use the greater than and less than range qualifiers with the `created:` and `repositories:` qualifiers. For more information, see the documentation on search qualifiers.", "b5f69412-9b01-4129-a184-2fe808d77109": "To add a custom dev container configuration to your Java project using Visual Studio Code and GitHub Codespaces, follow these step-by-step instructions:  1. Open your Java project in Visual Studio Code. 2. Click on the \"Codespaces\" icon in the left-hand menu and select \"Create codespace\". 3. Follow the prompts to create a new codespace. 4. Once your codespace is ready, open it in Visual Studio Code. 5. In your project directory, create a new file called \".devcontainer\" and add the following code snippet:  ```json {   \"name\": \"Java App\",   \"build\": {     \"config\": [\".m2/settings.xml\"]   },   \"env\": {     \"JAVA_HOME\": \"/opt/java\"   },   \"extends\": \"microsoft/java:1.10.0\",   \"settings\": {     \"java.debug.console\": \"integratedTerminal\",     \"java.debug.projectNature\": \"org.springframework.ide.eclipse.core.springNature\",     \"java.debug.stopOnEntry\": true,     \"java.debug.stepFilter\": \"i\",     \"java.debug.stepReturn\": true,     \"java.debug.stepLine\": true,     \"java.debug.stepInto\": \"i\",     \"java.debug.stepOver\": \"i\",     \"java.debug.stepOut\": \"i\",     \"java.debug.terminate\": true,     \"java.debug.vmArgs\": \"-Xms512m -Xmx1024m\",     \"java.project.facets\": [       \"java\"     ],     \"java.source\": \"17\",     \"java.target\": \"17\",     \"java.version\": \"17\",     \"maven.test.skip\": true,     \"spring.profiles.active\": \"dev\",     \"spring.datasource.url\": \"jdbc:h2:mem:testdb\",     \"spring.datasource.username\": \"sa\",     \"spring.dat", "acc418e6-82f1-4983-a490-35450f41040e": "The name of the GitHub repository can be obtained from the context information using the variable `CI_PROJECT_NAMESPACE` and `CI_PROJECT_PATH_SLUG`. By concatenating these two variables, we can get the name of the repository.  For example, if `CI_PROJECT_NAMESPACE` is `my-org` and `CI_PROJECT_PATH_SLUG` is `my-repo`, then the name of the repository is `my-org/my-repo`.  Therefore, the answer to the question is `CI_PROJECT_NAMESPACE` and `CI_PROJECT_PATH_SLUG` concatenated together.  If the context information didn't provide the `CI_PROJECT_NAMESPACE` and `CI_PROJECT_PATH_SLUG` variables, then we wouldn't be able to determine the name of the repository using this context information alone. In that case, we would say that we weren't given enough information to give an answer.", "6b0dfef0-9dda-43c2-906c-7c7a576766ec": "To access the {% data variables.product.prodname_copilot_chat %} feature in JetBrains, you need to follow these steps:  1. Open your JetBrains IDE (such as IntelliJ IDEA, PyCharm, or WebStorm). 2. At the right side of the JetBrains IDE window, click the {% data variables.product.prodname_copilot_chat %} icon in the Activity Bar.     !Screenshot of the {% data variables.product.prodname_copilot_chat %} icon in the Activity Bar.     If you don't see the {% data variables.product.prodname_copilot_chat %} icon, make sure that the {% data variables.product.prodname_copilot_chat %} plugin is installed and enabled in your JetBrains IDE.  3. The {% data variables.product.prodname_copilot_chat %} window will open, where you can interact with the AI-powered assistant to generate code suggestions, debugging tips, and more.     !Screenshot of the {% data variables.product.prodname_copilot_chat %} window in IntelliJ IDEA.  That's it! You're now ready to use {% data variables.product.prodname_copilot_chat %} in your JetBrains IDE. If you have any questions or issues, you can refer to the {% data variables.product.prodname_copilot %} FAQ or contact GitHub Support for further assistance.", "1b8fa083-55be-42b1-b618-2b39fba0b114": "The context information provided does not explicitly state whether the current pipeline was triggered by a GitHub action. However, the `CI_PIPELINE_TRIGGERED` variable is set to the value of `github.actions`, which could potentially be a boolean value indicating whether the pipeline was triggered by a GitHub action. Without further information about the value of `github.actions`, it's unclear whether the pipeline was triggered by a GitHub action or not. Therefore, it's not possible to give a definitive answer based on the provided context information.", "a8bf60b0-30ae-44ab-bca6-4b9025de6be5": "To exclude non-production code from analysis using the configuration options provided for interpreted languages, you can use the `paths` and `paths-ignore` options in the workflow file for {% data variables.product.prodname_codeql %} advanced setup for {% data variables.product.prodname_code_scanning %}. This allows you to specify which code should be analyzed and which should be ignored. This can help reduce the size of the results file by excluding non-production code from analysis. For more information, refer to the documentation provided by GitHub.", "c9d64722-8dd6-41a8-b319-c64657b99c26": "Existing assignments with Repl.it integration will remain private forever, but after September 30, 2021, the Repl.it icon on the assignment README will no longer work. It is suggested to remove the instructions from any README using the integration and let students know that the icon will no longer work. New private and public repls cannot be created from within GitHub Classroom, but private repls can still be created in Replit until September 30, 2021.", "1afd758f-d2f3-4220-b960-31afae1039a6": "According to the documentation philosophy of {% data variables.product.prodname_dotcom %}, they prioritize keeping up to date with the latest accessibility standards to create content that can be accessed by a broad group of users, including those with disabilities. This means that they use resources and guidance to reduce barriers for people with disabilities in their content creation process. By doing so, they ensure that their documentation is inclusive and accessible to a broad group of users, regardless of any disabilities they may have.", "b8d62638-714a-4533-956c-c50eb1bfb4e5": "The maximum number of organization variables that can be stored in GitHub Enterprise Server (GHES) version 3.8 or earlier is 1,000.  If the context doesn't give enough information to determine the version of GHES, then say that you weren't given enough information to determine the maximum number of organization variables that can be stored in GitHub Enterprise Server.  If the context mentions that the organization is using GHES version 3.9 or later, then say that the maximum number of organization variables is not limited by the version of GHES, but is still limited to 1,000 organization variables.  If the context mentions that the organization is using GHES version 4.0 or later, then say that the maximum number of organization variables is not limited by the version of GHES, but is still limited to 1,000 organization variables, and that the maximum number of repository variables is not limited by the version of GHES, but is still limited to 500 variables per repository.  If the context mentions that the organization is using GHES version 4.1 or later, then say that the maximum number of organization variables is not limited by the version of GHES, but is still limited to 1,000 organization variables, and that the maximum number of repository variables is not limited by the version of GHES, but is still limited to 500 variables per repository, and that the total combined size limit for organization and repository variables is not limited by the version of GHES, but is still 10 MB per workflow run.  If the context mentions that the organization is using GHES version 4.2 or later, then say that the maximum number of organization variables is not limited by the version of GHES, but is still limited to 1,000 organization variables, and that the maximum number of repository variables is not limited by the version of GHES, but is still limited to 500 variables per repository, and that the total combined size limit for organization and repository variables is not limited by the version of GHES, but is still 10 MB per workflow run, and that the maximum number of environment-level variables is not limited by the version of GHES, but", "2a7d79d3-58fb-41df-8735-bb75381bd5c8": "To reduce the amount of code being analyzed in a single workflow and decrease analysis time in {% data variables.product.prodname_codeql %} with advanced setup, you can:  1. Exclude test code: If your repository has a significant amount of test code, you can exclude it from the analysis. This can significantly reduce the amount of code being analyzed and, in turn, decrease analysis time.  2. Break analysis into multiple workflows: Instead of analyzing all of your code in a single workflow, you can break it down into multiple workflows that analyze smaller subsets of your code at a time. This can help to reduce the overall analysis time.  3. Run only during a schedule event: If your analysis is slow during `push` or `pull_request` events, you can configure your workflow to only trigger on the `schedule` event. This can help to reduce the analysis time during these events.  4. Check which queries or rules the workflow runs: If you're using {% data variables.product.prodname_codeql %} for your {% data variables.product.prodname_code_scanning %}, you can run only the queries or rules that you consider critical in workflows that run on pull requests. This can help to reduce the analysis time during these events.  5. Reduce the number of queries: If you have optimized the {% data variables.product.prodname_codeql %} database build and the process is still too long, you could reduce the number of queries being run in your workflow. This can help to significantly reduce the analysis time.  Overall, by implementing these strategies, you can help to reduce the analysis time in {% data variables.product.prodname_codeql %} with advanced setup, making it more efficient and less resource-intensive.", "3ecdc132-6b8d-4f25-af43-37a245233e07": "To view the recent workflow runs using the gh cli, you can use the `run list` subcommand. For example:  ```shell gh run list ```  This will list the recent workflow runs. If you want to limit the number of runs returned, you can use the `-L` or `--limit` flag. For example:  ```shell gh run list --limit 5 ```  To view details for a specific workflow run, you can use the `run view` subcommand. If you don't specify a run ID, gh will return an interactive menu for you to choose a recent run. For example:  ```shell gh run view ```  To view details for a specific job in the run, you can use the `-j` or `--job` flag. For example:  ```shell gh run view --job JOB_ID ```  To view the full log for a job, you can use the `--log` flag. For example:  ```shell gh run view --job JOB_ID --log ```  To include job steps in the output, you can use the `-v` or `--verbose` flag. For example:  ```shell gh run view --verbose ```  To exit with a non-zero status if the run failed, you can use the `--exit-status` flag. For example:  ```shell gh run view 0451 --exit-status && echo \"run pending or passed\" ```  Overall, the gh cli provides several options to view recent workflow runs and their details, making it easy to monitor and troubleshoot workflows.", "c60a4cf1-ee0f-4d4e-ac7c-bef1f0e00918": "A call to action (CTA) button is a button that emphasizes a link that we expect or encourage people to navigate to after reading an article or as part of completing the task that an article describes. These buttons should only take people to GitHub-owned domains. Other types of buttons that can be included on a landing page or article are not subject to this restriction, but they should still support user needs and be approved by the Content Strategy team through an issue in the `docs-strategy` repository. This is necessary to ensure that the buttons are consistent with the overall design and messaging of the {% data variables.product.prodname_docs %} site.", "0f5e8c27-f2dc-4aeb-9565-426bfa08c19e": "An organization owner can restrict the use of certain dev container images for codespaces created within their organization by adding a policy to do so. If the image specified in the dev container configuration for a repository does not match one of the allowed images, a message is displayed when someone tries to create a codespace for the repository: \"Codespace could not be created: Base image 'DETAILS FROM DEV CONTAINER CONFIGURATION' is not allowed based on an organization policy set by your organization owner.\" This policy is only applied when a codespace is created, and it does not apply to the default image. In a future release, the policy will also be applied when a container is rebuilt.", "2a75a133-b720-437f-8ef5-86d1ec01dfeb": "If a repository owner enables the setting to always suggest updating pull request branches, people with write permissions will always have the ability to update a pull request's head branch when it's not up to date with the base branch, even if the base branch doesn't require branches to be up to date before merging. This means that they can update the branch without having to wait for the base branch to be updated first. If the setting is not enabled, the ability to update is only available when the base branch requires branches to be up to date before merging and the branch is not up to date.", "89e05153-52bf-4ac2-9eec-76496b9a803c": "According to the GitHub Supplemental Terms, Customer Content in Customer\u2019s Private Repositories is considered Confidential Information by GitHub. GitHub will protect and keep strictly confidential this Customer Content in accordance with the applicable confidentiality provision in the Microsoft Customer Agreement. This means that GitHub will take measures to ensure that this information is not disclosed to unauthorized parties or used for any purpose other than providing the Service. Additionally, GitHub may only access Customer\u2019s Private Repositories with Customer\u2019s consent and knowledge, for support reasons, or when access is required for security reasons. GitHub will also treat Customer Content in Customer\u2019s Private Repositories as Confidential Information if additional access is enabled for certain services or features. However, GitHub reserves the right to access, review, and remove Content of Private Repositories if it believes the Content is in violation of the law or of these GitHub Supplemental Terms, or if required by law. GitHub will provide notice of such actions unless otherwise bound by requirements under law or if in response to a security threat or other risk to security. GitHub also has copyrights to the Service's look and feel, and Customer may not duplicate, copy, or reuse any portion of the HTML/CSS, Javascript, or visual design elements or concepts without express written permission from GitHub.", "72cc9633-0086-4759-84fc-6ee0337f63b7": "To cancel a free trial for an app on GitHub Marketplace for your personal account, follow these steps:  1. Go to your GitHub settings by clicking on your profile picture in the top right corner of the GitHub page and selecting \"Settings\" from the drop-down menu. 2. Click on \"Billing plans\" in the left-hand menu. 3. Find the app you signed up for during the free trial and click on \"Manage subscription\" next to it. 4. Click on \"Cancel subscription\" and then confirm the cancellation by clicking \"Confirm cancellation\".  Alternatively, you can follow these steps:  1. Go to your GitHub settings by clicking on your profile picture in the top right corner of the GitHub page and selecting \"Settings\" from the drop-down menu. 2. Click on \"Billing plans\" in the left-hand menu. 3. Click on \"Manage free trials\" at the top of the page. 4. Find the app you signed up for during the free trial and click on \"Cancel trial\" next to it. 5. Confirm the cancellation by clicking \"Confirm cancellation\".  Either of these methods will cancel your free trial for the app and prevent any further charges.", "5cf332bb-cf85-4f04-859a-1b2620c3ee71": "The purpose of subscribing to webhooks in the context of {% data variables.product.prodname_github_apps %} is to allow the app to receive notifications in real-time when certain events occur on a GitHub repository. For example, an app that provides code review services may subscribe to the pull_request webhook to receive notifications when a new pull request is created, allowing the app to automatically leave a comment with feedback.  When setting up a {% data variables.product.prodname_github_app %}, specific permissions determine what the app can do via the {% data variables.product.prodname_dotcom %} API, what they can do on behalf of a signed in user, and what webhooks the app can receive. An example of a specific permission that an app can select when setting up the {% data variables.product.prodname_github_app %} is the \"Write: Discussion comments\" permission, which allows the app to create, edit, and delete comments on discussion threads in a repository. This permission is necessary for an app that provides discussion management services to be able to create comments on behalf of the user.", "467dc1a3-535d-4ebe-a3ca-adc877989eb1": "Git can be configured to handle line endings automatically using the `git config core.autocrlf` command. The argument passed to this command depends on the operating system being used.  - On macOS, passing `input` as the argument ensures that line endings in files checked out are correct for macOS. - On Windows, passing `true` as the argument ensures that line endings in files checked out are correct for Windows. - On Linux, passing `input` as the argument ensures that line endings in files checked out are correct for Linux.  Additionally, a `.gitattributes` file can be created in the root of a repository to manage how Git reads line endings for that specific repository. This file overrides the `core.autocrlf` setting for all repository contributors, ensuring consistent behavior for all users, regardless of their Git settings and environment. The `.gitattributes` file looks like a table with two columns, where the left column contains the file name for Git to match, and the right column contains the line ending conversion to use.", "a9cf6c10-b4a3-473c-b0d3-cefaa6695477": "The purpose of defining inputs, outputs, and secrets for reusable workflows in GitHub Actions is to provide flexibility and customization options for users who want to reuse a workflow in multiple contexts. Inputs allow users to pass specific values or parameters to the reusable workflow, which can be used to tailor the workflow's behavior to the specific use case. Outputs allow the reusable workflow to return values or results to the calling workflow, which can be used to provide additional information or feedback. Secrets allow users to securely store sensitive information, such as passwords or API keys, that can be accessed by the reusable workflow without exposing them in the repository's codebase. In a calling workflow, these inputs, outputs, and secrets can be accessed and utilized by referencing the `inputs`, `outputs`, and `secrets` properties of the reusable workflow's job, respectively. This allows for a more modular and reusable workflow architecture, as users can easily adapt and customize reusable workflows to fit their specific needs.", "504eea1d-23a0-497d-8c0f-3905e4c2ef7b": "GitHub collects usage data from Marketplace and GitHub API, and this data is maintained in accordance with GitHub's privacy policies. Developers may have limited access to this usage data at GitHub's discretion. The usage data is used to operate and improve Marketplace. Developers' role in this process is limited to potential access to usage data, subject to GitHub's discretion.", "349a815d-53ba-4ba9-af23-dfcba877a981": "To customize your configuration for {% data variables.product.prodname_enterprise_backup_utilities %}, you can edit the `backup.config` file by following these steps:  1. If you previously upgraded {% data variables.product.prodname_enterprise_backup_utilities %} using Git, ensure that you copy your existing configuration from `backup.config` into the new file.  2. Set the `GHE_HOSTNAME` value to your primary {% data variables.product.prodname_ghe_server %} instance's hostname or IP address. If {% data variables.location.product_location %} is deployed as a cluster or in a high availability configuration using a load balancer, the `GHE_HOSTNAME` can be the load balancer hostname, as long as the load balancer allows SSH access over port 122 to {% data variables.location.product_location %}.  3. Save the changes and exit the text editor.  By customizing your configuration in this way, you can tailor {% data variables.product.prodname_enterprise_backup_utilities %} to suit your specific needs and requirements.", "9ad10738-8462-43bf-b61f-fcf047797908": "Yes, the documentation provided in the context information shows an example of using query parameters to fetch only the first page of results in the {% data variables.product.prodname_dotcom %} REST API using the `page` query parameter. Here's the example:  ```shell copy gh api -H 'Accept: application/vnd.github.v3+json' \\   GET /repos/{owner}/{repo}/issues \\   --query 'page=1' ```  In this example, replace `{owner}` and `{repo}` with the owner and repository you want to fetch issues from. The `--query 'page=1'` option is used to specify that you want to fetch only the first page of results. This can be useful to limit the number of results returned and improve performance.", "f359905b-002e-4ecc-bf17-5616b5b02204": "Using proper markup for row and column headers in tables is important because it helps assistive technology understand the relationships between cells in the table. For example, in a table where the first column describes the data values in the table but is not data itself, row headers are needed to make sense of the values in the table. This is because the values in the table are dependent on both the column header and the row header. Without proper markup, assistive technology may not be able to accurately convey the information in the table to users who rely on it. By using proper markup, users with visual impairments or other disabilities can better understand and navigate the table, making the content more accessible and inclusive.", "6e5daa99-7399-4726-a955-9b0fb2aec7eb": "When a public repository is made private, its public forks are split off into a new network and one of the existing public forks is chosen to be the new upstream repository. All other repositories are forked off of this new upstream. Subsequent pull requests go to this new upstream repository. In other words, a public repository's forks will remain public in their own separate repository network even after the upstream repository is made private. This allows the fork owners to continue to work and collaborate without interruption. If public forks were not moved into a separate network in this way, the owners of those forks would need to get the appropriate access permissions to pull changes from and submit pull requests to the (now private) upstream repository\u2014even though they didn't need those permissions before. If a public repository has anonymous Git read access enabled and the repository is made private, all of the repository's forks will lose anonymous Git read access and return to the default disabled setting. If a forked repository is made public, repository administrators can re-enable anonymous Git read access.", "704da88d-5def-437e-8f4a-3430a772e606": "If your teacher sets a hard cutoff deadline for an assignment on {% data variables.product.prodname_classroom %}, unless you or your group have received an extension, you will lose write access to your assignment repository after the deadline passes. This means that you will no longer be able to make changes or submit your assignment through the platform after the deadline has passed. It's important to monitor the deadline for an assignment through the README in your assignment repository, as the deadline shown updates automatically any time your teacher alters the assignment deadline.", "0099a804-ea22-4167-a702-52001cc3a348": "The limitations of liability outlined in the text material state that under no circumstances will the company be liable for indirect, special, incidental, consequential, exemplary, or punitive damages, including but not limited to damages for lost data, lost profits, or costs of procurement of substitute goods or services, regardless of the form of action or theory of liability. The company's aggregate liability to the user arising out of or related to the agreement for any cause whatsoever, and regardless of the form of action, whether in contract or in tort, will not exceed the greater of (i) the amounts the user actually paid the company under the agreement, or (ii) five hundred dollars ($500).", "9ce3e30f-2450-4b00-8b80-894c4b810988": "How can I ensure consistent installation of PowerShell dependencies across workflows and specify the required module versions?  2. What is the default installation policy for PowerShell Gallery modules and how can I explicitly set it to \"Trusted\" for installing dependencies?", "c4c7f4aa-c62c-48be-b2ac-72607c98d94f": "To add a new tool to {% data variables.product.prodname_docs %} documentation, the following guidelines should be followed:  1. The new tool should be added only when it is the only way to accurately document something for a specific user need. 2. The writer proposing the new tool should include it in a content design plan. 3. The content design plan should be reviewed, and alternative ways to address the documentation needs without adding a new tool should be considered. 4. If a new tool is the only way to create accurate documentation, it should be added. If there is an alternative content solution that does not add a new tool, that option should be used. 5. To add a new tool, an entry should be added to the `allTools` object in the `lib/all-tools.js` file as a key-value pair. 6. Tools should be added in alphabetical order, and only a maximum of eight different tools should be included in an article to prevent tool switcher tabs from overflowing with the article's table of contents. 7. The first tool tag should be selected as the default tool for an article, but a different default tool can be specified in the article's frontmatter. 8. Links to an article with a specific tool selected can be created by adding `?tool=TOOLNAME` to the end of the link. 9. {% data variables.product.prodname_docs %} documents and maintains tool tags for {% data variables.product.prodname_dotcom %} products, {% data variables.product.prodname_dotcom %}-developed tools, and select third-party extensions developed in collaboration with {% data variables.product.company_short %}. 10. New tools should be added only for {% data variables.product.prodname_dotcom %} products, {% data variables.product.prodname_dotcom %}-developed tools, or select third-party extensions developed in collaboration with {% data variables.product.company_short %}.", "11fd88d1-f574-41ad-a8ba-ae95a4274224": "The purpose of creating files with geoJSON, topoJSON, and STL diagrams in the context provided is not explicitly stated. However, based on the context, it can be inferred that these files may be used to visualize or represent geographic data, topographic data, or 3D models in the web interface of the product. Without further context, it is unclear why these specific file formats are being used or what specific use case they serve.", "5686d4ae-5064-4ece-90b8-814c640d9821": "To authenticate ARC with a {% data variables.product.pat_v1 %}, follow these steps:  1. Create a {% data variables.product.pat_v1 %} with the necessary scopes. For more information, see \"AUTOTITLE\".  1. In ARC, go to the \"Settings\" page for the runner.  1. In the \"Runner registration\" section, select \"Use a personal access token (PAT)\".  1. Enter the {% data variables.product.pat_v1 %} in the \"Personal access token\" field.  1. Save the changes.  {% endif %}", "b2f40042-981d-45f8-b976-1fcb3a82d975": "In the context of the Data Protection Agreement, the term \"Controller\" refers to the entity or organization that determines the purposes and means of processing Personal Data. This is different from the term \"Processor,\" which refers to the entity or organization that processes Personal Data on behalf of the Controller. The term \"Data Subject\" refers to the individual whose Personal Data is being processed. The distinction between these terms is important in the context of data protection laws, as it determines the respective responsibilities and obligations of the Controller and Processor in relation to the processing of Personal Data.", "5d67f8be-64be-4279-9b9e-236c29c2cffa": "To view the commit details of a specific author using the GitHub API, you can use the commits endpoint with the author's name and a specific date range. Here's an example URL:  `https://github.com/your-organization-or-personal-account/your-repository/commits?author=your-author-name&since=2021-01-01T00:00:00Z&until=2021-12-31T23:59:59Z`  Replace `your-organization-or-personal-account` and `your-repository` with your actual GitHub organization or personal account and repository name, respectively. Replace `your-author-name` with the name of the author you want to view commits for. Replace the date range with the specific dates you want to view commits for.  If you're missing expected commits on your GitHub timeline, there could be a few reasons:  1. The commit history was rewritten: If the commit history was rewritten, the commit author date and the commit date may be different. In this case, you can use the GitHub API to view the commit details with the author's name and a specific date range, as described above.  2. The commits were deleted: If the commits were deleted, they won't appear on your GitHub timeline. You can use the GitHub API to view the deleted commits, if they haven't been purged from the GitHub servers.  3. The commits were pushed to a different branch: If the commits were pushed to a different branch, they won't appear on your GitHub timeline unless you're viewing the commits for that branch.  4. The commits were merged: If the commits were merged, they won't appear on your GitHub timeline unless you're viewing the commits for the merged branch.  To troubleshoot missing expected commits, you can follow these steps:  1. Check the branch where the commits were pushed: Make sure you're viewing the commits for the correct branch.  2. Check the deleted commits: Use the GitHub API to view the deleted commits, if", "b77836ea-2f39-4c8e-8bbb-c836dfcdd16c": "According to the text, the terms or conditions of any prior communications, representations, understandings, and agreements, whether written or oral, that the parties may have had in relation to the subject matter of this agreement will be null and void, and of no effect.", "6e8eaccc-17f0-4f0f-936a-f47fadd867e6": "To enable debug logging and gather more detailed information about a workflow, job, or step that is not working as expected, you can follow these steps:  1. Open the repository containing the workflow in question. 2. Navigate to the Actions tab. 3. Select the workflow you want to debug. 4. Click the ellipsis (...) next to the workflow name. 5. Select Edit from the drop-down menu. 6. Add the following line to the YAML file, inside the job that you want to debug:     ```yaml    steps:      - name: Debug logging        run: |          echo \"::set-env::\" >> $GITHUB_ENV          echo \"DEBUG=true\" >> $GITHUB_ENV          echo \"::unset::DEBUG\" >> $GITHUB_ENV        shell: bash    ```  7. Save the changes to the YAML file. 8. Re-run the workflow to generate debug logs. 9. Navigate to the Actions tab and select the workflow run. 10. Click the ellipsis (...) next to the workflow run. 11. Select View logs from the drop-down menu. 12. Scroll down to the Debug console section to view the debug logs.  By following these steps, you can enable debug logging and gather more detailed information about a workflow, job, or step that is not working as expected. This information can help you diagnose and resolve issues more quickly.", "1bf72923-ed1a-4267-b945-ba70d4bf7520": "The API version used in the \"Create an issue\" endpoint in the provided context is not specified. The `--header \"X-GitHub-Api-Version: 2022-11-28\"` line is included in the curl command as an example of how to set the API version header, but it is not actually used in the provided context. To use a specific API version, you would replace `2022-11-28` with the desired version in the header value. If you are unsure which API version to use, you can omit the header altogether, and the request will use the default version for the endpoint. For more information, see the API versioning documentation.", "d870ced8-2ee4-4bb2-9ef1-92a63ea83927": "To check the maximum number of concurrent jobs you can run on {% data variables.product.prodname_dotcom %}-hosted runners, follow these steps:  1. Go to your repository or organization/enterprise on {% data variables.product.prodname_dotcom %}. 2. Click on \"Actions\" in the left-hand menu. 3. Click on \"Runner groups\" in the left-hand menu. 4. Click on the runner group you want to check. 5. Review the \"All jobs usage\" section, which lists the number of active jobs and the maximum number of jobs you can run.  This will tell you the maximum number of concurrent jobs you can run on {% data variables.product.prodname_dotcom %}-hosted runners for your plan.", "80f00437-9257-4f7b-9a53-619640bef8ea": "Since {% data variables.product.company_short %}-hosted macOS runners are more expensive than Linux and Windows runners, we recommend that you build only the code that you want to analyze for Swift code using Xcode. This can be achieved by configuring your Xcode project or workspace to only build the targets that contain the code you want to analyze. You can do this by selecting the targets in the project navigator, opening the \"Build Phases\" tab, and adding the targets to the \"Target Dependencies\" section. This will ensure that only the selected targets are built during the build process, reducing the overall build time and cost. Additionally, you can consider using self-hosted runners instead of {% data variables.product.company_short %}-hosted runners to further reduce costs.", "c22366b2-16eb-4939-8a91-a6a8bece4d14": "To determine if a discussion has been answered, one can check the \"isAnswered\" field of the Discussion type in the GraphQL query provided. This field returns a Boolean value indicating whether the discussion has been answered or not.  The significance of this is that it helps users to quickly identify discussions that have been resolved, as opposed to those that are still pending. This can be useful for users who are looking for specific information or assistance, as they can focus their efforts on discussions that have already been answered. Additionally, it can help to reduce the number of duplicate discussions that are created, as users can refer to existing answers instead of starting a new discussion.", "54e39e15-78b2-43c1-b88f-50d4b41f58c9": "Variables and secrets can be set in both GitLab CI/CD and {% data variables.product.prodname_actions %} through the configuration file or the respective UI. In GitLab CI/CD, variables can be defined in the .gitlab-ci.yml file or through the GitLab UI, while secrets can be created using the GitLab UI. In {% data variables.product.prodname_actions %}, variables can be set in the workflow configuration file or through the {% data variables.product.product_name %} UI, while secrets can be created using the {% data variables.product.product_name %} UI.", "0e0ec993-56c3-40e0-a4d5-9b435a9e7e62": "To configure your repository to publish your site using GitHub Pages and a custom workflow with GitHub Actions, follow these step-by-step instructions:  1. Create a new repository or navigate to an existing one.  2. In the repository settings, go to the Pages tab and select the source for your site. You can choose either the main branch or a specific branch or directory.  3. If you want to use a custom domain, go to the Custom domain section and follow the instructions to add your domain.  4. Go to the Actions tab and click on the \"New workflow\" button.  5. Choose the \"Set up a workflow yourself\" option and select the YAML file format.  6. Copy and paste the following YAML code into the editor:  ```yaml name: Publish site  on:   push:     branches:       - main  jobs:   publish:     runs-on: ubuntu-latest      steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up Node.js       uses: actions/setup-node@v2       with:         node-version: '14'      - name: Install dependencies       run: npm install      - name: Build site       run: npm run build      - name: Upload artifact       uses: actions/upload-pages-artifact@v2      - name: Deploy to GitHub Pages       uses: actions/deploy-pages@v2       with:         source: 'pages-artifact'         branch: '${{ github.ref }}'         path: '.' ```  7. Save the file and commit it to your repository.  8. Go to the Actions tab and click on the workflow you just created to run it.  9. Once the workflow completes successfully, go to the Pages tab to verify that your site is published.  Recommended best practices for creating a custom GitHub Actions workflow for publishing a site with GitHub Pages:  1. Use the `on` keyword", "4e5d85d0-538a-4e46-bd2c-5a8c8b865219": "The value of `CI_EXTERNAL_PULL_REQUEST_SOURCE_BRANCH_SHA` in the provided context is {% raw %}`${{ github.event.pull_request.head.sha }}`{% endraw %}. This variable contains the SHA (a unique identifier for a specific commit) of the branch that the pull request is based on. In other words, it represents the state of the code in the branch that the pull request is trying to merge into the target branch.", "c80b767f-280e-4e38-bb82-025f8a38adfb": "To create an Azure Active Directory tenant for configuring authentication and user provisioning with Azure AD for {% data variables.product.product_name %}, you can follow the steps provided in the Microsoft Docs. Here's a quick summary:  1. Go to the Azure portal and sign in with your Microsoft account. 2. Click on the \"Create a resource\" button in the left-hand menu and select \"Azure Active Directory\" from the list of options. 3. Follow the prompts to create a new Azure Active Directory tenant. You'll need to provide some basic information, such as your organization's name and domain name. 4. Once your tenant is created, you can add the {% data variables.product.product_name %} application and configure single sign-on and user provisioning, as described in the context information provided.  If you're not familiar with Azure Active Directory or need more detailed instructions, you can find step-by-step guides and tutorials in the Microsoft Docs.", "d30ce110-5690-452e-9379-6ad39b120837": "If you publish a package without linking it to a repository in {% data variables.product.prodname_registry %}, then the package won't be linked to a repository source when it is published. However, you can choose to connect the package to a repository later. On the other hand, if you publish a package with a linked repository in {% data variables.product.prodname_registry %}, then the repository specified in the `repository` key of your `package.json` file is used as the destination npm registry for {% data variables.product.prodname_registry %}. This means that the package is published to the specified repository in {% data variables.product.prodname_dotcom %}, and the repository source is updated when the package is published. However, if the `url` specified in the `repository` key is invalid, the package may still be published, but it won't be linked to the repository source as intended. Additionally, if you publish a package without linking it to a repository, the package won't inherit any permissions from the destination repository, while if you publish a package with a linked repository, only the repository source is updated, and the package doesn't inherit any permissions from the destination repository.", "8c47c47b-ef8e-4182-9f67-acfc83fd7519": "Subcontractors are permitted to obtain Customer Personal Data only to deliver the services the data importer has retained them to provide, and they are prohibited from using Customer Personal Data for any other purpose. (from the context information provided)", "fa42c30d-9059-437d-8ed4-d2e0ab32a3a2": "If the repository doesn't have a `go.mod` file, the recommended way to install a compatible version of Go for code scanning without using a workflow is to rearrange the repository directory structure suitable for addition to `GOPATH`, and use `go get` to install dependencies. The directory structure reverts to normal after extraction completes. This is mentioned in the context information provided.", "16fbe542-1184-4458-8f46-14cfe894fb55": "Users with permission to create workflows in their repositories can access and use starter workflows created by other organization members by navigating to their repository's Settings > Workflows > Starter workflows. The available starter workflows will be listed, and users can select the one they want to use as a starting point for their own workflow. The selected starter workflow will be copied and renamed, and users can then customize it as needed.", "591401ba-060e-4909-ad3f-b667bd9ed53d": "To view the list of people who have starred a public repository on Github, you can add `/stargazers` to the end of the repository URL. For example, to view the list of people who have starred the github/docs repository, visit https://github.com/github/docs/stargazers. This feature is available for public repositories and private repositories that you have access to.", "16322744-18e9-4c85-b08c-317abdef69a3": "To update {% data variables.product.prodname_desktop %} on your computer, follow these steps:  1. In the menu bar, select {% data variables.product.prodname_desktop %}, then click About {% data variables.product.prodname_desktop %}. 2. In the modal window, click Check for Updates. 3. If an update is available, quit and relaunch {% data variables.product.prodname_desktop %} to install the update.  Note: These steps are applicable for both Mac and Windows operating systems.", "a6f22a84-0845-4dff-be83-0358bd0b7b2e": "The `repository_advisory` activity stream in the context provided is significant because it contains repository-level activities related to security advisories in the {% data variables.product.prodname_advisory_database %}. This indicates that the organization is actively monitoring and addressing security vulnerabilities in their repositories, which is a best practice for maintaining the security and integrity of their codebase. By tracking these activities, the organization can stay informed about any potential security issues and take appropriate action to mitigate them.", "d775cefc-448a-4bda-9e27-2ab97974b994": "The policy settings selected by the organization owner determine the behavior of {% data variables.product.prodname_copilot %} for all organization members granted access to {% data variables.product.prodname_copilot_short %} through the organization. If explicit settings have been selected at the enterprise level, an organization owner cannot override those settings at the organization level.", "31b58890-a132-4b1d-8061-b8eea8243ad3": "The \"gh issue list\" command in this context is used to locate previously created issues by setting an alias called \"list-reports\" that filters the issue list based on specific criteria. The alias is defined using the \"gh alias set\" command, which sets the alias with the specified filtering options. The filtering options include the repository name, author name, and label name. The command is then used to retrieve the URLs of the previous two reports using the \"gh list-reports\" command with the alias. The URLs are stored in a variable called \"previous_report_url\" for later use in closing and/or commenting on old issues.", "9a6780e3-2fbd-41dd-b59c-506d258dc3ec": "Invoiced enterprise customers on {% data variables.product.prodname_dotcom %} can pay their current invoice using a credit card by accessing their enterprise account on {% data variables.product.prodname_dotcom %}, going to the settings tab, selecting the billing tab, clicking \"Pay invoice\" under \"Latest invoice\", and filling out the secure payment form.", "95cede14-33d1-44f0-b2de-c0c9be41c199": "To narrow down a code search using specialized qualifiers such as `repo:`, `language:`, and `path:`, you can add these qualifiers to your search query. For example, to search for a specific repository, you can use the `repo:` qualifier followed by the repository name. Here's an example:  ```text repo:octocat/Hello-World \"print('Hello, World!')\" ```  This query will search for the exact string `\"print('Hello, World!')\"` in the repository `octocat/Hello-World`.  To search for a specific programming language, you can use the `language:` qualifier followed by the programming language name. Here's an example:  ```text language:python \"def hello(): print('Hello, World!')\" ```  This query will search for the exact string `\"def hello(): print('Hello, World!')\"` in Python files.  To search for a specific file path, you can use the `path:` qualifier followed by the file path. Here's an example:  ```text path:README.md \"Hello, World!\" ```  This query will search for the exact string `\"Hello, World!\"` in files named `README.md`.  To search for an exact string, including whitespace, you can surround the string in quotes. For example:  ```text \"print('Hello, World!')\" ```  This query will search for the exact string `\"print('Hello, World!')\"`.  In summary, you can narrow down a code search using specialized qualifiers such as `repo:`, `language:`, and `path:` to search for specific repositories, programming languages, and file paths. You can also search for an exact string, including whitespace, by surrounding the string in quotes.", "8d242465-d0bf-40bc-bac5-301ab00a8979": "The recommended way to publish a package that should be accessible to all codespaces in an organization is to publish it with internal visibility. This will automatically make the package visible to all codespaces within the organization, unless the repository the codespace is launched from is public.", "a2356080-4bbe-4814-9934-5c448aabbfb3": "You should refer to the section \"Choosing the runner for a job\" in the documentation for GitHub Actions. This section provides guidance on how to select the appropriate runner for a job based on factors such as the operating system, language, and resources required by the workload. The documentation can be found on the GitHub website or by clicking the \"Learn more\" link provided in the context information.", "420a7dd0-ff85-43e4-97de-c7e6d71a0e98": "Repositories can gain access to hosted runners through runner groups, which can be either enterprise-level or organization-level. Enterprise administrators can grant access to enterprise-level runner groups for specific organizations, while organization owners can choose to use these groups or create their own organization-level runner groups. Once a repository has access to a runner group, the hosted runners associated with that group can be added to workflow files. This allows for the execution of automated workflows on self-hosted machines, providing greater flexibility and control over the CI/CD process.", "4c877ee0-386e-4f54-805d-7891b2099acf": "The `id-token: write` permission in the context of the `pypi-publish` job is required to enable trusted publishing using the `pypa/gh-action-pypi-publish` action. Trusted publishing allows GitHub to securely exchange an OIDC token for a PyPI API token, which can then be used to upload packages to PyPI without the need to provide a username and password. By giving the `pypi-publish` job this permission, the `pypa/gh-action-pypi-publish` action is able to access the necessary tokens to perform the publishing process. Without this permission, the action would prompt for a username and password, which is not recommended for storing sensitive information in a GitHub repository.", "65397f82-f309-4b86-b926-a68f0cfb2cbd": "To unapply a filter in Classroom, select the associated dropdown menu, then click the filter once more. A {% octicon \"check\" aria-hidden=\"true\" %} is displayed alongside the name of an applied filter, while unapplied filters only display their names.", "3b34f715-fd6d-4b13-9553-7c86a93cfa92": "To find out how many jobs are currently being executed in this pipeline, we can use the `CI_NODE_TOTAL` and `CI_NODE_INDEX` variables.   The `CI_NODE_INDEX` variable represents the current job's index in the current strategy, and the `CI_NODE_TOTAL` variable represents the total number of jobs in the current strategy.   So, to calculate the number of jobs currently being executed, we can subtract the current job's index from the total number of jobs in the current strategy:  ``` number_of_jobs = CI_NODE_TOTAL - CI_NODE_INDEX + 1 ```  However, we don't have information about the current strategy, so we can't determine the number of jobs currently being executed without this information.  In summary, we need more context to answer this question.", "0ad849c7-9230-4878-aae6-393a2b11aeac": "To publish a package to {% data variables.product.prodname_registry %} using a supported package client, you need to create or use an existing {% data variables.product.pat_v1 %} with the appropriate scopes for the task you want to accomplish, authenticate to {% data variables.product.prodname_registry %} using your {% data variables.product.pat_v1 %}, and publish the package using the instructions for your package client.  To ensure that the README and description provide clear information about each package in a connected repository, you should make sure that they contain installation and usage instructions, as well as any other relevant details. It's also a good idea to clearly distinguish between different packages connected to the same repository to prevent confusion. This can be achieved by providing information about each package in both the README and the description.", "f38260ae-a0ab-4a4e-a4fb-e5bb6dd953e0": "To ensure that the 'clientMutationId' argument in the 'addComment' mutation is passed as a string, we need to wrap the value in quotes when passing it as an argument. In the provided context, the error message suggests that the 'clientMutationId' argument is expected to be a string, and wrapping the value in quotes will transform it from an integer into a string, which is the expected type. Therefore, we can modify the mutation query as follows:  ```graphql mutation {   addComment(input:{clientMutationId: \"1234\", subjectId: \"MDA6SXNzdWUyMjcyMDA2MTT=\", body: \"Looks good to me!\"}) {     clientMutationId     commentEdge {       node {         body         repository {           id           name           nameWithOwner         }         issue {           number         }       }     }   } } ```  By wrapping the 'clientMutationId' value in quotes, we ensure that it is passed as a string, which meets the expected type requirement.", "206ee8d4-be31-46b2-b6a5-040e5e985e0e": "The Actions secrets tab should be accessed in the repository settings to manage secrets and variables using the web interface. This is mentioned in the context information provided.    How can I limit the permissions granted to credentials generated for GitHub Actions?  To limit the permissions granted to credentials generated for GitHub Actions, it is recommended to grant the minimum permissions possible. For example, instead of using personal credentials, use deploy keys or a service account. Consider granting read-only permissions if that's all that is needed, and limit access as much as possible.  When generating a {% data variables.product.pat_v1 %}, select the fewest scopes necessary. When generating a {% data variables.product.pat_v2 %}, select the minimum permissions and repository access required.  Instead of using a {% data variables.product.pat_generic %}, consider using a {% data variables.product.prodname_github_app %}, which uses fine-grained permissions and short-lived tokens. Unlike a {% data variables.product.pat_generic %}, a {% data variables.product.prodname_github_app %} is not tied to a user, so the workflow will continue to work even if the user who installed the app leaves your organization.  Users with collaborator access to a repository can use the REST API to manage secrets for that repository, and users with admin access to an organization can use the REST API to manage secrets for that organization.    How can I create secrets for a repository using the web interface?  To create secrets for a repository using the web interface, follow these steps:  1. Go to the repository for which you want to create secrets. 2. Click on the settings icon in the top right corner of the repository page. 3. Click on \"Settings\" in the drop-down menu. 4. Click on \"Secrets\" and \"Actions secrets\" tab. 5. Click on the \"New repository secret\" button. 6. Enter a name for the secret and the value you want to assign to it. 7. Click on the \"Add secret\" button to save the secret.  Alternatively, you can also create secrets using the GitHub CLI or the REST API. For more information, see the GitHub", "0e4a47dc-f928-45ce-9da8-a89aeafb0816": "The effect of using the command line option \"--no-database-extension-packs\" in the context information provided is to omit extension packs stored in the database during database creation, either from a Code Scanning configuration file or from extension files stored in the 'extensions' directory of the analyzed codebase. This option is marked as advanced, indicating that it may have unintended consequences if used incorrectly.", "7034f431-f57f-43c8-9561-fe7a097ea211": "Based on the given context, it is unclear whether the author has an opinion on the importance of language design. The provided information only mentions the `--search-path` option for setting up compilation environment, and the `--local-checking`, `--no-metadata-verification`, `--compilation-cache-size`, and `--fail-on-ambiguous-relation-name` options for advanced compilation settings. There is no explicit statement or indication about the author's opinion on the importance of language design.", "83082aa1-b584-4b8a-b2fb-ef0611640f1b": "To determine the `billing_cycle` for a specific user's account, we need to refer to the context information provided. The `billing_cycle` can be either `yearly` or `monthly`. If the `account` owner has a free GitHub plan and has purchased a free {% data variables.product.prodname_marketplace %} plan, then `billing_cycle` will be `nil`.  Based on the context information, we don't have enough information to determine whether the `billing_cycle` is `yearly` or `monthly`. The context only provides the `billing_cycle` for the `account` owner, but it doesn't specify whether it's for the specific user we're interested in. Therefore, we cannot say whether the `billing_cycle` for the specific user's account is `yearly` or `monthly`. More context is needed to make an informed decision.", "4839e9b1-fd2e-432a-b7c7-07417cb9575d": "If you need access to another repository in an existing codespace, or if the permissions you need are specific to you and don't apply to other contributors, you can create a personal access token (PAT) with access to the repository and add the token to your codespace. We recommend you limit the token's access by using a PAT version 2, selecting only the repositories to which you need access, and giving the required access to the Contents permission only. For more information, see the documentation on creating a PAT.  You can then add the token as an environment variable in a codespace, or as a secret for GitHub Codespaces. If you create a secret, you should only allow certain trusted repositories to access the secret. When you add a new secret, you will be prompted to reload your existing codespace to pull in the new secret. For more information, see the documentation on creating a secret.  To use the token to authenticate in your codespace, you have the following options. When you create the environment variable or secret, you can use the name `GH_TOKEN`. The `GH_TOKEN` variable is used by default in GitHub CLI operations, so you can clone the repository using the command `gh repo clone <repository>`.  If you didn't create the codespace from the repository you need access to, then you'll need to follow these steps to grant the necessary permissions. By default, the `GITHUB_TOKEN` in a codespace only has read and write access to the repository from which you created the codespace. This means you may not be able to clone or push to other repositories, even if you have the necessary permissions.  To grant access to another repository, you can either give codespaces access to that repository by listing additional permissions in your dev container configuration, or you can create a PAT with access to the repository and add it to your codespace. If you choose to create a PAT, we recommend limiting the token's access by using a PAT version 2, selecting only the repositories to which you need access, and giving the required access to the Contents permission only.  Once you've granted the necessary permissions, you can add the token as an environment variable in a codespace, or as a secret for GitHub Codes", "4725d1c8-da77-4ceb-84d3-3c7b7eff38aa": "The `configuration_url` attribute in pre-receive hooks is significant because it provides a URL for the endpoint where enforcement is set. This allows for the configuration of the pre-receive hook to be centralized and managed globally, rather than having to configure it separately for each repository. Only site admins are able to access this global configuration, which ensures consistency and standardization across all repositories. This can be particularly useful for organizations with multiple repositories that follow similar workflows or have similar requirements for pre-receive hook behavior.", "0e99f453-a3a6-403e-b52d-4c58da045c49": "Without further context, it is unclear what \"AUTOTITLE\" refers to. If \"AUTOTITLE\" is referring to a specific document feature, it could potentially differ from other document features in terms of functionality based on the specific software or document type being used. More information is needed to accurately answer this question.", "e3702fa9-67de-4e92-b26c-909d83523d3c": "The \"Active Assignment\" view in the {% data variables.product.prodname_classroom %} extension pack is a component of the {% data variables.product.prodname_classroom %} extension that allows students to work on and submit assignments directly from their local development environment. It provides a view alongside the tree-view of files in the repository, where students can push their commits to the latest version of remote by clicking the \"sync changes\" button. This abstracts away source control with Git, making it easier for instructors to teach Git at their own pace. Additionally, syncing changes triggers \"Tests\" to run if a teacher has configured autograding for their assignment. By using the \"Active Assignment\" view, students can collaborate on group projects by starting a Live Share session with anyone in the group node, and they will immediately share the entire context of the repository with them. Overall, the \"Active Assignment\" view simplifies the process of submitting assignments and allows students to focus on learning the material rather than managing Git.", "3bad8f27-6f0d-4351-91f8-1f8bf43ea76d": "Based on the context information provided, it is recommended to use the `default` query suite with default setup for {% data variables.product.prodname_code_scanning %} on {% data variables.product.prodname_dotcom %}. The `default` query suite is highly precise and returns fewer false positive {% data variables.product.prodname_code_scanning %} results compared to the `security-extended` query suite. However, if you prefer to have a greater number of queries with slightly lower precision and severity, you can use the `security-extended` query suite, which is also available for use with default setup for {% data variables.product.prodname_code_scanning %} on {% data variables.product.prodname_dotcom %}.", "6055c905-6925-4304-a440-f249fecf962a": "{% ifversion dependabot-grouped-security-updates %}  1. If {% data variables.product.prodname_dependabot %} fails to update a dependency in a grouped pull request, it will show the failed update in the logs and job summary. To handle this scenario, you should use the `@dependabot recreate` comment on the pull request to build the group again. If the dependency still fails to update, you can use the `exclude-patterns` configuration to exclude the dependency from the group. {% data variables.product.prodname_dependabot %} will then raise a separate pull request to update the dependency.  2. If a grouped pull request for security updates fails or is unable to be merged, we recommend manually opening pull requests to bump the versions of breaking changes. When you manually update a package that is included in a grouped pull request, {% data variables.product.prodname_dependabot %} will rebase the pull request so it does not include the manually updated package. This ensures that the grouped pull request remains focused on security updates, while still allowing for manual updates to specific packages as needed.", "00198649-e880-4c62-bd84-f1a1dc5250db": "Access control for {% data variables.product.prodname_pages %} sites is a feature that allows you to restrict access to your project site by publishing it privately. With access control, only people with read access to the repository the site is published from can access the privately published site. This can be used to share internal documentation or knowledge bases with members of your enterprise. Access control is available for project sites that are published from a private or internal repository that is owned by the organization, but it is not available for organization sites. If your organization uses {% data variables.product.prodname_ghe_cloud %} without {% data variables.product.prodname_emus %}, you can choose to publish your project sites privately or publicly to anyone on the internet.", "0c8dd673-e8d6-4d05-88dd-58d339d86b86": "Based on the provided context information, the syntax for uploading artifacts using {% data variables.product.prodname_actions %} is:  ```yaml - name: Upload math result for job 1   uses: {% data reusables.actions.action-upload-artifact %}   with:     name: homework     path: math-homework.txt ```  This syntax is used in a {% data variables.product.prodname_actions %} workflow to upload an artifact named \"homework\" from the path \"math-homework.txt\" to the repository. The action used is \"action-upload-artifact\" which is provided by GitHub. For more information, you can refer to the documentation provided by GitHub.", "985f0d1e-0359-4919-9527-c7d5f16b0e20": "To ensure that {% data variables.product.prodname_desktop %} is running in the correct compatibility mode, follow these steps:  1. Locate the {% data variables.product.prodname_desktop %} shortcut on your computer. 2. Right-click on the shortcut or hold down the Ctrl key and left-click on the shortcut to open the context menu. 3. Select Properties from the context menu. 4. In the Properties dialog box, select the Compatibility tab. 5. In the \"Compatibility mode\" section, ensure that the Run this program in compatibility mode checkbox is deselected. 6. Click OK to save the changes and close the Properties dialog box.  By following these steps, you can ensure that {% data variables.product.prodname_desktop %} is running in the correct compatibility mode for your operating system. If you are unsure which compatibility mode to use, it is recommended to leave this option deselected to allow {% data variables.product.prodname_desktop %} to automatically detect and use the correct compatibility mode.", "f396ee21-a0a4-4318-b4a3-9e82527e76ae": "You should select the option to prompt users to authorize your app during installation if your app needs to generate a user access token to make API requests on the user's behalf and attribute app activity to the user. This option provides the benefit of allowing your app to generate a user access token without requiring the user to manually authorize your app at a later time, which can improve the user experience and reduce the likelihood of errors or confusion. Additionally, if a user authorizes your app during installation, your app can attribute app activity to the user, which can provide more accurate usage data and analytics. However, if your app does not need to generate a user access token, this option is not necessary and can be ignored.", "cc1cd3aa-dc30-44b8-b25d-b0c4b7088bb8": "Artifacts can be uploaded as files and directories in both GitLab CI/CD and {% data variables.product.prodname_actions %} by specifying the paths of the files or directories to be uploaded in the \"artifacts\" section of the configuration file. The syntax for specifying the paths may differ slightly between the two systems, but the concept is the same.  In GitLab CI/CD, the \"artifacts\" section is included in the job definition, and the paths to be uploaded are specified as an array of strings. In {% data variables.product.prodname_actions %}, the \"artifacts\" section is included in the job definition, and the paths to be uploaded are specified as a key-value pair, where the key is \"paths\" and the value is an array of strings.  Here's an example of the syntax for both systems:  GitLab CI/CD syntax:  ```yaml script:   - echo \"Hello, world!\" > hello.txt   - echo \"This is a test.\" >> hello.txt   - zip -r hello.zip hello.txt  artifacts:   paths:     - hello.txt     - hello.zip ```  {% data variables.product.prodname_actions %} syntax:  ```yaml jobs:   test:     runs-on: ubuntu-latest     steps:     - name: Checkout       uses: {% data reusables.actions.action-checkout %}     - name: Run a simple script       run: echo \"Hello, world!\" > hello.txt && echo \"This is a test.\" >> hello.txt && zip -r hello.zip hello.txt     - name: Upload artifacts       uses: {% data reusables.actions.action-upload-artifact %}       with:         name: My artifacts         path: |           hello.zip ```  In both examples, the \"artifacts\" section is used to specify the paths of the files or directories to be uploaded. In the GitLab CI/CD example,", "fae033db-1da7-4b3c-ae53-d3d95a5802ab": "To manage tags for a specific repository using the REST API, you can follow these steps:  1. Obtain an access token with the necessary scopes (e.g., repo, write:discussion, write:gollum) using the OAuth app or personal access tokens.  2. Send a GET request to the `/repos/{owner}/{repo}/tags` endpoint to retrieve a list of all tags for the specified repository.  3. To create a new tag, send a POST request to the `/repos/{owner}/{repo}/tags` endpoint with the tag name and an optional message in the request body.  4. To update an existing tag, send a PUT request to the `/repos/{owner}/{repo}/tags/{tagname}` endpoint with the updated tag name and message.  5. To delete a tag, send a DELETE request to the `/repos/{owner}/{repo}/tags/{tagname}` endpoint.  6. To view the details of a specific tag, send a GET request to the `/repos/{owner}/{repo}/tags/{tagname}` endpoint.  7. To list all the repositories that a specific tag is associated with, send a GET request to the `/repos/{owner}/{repo}/tags/{tagname}/repos` endpoint.  8. To remove a tag from a specific repository, send a DELETE request to the `/repos/{owner}/{repo}/tags/{tagname}` endpoint with the `--force` flag.  These requests can be made using any HTTP client, such as cURL, Postman, or a programming language's HTTP library. The responses will contain JSON data that can be parsed and processed as needed.", "7a956b73-daff-4fec-ab35-9fc04f583464": "When an organization is removed from an enterprise account, billing, identity management, and 2FA requirements that were previously governed by the enterprise will no longer apply to the organization. The organization will also be downgraded to the free plan. Additionally, the organization will be governed by GitHub's standard Terms of Service, and any internal repositories within the organization will be converted to private repositories.", "9e28d050-0339-4b0e-a913-4c131a1ebca6": "Based on the provided context information, it is unclear what the question is. Please provide more context or clarify the question.", "e19ae90b-ac10-42fd-af61-0afda07cc0ab": "The `--automation-id` option in the CodeQL CLI is used to differentiate between multiple analyses of the same codebase when presenting the results in Code Scanning. It should be a unique identifier that distinguishes each analysis, and it should differ between each analysis for the same codebase. This value is used as the `.automationId` property in SARIF v1, the `.automationLogicalId` property in SARIF v2, and the `.automationDetails.id` property in SARIF v2.1.0.  For example, if you're running multiple analyses for the same codebase, you could use the following command to differentiate between them:  ``` codeql analyze --automation-id=analysis1 my-repo.ql codeql analyze --automation-id=analysis2 my-repo.ql ```  To use this option consistently between runs of the same analysis for different versions of the codebase, you should ensure that the `--automation-id` value is consistent between runs of the same analysis for different versions of the codebase. This will help Code Scanning to understand that these analyses supplement each other rather than supersede each other.  The `--common-caches` option in the CodeQL CLI is used to control the location of cached data on disk that will persist between several runs of the CLI. This includes downloaded QL packs and compiled query plans. The default location for this cache is a directory named `.codeql` in the user's home directory, and it will be created if it doesn't already exist. This cache can be controlled explicitly by providing a directory path with the `--common-caches` option. This can be useful for sharing cached data between multiple users or machines.", "65913fc6-a92a-4c2e-803d-5792c3e76023": "The syntax for caching and artifacts in GitLab CI/CD and {% data variables.product.prodname_actions %} is provided in the context information. For caching, the syntax is shown in the examples provided for both GitLab CI/CD and {% data variables.product.prodname_actions %}. For artifacts, the syntax is also provided for both systems.  In GitLab CI/CD, the syntax for artifacts is included in the `artifacts` section of the job configuration. The `paths` key is used to specify the files or directories to be uploaded as artifacts.  In {% data variables.product.prodname_actions %}, the syntax for artifacts is included in the `artifacts` section of the job configuration. The `name` key is used to specify a name for the artifact, and the `path` key is used to specify the files or directories to be uploaded as artifacts.  The syntax for caching in {% data variables.product.prodname_actions %} is different from that of GitLab CI/CD. In {% data variables.product.prodname_actions %}, the `cache` section is used to specify the cache settings, and the `key` and `paths` keys are used to specify the cache key and the files or directories to be cached, respectively. The `restore-keys` key is used to specify the prefix for the cache keys to be restored.  In summary, the syntax for caching and artifacts in GitLab CI/CD and {% data variables.product.prodname_actions %} is similar, but there are some differences in the syntax for caching in {% data variables.product.prodname_actions %}.", "10d33eb0-f3ce-43bd-bfec-bf3435359be0": "To fix the issue \"Could not create temporary directory: Permission denied\" when checking for updates on {% data variables.product.prodname_desktop %} on macOS, you need to follow these steps:  1. Close {% data variables.product.prodname_desktop %}. 2. Open \"Finder\" and navigate to `~/Library/Caches/`. 3. Right-click `com.github.GitHubClient.ShipIt` and then click **Get Info**. 4. Click the arrow to the left of \"Sharing & Permissions.\" 5. If the Privilege to the right of your user account does not say \"Read & Write,\" click the text and then click **Read & Write**.  This will grant the necessary permissions to the `com.github.GitHubClient.ShipIt` directory, which {% data variables.product.prodname_desktop %} uses to create and unpack temporary files as part of updating the application. After completing these steps, you should be able to successfully check for updates in {% data variables.product.prodname_desktop %}.", "64a71deb-fd5e-4a16-8c63-8390e3b2f9d5": "The available options in the dropdown menu for managing the {% data variables.product.prodname_copilot_chat %} beta for organizations under my enterprise are:  1. Allowed: Enables the {% data variables.product.prodname_copilot_chat %} beta for all organizations under your enterprise. 2. Blocked: Disables the {% data variables.product.prodname_copilot_chat %} beta for all organizations under your enterprise. 3. No policy: Allows each organization under your enterprise to set their own policy for the {% data variables.product.prodname_copilot_chat %} beta.    How do I install or update the {% data variables.product.prodname_copilot %} plugin in JetBrains to use the {% data variables.product.prodname_copilot_chat %} beta?  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  To install or update the {% data variables.product.prodname_copilot %} plugin in JetBrains to use the {% data variables.product.prodname_copilot_chat %} beta, follow these steps:  1. Open your JetBrains IDE and go to **Settings/Preferences** (on Windows, Linux, or Mac). 2. In the left-side menu, click **Plugins**. 3. At the top of the dialog box, click **Marketplace**. 4. In the search bar, type \"GitHub Copilot\" and press Enter. 5. Find the {% data variables.product.prodname_copilot %} plugin and click **Install**. 6. Follow the prompts to install the plugin. 7. After the plugin is installed, restart your JetBrains IDE. 8. To use the {% data variables.product.prodname_copilot_chat %} beta, follow the instructions provided by GitHub to enable the beta for your enterprise.    How do I manage the {% data variables.product.prodname_copilot_chat %} beta for organizations under my enterprise in GitHub", "76e466d0-3130-4089-80f4-198a6ae18f89": "Based on the repeated use of \"AUTOTITLE,\" it is unclear what the main idea or topic of the text material is. Without further context or information, it is impossible to summarize the main idea or topic presented in the text.", "d4371c0e-b278-4581-ad61-9bc503620361": "To schedule a workflow to automatically trigger at a regular interval using `cron` syntax, you can add a `schedule` event to your workflow configuration file. Here's an example:  ```yaml name: My Workflow  on:   workflow_dispatch:   schedule:     - cron: '40 19 * * *' # once a day at 19:40 UTC / 11:40 PST  jobs:   # ... ```  In this example, the workflow will be triggered automatically every day at 19:40 UTC / 11:40 PST using the `cron` syntax. The `cron` syntax is a standard Unix scheduling format that specifies the minute (0-59), hour (0-23), day of the month (1-31), month (1-12), and day of the week (0-6, where 0 and 6 are Sunday) when the workflow should be triggered.  For more information on how to use `cron` syntax, you can refer to the official documentation: https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#cron-syntax-for-scheduled-workflows.", "0228c325-a250-42f4-8fee-fbd148bfbef9": "Regular expressions can be used in the `path:` qualifier to search for specific files or directories by escaping special characters and using wildcard characters like `*` and `?`. For example, to search for files with the extension `.txt`, you can use the regular expression `*.txt`. This will match any file name that ends with `.txt`.  2. The `symbol:` qualifier can be used to search for symbol definitions in code using the open source Tree-sitter parser ecosystem. This feature is supported by several programming languages, including Go, JavaScript, TypeScript, Python, and C++.  To search for a method called `deleteRows`, you can use the following query:  ```text language:javascript symbol:deleteRows ```  This will search for the symbol `deleteRows` in JavaScript files. If the method is a member of a class, you can also search for it using a prefix of the class name. For example, to search for the `deleteRows` method of a class called `Table`, you can use:  ```text language:javascript symbol:Table.deleteRows ```  This will search for the `deleteRows` method of the `Table` class in JavaScript files.", "b5727836-4eb4-4e3d-a629-fa0d199a5d39": "If you are unsure whether a proposed gift or expense falls within the acceptable limits, you should consult with Legal for guidance before proceeding. The process for seeking approval for gifts and expenses exceeding $150 is to obtain written approval from Legal in advance. This applies to both giving and receiving gifts, as both situations can potentially raise issues of improper influence or bribery. It's always better to err on the side of caution and seek approval rather than risk violating our company's policies and potentially damaging our reputation.", "1cf4249e-eb95-4abe-b255-9294cf0d79cb": "To automate the process of building and pushing Docker images to {% data variables.product.prodname_registry %} using GitHub Actions, follow these steps:  1. Create a new repository on GitHub and add your Dockerfile to the root directory.  2. Create a new workflow file named `build-push.yaml` in the `.github/workflows` directory of your repository.  3. Add the following YAML configuration to your `build-push.yaml` file:  ```yaml name: Build and Push Docker Image  on:   push:     branches:       - main  jobs:   build-push:     runs-on: ubuntu-latest      steps:     - name: Checkout       uses: actions/checkout@v2      - name: Login to Docker Hub       uses: docker/login-action@v1       with:         username: ${{ github.actor }}         password: ${{ secrets.DOCKERHUB_TOKEN }}      - name: Set up metadata       uses: actions/setup-metadata@v1       with:         images: my-image:latest      - name: Build and push Docker image       uses: docker/build-push-action@v2       with:         context: .         push: true ```  In this example, replace `my-image` with the name of your Docker image and `latest` with the tag you want to use.  4. Save the file and commit your changes.  5. Go to your repository's Settings > Secrets > New repository secret and create a new secret named `DOCKERHUB_TOKEN` with your Docker Hub API token.  6. Push a change to your repository to trigger the workflow.  The `login-action` options required for {% data variables.product.prodname_registry %} are:  - `registry`: Must be set to `ghcr.io` for {% data variables.product.prodname_registry %}. This option specifies the registry where the images will be pushed. - `username`", "abab28e9-c540-4c31-b251-fdb0020c6a67": "The purpose of {% data variables.product.prodname_dependabot %} closing old pull requests and opening new ones is not explicitly stated in the given context information. However, based on the context, it can be inferred that {% data variables.product.prodname_dependabot %} is a tool that automatically manages dependencies in a repository, and it may be closing old pull requests related to dependency updates and opening new ones to apply the latest security updates. This feature is called \"grouped {% data variables.product.prodname_dependabot_security_updates %}\" and can be enabled or disabled for individual repositories or organizations. Additionally, the behavior of {% data variables.product.prodname_dependabot %} can be customized using a configuration file.", "394d3c8a-7e50-4a2f-a100-507eda75e4e5": "To convert an Azure DevOps pipeline to GitHub Actions using the gh actions-importer tool, follow these steps:  1. First, make sure you have the gh CLI installed on your local machine. You can download it from the official GitHub website (https://github.com/cli/cli/releases) and follow the installation instructions for your operating system.  2. Next, open your terminal and navigate to the root directory of your GitHub repository.  3. Run the following command to migrate your Azure DevOps pipeline to GitHub Actions:  ```shell gh actions-importer migrate azure-devops pipeline --pipeline-id :pipeline_id --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate ```  Replace `:pipeline_id` with the ID of the pipeline you want to convert, `https://github.com/octo-org/octo-repo` with the URL of your GitHub repository, and `tmp/migrate` with the desired output directory.  4. The command will generate a pull request on your GitHub repository with the converted workflow. Review the pull request and merge it to apply the changes.  To convert an Azure DevOps release pipeline to GitHub Actions using the gh actions-importer tool, follow these steps:  1. First, make sure you have the gh CLI installed on your local machine. You can download it from the official GitHub website (https://github.com/cli/cli/releases) and follow the installation instructions for your operating system.  2. Next, open your terminal and navigate to the root directory of your GitHub repository.  3. Run the following command to migrate your Azure DevOps release pipeline to GitHub Actions:  ```shell gh actions-importer migrate azure-devops release --pipeline-id :pipeline_id --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate ```  Replace `:pipeline_id` with the ID of the pipeline you want to convert, `https://github.com/octo-org/octo-repo` with", "fd2318bd-a563-4c20-b743-81a5e21d17d8": "SSH (Secure Shell) is a network protocol that allows secure remote access to a computer or server. In the context provided, SSH can be used for authentication and commit signing in {% ifversion fpt or ghec or ghes %}{% data variables.location.product_location %}{% elsif ghae %}{% data variables.product.product_name %}{% endif %}. Authentication refers to the process of verifying the identity of a user attempting to access a system or repository. By adding an SSH key to your account, you can authenticate yourself without the need for a password, making it a more secure and convenient way to access your repositories. Commit signing, on the other hand, is the process of digitally signing Git commits and tags. By using SSH for commit signing, you can add an extra layer of security to your commits by verifying that they were indeed made by you. This can be especially useful in collaborative environments where multiple developers are working on the same project.", "c574287e-b115-487a-a29b-b8265139f470": "The purpose of the `login-action` in the provided workflow is to log in to the GitHub Docker Registry using the `login-action` provided by GitHub. This action is necessary to authenticate the workflow with the registry and allow it to push the Docker image.  The purpose of the `build-push-action` in the provided workflow is to build and push the Docker image to the GitHub Package Registry. This action is provided by GitHub and is used to automate the process of building and publishing Docker images to the registry. It extracts labels and tags for the Docker image and uses them to publish the image to the registry.  If the context didn't provide enough information to determine the specifics of the `login-action` and `build-push-action`, such as the version of GitHub being used, then it would be necessary to consult the documentation or the specific workflow being used to determine the exact implementation of these actions.", "78b2849d-4126-48cd-9194-cbee374326df": "Customers under a Microsoft Enterprise Agreement can enable {% data variables.product.prodname_copilot %} for their enterprise account by following the steps provided in the \"Enabling {% data variables.product.prodname_copilot %} for your enterprise account\" section. This involves logging in to their Microsoft Azure DevOps organization, navigating to the \"Billing and subscriptions\" page, selecting their GitHub Enterprise subscription, and then enabling {% data variables.product.prodname_copilot %} for their organization. If they have not previously authorized {% data variables.product.prodname_vscode %} in their {% data variables.product.prodname_dotcom %} account, they will also need to authorize {% data variables.product.prodname_vscode %} in their browser.", "ba835943-93ad-4ef5-8775-0653a2319470": "To migrate your enterprise to a new tenant on your IdP using {% data variables.product.prodname_emus %} without causing disruption to integrations and automated flows, you should follow these steps:  1. Determine whether the normalized SCIM `userName` attribute values will remain the same for your {% data variables.enterprise.prodname_managed_users %} in the new environment.  2. If the normalized SCIM `userName` attribute values will remain the same, you can complete the migration by yourself. Here's how:     a. Create a new SAML configuration for your new tenant on your IdP.     b. Configure your new tenant in {% data variables.product.prodname_emus %}.     c. Test your new configuration to ensure that it's working correctly.     d. Disable your current SAML configuration in {% data variables.product.prodname_emus %}.     e. Wait for the {% data variables.product.pat_generic_plural %} and SSH keys associated with your {% data variables.enterprise.prodname_managed_users %} to be deleted.     f. Create and deploy new keys to your integrations where necessary.  3. If the normalized SCIM `userName` attribute values will change, {% data variables.product.company_short %} will need to help with your migration. Contact {% data variables.product.company_short %} support for assistance.  Remember, migrating to a new IdP or tenant can cause disruption to integrations and automated flows in your enterprise, so it's important to plan for a migration window during which you can create and deploy new keys to your integrations where necessary.", "1d76aee8-2764-4007-b802-c2e351d48063": "To reprovision a suspended user in {% data variables.product.prodname_dotcom %} using the REST API, you need to follow these steps:  1. Set the user record's `active` attribute to `true` using a `POST`, `PUT`, or `PATCH` request.  2. Send the request to the REST API endpoint for SCIM operations.  3. If the user was permanently suspended, a subsequent provisioning event will create a new account for the person who's signing in.  4. The newly provisioned account will have no relationship to the original account.  5. For more information, you can refer to the REST API documentation for SCIM operations, specifically \"Provision a SCIM enterprise user\", \"Set SCIM information for a provisioned enterprise user,\" or \"Update an attribute for a SCIM enterprise user\".  6. If you're experiencing rate-limiting or want to review any requests to the REST API endpoints for SCIM, you can refer to the \"About rate limits\" documentation or filter events from the `EnterpriseUsersScim` or `EnterpriseGroupsScim` controllers in the audit log.  7. If your integration manages access using groups on your IdP, you can review `external_group` category events in your enterprise's audit log to confirm the success of SCIM calls, or view troubleshooting information in the web interface for {% data variables.product.prodname_dotcom %}.", "1688c9aa-8cbc-43af-8626-092585bf5e8b": "According to the context information provided, when a private repository is deleted, all of its private forks are also deleted.", "755297a6-e56b-4e48-b9bc-a16810097aaf": "To confirm authentication in the \"{% data variables.product.prodname_vscode %}\" dialogue box, click **Open**. This step is necessary when setting up {% data variables.product.prodname_copilot %} for the first time or when re-authenticating your account. The dialogue box will appear when you open a repository or create a new one in {% data variables.product.prodname_vscode %} and {% data variables.product.prodname_copilot %} is not yet configured. Clicking **Open** will allow {% data variables.product.prodname_copilot %} to access your GitHub repositories and provide suggestions based on your code and the code of other GitHub users.", "a26d0db4-3368-468e-bb93-ab7d135302ca": "The purpose of using teams to manage access to repositories is to simplify the process of granting or denying access to multiple repositories to a group of users. Instead of adding each user individually to each repository, you can add them to a team and grant access to all repositories associated with that team. This approach makes it easier to manage access and reduces the risk of errors or oversights.  To manage team members through an identity provider (IdP), you can link your GitHub teams to the corresponding groups in your IdP. This integration allows you to automatically add or remove users from your GitHub teams based on their membership in the corresponding IdP groups. This approach ensures that your GitHub teams are always up-to-date with your organization's user management system, reducing the need for manual updates and minimizing the risk of errors.", "ffe7e564-59a7-4bf2-b9f6-e2b67d022f6b": "The warning message will inform you that deleting your organization account will remove all repositories, forks of private repositories, wikis, issues, pull requests, and project or organization pages. It will also state that your billing will end and, after 90 days, the organization name becomes available for use on a new user or organization account. If you have a {% data variables.product.prodname_ghe_cloud %} or {% data variables.product.prodname_ghe_server %} account, a note will also inform you that a site administrator for {% data variables.location.product_location %} may be able to partially restore a deleted organization. If you have a {% data variables.product.prodname_ghe_cloud %} account, you will also be prompted to enter the organization name to confirm the deletion.", "fbb8eaaa-5aa5-48f8-a22e-2cc296873b3f": "In {% ifversion fpt %}FPT{% endif %} and GHEC environments, the \"AUTOTITLE\" feature is used to automatically generate a title for a repository based on its contents. However, the implementation and behavior of this feature may differ between the two environments. Without further information, it is unclear how these differences manifest. If you have access to specific documentation or resources for both FPT and GHEC, you may be able to compare and contrast their \"AUTOTITLE\" features. Otherwise, you may need to consult the official documentation or contact support for more details.", "133ccae1-8adf-4bf9-ba0f-ddb07f542485": "The `consul-datacenter` and `datacenter` properties in the context provided are used to specify the datacenter name for Consul and the datacenter name for the cluster, respectively. This is necessary for multi-datacenter deployments of Consul, where multiple clusters can be managed as a single logical entity. In this context, all nodes are part of the primary datacenter, which is specified by setting both properties to \"primary\". This ensures that all nodes in the cluster are aware of each other and can communicate and share resources within the same datacenter.", "6cd4eb4c-8c6e-4441-abea-1fc99ba9c5d5": "GitHub SIRT carries out the following activities for incident resolution:  - Engages relevant internal teams to eradicate, restore, and secure. - Collection and storage of evidence for internal use as well as potential law enforcement involvement. - Notification to affected constituents. - Postmortem authoring with lessons learned and post-incident repair items.  The responsible party for eradicating, restoring, and securing during an incident is GitHub SIRT, as they engage relevant internal teams to carry out these activities.", "1464499c-3258-4ce3-8dee-c1c78f68f2d9": "To search for events performed by a specific user on GitHub using the {% data variables.product.product_name %} search feature, you can use the `actor` qualifier followed by the username of the user you want to search for. For example, `actor:octocat` will find all events performed by the user with the username \"octocat\".  2. To search for events related to a specific organization on GitHub using the {% data variables.product.product_name %} search feature, you can use the `org` qualifier followed by the name of the organization you want to search for. For example, `org:my-org` will find all events that occurred for the organization \"my-org\".  3. To search for specific types of events on GitHub using the {% data variables.product.product_name %} search feature, you can use the `action` qualifier followed by the category and associated event you want to search for. Here's a list of categories and associated events:     | Category name | Description", "44291d85-c5a9-4400-ab24-81a6bcb0decc": "The URL parameter 'scope' in the context provided is used to define the scopes requested by the application. It allows the user to grant specific permissions to the application, such as access to their email address or the ability to create repositories on their behalf. This parameter helps to ensure that the user has control over the level of access granted to the application. Without this parameter, the application may request excessive permissions, which could potentially compromise the user's data.", "3e3b9d0b-ac32-4a9e-bfd1-4835355a8c49": "To view the status of each job and step in a workflow, you can go to your repository's Actions tab and select the workflow run you want to view. From there, you can click on the job to expand it and see the status of each step within that job. Alternatively, you can click on the ellipsis (...) next to the job and select View run logs to see detailed logs for that job. This information can also be found in the workflow run history, which can be accessed by clicking on the workflow run and then selecting the History tab.", "8907418e-c1a9-47f6-9842-36f15fb8113c": "To alert your community about the security vulnerability in previous versions of your project, as suggested in the context information, you should follow these specific steps:  1. Check the \"AUTOTITLE\" section of your project's documentation or configuration file to ensure that it accurately reflects the nature of the security vulnerability. If it doesn't, update it to provide clear and concise information about the issue.  2. Compose an email or message to your community, explaining the nature of the security vulnerability and its potential impact. Be sure to include details such as the affected versions of your project, the specific security issue, and any steps that users can take to mitigate the risk.  3. Provide clear instructions for users to upgrade to the latest version of your project, which should contain a fix for the security vulnerability. If upgrading is not possible, suggest alternative workarounds or mitigation strategies.  4. Encourage your community to report any further issues or concerns related to the security vulnerability, and assure them that you will provide regular updates as the situation develops.  5. Test the updated version of your project thoroughly to ensure that it addresses the security vulnerability and does not introduce any new issues.  6. Release the updated version of your project, and communicate the availability of the fix to your community.  7. Monitor your project's issue tracker and user feedback for any further reports of the security vulnerability, and respond promptly to any new issues that arise.  By following these steps, you can ensure that your community is informed about the security vulnerability in a timely and effective manner, and that they have the information they need to protect themselves and their systems.", "15cb90cd-3dbd-4459-a6cf-860cb0044ed2": "According to the context provided, GitHub uses required cookies for essential website functions and to provide services, analytics cookies to understand website usage and provide personalized advertising, social media cookies to show ads and content based on social media profiles and activity, and advertising cookies to show new ads based on previous ad interactions.", "7d52306c-32de-430d-8a6f-fb14c3a3310a": "To set up streaming to Azure Event Hubs in {% data variables.product.prodname_dotcom %}, follow these steps:  1. Log in to your Microsoft Azure account and navigate to the Event Hubs service. 2. Create a new event hub namespace or select an existing one. 3. Create a new event hub instance within the namespace or select an existing one. 4. In the Azure portal, go to the event hub instance you want to stream to and click on \"Shared Access Policies\" in the left-hand menu. 5. Select a shared access policy or create a new one. 6. Copy the connection string from the \"Connection string-primary key\" field. 7. Go to {% data variables.product.prodname_dotcom %} and navigate to the Log Streaming tab. 8. Select \"Configure stream\" from the dropdown menu and choose \"Azure Event Hubs\". 9. Enter the name of the Azure Event Hubs instance and paste the connection string you copied in step 6. 10. Click \"Check endpoint\" to verify that {% data variables.product.prodname_dotcom %} can connect and write to the Azure Event Hubs endpoint.  To set up streaming to Datadog in {% data variables.product.prodname_dotcom %}, follow these steps:  1. Log in to your Datadog account and navigate to the \"Integrations\" section. 2. Search for \"GitHub\" and select \"GitHub\" from the list of integrations. 3. Click \"Create API key\" and copy the API key. 4. Go to {% data variables.product.prodname_dotcom %} and navigate to the Log Streaming tab. 5. Select \"Configure stream\" from the dropdown menu and choose \"Datadog\". 6. Enter your Datadog API key and select the organization and repository you want to stream from. 7. Click \"Check endpoint\" to verify that {% data variables.product.prodname_dotcom %} can connect and write to the Datadog endpoint. 8. To view your audit log data in Datadog, go to the \"Logs\" section and filter by \"github.audit.streaming\".  If you don't", "3403c335-3a41-466f-8e2b-927957b45906": "The purpose of the \"codeql database analyze\" command, as described in the context information, is to generate detailed information about alerts and diagnostic data from the database generation step, as well as summary metrics. This command can also be used to export and upload diagnostic information to GitHub even if the {% data variables.product.prodname_codeql %} analysis fails. This information can be useful for determining the number of problems in the code and whether there were errors generating the {% data variables.product.prodname_codeql %} database, especially for repositories with few alerts.", "afe1cee9-952c-49bb-83de-f58666545aae": "Pre-receive hooks are scripts that run automatically on GitHub before a push is accepted into a repository. Their purpose is to validate the changes being pushed and ensure that they meet certain criteria, such as passing automated tests or adhering to coding standards. If the hook script fails, the push is rejected and the changes are not merged into the repository. Pre-receive hooks can help prevent errors and ensure the quality and consistency of the codebase.", "5e319927-ff5e-4c99-9333-4c5301844681": "The `$(Agent.WorkFolder)` variable in the context information represents the directory where the agent performs the build and test tasks. It is equivalent to the `GITHUB_WORKSPACE` variable in GitHub Actions, and it is where the source code is checked out and where the build process takes place. The value of this variable can be accessed and used in scripts and build configurations to perform tasks specific to the current build context.", "967d6f73-1009-40bc-97cd-2e55937d21da": "Yes, one frequently used Git command that is included in the \"Using Git\" cheat sheet is `git pull`. This command combines `git fetch` and `git merge` into a single command, fetching changes from a remote repository and merging them into the current branch. It is commonly used to update a local repository with changes made by other collaborators.", "038931a5-b5d8-4e5c-b220-495e417cdc11": "Multiple GitHub tokens that are pending approval for an organization can be approved or denied simultaneously in the GitHub interface by following these steps:  1. Access the organization settings by clicking on the organization name in the top left corner of the GitHub page, then selecting \"Settings\" from the drop-down menu. 2. In the left sidebar, click on \"Developer settings\" and then select \"Personal access tokens\" from the drop-down menu. 3. In the \"Personal access tokens\" page, click on \"Pending requests\" to view any pending requests for GitHub tokens. 4. Use the checkboxes to select the tokens that you want to approve or deny. 5. Select the \"request selected...\" drop-down menu and choose either \"Approve...\" or \"Deny...\" to perform the desired action on the selected tokens.  This feature allows for more efficient management of GitHub tokens, especially for organizations with multiple developers requesting access.", "7f0311eb-1144-425f-9c85-21adf3385911": "Registered developers in the Program are prohibited from granting any rights, titles, or interests in or to GitHub's trademarks to any party. They are also restricted from challenging or assisting others in challenging GitHub's registration or enforcement of its trademarks. Additionally, they cannot attempt to adopt, use, apply for, or register any trademark, service mark, logo, URL, Internet domain name, or symbol that is confusingly similar to GitHub's trademarks. Regarding confidential information, developers must keep any non-public information provided by GitHub secret and use it only for the authorized purposes of the Program. They must also prevent unauthorized use, disclosure, publication, or dissemination of the confidential information and limit disclosure to their employees and contractors who need to know it for the Program's purposes. Unauthorized disclosure or use of confidential information could result in negative consequences for the developer.", "26346278-869e-49c5-8537-9e5d042e6a09": "The {% data variables.product.prodname_codeql_cli %} can analyze dynamic languages such as JavaScript and Python, as well as compiled languages such as C/C++, C#, and Java. It can be used for code scanning in third-party continuous integration (CI) systems by running {% data variables.product.prodname_codeql %} analyses using queries provided by {% data variables.product.prodname_dotcom %} engineers and the open source community, generating code scanning alerts, and creating {% data variables.product.prodname_codeql %} databases to use in the {% data variables.product.prodname_codeql %} for {% data variables.product.prodname_vscode %} extension. This allows developers and security researchers to secure their code using {% data variables.product.prodname_codeql %} analysis, even when using external CI systems.", "71d99ee7-c79e-45fc-b170-b798e5cfd1d7": "{% data variables.product.prodname_emus %} is a feature provided by {% data variables.product.company_short %} that allows managing single sign-on (SSO) for unauthenticated users in an enterprise. It can be used to customize the behavior of {% data variables.product.company_short %} when an unauthenticated user attempts to access private resources in the enterprise. By default, {% data variables.product.company_short %} displays a 404 error to hide the existence of private resources, but with {% data variables.product.prodname_emus %}, you can choose to redirect the user to a custom login page or display a custom message instead of the error. This can prevent confusion for developers and provide a more consistent user experience.", "46921fd8-0a01-4f2a-9c19-8136a09f0ffd": "The purpose of the \"repair offset\" used in repair jobs for parallelization in {% data variables.location.product_location %} is to allow multiple background jobs to synchronize work based on an offset into the database table for the record being reconciled. This helps to minimize the effects on I/O performance and reduce the chances of operations timing out during the repair job, which reconciles the search index with database and Git repository data.", "3b2ee4bc-59e8-4576-8c4d-a9762303a42f": "To activate or deactivate the {% data variables.product.prodname_github_app %} webhook, follow these steps:  1. Go to the {% data variables.product.prodname_github_apps %} settings page for your app. 2. Click the \"Webhooks\" tab. 3. To activate the webhook, toggle the switch next to \"Active\" to the right (green). 4. To deactivate the webhook, toggle the switch next to \"Active\" to the left (gray).  Note: If you're not sure whether the webhook is currently active or not, check the position of the switch. If it's to the right (green), the webhook is active. If it's to the left (gray), the webhook is deactivated.", "ec50462c-b6be-466a-9b37-8ead6c064d9a": "Dependencies can be cached in a GitHub Actions workflow using Maven by using the `setup-java` action or the `cache` action for custom and more advanced configuration. The cache key will be the hashed contents of `pom.xml`, so changes to `pom.xml` will invalidate the cache. This will save the contents of the local Maven repository, located in the `.m2` directory of the runner's home directory, and restore it in future workflow runs, so dependencies don't need to be downloaded from remote Maven repositories.", "deb275e3-464b-4350-9c8c-669b9d5afa3c": "Many U.S. congresspeople follow policies that prohibit or restrict accepting gifts or invitations from customers or prospective customers, especially if the value is considered \"of value.\" It's always best to ask them what their specific policies are to avoid any potential violations. Some examples of gifts or invitations that might be considered improper include first class airfare, winery tours, recommendations for prestigious private schools, tickets to major events, and contributions to charities. However, some smaller gifts, such as swag, reasonably priced dinners, and small boxes of chocolates, are generally allowed. If in doubt, it's best to consult with legal counsel before accepting any gifts or invitations.", "c4693422-bf40-4511-a638-1481fb779503": "According to the provided context, the GitHub Products agreement supersedes any prior proposals, agreements, or communications (oral or written) between the user and GitHub regarding the GitHub Products. This agreement governs the user's use of the GitHub Products and includes applicable GitHub Products terms and GitHub's Privacy Statement, as outlined in the Order Forms. Therefore, any prior agreements or communications related to the GitHub Products are no longer valid or applicable.", "203b2233-958d-4253-a187-e609f93cd737": "To configure credentials for using GitLab and GitHub Actions Importer, you need to follow these steps:  1. Create a GitLab personal access token (PAT) with the required scopes. For more information, see the GitLab documentation on creating a PAT.  2. Create a GitHub {% data variables.product.pat_v1 %} with the required scopes. For more information, see the GitHub documentation on creating a PAT.  3. Run the `configure` command in the GitHub CLI with the following arguments:     ```    gh actions-importer configure --gitlab-token <GITLAB_PAT> --github-token <GITHUB_PAT>    ```     Replace `<GITLAB_PAT>` and `<GITHUB_PAT>` with the respective tokens you created in steps 1 and 2.     This command will save your credentials securely and allow you to use {% data variables.product.prodname_actions_importer %} to migrate your GitLab pipelines to GitHub Actions.     Note: If you're using an organization account, you can also set the `--audit` flag to enable auditing of GitLab pipelines during the migration process. This feature is only available for organization accounts.     For more information on the `configure` command, see the GitHub CLI documentation on the `actions-importer` extension.", "86c8eca9-c3e4-407d-845d-0ec871d3b297": "To authenticate to a repository that you did not create using Git in a codespace, you need to have permission to access the repository. If the repository is a public repository, you can clone it using HTTPS or GitHub CLI's `gh repo clone` command. If the repository is a private repository, you need to have a personal access token (PAT) with the necessary permissions.  Here's how you can authenticate to a private repository using a PAT:  1. Go to your {% data variables.product.prodname_dotcom %} settings. 2. Click on Developer settings. 3. Click on Personal access tokens. 4. Click on the Generate new token button. 5. Enter a name for the token, select the scopes you need, and click on the Generate token button. 6. Copy the generated token. 7. In your codespace, run the following command to set the `GIT_AUTHORIZATION` environment variable:  ```shell export GIT_AUTHORIZATION=\"Bearer <YOUR_PAT>\" ```  Replace `<YOUR_PAT>` with the PAT you copied in step 6.  8. Clone the repository using HTTPS or GitHub CLI's `gh repo clone` command.  If you're using HTTPS, you can clone the repository using the following command:  ```shell git clone https://github.com/<OWNER>/<REPOSITORY>.git ```  Replace `<OWNER>` and `<REPOSITORY>` with the owner and name of the repository you want to clone.  If you're using GitHub CLI's `gh repo clone` command, you can clone the repository using the following command:  ```shell gh repo clone <OWNER>/<REPOSITORY> ```  Replace `<OWNER>` and `<REPOSITORY>` with the owner and name of the repository you want to clone.  After cloning the repository, you can use Git to push, pull, and make changes to the repository.  Note: It's recommended to use HTTPS instead of SSH", "6f57b283-1dfe-440b-a5ef-8ee8e12e80c9": "To display the security and analysis settings for an organization, you can click the name of the organization. On the \"Code security & analysis\" settings page, you can scroll to the \"{% data variables.product.prodname_GH_advanced_security %} repositories\" section to see a detailed breakdown of usage by repository for that organization. This information includes the number of unique committers, the number of commits, and the number of {% data variables.product.prodname_advanced_security %} licenses being used in the organization's repositories. If the context information mentions a UI update, then the information may also include a summary of the organization's license usage and a breakdown of usage by repository for the entire organization, as well as the ability to download a CSV file with detailed usage information.", "74eb0bb6-75d8-4506-a912-c8bfd30787ad": "To filter the \"Security risk\" view to display only private repositories with at least one security vulnerability, you can use the following qualifiers:  - `is:private` to display only private repositories. - `vulnerability-alerts:any` to display only repositories with at least one security vulnerability.  Here's the filter syntax:  ``` is:private vulnerability-alerts:any ```  This will display a list of private repositories with at least one security vulnerability.    How can I filter the \"Security coverage\" view to display only repositories with enabled code scanning and advanced security features?  Answer:  To filter the \"Security coverage\" view to display only repositories with enabled code scanning and advanced security features, you can use the following qualifiers:  - `code-scanning-push-protection:enabled` to display only repositories with enabled code scanning. - `advanced-security:enabled` to display only repositories with enabled advanced security features.  Here's the filter syntax:  ``` code-scanning-push-protection:enabled advanced-security:enabled ```  This will display a list of repositories with both enabled code scanning and advanced security features.    How can I filter the \"Security risk\" view to display only repositories with at least one security vulnerability and enabled code scanning?  Answer:  To filter the \"Security risk\" view to display only repositories with at least one security vulnerability and enabled code scanning, you can use the following qualifiers:  - `vulnerability-alerts:any` to display only repositories with at least one security vulnerability. - `code-scanning-push-protection:enabled` to display only repositories with enabled code scanning.  Here's the filter syntax:  ``` vulnerability-alerts:any code-scanning-push-protection:enabled ```  This will display a list of repositories with both at least one security vulnerability and enabled code scanning.    How can I filter the \"Security coverage", "962f5928-e72d-4064-a698-9caed8d4b135": "To give access to migrated repositories in {% data variables.product.prodname_dotcom %} after migration from Bitbucket Server, you can create teams, add organization members to those teams, and give each team access to the repository. This is necessary because permissions work differently in {% data variables.product.prodname_dotcom %} than in Bitbucket Server, and {% data variables.product.prodname_importer_proper_name %} does not attempt to migrate repository permissions from Bitbucket Server. For more information on creating teams, adding organization members to teams, and giving teams access to repositories, refer to the documentation provided in the context information.", "5c74f51a-ed60-4bfd-a048-5320f05a8c33": "To set up a subtree merge, follow these steps:  1. Create a new directory and navigate to it.     ```shell    mkdir test    cd test    ```  2. Initialize a new Git repository.     ```shell    $ git init    > Initialized empty Git repository in /Users/octocat/tmp/test/.git/    ```  3. Add a new remote URL pointing to the separate project that you're interested in.     ```shell    $ git remote add -f spoon-knife https://github.com/octocat/Spoon-Knife.git    > Updating spoon-knife    > warning: no common commits    > remote: Counting objects: 1732, done.    > remote: Compressing objects: 100% (750/750), done.    > remote: Total 1732 (delta 1086), reused 1558 (delta 967)    > Receiving objects: 100% (1732/1732), 528.19 KiB | 621 KiB/s, done.    > Resolving deltas: 100% (1086/1086), done.    > From https://github.com/octocat/Spoon-Knife    >  * [new branch]      main     -> Spoon-Knife/main    ```  4. Merge the `Spoon-Knife` project into the local Git project. This doesn't change any of your files locally, but it does prepare Git for the next step.     If you're using Git 2.9 or above:     ```shell    $ git merge -s ours    ```     This command performs a \"subtree merge\" using the `ours` strategy, which means that the changes from the `Spoon-Knife` repository will be merged into the current repository, but any conflicts will be resolved by keeping the current repository's version.     Note: If you're using an older version of", "e00664d7-56e7-47da-8747-699c414ba1c8": "To ensure that your GitHub Actions workflow has access to your package in a personal repository, follow these steps:  1. Go to your personal repository that contains the workflow. 2. Click on \"Settings\" in the repository menu. 3. Click on \"Packages\" in the left-hand menu. 4. Click on the package for which you want to grant access to your workflow. 5. Click on \"Access\" in the left-hand menu. 6. Click on \"Add repository\" and select the repository where your workflow is stored. 7. Select the role you want to grant to the repository. The available roles are \"Maintain\" and \"Read\". 8. Click on \"Save\" to add the repository and grant access to your workflow.  To further customize access to your package for your personal account, you can follow these steps:  1. Go to your personal repository that contains the package. 2. Click on \"Settings\" in the repository menu. 3. Click on \"Packages\" in the left-hand menu. 4. Click on \"Settings\" in the left-hand menu. 5. Scroll down to the \"Access\" section and click on \"Configure access\". 6. Choose whether you want to grant access to all users, specific users, or specific teams. 7. If you choose to grant access to specific users or teams, enter their GitHub usernames or team names. 8. Select the role you want to grant to the users or teams. The available roles are \"Maintain\" and \"Read\". 9. Click on \"Save\" to save your changes and grant access to your package.  To ensure that your GitHub Actions workflow has access to your package for an organization, follow these steps:  1. Go to your organization's repository that contains the workflow. 2. Click on \"Settings\" in the repository menu. 3. Click on \"Packages\" in the left-hand menu. 4. Click on the package for which you want to grant access to your workflow. 5. Click on \"Access\" in the left-hand menu. 6. Click on \"Add repository\" and select the repository where your workflow is stored. 7. Select the role you want to grant to the", "7f86dd5c-3179-4468-b1d1-c0798af9290d": "The default branch name used by {% data variables.product.product_name %} for new repositories created on {% data variables.location.product_location %} is not explicitly stated in the context information provided. However, the context does mention that a new repository created on {% data variables.location.product_location %} contains one branch, which is the default branch. It also provides a link to \"AUTOTITLE\" for further reading, which may provide more information about the default branch name. If you do not have prior knowledge about this, then you cannot give an answer without further information.", "5dde17d0-a699-494e-808e-af22a556332e": "To grant a GitHub Codespace access to additional repositories beyond the one it was created from, you can list additional permissions in your dev container configuration. This will allow users to authorize the additional permissions when they create a codespace. However, it will not change the permissions of an existing codespace. If you need access to another repository in an existing codespace, or if the permissions you need are specific to you and don't apply to other contributors, you can create a personal access token (PAT) with access to the repository and add the token to your codespace. We recommend you limit the token's access by using a PAT version 2, selecting only the repositories to which you need access, and giving the required access to the Contents permission only. You can then add the token as an environment variable in a codespace, or as a secret for GitHub Codespaces. If you create a secret, you should only allow certain trusted repositories to access the secret. When you add a new secret, you will be prompted to reload your existing codespace to pull in the new secret.", "5ac0a224-0a53-4c0d-ac8d-8d17dd99ab8e": "To block a user in a comment on GitHub, follow these steps:  1. Navigate to the comment made by the user you want to block. 2. Click the three horizontal dots icon in the upper-right corner of the comment. 3. Select \"Block user\" from the dropdown menu. 4. If you want to set a time limit for the block, select the \"Block user\" dropdown menu and choose the desired time limit. 5. If you want to hide all comments made by the user in the organization, select \"Hide this user's comments\" and choose a reason. 6. If you want to notify the user why they're being blocked, select \"Send a notification to this user\". 7. Click \"Block user from organization\" or \"Block user from organization and send message\" to complete the process.  To block an organization member on GitHub, follow these steps:  1. First, remove the user from the organization by following the instructions in \"AUTOTITLE\". 2. Go to the organization settings by following the instructions in \"AUTOTITLE\". 3. Click \"Block users\" under the \"Membership\" section. 4. Type the username of the user you want to block in the text field. 5. If you want to set a time limit for the block, select the \"Block options\" dropdown menu and choose the desired time limit. 6. Click \"Block user\" to complete the process. 7. Optionally, you can add a note to describe why the user was blocked by clicking \"Add Note\". The note will be visible to the owners and moderators of the organization.", "0269cca9-eec4-4040-84bf-64e334c5ece5": "The \"shared_steps\" section in the CircleCI configuration file defines a set of steps that can be reused by other jobs or steps in the configuration file. In this case, it includes common setup tasks such as checking out the repository, restoring cached dependencies, running bundle install, caching dependencies, waiting for the database, setting up the environment, setting up the database, and running tests. By defining these steps as a shared resource, they can be easily reused by other jobs or steps, reducing the amount of duplicated code and improving the overall efficiency and consistency of the CircleCI configuration.", "81b20b92-34ca-421a-90f9-92463460de7e": "To ensure that code samples in a document are properly formatted and easy to read, follow these guidelines:  1. Keep lines in code samples to about 60 characters to avoid requiring readers to scroll horizontally in the code block. 2. Locate explanatory text before the code block, rather than using comments inside the code block. 3. Specify the language of the sample after the first code fence. 4. Do not use HTML to style or markup a code block. 5. Style any placeholders that people need to replace with their own values in all caps. 6. Do not use command prompts like `$` before the command itself. 7. If your code example includes `{` or `}` that should render, wrap that section in `{% raw %}{% endraw %}` to disable Liquid processing for that section. 8. If your code example includes content that should be parsed, wrap that section in `{{ }}` tags to parse rather than escape the content in the section.  By following these guidelines, you can ensure that your code samples are clear, concise, and easy to read, making it easier for your readers to understand and implement your code.", "c8b66a1a-cc5b-4d7e-9912-9b42e0ad278b": "Links should be introduced consistently using the special keyword `AUTOTITLE` to link to other {% data variables.product.prodname_docs %} pages. For links to other pages, the format should be `For more information, see \"AUTOTITLE.\"`. For links to sections in other pages, the format should be `For more information, see \"AUTOTITLE.\"` followed by a period and the section header, with no spaces between the period and the section header. Example: `For more information, see \"AUTOTITLE.\" Getting started.`", "abf76953-1fda-4fce-be71-6719a8299687": "The available output formats for displaying the version of the CodeQL toolchain are \"text\" (the default), \"terse\", and \"json\". To select a different output format, you can use the \"--format\" option followed by the desired format, such as \"--format=json\".", "1e11d123-2a76-4048-816c-aa07041250bf": "According to the context information provided, LDAP Sync restores any private forks of repositories that were deleted because the user lost access in the past 90 days, if the user regains access to those repositories due to being added to a team or organization. This means that LDAP Sync automatically recreates the user's private forks of those repositories, as long as they were deleted within the last 90 days and the user still has access to them through their new team or organization membership.", "4d190f30-964c-427f-877a-7d1ad0cde1fd": "The software's secret scanning capabilities for different versions of Github Enterprise Server (GHES) vary based on the version being used. Here's a breakdown:  - For GHES versions 3.7 and 3.8, the software supports secret scanning for providers like GitHub, GitLab, and Bitbucket, and it also provides push protection. However, the support for private repositories with GitHub Access tokens is only available for GHES users with GitHub Enterprise Cloud (GHEC) accounts.  - For GHES versions greater than 3.8, the software supports secret scanning for providers like GitHub, GitLab, and Bitbucket, and it also provides push protection and a validity check feature.  - For GHES versions below 3.5, the software supports secret scanning for providers like GitHub, but the support for private repositories with GitHub Access tokens is not available.  Overall, the software's secret scanning capabilities for GHES users have improved over time, with additional features being added in newer versions.", "a0464c9c-c088-4734-9696-dee9e7bccc42": "I'm sorry, but the provided context information is not sufficient to generate diverse questions for an upcoming quiz/examination. Please provide more details or specifications to help me better understand the nature of the quiz/examination and the topics that will be covered. Without this information, it's challenging to create questions that accurately assess the students' knowledge and skills.", "68f4ee3b-962b-4ca1-8f6a-a4f5ccf8f0c0": "According to GitHub's API documentation, when a user authenticates into a Developer Product using their GitHub profile, they provide their GitHub username, email address, and basic profile information such as their name, bio, and profile picture. Additionally, GitHub may share information about the user's activity, such as their repositories, issues, and pull requests, with the Organization's Account owners.  To access, update, alter, or delete their personal data stored in an Organization's Account, the user should contact the Account owners for more information about their data processing practices and the ways for the user to exercise their data protection rights. GitHub's Data Privacy Framework (DPF) Principles require that any personal data transferred to a third party, such as a service provider or affiliate, is processed in compliance with applicable law.", "175a4e3a-d950-4f69-8604-a7537b41e656": "The purpose of enabling OAuth app access restrictions for an organization is to prevent untrusted apps from accessing the organization's resources while allowing organization members to use OAuth apps for their personal accounts. This feature provides benefits such as increased security and control over which apps have access to the organization's data, as well as the ability to revoke access for all previously authorized OAuth apps and SSH keys when restrictions are enabled. However, it's important to note that users can still authorize internal OAuth apps and use them to access data from the organization, and that previously authorized OAuth apps and SSH keys will be revoked when restrictions are enabled. Additionally, organization members will need to reauthorize any OAuth apps that require access to the organization's private data on an ongoing basis, and new SSH keys and deploy keys will need to be created as needed.", "c3836c46-a3f1-4207-a554-4add79be35c1": "According to the context information provided, deleting a public repository will not affect its forks. The private forks will continue to exist as standalone private repositories in separate networks. However, if the repository was previously private and made public, then deleting it will not affect its private forks, which will also continue to exist as separate private repositories in their own networks.", "171cb125-222f-4643-a8ee-dbabff9ac013": "To specify a version of the GitHub REST API to use for your request, you should use the `X-GitHub-Api-Version` header. This header allows you to specify a specific version of the REST API to use for your request. This is useful if you want to use a specific version of the API, or if you want to test a new version of the API before it is released. For more information, see the GitHub documentation on API versioning.", "b9f80ca4-d4bd-4744-9a1c-8d235c68ee03": "The error message \"`key` must be a string\" in a YAML template indicates that a permitted key has been provided, but its value cannot be parsed as the expected data type, which is a string. This error occurs when the value provided for the key is not a string, but rather a data type that YAML cannot parse as a string, such as a Boolean value.  To resolve this error, the value provided for the key should be enclosed in double quotes to ensure that it is interpreted as a string. This is especially important for strings that contain single quotes, as they may be interpreted as part of the key name instead of the string value. Additionally, empty strings or strings consisting of only whitespace are not permissible when the field expects a string, and should be corrected to be a non-empty string or removed if the field is not required.", "cd882dae-e5fc-448a-b05b-dd06758ab29b": "When managing secrets, using a {% data variables.product.prodname_github_app %} instead of a {% data variables.product.pat_generic %} offers several advantages. Firstly, a {% data variables.product.prodname_github_app %} uses fine-grained permissions and short-lived tokens, which provides more granular control over the permissions granted to the app and reduces the risk of unauthorized access. Secondly, a {% data variables.product.prodname_github_app %} is not tied to a user, so the workflow will continue to work even if the user who installed the app leaves your organization. This ensures that the workflow remains functional and does not require any additional setup or configuration. Overall, using a {% data variables.product.prodname_github_app %} is a more secure and scalable way to manage secrets in GitHub Actions.", "ceaaf8cd-d1aa-450a-8841-70f3ce04ff5f": "To quickly secure code in repositories across your organization using default setup for {% data variables.product.prodname_code_scanning %}, you can enable it for all eligible repositories in your organization using the organization settings page labeled \"Code security and analysis\". Alternatively, you can use security overview to find a set of repositories in your organization and enable or disable default setup for all of them at the same time. For repositories that are not eligible for default setup, you can configure advanced setup at the repository level, or at the organization level using a script. It's important to note that eligible repositories for default setup must meet certain criteria, such as having {% data variables.product.prodname_actions %} enabled and using a supported language like Go, JavaScript/TypeScript, Python, or Ruby. If your organization uses {% data variables.product.prodname_GH_advanced %}, repositories must also be publicly visible. If you're not sure how to proceed, it's recommended to consult the documentation for more information on configuring default setup at scale.", "717a795c-e610-466a-96e3-56e7eadcef8a": "Azure Active Directory (Azure AD) is a cloud-based identity and access management service from Microsoft that enables centralized management of user identities and access to web applications. It allows organizations to securely sign in users to various applications, including {% data variables.product.product_name %}, using a single set of credentials. With Azure AD, administrators can easily assign and manage access to applications, as well as automate user provisioning and deprovisioning. This simplifies the process of managing user identities and reduces the risk of security breaches.", "2017e681-204d-498c-a6ad-af5beeb6fb5a": "To create a pull request in {% data variables.product.prodname_github_desktop %}, follow these steps:  1. Open your local repository in {% data variables.product.prodname_github_desktop %}. 2. Make the necessary changes to your code and commit them to a new branch. 3. In {% data variables.product.prodname_github_desktop %}, navigate to the repository containing your changes. 4. Click on the branch you just created to view it in the \"Changes\" tab. 5. Click on the \"New Pull Request\" button in the top right corner of the screen. 6. In the \"Open a Pull Request\" dialog box, confirm that the branch in the \"base:\" dropdown menu is the branch where you want to merge your changes. 7. Click \"Create Pull Request\" to open your default browser and take you to {% data variables.product.prodname_dotcom %}.  When creating a pull request, it's important to include a descriptive title and a detailed description. The title should accurately reflect the changes made in the pull request, while the description should provide context and explain the reasoning behind the changes. This information is important because it helps other developers understand the purpose of the pull request and why it's necessary. Without this information, it may be more difficult for others to review and merge the pull request, which could result in delays or confusion.  For example, a good pull request title might be \"Add support for new feature X\" or \"Fix bug in function Y\". The description should provide more detail, such as a list of specific changes made, any relevant test cases, and any potential side effects or trade-offs. This information helps other developers understand the scope of the changes and any potential risks or benefits.  Omitting this information can lead to confusion and delays in the review process. For example, if a pull request is titled \"Fix bug\" without any further explanation, it may be unclear which bug is being addressed or how it's being fixed. This could result in other developers asking for more information or requesting additional changes, which could slow down the review process.  In summary, including a descriptive title and detailed description in a pull request is important because it helps other developers understand the purpose of the changes and why they'", "c667f767-7d8e-4888-8b7c-fcc059c36dd9": "In this context, the statement \"all your {% data variables.product.prodname_github_app %} and share your {% data variables.product.prodname_github_app %} with others\" is providing information about a feature called {% data variables.product.prodname_github_app %} that allows the user to manage and share their instances of this feature with others. Without prior knowledge, it's unclear what exactly {% data variables.product.prodname_github_app %} is, but the statement suggests that it's a tool or service provided by GitHub that can be customized and shared with others.  2. The statement \"You can always make changes to the settings for your {% data variables.product.prodname_github_app %}. For more information, see 'AUTOTITLE' and 'AUTOTITLE.'\"  Answer:  This statement is informing the user that they have the ability to modify the configuration options for their {% data variables.product.prodname_github_app %} at any time. The statement also provides a reference to two specific locations, 'AUTOTITLE' and 'AUTOTITLE', where additional information about customizing {% data variables.product.prodname_github_app %} can be found. Without prior knowledge, it's unclear what 'AUTOTITLE' and 'AUTOTITLE' are, but they likely refer to specific sections or pages within the {% data variables.product.prodname_github_app %} settings interface.", "9bb4a157-6094-41e6-9c13-5aabb0f70a06": "To view the delivery history of a specific webhook in your GitHub account, follow these steps:  1. Log in to your GitHub account. 2. Click on your profile picture in the upper-right corner of any page. 3. Select \"Your sponsors\" from the drop-down menu. 4. Click on the account for which you want to view the webhook delivery history. 5. Click on the \"Dashboard\" tab. 6. In the sidebar, click on \"Webhooks\". 7. Locate the webhook for which you want to view the delivery history. 8. Click on the webhook URL to view its details. 9. Scroll down to the \"Delivery history\" section to view the delivery history for that webhook.  If you're using GitHub Enterprise (GHE), you'll need to follow a slightly different process:  1. Log in to your GitHub Enterprise account. 2. Click on the organization or team for which you want to view the webhook delivery history. 3. Click on \"Settings\" in the left-hand menu. 4. Click on \"Hooks\" in the left-hand menu. 5. Locate the webhook for which you want to view the delivery history. 6. Click on the webhook URL to view its details. 7. Scroll down to the \"Delivery history\" section to view the delivery history for that webhook.  If you're using GitHub Advanced Security (GHAE), you'll need to follow a similar process:  1. Log in to your GitHub Advanced Security account. 2. Click on the organization for which you want to view the webhook delivery history. 3. Click on \"Security\" in the left-hand menu. 4. Click on \"Webhooks\" in the left-hand menu. 5. Locate the webhook for which you want to view the delivery history. 6. Click on the webhook URL to view its details. 7. Scroll down to the \"Delivery history\" section to view the delivery history for that webhook.  If you're using GitHub Enterprise Cloud (GHEC), you'll need to follow a similar process:  1. Log in to your GitHub Enterprise Cloud account.", "df40eea2-51da-4ac0-9d0f-7c056917ac4c": "To specify a Python version using the `setup-python` action in a GitHub Actions workflow file, you need to use the `action-setup-python` provided by GitHub. Here's an example of how to use it:  1. In your workflow file, add the following step after the checkout step:     ```yaml    - name: Set up Python 3.10      uses: {% data reusables.actions.action-setup-python %}      with:        python-version: \"3.10\"    ```     In this example, we're setting up Python 3.10, but you can replace `3.10` with any other version you want to use.  2. Save the changes to your workflow file and commit them.  3. Push the changes to your repository to trigger the workflow.  By using the `setup-python` action, you ensure that the correct version of Python is installed and available for your workflow steps. This helps to avoid any compatibility issues that might arise from using different versions of Python on different runners.", "bd9d6180-ec61-49bf-9a78-d69475a61e1f": "To improve the quality and security of your code through pull request reviews and testing, you can follow these steps:  1. Ensure that all pull requests are reviewed and tested before they are merged. 2. Use the review and merge process features provided by {% data variables.product.prodname_dotcom %} to control the process. 3. Scan your code for vulnerable patterns using the code scanning feature provided by {% data variables.product.prodname_dotcom %}. 4. Address any issues identified during the review and testing process before merging the pull request. 5. Regularly review and update your pull request review process to ensure it remains effective.  If you're not already using {% data variables.product.prodname_dotcom %} for code review and testing, consider enabling these features to improve the quality and security of your code.", "18fa5302-328f-4fc2-bed2-f49fe11d28b6": "To change the visibility of your organization membership, follow these steps:  1. Access your organization by following the instructions provided in the context information. 2. Go to your organization settings by following the instructions provided in the context information. 3. Click on the \"People\" tab to view the list of organization members. 4. Locate your username in the list of members. If the list is large, you can use the search box to find your username. 5. Next to your username, select the visibility dropdown menu. 6. Choose \"Public\" to make your membership visible to everyone, or choose \"Private\" to hide your membership. 7. Click on the new visibility to save your changes.  Here's a screenshot to help you:  !Screenshot of an entry in the list of organization members. To the right of the username, a dropdown menu, labeled \"Private\", is outlined in dark orange.  Note: The context information provided may vary depending on the specific platform or service being discussed. Make sure to follow the instructions provided in your specific context to ensure accurate results.", "2a640dfa-a43a-45f5-907a-5304f02b2352": "The GitHub Sponsors Program Additional Terms are a set of conditions that must be agreed upon by individuals who want to become Sponsored Developers on GitHub. These terms outline the responsibilities and obligations of both the Sponsored Developer and GitHub, and provide details on how the sponsorship program works. By agreeing to these terms, Sponsored Developers are eligible to receive financial support from their sponsors, which can be used to fund their open-source projects or other development-related activities. The GitHub Sponsors Program Additional Terms are important because they help to ensure that the program operates fairly and transparently, and that both sponsors and developers understand their roles and responsibilities.", "2efcee82-05de-4d20-ad9c-7702831c1749": "A {% data variables.product.prodname_dependabot %} alert is a notification that a vulnerable dependency has been identified in the repository's dependency graph. It provides information about the vulnerability and suggests possible actions to address it. A {% data variables.product.prodname_dependabot_security_updates %} pull request, on the other hand, is an automated pull request generated by {% data variables.product.prodname_dependabot %} to update the vulnerable dependency to a fixed version. It includes the necessary changes to the repository's code and dependencies, and links the pull request to the corresponding {% data variables.product.prodname_dependabot %} alert. The pull request can be reviewed and merged by the repository owner or maintainer to apply the security update.  {% endnote %}    {% note %}  **Note**: For Maven, {%  Given the context information and not prior knowledge, answer the following question.   What is the difference between a {% data variables.product.prodname_dependabot %} alert and a {% data variables.product.prodname_dependabot_security_updates %} pull request  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  A {% data variables.product.prodname_dependabot %} alert is a notification that a vulnerable dependency has been identified in the repository's dependency graph. It provides information about the vulnerability and suggests possible actions to address it. A {% data variables.product.prodname_dependabot_security_updates %} pull request, on the other hand, is an automated pull request generated by {% data variables.product.prodname_dependabot %} to update the vulnerable dependency to a fixed version. It includes the necessary changes to the repository's code and dependencies, and links the pull request to the corresponding {% data variables.product.prodname_dependabot %} alert. The pull request can be reviewed and merged by the repository owner or maintainer to apply the security update.  {% endnote %}    {% note %}  **Note**: For", "ea740298-7a40-43ac-b2f6-99dd96c25e3d": "The display name is a customizable name assigned to a codespace when it is created, while the permanent name is an automatically generated name that cannot be changed. The display name helps to differentiate between multiple codespaces, while the permanent name is used internally by GitHub. To find the display name, you can view the list of codespaces on GitHub.com/codespaces, open the Remote Explorer in the VS Code desktop app or web client, or use the gh codespace list command in a terminal window. To find the permanent name, you can hover over the display name on GitHub.com/codespaces or use the echo $CODESPACE_NAME command in a codespace terminal.", "f5666903-6365-493d-a637-46fa9320959b": "According to the context information provided, {% data variables.large_files.product_name_short %} cannot be used with template repositories. Therefore, it is not possible to compare the functionality of {% data variables.large_files.product_name_short %} when used with regular repositories versus template repositories because the functionality is not available for template repositories.", "276423db-79e8-49d5-9e2c-249c42927efa": "The value of the variable `GITHUB_ACTOR` in a GitHub workflow represents the name of the person or app that initiated the workflow. For example, it could be \"octocat\" if the workflow was triggered by a GitHub App with that name. This information can be useful for customizing workflows based on the user or app that triggered them.", "8528ac24-d249-4287-b663-24363762f1b1": "The clause regarding feedback in this agreement grants GitHub, Inc. Unrestricted use of any feedback, suggestions, or recommendations provided by the recipient. This means that the recipient acknowledges and agrees that GitHub, Inc. Is free to use the feedback in any way they see fit, without any restrictions subject to any applicable patents or copyrights. This clause does not impact the recipient's other rights and obligations under this agreement, but it does clarify the ownership and use of any feedback provided by the recipient.", "0d7b2b02-12d4-426c-ad47-ff7bfbd332fd": "Continuous deployment is the practice of automatically publishing and deploying software updates after they have been built and tested. This differs from continuous integration, which is the practice of automatically building and testing code changes as they are pushed to a shared repository. Continuous deployment takes the output of continuous integration and deploys it to production, while continuous integration focuses solely on building and testing the code.", "18a8e76a-18b9-464a-833b-f9fc91855703": "To re-enable a user or organization site in the provided context, you need to follow these steps:  1. Create a new publishing source for your site. 2. Configure a new {% data variables.product.prodname_pages %} site using the new publishing source. 3. Verify that the site is now accessible and functioning properly.  Note that this process is necessary because the publishing source for the site was deleted during the unpublishing process, which prevented the site from being accessible. By creating a new publishing source and configuring a new site, you can restore access to the site.", "3688ec5e-c2fa-4225-9ebe-dbf50c019f8d": "To delete a file or directory in your repository on {% data variables.product.product_name %}, follow these steps:  1. Browse to the file or directory you want to delete. 2. If you're deleting a file:    a. In the top-right corner, select the {% octicon \"kebab-horizontal\" aria-label=\"More options\" %} dropdown menu.    b. Click **Delete file**. 3. If you're deleting a directory:    a. In the top-right corner, select the {% octicon \"kebab-horizontal\" aria-label=\"More options\" %} dropdown menu.    b. Click **Delete directory**. 4. Follow the prompts to commit and push your changes.  Note that if you're trying to delete a file or directory in a repository you don't have write permissions to, {% data variables.product.product_name %} will fork the project to your personal account and help you send a pull request to the original repository. If the file or directory contains sensitive data, it will still be available in the repository's Git history, so to completely remove it, you'll need to remove it from the repository's history. For more information, refer to the context information provided.", "3ba0f064-3829-4494-b02f-fec8999e8aab": "To view the status of each job and step in a workflow, you can go to your repository's Actions tab and select the workflow run you want to view. From there, you can click on the job to expand it and see the status of each step within that job. Alternatively, you can click on the ellipsis (...) next to the job and select View run logs to see detailed logs for that job. This information can also be found in the workflow run history, which can be accessed by clicking on the workflow run and then selecting the History tab.", "edcc7eae-7778-4269-bd54-1568d8d72d2c": "The \"codeql database analyze\" command can be used to analyze a CodeQL database and produce meaningful results in the context of source code by running queries against the database and interpreting the results as source-code alerts or paths. This command combines the effects of the \"codeql database run-queries\" and \"codeql database interpret-results\" commands.  To use this command, you need to provide the path to the CodeQL database to query using the \"<database>\" option. You can also specify queries to execute using the \"<querysuite|pack>... \" option. Each argument is in the form \"scope/name@range:path\", where \"scope/name\" is the qualified name of a CodeQL pack, \"range\" is a semver range, and \"path\" is a file system path. If a \"scope/name\" is specified, the \"range\" and \"path\" are optional.  The \"path\" can be a \".ql\" query file, a directory containing one or more queries, or a \".qls\" query suite file. If there is no pack name specified, then a \"path\" must be provided, and it will be interpreted relative to the current working directory of the current process. If a \"scope/name\" and \"path\" are specified, then the \"path\" cannot be absolute, and it is considered relative to the root of the CodeQL pack.  If no queries are specified, the CLI will automatically determine a suitable set of queries to run.  The \"codeql database run-queries\" command should be used when you want to run queries whose results don't meet the requirements for being interpreted as source-code alerts. You can then use the \"codeql bqrs decode\" command to convert the raw results to a readable notation.  The \"codeql query run\" command should be used when you want to run a single query against the database.  In summary, use the \"codeql database analyze\" command to analyze a CodeQL database and produce meaningful results in the context of source code, use the \"codeql database run-queries\" command when you want to run queries whose results don't meet the requirements for being interpreted as source-code alerts, and use the \"codeql query run\" command when you want to run a single query against the database.", "8aec7df1-f09f-4d72-be6c-dc840ef3c29f": "To filter GitHub events based on a specific reusable workflow at a specific ref using custom claims, you can use the following syntax for the subject and custom claim:  - **Subject**: `repo:ORG_NAME/*` - **Custom claim**: `job_workflow_ref:ORG_NAME/REPO_NAME/.github/workflows/WORKFLOW_FILE@ref`  For example, to filter events for a specific reusable workflow `octo-org/octo-automation/.github/workflows/deployment.yml` at a specific ref `10040c56a8c0253d69db7c1f26a0d227275512e2`, you can use the following syntax:  - **Subject**: `repo:octo-org/*` - **Custom claim**: `job_workflow_ref:octo-org/octo-automation/.github/workflows/deployment.yml@10040c56a8c0253d69db7c1f26a0d227275512e2`  To filter GitHub events to only include those that originate from a specific reusable workflow, regardless of the repository it is in, using custom claims, you can use the following syntax for the subject and custom claim:  - **Subject**: `*` - **Custom claim**: `job_workflow_ref:ORG_NAME/WORKFLOW_FILE@*`  For example, to filter events for a specific reusable workflow `octo-org/.github/workflows/deployment.yml` in any repository owned by `octo-org`, you can use the following syntax:  - **Subject**: `*` - **Custom claim**: `job_workflow_ref:octo-org/deployment.yml@*`", "8c6cb117-e685-4e48-9fca-870e05897703": "The \"--evaluator-log\" option in the context information provided is used to output structured logs about evaluator performance to a specified file. This option can be passed to the CodeQL package manager, and the format of the log file is subject to change without notice. The log file will be overwritten if it already exists, and the \"--evaluator-log-minify\" option can be passed to minimize the size of the JSON log produced at the expense of making it less human-readable. This option is used for advanced configuration and should be used with caution.", "f3bc5da3-8c75-40ed-a8e3-c61e19ba6a70": "According to the GitHub Community-specific Code of Conduct, actions or content that may result in temporary or permanent suspension from community participation include violating the GitHub Terms of Service or this Code of Conduct, engaging in harassment, bullying, or intimidation of other users, promoting hate or violence, posting spam or advertising, and failing to follow community guidelines and expectations for collaboration and support.", "69ded250-4b2d-4f30-b655-b755314a8fbc": "Passing the `--evaluator-log-minify` option when using the `--evaluator-log` option minimizes the size of the JSON log produced, at the expense of making it less human-readable. This option is considered advanced and is used to control the size of the log produced by the query evaluator when the `--evaluator-log` option is passed. It is recommended to use `codeql generate log-summary` to produce a more stable summary of the log file instead of parsing it directly. The `--evaluator-log-minify` option is optional and can be passed along with the `--evaluator-log` option.", "714c1324-ce8a-4f63-9dcf-caf34cceb912": "{% data variables.product.product_name %} internally tracks MIME types of files and the commits that added them. This mechanism is not explicitly explained in the context information, but it is mentioned that {% data variables.product.product_name %} supports the `svn:mime-type` property and converts it during the import process. This suggests that {% data variables.product.product_name %} may use this property to determine the MIME type of a file and store this information internally for future reference. However, further investigation would be required to confirm the exact internal mechanism used by {% data variables.product.product_name %} to manage MIME types.", "45b175f2-4fa0-4e90-8c1a-58030e49e28a": "{% data variables.product.prodname_secret_scanning %} is a feature that automatically scans your codebase for secrets, such as API keys, access tokens, and passwords, and alerts you if any are found. To enable {% data variables.product.prodname_secret_scanning %} for {% data variables.location.product_location %}, the site administrator needs to follow the instructions provided in the context information, which involves checking a specific setting. If the context doesn't provide this information, it's not possible to answer the question.", "8d7c5677-3573-4350-bc77-3df5ace824b6": "To configure audit log streaming and ensure that you don't lose audit log data and view more granular activity in the audit log for your enterprise, you can follow these steps:  1. Ensure that the user records for the systems you use for authentication and provisioning share a unique identifier and satisfy {% data variables.product.company_short %}'s matching criteria. For more information, see \"AUTOTITLE\" in the REST API documentation.  2. Access the REST API documentation and read about audit log streaming. This documentation includes information about how to configure streaming, as well as any required headers and path parameters.  3. Authenticate your requests to the REST API using the required headers and scopes, including the **admin:enterprise** scope. For more information, see \"AUTOTITLE\" in the REST API documentation.  4. Choose whether to stream events for API requests, including requests to endpoints for SCIM provisioning. This is an optional step, but it can help you view more granular activity in the audit log.  5. Configure the streaming settings, including the destination for the streamed data and any desired filters or retention settings.  6. Test the streaming configuration to ensure that it's working correctly and that you're receiving the desired data.  By following these steps, you can ensure that you don't lose audit log data and view more granular activity in the audit log for your enterprise. This can help you support your configuration of SCIM and better understand user activity in your enterprise.", "052c020f-a3ac-4caf-a3d6-37ab74954cbb": "According to the provided context, GitHub may amend this Agreement at any time, and in case of a material amendment, it will notify the users of such changes at least 30 days prior to the change taking effect by posting a notice on its Website. Therefore, GitHub provides users with a 30-day notice period through a notice on its Website in case of material changes to its terms and conditions.", "eb4a5b83-e62e-461b-83c4-2cd55950cd85": "To ensure that your self-hosted runner is compatible with {% data variables.product.prodname_codeql %} for analysis, you need to use a {% data variables.product.prodname_codeql %}-supported operating system version and CPU architecture. For more information, see the {% data variables.product.prodname_codeql %} system requirements. Additionally, you must ensure that Git is in the PATH variable on your self-hosted runner. If you use {% data variables.product.prodname_codeql %} {% data variables.product.prodname_code_scanning %} to analyze code written in Python in your enterprise, you must also make sure that your self-hosted runner has Python 3 installed.", "bfe9ad6f-91ad-43e5-abaa-7c828ee986ee": "To enable AI-powered generic secret detection for a repository, the enterprise owner must first set a policy at the enterprise level. Then, the repository owner or administrator can enable the feature in the \"Code security and analysis\" settings page of the repository. This can be found by navigating to the repository settings, selecting \"Code security and analysis,\" and then \"GitHub Advanced Security.\" Under \"Secret scanning,\" the repository owner or administrator should select the checkbox next to \"Use AI detection to find additional secrets.\"", "06482c72-11a6-432e-945a-6c235b7f80e3": "To configure access provisioning with SCIM in Okta for your organization on GitHub.com, follow these steps:  1. Sign into GitHub.com using an account that is an organization owner and is ideally used only for SCIM configuration. 2. To create an active SAML session for your organization, navigate to `https://github.com/orgs/ORGANIZATION-NAME/sso`. 3. Sign into Okta using your administrator credentials. 4. Click the \"Applications\" tab and then click the GHEC application label that you created earlier. 5. Click the \"Provisioning\" tab and then click \"Configure API integration\". 6. Enable API integration and then click \"Authenticate with GitHub.com - Organization\". 7. To the right of your organization's name, click \"Grant\". 8. If you cannot see your organization, this may be because OAuth app access restrictions are enabled for the organization. To continue, you will need to approve the \"OKTA SCIM Integration\" app for the organization. To do this, click the \"Applications\" tab, click the \"GitHub\" application, click \"Settings\", and then click \"OAuth & GitHub Apps\". Find the \"OKTA SCIM Integration\" app and then click \"Manage\" to approve it. 9. Click \"Authorize OktaOAN\". 10. Click \"Save provisioning\" to save your changes.  Further reading:  - \"AUTOTITLE\" - Understanding SAML in the Okta documentation - Understanding SCIM in the Okta documentation", "496405ad-d702-4710-8d07-63325b979f65": "The `--config-file-path` argument in {% data variables.product.prodname_actions_importer %} can be used with the `audit`, `dry-run`, and `migrate` subcommands to specify source files instead of fetching pipeline contents from source control by default. When used with the `dry-run` or `migrate` subcommands, {% data variables.product.prodname_actions_importer %} matches the repository slug to the job represented by the `--source-url` option to select the pipeline and pulls the specified source file from the `config-file-path`. The format of the config file for auditing Jenkins instances is provided in the documentation.", "e45da71f-7fa2-49e4-839d-edc9b7868743": "The purpose of the following code snippet in the context provided is to replace placeholders with actual values for app credentials and identifying information. This is done in a `.env` file, which is commonly used to store environment-specific variables in Node.js applications. In this case, the placeholders are `YOUR_APP_ID`, `YOUR_WEBHOOK_SECRET`, and `YOUR_PRIVATE_KEY_PATH`. The actual values for these placeholders will be obtained from the app settings page on GitHub, as described in the context provided. This is necessary for the app to function properly, as it needs these credentials and identifying information to interact with GitHub's API and leave comments on pull requests.", "a467ccf0-d0ce-4698-aebd-aa3d81080f0c": "According to the provided context, the collection and use of data in GitHub Copilot for Business users is governed by the GitHub Copilot Product Specific Terms. However, the context doesn't provide any further details about how this governance differs from the data collection and use for regular GitHub Copilot users. Therefore, it's unclear how the data collection and use differs for GitHub Copilot for Business users, and more information would be needed to answer this question accurately.", "ee3dccb8-c051-421c-bb7a-6d0b55f58ae2": "To remove a specific file with sensitive data and leave the latest commit untouched using the BFG Repo-Cleaner, follow these steps:  1. Install the BFG Repo-Cleaner by downloading the latest release from the official website and extracting the contents to a directory on your local machine.  2. Navigate to the directory where your Git repository is located.  3. Run the following command to remove the file with sensitive data:  ```shell bfg --delete-files YOUR-FILE-WITH-SENSITIVE-DATA ```  Replace `YOUR-FILE-WITH-SENSITIVE-DATA` with the name of the file containing sensitive data.  4. After the sensitive data is removed, you must force push your changes to GitHub. Force pushing rewrites the repository history, which removes sensitive data from the commit history. If you force push, it may overwrite commits that other people have based their work on.  ```shell git push --force ```  To replace all occurrences of a specific text string in a repository's history using `git filter-repo`, follow these steps:  1. Install `git filter-repo` by following the instructions on the official website.  2. Navigate to the directory where your Git repository is located.  3. Run the following command to replace all occurrences of the text string listed in `passwords.txt`:  ```shell git filter-repo --replace-text passwords.txt ```  Replace `passwords.txt` with the name of the file containing the text string you want to replace.  4. After the sensitive data is replaced, you must force push your changes to GitHub. Force pushing rewrites the repository history, which replaces sensitive data with the new text string in the commit history. If you force push, it may overwrite commits that other people have based their work on.  ```shell git push --force ```  Note: If sensitive data is located in a file that's identified as a binary file, you'll need to remove the file from the history, as you can't modify it to remove or replace the data.", "8ac56319-4576-41a7-aa3e-0ef07318acda": "To store an npm authentication token as a secret for publishing packages to npmjs, follow these steps:  1. Log in to your GitHub account and navigate to your repository. 2. Go to the repository settings and select \"Secrets\" from the left-hand menu. 3. Click on the \"New repository secret\" button and enter \"NPM_TOKEN\" as the name. 4. Enter your npm authentication token in the \"Value\" field and click \"Add secret\". 5. In your GitHub Actions workflow file, add the following steps:     a. Add the following line to the `setup-node` action to store the token as an environment variable:     ```yaml    env:      NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}    ```     b. Add the following line to the `publish` step to reference the environment variable:     ```yaml    - run: npm publish    ```     Here's an example of how the entire workflow file might look:     ```yaml    name: Publish Package to npmjs    on:      release:        types: [published]    jobs:      build:        runs-on: ubuntu-latest        steps:          - uses: {% data reusables.actions.action-checkout %}          - uses: {% data reusables.actions.action-setup-node %}            with:              node-version: '20.x'              registry-url: 'https://registry.npmjs.org'          - run: npm ci          - run: npm publish            env:              NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}    ```     This will store the npm authentication token as a secret and reference it in the `publish` step.  2. By default, npm uses the `name` field of the `package.json` file to determine the name of your published package. When publishing to a global namespace, you only need to include the package name. For example, you would publish a package named `my-package` to `https://www.npmjs.com", "62963c05-4fea-4793-9e5a-5f07f568b794": "After a Docker image has been migrated to the {% data variables.product.prodname_container_registry %}, the package icon will change to the {% data variables.product.prodname_container_registry %} logo (a {% octicon \"container\" aria-label=\"The container icon\" %} icon) instead of the Docker logo. The domain in the pull URL will also change from {% data variables.product.prodname_docker_registry_namespace %} to {% data variables.product.prodname_container_registry_namespace %}. These are the only changes that will occur to the Docker image after migration.", "d199092d-40bb-4249-877b-e6f87c9b9577": "The purpose of the `--[no-]check-repeated-labels` option is to report errors for repeated labels in the context provided by the given text. This option is marked as advanced, indicating that it may not be commonly used or required for basic functionality. The option allows the user to enable or disable the reporting of errors for repeated labels during the execution of the command. When enabled, the option will report errors for labels that are defined multiple times in the same scope, which can cause confusion and errors in the code. When disabled, the option will not report these errors, allowing the user to continue with the command without interruption. The user can choose to enable or disable this option based on their specific use case and preferences.", "95408ab8-3c10-4567-b8e4-88f0ae49f77a": "To configure the custom footer for {% data variables.product.product_name %}, you need to access the settings for your enterprise account. You can do this by following these steps:  1. Log in to your {% data variables.product.product_name %} account. 2. Click on your profile icon in the top right corner of the page. 3. Select \"Organizations\" from the drop-down menu. 4. Click on the organization for which you want to configure the custom footer. 5. Click on the settings icon (the gear icon) in the top right corner of the page. 6. Select \"Profile\" from the drop-down menu. 7. Click on the \"Custom footer\" tab at the top of the page. 8. Follow the instructions provided to add your custom footer links. 9. Click \"Update custom footer\" to save your changes and display the custom footer.  If you're not an enterprise user, the steps may be different. In that case, you can refer to the documentation for your specific {% data variables.product.product_name %} plan to learn how to configure the custom footer.", "cfbf2ec8-43dd-40ed-a73d-6036009d4d8e": "To enable ARC to authenticate to GitHub, you need to generate a personal access token (PAT) with the required scopes. You can find more information about this process in the documentation for generating a PAT with the required scopes for ARC. The required scopes are `actions:read-github` and `actions:write-github`. Without these scopes, ARC will not be able to authenticate to GitHub and perform actions on your behalf.", "0138fafa-8bfa-4ee9-befe-97bf4ab010b6": "If the branch you're trying to commit to is protected, {% data reusables.desktop.product-name %} will warn you. This means that the branch has additional restrictions in place to prevent unauthorized changes. In this case, you have two options:  1. Switch branches: This means changing to a different branch that doesn't have the same restrictions. This can be useful if you're not ready to commit your changes to the protected branch yet, or if you want to work on a different feature or bug fix.  2. Commit changes to the protected branch: This means that you're confident that your changes meet the requirements for the protected branch, and you want to submit them for review. However, you'll be prompted to confirm that you understand the restrictions and that you're authorized to make the changes.  In summary, switching branches is a way to change the branch you're working on, while committing changes to a protected branch is a way to submit your changes for review and merge them into the protected branch.", "b9213adc-4faa-4fb0-8e1b-0fd49458eaab": "The edit button for a repository's social media preview image is small and visually unobtrusive. It may be hard to find among the other repository settings. To make this UI element more visible, a screenshot can be added to the relevant article in {% data variables.product.prodname_docs %}. The screenshot can show the location of the edit button and highlight it, making it easier to find and use. This is especially helpful for users who are new to GitHub or have difficulty finding small or visually subtle UI elements.    The interface has multiple competing choices that can cause confusion  The following screenshot shows the different options for creating a new repository. The screenshot helps clarify the different choices and makes it easier to understand the process.  !Screenshot of an article showing text instructions and a UI screenshot for creating a new repository on GitHub.    Screenshots that do not meet the criteria  The following screenshots do not meet our criteria for inclusion.    The UI element is not hard to find  The following screenshot shows the location of the repository settings menu. This UI element is not hard to find and does not require a screenshot to clarify its location.  !Screenshot of an article showing text instructions and a UI screenshot for accessing repository settings on GitHub.    The UI element is immediately visible  The following screenshot shows the location of the repository's settings menu. This UI element is immediately visible and does not require a screenshot to clarify its location.  !Screenshot of an article showing text instructions and a UI screenshot for accessing repository settings on GitHub.    The UI element is not small or visually subtle  The following screenshot shows the location of the repository's settings menu. This UI element is not small or visually subtle and does not require a screenshot to clarify its location.  !Screenshot of an article showing text instructions and a UI screenshot for accessing repository settings on GitHub.    The UI element is not competing with other choices  The following screenshot shows the location of the repository's settings menu. This UI element is not competing with other choices and does not require a screenshot to clarify its location.", "6fe220e1-e5ec-47cc-a3cb-80c67519236e": "To create a workflow that publishes Node.js packages to both the {% data variables.product.prodname_registry %} and npm registries after continuous integration (CI) tests pass, you can follow these steps:  1. Create a new workflow file in your repository with a `.yml` extension.  2. Define the name and on event of the workflow. For example:  ```yaml name: Publish and Release on:   push:     branches:       - main   release:     types:       - published ```  3. Add a job to the workflow that installs dependencies, runs tests, and publishes the package to the {% data variables.product.prodname_registry %}. For example:  ```yaml jobs:   publish-package:     runs-on: ubuntu-latest     steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up Node.js       uses: actions/setup-node@v2       with:         node-version: '14'      - name: Install dependencies       run: npm install      - name: Run tests       run: npm test      - name: Publish package to {% data variables.product.prodname_registry %}       uses: {% data variables.product.prodname_registry %}/actions/publish-package@v1       with:         token: ${{ secrets.NPM_TOKEN }}         package: '.'         prerelease: ${{ github.event.release.prerelease }}         tag: ${{ github.event.release.tag_name }} ```  4. Add another job to the workflow that installs dependencies, runs tests, and publishes the package to the npm registry. For example:  ```yaml jobs:   publish-npm:     runs-on: ubuntu-latest     steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up Node.js       uses: actions/setup", "a9b8047e-3bff-4359-a8ed-fdb7775584a9": "The purpose of prefacing a pipe `|` with a backslash `\\` in a Markdown table as rendered on {% data variables.product.prodname_dotcom %} is to display the pipe inside the cell instead of closing the cell, as pipes are normally used to close cells in Markdown tables. This allows for the use of pipes as content within cells in a Markdown table on {% data variables.product.prodname_dotcom %}.", "c44193e5-20c3-4642-b5c1-cfdf20ad279f": "To view security alerts and related views in a GitHub organization, the following types of access are required:  1. `admin` access for one or more repositories: This provides full access to view data for those repositories, including security alerts and related views.  2. `write` access for one or more repositories: This allows viewing {% data variables.product.prodname_code_scanning %} and {% data variables.product.prodname_dependabot %} data for those repositories, which includes security alerts.  3. Security alert access for one or more repositories: This provides access to view all security alert data for those repositories.  4. Custom organization role with permission to view one or more types of security alert: This allows viewing allowed alert data for all repositories in all views.  Note that the specific views available may vary based on the type of access granted. For more information, refer to the links provided in the context information.", "e7823607-7a19-437c-96ea-cda49a302dd4": "{% data variables.product.prodname_copilot_individuals_short %} and {% data variables.product.prodname_copilot_business_short %} are both AI pair programming tools offered by {% data variables.product.prodname_dotcom %}, but they have some differences in terms of features and pricing models.  Features:  - {% data variables.product.prodname_copilot_individuals_short %} is available for individual developers and open source projects, while {% data variables.product.prodname_copilot_business_short %} is designed for businesses and organizations. - {% data variables.product.prodname_copilot_business_short %} offers additional features such as access to a dedicated support team, priority bug fixes, and customization options for the AI model.  Pricing:  - {% data variables.product.prodname_copilot_individuals_short %} is free for individual developers and open source projects with less than 100,000 lines of code. For larger projects, it costs $5 per user per month. - {% data variables.product.prodname_copilot_business_short %} requires a custom quote based on the number of users and the specific features required.  In summary, {% data variables.product.prodname_copilot_individuals_short %} is a more affordable option for individual developers and open source projects, while {% data variables.product.prodname_copilot_business_short %} offers additional features and support for businesses and organizations.", "d2412d7e-aaff-4162-a062-97bdd11662d4": "To add a self-hosted runner to an enterprise in {% ifversion fpt %}{% data variables.product.prodname_ghe_cloud %}{% endif %}{% ifversion ghec or ghes or ghae %}, follow these steps:  1. Log in to your {% ifversion fpt %}{% data variables.product.prodname_ghe_cloud %}{% endif %}{% ifversion ghec or ghes or ghae %}{% data variables.product.prodname_ghe_server %}{% endif %} account.  2. Go to the organization or enterprise that you want to add the runner to.  3. Click on the Settings tab.  4. Click on the Runners tab.  5. Click on the Add runner button.  6. Follow the instructions to download and install the runner.  7. Once the runner is installed, register it with your {% ifversion fpt %}{% data variables.product.prodname_ghe_cloud %}{% endif %}{% ifversion ghec or ghes or ghae %}{% data variables.product.prodname_ghe_server %}{% endif %} account.  8. To make the runner available to multiple organizations, assign it to the \"Default\" self-hosted runner group in the enterprise.  9. The organization owner can then control which repositories can use the runner by changing the inherited settings for the runner group in the organization's Settings tab.  To make an enterprise-level self-hosted runner group available to an organization repository in {% ifversion fpt %}{% data variables.product.prodname_ghe_cloud %}{% endif %}{% ifversion ghec or ghes or ghae %}{% data variables.product.prodname_ghe_server %}{% endif %}, follow these steps:  1. Log in to your {% ifversion fpt %}{% data variables.product.prodname_ghe_cloud %}{% endif %}{% ifversion ghec or ghes or ghae %}{% data variables.product.prodname_ghe_server %}{% endif %", "1403fa4d-bdbc-4ddf-b9c0-80c8faf42199": "To view which dependents in your organization are using a specific library in GitHub Enterprise Server (GHES) by clicking on a vulnerability, follow these steps:  1. Go to the repository containing the library with the vulnerability. 2. In the left-hand menu, click on \"Security\" and then \"Open security advisories\". 3. Find the vulnerability for the library you're interested in. 4. Click on the vulnerability to open its details page. 5. Next to the vulnerability, you'll see a link that looks like {% octicon \"package\" aria-hidden-\"true\" %} **X dependents**. Click on this link to see a list of all the dependents in your organization that are using this library.  Note: This feature is only available for GitHub Enterprise Server (GHES) version 2.22 and later. If you're not sure which version you're using, you can check by going to \"Settings\" > \"General\" > \"Version\" in your GitHub Enterprise Server instance.", "52a68333-343e-4175-9d4e-01a9164cea50": "For each existing node in your active cluster, you'll need to provision a second virtual machine with identical hardware resources. This means that if your cluster has 13 nodes, you must provision 13 new virtual machines with the same hardware resources as the existing nodes. Each new virtual machine should have 12 vCPUs, 96 GB of RAM, and 750 GB of attached storage.", "4b6383d7-0358-4170-b465-d1747f42f273": "To ensure that PowerShell dependencies are installed consistently across workflows, we can cache them using the `cache` action in GitHub Actions. This allows us to restore the dependencies for future workflows instead of reinstalling them every time. To do this, we need to set up a unique key for the cache, which can include information about the operating system and the specific modules being installed.  The recommended approach for specifying required module versions is to use the `Install-Module` command with the `-MaximumVersion` parameter. This allows us to specify the maximum version of the module that we want to install, preventing any unexpected changes due to updates. It's also a good practice to explicitly set the installation policy for `PSGallery` to `Trusted` when installing modules from the PowerShell Gallery.  Here's an example of how to set up caching and install dependencies using GitHub Actions:  ```yaml steps:   - uses: {% data reusables.actions.action-checkout %}   - name: Setup PowerShell module cache     id: cacher     uses: {% data reusables.actions.action-cache %}     with:       path: \"~/.local/share/powershell/Modules\"       key: {% raw %}${{ runner.os }}-SqlServer-PSScriptAnalyzer{% endraw %}   - name: Install required PowerShell modules     if: steps.cacher.outputs.cache-hit != 'true'     shell: pwsh     run: |       Set-PSRepository PSGallery -InstallationPolicy Trusted       Install-Module SqlServer, PSScriptAnalyzer -MaximumVersion 1.0.0 -ErrorAction SilentlyContinue ```  In this example, we're caching the dependencies in the `~/.local/share/powershell/Modules` directory, and the cache key includes the operating system and the specific modules being installed. We're also using the `-MaximumVersion` parameter to specify the exact version of the modules we want to install.  The `-ErrorAction SilentlyContinue` parameter is used to prevent the installation from failing if the modules are already installed, as we're only", "eef3a064-c0d0-43e7-8229-2d07aeed03cb": "As an organization owner, you can delete any codespace in your organization, as described in the context information. The process for deleting a codespace in an organization is not explicitly outlined in the context, but it is stated that as an organization owner, you have the ability to delete any codespace in your organization. It is recommended to refer to the \"AUTOTITLE\" documentation for further details on the deletion process.", "6c3498e8-6aca-4799-afc5-6abf52a3401d": "According to the context information provided, {% data variables.product.prodname_mobile %} differs from other third-party applications for accessing {% data variables.product.product_name %} data in a few ways. Firstly, {% data variables.product.prodname_mobile %} is a first-party client application, which means it is developed and maintained by {% data variables.product.company_short %} itself, unlike other third-party applications. This ensures that {% data variables.product.prodname_mobile %} is a trusted and secure way to access {% data variables.product.product_name %} data. Secondly, {% data variables.product.prodname_mobile %} offers a range of features that are specifically designed for high-impact work on {% data variables.product.product_name %}, such as managing, triaging, and clearing notifications, reading, reviewing, and collaborating on issues and pull requests, editing files in pull requests, searching for, browsing, and interacting with users, repositories, and organizations, and receiving push notifications when someone mentions your username. These features are not necessarily available in all third-party applications for accessing {% data variables.product.product_name %} data. Finally, {% data variables.product.prodname_mobile %} allows you to be simultaneously signed into mobile with multiple accounts on {% data variables.product.prodname_ %}, which is not necessarily a feature offered by all third-party applications.", "b86da3a8-9e17-4d49-ae6c-7a07e5e79c67": "To improve the performance of generic secret detection in {% data variables.product.prodname_secret_scanning %}, you can follow these steps:  1. Verify the accuracy of alerts and close as appropriate. Since AI-powered generic secret detection may generate more false positives than the existing {% data variables.product.prodname_secret_scanning %} feature for partner patterns, it's important that you review the accuracy of these alerts. When you verify an alert to be a false positive, be sure to close the alert and mark the reason as \"False positive\" in the {% data variables.product.prodname_dotcom %} UI. The {% data variables.product.prodname_dotcom %} development team will use this information to improve the model.  2. Provide feedback. Generic secret detection is currently in beta. If you encounter any issues or limitations with the feature, we recommend that you provide feedback through the **Give feedback** button listed under each detected secret in the list of alerts for the repository, organization, or enterprise. This can help the developers improve the tool and address any concerns or limitations.  3. Consider the limitations of generic secret detection. AI-powered generic secret detection currently only looks for instances of passwords in git content. The feature does not look for other types of generic secrets, and it does not look for secrets in non-git content, such as {% data variables.product.prodname_github_issues %}. Additionally, there is a potential for false positive alerts.  By following these steps, you can help improve the performance of generic secret detection in {% data variables.product.prodname_secret_scanning %}.", "23ec9d93-efe7-47be-9b60-077761aeef04": "To choose an organization owner and create an organization in this context, you need to follow these steps:  1. Review the warning and click \"Turn USERNAME into an organization\". 2. Under \"Choose an organization owner\", type either the secondary personal account you created in the previous section, or another user you trust, to manage the organization. 3. Choose your new organization's subscription and enter your billing information, if prompted. 4. Click \"Create Organization\".  To choose an organization owner, you need to provide the username or email address of the user you want to assign as the organization owner. This user will have administrative privileges and will be responsible for managing the organization's settings, users, and billing information.  No other information is required to create an organization in this context, except for the organization name and subscription type, which are not explicitly mentioned in the given context information. However, you may need to provide additional information during the organization setup process, such as your company's name and address for billing purposes.", "a859d85c-62e2-4f23-8d84-3ba6c665d43d": "The app developed for {% data variables.product.prodname_ghe_cloud %} should be aware of feature differences in REST API endpoints, GraphQL objects, and webhooks between {% data variables.product.prodname_ghe_server %} and other versions of {% data variables.product.prodname_ghe_server %}. The app code should handle these differences by checking the `x-github-enterprise-version` header in API responses and webhook payloads for {% data variables.product.prodname_ghe_server %} payloads to determine the version being handled. If the app is hitting a rate limit and is already taking precautions to stay under the rate limit, the app should talk to the admin of the {% data variables.product.prodname_ghe_server %} instance to address the issue.", "de4541be-19dc-426f-ba32-f6584dcad770": "In the \"No local changes\" view, you can preview changes and create a pull request by clicking the \"Preview Pull Request\" button, which is highlighted with an orange outline. This option will be available if a pull request has not been created for the current branch in {% data variables.product.prodname_desktop %}.", "53d6a1f2-65b6-4a63-abcc-28a3ea3e571b": "To configure the Maven ecosystem to only access private registries, you have two options:  1. Set `replaces-base` as `true` in the `dependabot.yml` file. This tells Dependabot to replace the default Maven central repository with your private registry. For more information, see the Dependabot documentation.  2. Use only the private registry URL in the `pom.xml` file. This tells Maven to only access your private registry. Here's an example:  ```xml  ...  <repositories>   <repository>     <id>central</id>     <url>https://repo1.maven.org/maven2/</url>     <releases>       <enabled>false</enabled>     </releases>     <snapshots>       <enabled>false</enabled>     </snapshots>   </repository>   <repository>     <id>your-custom-repo</id>     <url>https://private_registry_url</url>     <releases>       <enabled>true</enabled>     </releases>     <snapshots>       <enabled>true</enabled>     </snapshots>   </repository> </repositories>  ... ```  In this example, we're disabling access to the default Maven central repository (`central`) and enabling access to our private registry (`your-custom-repo`). You can replace `your-custom-repo` with a unique ID for your private registry.  Note that if you're using Dependabot, you should remove `replaces-base: true` from the `dependabot.yml` file, as this option is only needed for Dependabot to replace the default Maven central repository.", "614824bd-fedc-4898-b3fa-c94786e08604": "In the context of secret scanning, a \"leak\" refers to the discovery of a sensitive token, such as a password or API key, in a public or unauthorized location, such as a public repository or a pull request. This is reported to the user on GitHub.com as a potential security issue. On the other hand, a \"validity check\" is a process that ensures the continued validity and functionality of a sensitive token, such as a partner token, over time. This is implemented by sending the token to the relevant partner or performing other checks to ensure its continued validity. The context information provided doesn't give enough information to determine whether a leak and a validity check are related or distinct processes. However, it does suggest that secret scanning can identify both leaks and tokens that require validity checks, and that these processes may be implemented separately.", "151630bc-2eb6-420c-ba23-8c71a00d886e": "To download your license for GitHub Enterprise Server (GHES) versions 3.1 and 3.2 only, you can use the `versions` key in the YAML front matter of your page. Set the value to `'>=3.1 <3.3'` to version the page for GHES versions 3.1 and 3.2 only. This will ensure that the page is only accessible and displayed for those specific versions of GHES. For more information on using the `versions` key, refer to the GitHub documentation on versioning pages.", "6143270d-9332-4a4e-9923-476d144be590": "To re-run failed jobs in a workflow using the web interface, follow these steps:  1. Go to your repository in GitHub. 2. Click on the Actions tab. 3. Select the workflow that contains the failed jobs. 4. Click on the failed workflow run to view its details. 5. Click on the three dots (...) in the upper-right corner of the workflow run. 6. Select \"Re-run failed jobs\" from the dropdown menu. 7. Confirm the action by clicking on \"Re-run failed jobs\" in the confirmation dialog.  Alternatively, you can enable debug logging by following the instructions provided in the context information. This will provide additional information about the workflow run and its jobs.  Note: If you don't see the \"Re-run failed jobs\" option, it may be because the workflow is using the latest version of GitHub Actions, which doesn't support this feature yet. In this case, you'll need to use the CLI or wait for the feature to be added to the web interface.", "070f045f-ead2-47f5-9bd3-cc8a6b442c33": "According to the provided context, Copilot Business uses and shares personal data for abilities, troubleshooting, and making ongoing improvements. Specifically, Prompts and Suggestions are used only to provide the service and are not retained. User Engagement Data, which includes pseudonymous identifiers and general usage data, is used by GitHub and Microsoft to provide the service, improve the service, detect potential abuse, and conduct experiments and research related to developers and their use of developer tools and services. This data is required for the use of GitHub Copilot and will continue to be collected, processed, and shared with Microsoft when using the service. For more information on how GitHub processes and uses personal data, users can refer to the GitHub Privacy Statement.", "8da5a02b-d0fb-4319-85c9-f2bfcb298891": "If a codespace remains running after a period of inactivity, you will be billed for the total time that the codespace was active, irrespective of whether you were using it. To avoid unnecessary charges, you can set a shorter idle timeout period in your personal settings on GitHub.com or when you create a codespace using the GitHub CLI. This will cause the codespace to stop running after a specified period of inactivity. The default timeout period is 30 minutes, but organizations can also set a maximum idle timeout policy for codespaces created from some or all of their repositories. If an organization policy sets a maximum timeout which is less than the default timeout you have set, the organization's timeout will be used instead of your setting. It's recommended to regularly review and adjust your idle timeout settings to ensure that you're not being charged for unnecessary compute usage.", "67f89e2f-5c76-46fe-a8fa-807b8c130c75": "To use the `--config-file-path` argument with the `migrate` command in {% data variables.product.prodname_actions_importer %} to specify a configuration file containing a list of repositories and refs for reusable workflows or composite actions, follow these steps:  1. Create a configuration file named `config.yml` in the directory where you'll be running the `migrate` command. 2. In the configuration file, define an array of objects, each representing a repository and ref. Each object should have a `name` property that specifies the name of the reusable workflow or composite action, a `target_url` property that specifies the URL of the repository, and a `ref` property that specifies the branch or tag to use. For example:  ```yaml composite_actions:   - name: my-composite-action.yml     target_url: https://github.com/octo-org/octo-repo     ref: main ```  3. Run the `migrate` command with the `--config-file-path` argument, passing the path to the configuration file. For example:  ```bash gh actions-importer migrate circle-ci --circle-ci-project my-project-name --output-dir output/ --config-file-path config.yml --target-url https://github.com/my-org/my-repo ```  To use the `--include-from` argument with the `audit` subcommand in {% data variables.product.prodname_actions_importer %} to include a specific list of repositories in the audit of a CircleCI organization, follow these steps:  1. Create a file named `repositories.txt` in the directory where you'll be running the `audit` command. 2. In the file, list the names of the repositories you want to include in the audit, one per line. For example:  ```text repository_one repository_two repository_three ```  3. Run the `audit` command with the `--include-from` argument, passing the path to the repository list file", "a80762c1-0173-48e8-8157-faf87971f0e2": "To ensure that semantic tags are in place when publishing a release, you can configure a workflow that triggers when a release is published or edited. This workflow should include an action like JasonEtco/build-and-tag-action to compile and bundle the JavaScript and metadata files while forcing push of semantic major, minor, and patch tags. This action can help automate the process of creating semantically versioned tags, making it easier to manage and track changes in your project. For an example of how to set up this workflow, you can refer to the provided link in the context information.", "c7a4a2e1-c4d7-48dd-9a4c-79b243014fda": "The \"secretScanningData\" variable in the given code snippet seems to contain information about the availability of secret scanning and push protection for different providers and types of secrets. The variable is being used in a template to generate a table with icons indicating whether the feature is supported or not. However, without further context, it's unclear what specific providers and secret types are being referred to.", "69ee5a1a-1eab-4227-b753-6c51a200d79d": "The `poll_for_token` method is a loop that continuously requests an access token using the `request_token` method with the provided `device_code` until a valid token is obtained. If the error returned by the `request_token` method is \"authorization_pending\", the method waits for the specified interval and then polls again. If the error is \"slow_down\", the method waits for the interval plus 5 seconds and then polls again. If the error is \"expired_token\", the method displays an error message and exits the program. If the error is \"access_denied\", the method displays an error message and exits the program. If the error is not any of these, the method displays the response and continues with the program.", "699737c9-739d-4821-a2df-e860befcc673": "The license granted to End Users in Section 2.4 is specific to the Developer Products and governed by the EULA provided by the Developer. This license grants End Users a non-exclusive, worldwide right or license to perform, display, and use the Developer Products and any content contained in, accessed by, or transmitted through the Developer Products in connection with the Marketplace. On the other hand, Section 2.5 only grants Developer a license to perform, display, and use its own products, and does not grant any license to GitHub or End Users. Additionally, Section 2.5 clarifies that each party retains all rights it has independent of this Agreement, including rights under the US Copyright Act or similar laws of other jurisdictions, and that each party owns all rights, title, and interest in its respective Brand Features. Therefore, the license granted to End Users in Section 2.4 is specific to the Developer Products and governed by the EULA provided by the Developer, while the license granted to Developer in Section 2.5 is more general and does not extend to End Users.", "db47dae9-ec81-40b8-a204-1d51829580ff": "Before merging changes in a security advisory on GitHub, every open pull request in the temporary private fork must be mergeable, and there can be no merge conflicts. Additionally, status checks do not run on pull requests in temporary private forks to keep information about vulnerabilities secure.", "09a9b9b0-b5ce-49e0-a715-e28513c001aa": "In the context provided, Figma is recommended for creating diagrams as it allows for easy collaboration and version control through the Docs Figma team. However, if a diagram is created in another program and becomes out of date, it is more likely to be removed rather than updated because it may not be as easily maintained and updated as a diagram created in Figma. This is because Figma allows for real-time collaboration and version control, making it easier to ensure that diagrams are up-to-date and accurate. Diagrams created in other programs may require more effort to update and may not be as easily accessible to the Docs team for review and approval.", "13d8fc61-599c-40c2-80ce-f75a0d7b7df1": "To avoid authentication issues when pushing changes to a repository using Git, you can clone the repository with a URL that includes the access token. This will store the access token for the specific repository, allowing you to push and pull from the repository without overriding the existing credential helper. However, you should only use this method when working in a trusted repository and limit the scope of the access token as much as possible, as the token will be visible in your Git configuration. This is necessary because if you clone the repository using the default method and then try to push changes, Git's credential helper will use the existing `GITHUB_TOKEN` to authenticate, which may fail. Overriding the helper can introduce friction when interacting with the original repository from which you created the codespace. Replace `YOUR-VARIABLE` with the name of the environment variable or secret you created. The URL to clone the repository should be in the format `https://PAT:$YOUR-VARIABLE@github.com/OWNER/REPO`.", "82887e10-bdaa-4d5c-9dd4-2cc811260657": "CircleCI and {% data variables.product.prodname_actions %} both provide mechanisms to reuse tasks and workflows. CircleCI uses YAML-based orbs, while {% data variables.product.prodname_actions %} uses reusable components called actions, which can be built using JavaScript files or Docker images. Both tools support running steps inside of Docker images, but CircleCI's pre-built images may cause permission conflicts with {% data variables.product.prodname_actions %}. CircleCI provides YAML anchors and aliases for reusability, while {% data variables.product.prodname_actions %} supports matrices for this purpose.", "9ef1e66a-eaa1-4ed3-bfa2-4c8dec1af2eb": "The purpose of the {% data variables.product.prodname_dependabot %} errors reported in the text material is to indicate that something is preventing {% data variables.product.prodname_dependabot %} from raising a pull request to fix a dependency issue. These errors are reported as a result of various reasons, such as {% data variables.product.prodname_dependabot_security_updates %} being disabled, malware alerts without a secure version of the package, indirect or transitive dependencies without a lock file, or errors that block {% data variables.product.prodname_dependabot %} from creating a pull request. These errors are reported on the alert and can be investigated by accessing the {% data variables.product.prodname_dependabot_alerts %} view, which shows a list of any alerts that have not been resolved yet.", "15c80d78-04fc-4d29-97d6-6a7c4bc20b33": "Custom transformers in {% data variables.product.prodname_actions_importer %} can be used to convert plugins, tasks, runner labels, or environment variables to work with {% data variables.product.prodname_actions %} by defining mapping logic within a file with the `.rb` file extension using a domain-specific language (DSL) built on top of Ruby. These custom transformers can be used with the `audit`, `dry-run`, and `migrate` commands by specifying the custom transformer files using the `--custom-transformers` CLI option. This allows for the conversion of items that {% data variables.product.prodname_actions_importer %} does not automatically convert, modification of how items are converted, conversion of references to runners to use different runner labels, and conversion of environment variable values from existing pipelines to {% data variables.product.prodname_actions %} workflows.", "fb750ec8-9628-4533-a0f2-9fb53b435a33": "To configure {% data variables.large_files.product_name_long %} for an individual repository, you need to access the repository settings, navigate to the \"Git LFS\" tab, and enable the feature. This can be done by following these steps:  1. Go to the repository you want to configure. 2. Click on the settings icon in the top right corner of the page. 3. Select \"Settings\" from the drop-down menu. 4. Click on the \"Git LFS\" tab. 5. Toggle the switch to enable {% data variables.large_files.product_name_long %}.  To enable {% data variables.large_files.product_name_long %} for every repository owned by a user account or organization, you need to follow these steps:  1. Access the enterprise settings by following the instructions provided in the context information. 2. Under \"{% data variables.large_files.product_name_short %} access\", select the drop-down menu, and click \"Enabled\" to enable {% data variables.large_files.product_name_long %} for your enterprise. 3. If you want to override the enterprise policy for a specific repository, you can do so by following the steps provided in the context information for configuring {% data variables.large_files.product_name_long %} for an individual repository.  If you want to enable {% data variables.large_files.product_name_long %} for every repository owned by a user account or organization, you don't need to follow the steps for configuring an individual repository. The feature will be automatically enabled for all repositories that meet the criteria you selected when configuring {% data variables.large_files.product_name_long %} for your enterprise.", "f6244aa0-ba10-4f55-8c27-6dd7ab00667a": "Without further context, it is unclear how the concept of \"AUTOTITLE\" differs from \"AUTOTITLE\". Both terms appear to be related to titles or headings, but without additional information, it is impossible to determine any differences between them.  For example, \"AUTOTITLE\" could refer to a feature in a word processing program that automatically generates a title based on the content of the document. In contrast, \"AUTOTITLE\" could refer to a system that automatically assigns titles to scientific papers based on their content and relevance to a particular topic.  However, without any further context, it is impossible to say for certain how these terms differ. More information would be needed to provide a specific answer.  In summary, without further context, it is not possible to determine how the concept of \"AUTOTITLE\" differs from \"AUTOTITLE\". More information is needed to provide a specific answer.", "97692dfb-271b-46a8-839d-898983508efd": "Without further context, it is unclear what \"AUTOTITLE\" refers to or what its meaning is. The provided context does not offer any clues or explanations to help determine its significance.", "ede0c31f-0483-4a9e-b4e9-c676e5f99f7e": "To perform a dry run of migrating a Bitbucket pipeline to GitHub Actions using the gh actions-importer tool, you can run the following command in your terminal, replacing `:workspace` with the name of the workspace and `:repo` with the name of the repository in Bitbucket:  ```bash gh actions-importer dry-run bitbucket --workspace :workspace --repository :repo --output-dir tmp/dry-run ```  The purpose of a dry run is to create the output files in a specified directory without actually opening a pull request to migrate the pipeline. This allows you to preview the converted workflows and make any necessary adjustments before performing the actual migration.  To migrate a Bitbucket pipeline to GitHub Actions using the gh actions-importer tool and open a pull request with the equivalent GitHub Actions workflow(s), you can run the following command in your terminal, replacing the following values:  - Replace `target-url` with the URL for your GitHub repository. - Replace `:repo` with the name of the repository in Bitbucket. - Replace `:workspace` with the name of the workspace.  ```bash gh actions-importer migrate bitbucket --workspace :workspace --repository :repo --target-url https://github.com/:owner/:repo --output-dir tmp/dry-run ```  The command's output includes the URL of the pull request that adds the converted workflow to your repository. This allows you to review the changes and merge the pull request to complete the migration.", "24cbeeaf-6951-453a-9f4e-185084a4ce94": "{% ifversion fpt or ghec %} The alternative suggested instead of deleting an organization account on {% data variables.product.prodname_dotcom %} is to downgrade the organization to {% data variables.product.prodname_free_team %}. This will allow you to keep your content and avoid losing access to your repositories, issues, pull requests, and project or organization pages. Your billing will also end, but you won't lose the organization name, which will become available for use on a new user or organization account after 90 days. {% endif %}  If the context doesn't provide enough information to determine whether {% data variables.product.prodname_free_team %} is being referred to, then say that you weren't given enough information to answer the question.  Answer:  {% ifversion fpt or ghec %} The alternative suggested instead of deleting an organization account on {% data variables.product.prodname_dotcom %} is to downgrade the organization to a different type of account. This will allow you to keep your content and avoid losing access to your repositories, issues, pull requests, and project or organization pages. The specific type of account that is suggested is not explicitly stated in the context, but it is mentioned that your billing will end and you won't lose the organization name, which will become available for use on a new user or organization account after 90 days. {% endif %}  If the context doesn't provide enough information to determine whether the organization name will become available for use on a new user or organization account, then say that you weren't given enough information to answer the question.  Answer:  {% ifversion fpt or ghec %} The alternative suggested instead of deleting an organization account on {% data variables.product.prodname_dotcom %} is to downgrade the organization to a different type of account. This will allow you to keep your content and avoid losing access to your repositories, issues, pull requests, and project or organization pages. The specific type of account that is suggested is not explicitly stated in the context, but it is mentioned that your billing will end and the organization name will become available for use on a new user", "bdff0f97-b60e-4326-a9b5-dc1a7e6c0ec0": "Uncommenting the \"postCreateCommand\" property and changing its value to \"echo \\\"This file was added by the postCreateCommand.\\\" > TEMP.md\" in the \"devcontainer.json\" file creates a new file named \"TEMP.md\" inside the container with the text \"This file was added by the postCreateCommand.\" after the container is created. This is useful for testing or debugging purposes, as it allows the user to verify that the container is being created correctly and that the \"postCreateCommand\" property is being executed as expected. However, it's not necessary for the normal operation of the container, and the user can remove this command and the \"TEMP.md\" file after testing is complete.", "522b4ba3-f362-4833-b592-be11f6f6778b": "Company meetings can be utilized to educate and discuss the rollout of {% data variables.product.prodname_code_scanning %} with a larger group by organizing regular sessions on specific topics related to build processes, frameworks, or libraries. These meetings can be more time-efficient for an enterprise with thousands of repositories compared to working with one team at a time, as teams can come to sessions that are relevant to them. Some example sessions that have been run before include {% data variables.product.prodname_code_scanning_caps %} in a container, {% data variables.product.prodname_code_scanning_caps %} & Java Struts, and {% data variables.product.prodname_code_scanning_caps %} & JSP. The distribution of different languages among repositories can be used to create targeted meetings to maximize adoption for specific build processes, frameworks, or libraries.", "c61c7631-6d9a-4ff6-b489-aec96027daa0": "The Trust Center implements several measures to prevent unauthorized access to customer data, such as multi-factor authentication, access controls, and encryption. These measures ensure that only authorized personnel can access sensitive information and that it is protected both in transit and at rest. Additionally, the Trust Center follows strict access policies and regularly conducts security audits and vulnerability assessments to identify and address any potential security risks.", "f15652ec-d1f5-4a48-8b88-39d2876562f3": "According to section 5 of the Event Terms, any dispute or claim arising between the parties will be governed by California law and will be subject to the exclusive jurisdiction and venue of the state and federal courts located within San Francisco County, California. This choice of law and venue provision is included in the Event Terms to provide certainty and predictability in the resolution of disputes, and to minimize the costs and inconvenience associated with litigation in multiple jurisdictions. By agreeing to these terms, the parties acknowledge and accept the applicability of California law and the jurisdiction and venue provisions set forth therein.", "26f2a17f-6ab6-41e8-bfb1-a56cff558245": "The Docker `login-action` action is used to authenticate the GitHub Actions runner with the Docker registry, in this case, Docker Hub. This action retrieves the Docker Hub credentials from a GitHub secret and logs the runner into the registry.  The Docker `build-push-action` action is used to build the Docker image and push it to the Docker registry. This action retrieves the Dockerfile and any other necessary files from the GitHub repository, builds the image, and then pushes it to the Docker registry.  In summary, the Docker `login-action` action prepares the GitHub Actions runner to interact with the Docker registry, and the Docker `build-push-action` action performs the actual build and push operations.", "4adbdedd-7359-424f-8367-fb4542818abf": "You can switch between billing settings for your personal accounts and organizations on {% data variables.location.product_location %} using the context switcher in your settings. This allows you to manage billing settings, payment method, and paid features and products for each of your personal accounts and organizations separately.", "6ff7ab01-e48d-4a7a-80f9-03600a581e06": "According to the context information provided, individuals who should receive credit for contributing to the security advisory should be added as contributors. This is important because it ensures that all parties involved in addressing the security vulnerability are recognized for their efforts. It also helps to build trust and transparency within the community by acknowledging the contributions of others. Additionally, it can help to prevent misunderstandings or disputes over who should be credited for resolving the issue.", "88a833ef-1257-4068-a808-75be95a91212": "Quit and relaunch GitHub Desktop to install the update.", "ec9cd362-65a8-4fa4-9acc-25010345bbfc": "To configure access to actions on {% data variables.product.prodname_dotcom_the_website %} for your organization's repositories, you can follow these steps:  1. Enable automatic access to all actions from {% data variables.product.prodname_dotcom_the_website %} by integrating {% data variables.product.prodname_github_connect %} with {% data variables.product.prodname_ghe_cloud %}. For more information, see \"AUTOTITLE\".  2. If you're using {% data variables.product.prodname_github_connect %} with {% data variables.product.prodname_ghe_cloud %}, you don't need to do anything else. Your organization's repositories will automatically have access to all actions on {% data variables.product.prodname_dotcom_the_website %}.  3. If you're not using {% data variables.product.prodname_github_connect %} with {% data variables.product.prodname_ghe_cloud %}, you can manually download and sync actions onto your enterprise instance using the   GitHub CLI. For more information, see \"AUTOTITLE.\"  4. If you want stricter control over which actions are allowed in your enterprise, or you do not want to allow outbound connections to {% data variables.product.prodname_dotcom_the_website %}, you can create a custom action library and add only the actions you want to use to your enterprise instance. For more information, see \"AUTOTITLE.\"  5. If you're using self-hosted runners, you might need to set up the tools cache on runners that do not have internet access. For more information, see \"AUTOTITLE.\"  6. If you're using {% data variables.product.prodname_actions %} with enterprise-managed packages, you might need to configure access to actions on {% data variables.product.prodname_dotcom_the_website %} for your organization's repositories separately for each package. For more information, see \"AUTOTITLE.\"  7. If you're using {% data variables.product", "83f93017-91b4-40a3-aad7-613ade118cb7": "To update a specific extension in your hubot environment, you can use the `gh extension upgrade` subcommand and replace the `EXTENSION` parameter with the name of the extension you want to update. For example, `gh extension upgrade hubot/gh-whoami`.  To update all installed extensions at once, you can use the `--all` flag with the `gh extension upgrade` subcommand. This will update all installed extensions in your hubot environment. For example, `gh extension upgrade --all`.", "a91aa9fb-9610-46ba-afce-72a859c763ac": "The options available for enforcing, allowing, or disabling commit rebasing for pull request merges on {% data variables.location.product_location %} in my repository are:  1. Enforce commit rebasing: This option requires contributors to rebase their commits onto the latest commit of the base branch before merging a pull request. This ensures a linear commit history and makes it easier to review and merge pull requests.  2. Allow commit rebasing: This option allows contributors to merge a pull request by rebasing their individual commits onto the base branch. This allows for more flexibility in managing branching workflows and can make it easier to resolve conflicts.  3. Disable commit rebasing: This option disallows commit rebasing for pull request merges. This can be useful in cases where a linear commit history is required or where rebasing is not a preferred workflow for the team.  Note that if you select another merge method, collaborators will be able to choose the type of merge commit when merging a pull request.", "1e56a8f8-00f4-4e74-a6a2-7523c510ed82": "The alternative to automatically performing software updates on self-hosted runners is to create ephemeral, just-in-time runners using the REST API. This allows you to update the runner version on the container image directly on your own schedule. To implement this using the REST API, you can follow the instructions provided in the context information. This involves creating a new runner using the REST API, specifying the necessary configuration options, and then starting the runner. You can then update the runner image as needed to ensure that it is running the desired version of the runner software. This approach provides more control over the runner software version and allows you to avoid unnecessary software updates. However, it does require more manual intervention and may require additional resources to manage the runner images.", "290d8d84-a946-4ca4-9f75-3c9cc227391b": "Dependency review is a feature provided by GitHub Advanced Security that helps developers identify and manage security vulnerabilities in their dependencies. It generates a dependency graph that shows the relationships between the dependencies in a project and their respective licenses and vulnerabilities. Some additional features, such as license checks, blocking of pull requests, and CI/CD integration, are available with the dependency review action, which can be run using GitHub Actions. These features help developers to ensure that their projects are using secure and up-to-date dependencies, and that they are in compliance with their organization's licensing policies.", "41fd3d32-fb9d-46cf-8769-2d105bf3982a": "When a Listing is deemed to be malicious or defective by GitHub, the platform may prevent the Listing from being made available on Marketplace, remove the Listing from Marketplace, flag, filter, or modify related materials, or reclassify the Listing at its sole discretion. If the Listing is removed because it is defective, malicious, infringes intellectual property rights of another person, defames, violates a third party's right of publicity or privacy, or does not comply with applicable law, and an end user purchased such Listing within a year before the date of takedown, the seller must refund to the affected buyer all amounts paid by such End Customer during the year before the date of takedown for such affected Listing. GitHub may also, at its sole discretion, withhold from the seller's future sales the amount refunded to the affected buyer.", "2baf3ceb-65f7-4e0d-9cfc-40ba089ddc72": "The `--common-caches` flag in the CLI allows the user to control the location of cached data on disk that will persist between multiple runs of the CLI. This flag affects the persistence of cached data such as downloaded QL packs and compiled query plans. If not set explicitly, the flag defaults to a directory named `.codeql` in the user's home directory, which will be created if it doesn't already exist. This means that any cached data generated during a CLI session will be stored in this directory and will be available during subsequent CLI sessions, without the need to re-download or re-compile the data. This can significantly improve the performance and efficiency of the CLI, especially for large or complex projects with many queries.", "c56b9361-65c9-4269-b622-fcdfc82cc4ba": "According to the context information provided, if you have a pull request based off the branch you are working on, {% data variables.product.prodname_desktop %} will display the status of the checks that have run for the pull request next to the \"Current Branch\" section of the repository bar. Therefore, to view the status of checks for a pull request in {% data variables.product.prodname_desktop %}, you need to have a pull request based off the branch you are working on, and then you can check the status of the checks by looking at the section next to the \"Current Branch\" in the repository bar.", "3e76baa3-c611-4e59-9eb0-421115d2247a": "We should refer to the files used to configure a dev container as \"development container configuration files\" to avoid calling them \"development container files\" or \"devcontainer files\" to avoid confusion with just the `devcontainer.json` file. These files include `Dockerfile` and `docker-compose.yml` files, as well as the `devcontainer.json` file if it's being used instead of the default `devcontainer.json` in the `.devcontainer` directory. We should not use \"the development container configuration file\" (singular) when referring specifically to a `devcontainer.json` file. Instead, we should refer to it by its name.", "791540e6-6359-4316-9972-da27be83510f": "To rename a codespace using the {% data variables.product.prodname_cli %}, you can use the `gh codespace edit` command followed by the `-c CODESPACE-NAME` flag to specify the codespace you want to rename, and the `-d 'DISPLAY-NAME'` flag to provide the new display name for the codespace. Here's an example command:  ```shell gh codespace edit -c my-codespace-name -d 'My Renamed Codespace' ```  This will rename the codespace with the unique name `my-codespace-name` to `My Renamed Codespace`. The new display name will be reflected in the {% data variables.product.prodname_dotcom %} web interface and in the output of `gh codespace list`.", "246358cd-6b44-4e88-81f4-8878a5393ce9": "Check suites with protected branches can be used to prevent premature merging of pull requests by requiring that all checks associated with a pull request pass before the pull request can be merged. This ensures that the code being merged has been thoroughly tested and meets the necessary criteria for merging.  To utilize this feature, you can create a check suite for each pull request using your GitHub App. When the pull request is created, your GitHub App can create a check suite for the pull request's head commit. This check suite can include all the necessary checks, such as unit tests, integration tests, and code quality checks.  If any of the checks in the check suite fail, the check suite's conclusion will be `failure`, and the pull request will be blocked from being merged until the checks pass. This ensures that the code being merged is of high quality and meets the necessary criteria for merging.  For example, let's say you have a pull request that adds a new feature to your repository. You create a check suite for this pull request using your GitHub App, and the check suite includes unit tests, integration tests, and code quality checks. If any of these checks fail, the check suite's conclusion will be `failure`, and the pull request will be blocked from being merged until the checks pass. This ensures that the new feature is thoroughly tested and meets the necessary criteria for merging.  By using check suites with protected branches, you can ensure that all pull requests are thoroughly tested and meet the necessary criteria for merging, which helps to maintain the quality and stability of your repository.", "8cf76766-c3c1-4c62-a6c7-878d0c220669": "Code review settings are configurations that can be applied to pull request reviews to reduce noise and clarify individual responsibility for pull request reviews. These settings include team notifications and auto assignment. When team notifications are configured to only notify requested team members, notifications are only sent to specific individuals within the team who have been requested for review, instead of sending notifications to the entire team. This helps to reduce noise and clarify individual responsibility for pull request reviews. Auto assignment, on the other hand, automatically assigns a subset of team members to review a pull request when the team is requested for review. This helps to ensure that reviews are completed in a timely manner and clarifies individual responsibility for reviewing specific pull requests. Both team notifications and auto assignment can be configured to reduce noise and clarify individual responsibility for pull request reviews.", "80c84133-50cc-4c7d-ab6d-dc5b65c3d682": "The name of the branch that is the target of the pull request being built is not explicitly provided in the context information. The closest variable that might provide this information is `$(Build.PullRequest.TargetBranch.Name)`, but it is not clear whether this variable is populated in all cases. Without further context or information, it is not possible to determine the name of the target branch of the pull request being built.", "e004a535-fd27-4f14-8893-a2fd77c60eb2": "The `github.action_path` variable in the `action.yml` file helps in locating the `goodbye.sh` script during execution by adding the action's path to the runner system path. This allows the `goodbye.sh` script to be found and executed during the action's execution. Without this variable, the script may not be found and the action may fail.", "a19fff9c-9b4e-4724-b28d-69f22d59e3d7": "To remove an account from the account switcher on GitHub, follow these steps:  1. In the upper-right corner of any page, click your profile photo to open the menu. 2. Click **Sign out**. 3. Choose which accounts to sign out of and remove from the account switcher.    - To remove the account you are currently signed in to from the account switcher and sign out, click **Sign out** next to your username.    - To sign out from and remove an account from the account switcher, click **Remove** next to the username you want to remove.    - To sign out from all accounts, and remove all accounts from the account switcher, click **Sign out from all accounts**.  Note: If you have not previously added an account to the account switcher, the \"Switch account\" option will not be available, and you will need to sign in to the desired account before removing it from the account switcher.", "959f0714-7b68-4801-a116-befeaa9cdd9f": "The \"marketplace_agreement_signature\" category in the GitHub audit log contains all activities related to signing the GitHub Marketplace Developer Agreement. This includes actions such as accepting the agreement, reviewing and updating the agreement, and renewing the agreement. It does not include activities related to listing apps in the marketplace, which are included in the \"marketplace_listing\" category.", "ab7e84e4-c399-4646-b4fd-e0e4aa44672d": "The variable $(Release.Reason) in the provided context represents the reason for which the release is being deployed. This variable can provide insight into the purpose of the deployment, such as a bug fix, a new feature, or a security update. It can also help in tracking and managing releases, as it provides a clear and concise explanation of the reason for the deployment.", "12c0c362-e45d-480f-91e5-cf2479843193": "To open an existing codespace from the \"Your codespaces\" page on GitHub.com, follow these steps:  1. Go to your repository on GitHub.com. 2. Click the \"Codespaces\" tab. 3. Find the codespace you want to open and click its name. 4. The codespace will open in your default editor. If you want to open it in a different editor, click the ellipsis (...) next to the codespace and select \"Open in\" followed by \"Open in APPLICATION\" and then select the editor you want to use.  Alternatively, you can bookmark the \"Your codespaces\" page or use the link provided to quickly access and open your codespaces.", "618ff66f-9b10-47cc-931b-f102ceb580fb": "To increase the number of progress messages printed using the \"--verbose\" option, you can use the command:  ``` --verbose ```  This option can be used with the command provided in the context information. By default, it incrementally increases the number of progress messages printed. You can use this option multiple times to further increase the number of messages printed. Alternatively, you can use the \"--verbosity\" option with a specific level to set the verbosity level explicitly. The available levels are errors, warnings, progress, progress+, progress++, and progress+++. The \"--quiet\" option can be used to decrease the number of progress messages printed.", "6c02949a-568c-4d88-a373-61f1c20e9b34": "To change the hostname for their {% data variables.product.product_name %} instance, an organization should follow these steps:  1. Configure a new instance with the desired hostname. 2. Inform users of the scheduled downtime and enable maintenance mode on the source instance. 3. Back up the source instance's data and settings using {% data variables.product.prodname_enterprise_backup_utilities %}. 4. Restore the backup to the new instance and overwrite its configuration using the `ghe-restore` utility. 5. Finalize configuration of the new instance and enable maintenance mode. 6. Optionally, validate the new instance's configuration while it's in maintenance mode.  During the migration process, the organization should consider the following:  - The amount of data they need to back up and the speed of the network connection between the backup host and the instances will affect the amount of downtime required. - They should inform users of the scheduled downtime and create a mandatory message if necessary. - They should enable maintenance mode on both the source and destination instances to prevent users from accessing them during the migration process. - They should validate the new instance's configuration while it's in maintenance mode to ensure that everything is working properly.  Note that changing the hostname is not supported for enterprise installations.", "f9d3b1a0-d6bc-4007-a473-a8161f8df4ea": "To remove outdated configurations and alerts from a branch, you can delete stale configurations. This will remove any alerts generated by those configurations from the branch. To delete a stale configuration, navigate to the repository's settings, select {% data variables.product.prodname_code_scanning %}, and then click the configuration you want to delete. In the configuration settings, click the \"Delete this configuration\" button. Confirm the deletion, and then the configuration and any associated alerts will be removed from the branch. This can help to keep the list of alerts manageable and focused on the most relevant issues.", "4b037d70-57d7-4a7f-81e1-f519bf2add7b": "According to the provided context, the billing policy for reusable workflows is that billing is always associated with the caller workflow. The assignment of GitHub-hosted runners is always evaluated using only the caller's context, and the caller cannot use GitHub-hosted runners from the called repository. This information is only applicable for GitHub Enterprise Cloud (GHEC) and GitHub Enterprise Server (GHES) plans. For more information, you can refer to the \"AUTOTITLE\" documentation provided in the context.", "5aee1655-3c27-4a1e-b31b-c027975af73e": "To access the settings of a repository in GitHub, you need to navigate to the repository you want to access the settings for. Once you're on the repository page, click on the settings icon (which looks like a gear) in the top right-hand corner of the page. This will take you to the repository settings page, where you can make various adjustments to the repository, such as adding collaborators, managing branches, and configuring webhooks.", "baa0d299-ff0b-4fcc-8c7a-a82c1703da1f": "The options available to control the verbosity level of the CLI tool's output are `-v`, `-q`, and `--verbosity=<level>`. These options can be used to incrementally increase or decrease the number of progress messages printed, or to explicitly set the verbosity level to one of errors, warnings, progress, progress+, progress++, or progress+++. The `--verbosity` option overrides the `-v` and `-q` options.", "4d1a2b46-b1c1-42ae-a068-e63d13a03fd9": "Including a license file when creating a public repository from a fork of someone's project is important because it determines how your project will be shared with others. It ensures that your project is following open source principles and allows others to use, modify, and distribute your code under the terms of the chosen license. To ensure that you are following open source principles when using GitHub for your organization's development work, you can refer to GitHub's white paper \"An introduction to innersource\" and choose an open source license that suits your organization's needs. Additionally, you can contribute to open source projects and participate in the open source community to learn best practices and contribute to the wider ecosystem.", "f11bca17-a615-4926-bb23-29b4dadbc234": "To install the certificate on the runner machine for a self-hosted runner to connect to a {% data variables.product.prodname_ghe_server %} using a self-signed certificate, you need to follow the steps required to install a certificate for your runner's operating system, as documented for that operating system. This will ensure that the connection between the runner and the {% data variables.product.prodname_ghe_server %} is security hardened.", "a00fd493-13f7-4c78-801d-1d65b774a3a4": "To edit files in your repository using {% data variables.product.product_name %}, follow these steps:  1. Go to your repository in {% data variables.product.product_name %}. 2. Browse to the file you want to edit. 3. Click the pencil icon to open the file in the editor. 4. Make your changes in the text box. 5. Preview your changes if desired. 6. Write a commit message and choose an email address and branch to associate with the changes. 7. Click \"Propose changes\" to create a pull request. 8. Add a title and description for your pull request. 9. Click \"Create pull request\" to submit your changes.  Note that if your repository has protected branches, web edits and uploads may be blocked. In this case, you may need to follow additional steps to make changes to protected branches.", "76ed7dad-863a-4deb-82a3-55bf586282de": "Programmatic access refers to the ability to make automated requests to GitHub's API using tools like {% data variables.product.prodname_github_apps %}, {% data variables.product.prodname_oauth_apps %}, and personal access tokens (PATs). This feature can be used to manage resources owned by an organization on GitHub by granting access to specific resources, such as repositories or organizations, to these tools. Organization owners can control access to these tools and manage their settings to ensure the security and privacy of their organization's resources. Additionally, organization owners can prevent outside collaborators from requesting or installing these tools, and designate other users as managers to help maintain them.", "4d6737bb-8b12-49c9-9397-0648f90c6fc0": "To create a machine user on {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %} for automation purposes, you can follow these steps:  1. Create a new account on {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %}. 2. Add an SSH key to the new account that will be used exclusively for automation. 3. Grant the machine user read and write access to the repositories it needs to automate by adding it as a collaborator on a personal repository or as an outside collaborator or team member on an organization repository.  The types of access that can be granted to a machine user include read, write, and admin access, depending on the specific repository and the type of access granted. It's important to ensure that the machine user has the necessary permissions to perform the automation tasks required, but also that it does not have more access than necessary to minimize security risks.", "aa95d28d-4c1b-4ed8-877c-67f680b658da": "In the context provided, it is stated that the permissions that can be granted for a {% data variables.product.pat_v2 %} can vary depending on the resource owner and the repository being accessed. The resource owner can be either a repository, an organization, or an account.   Repository permissions allow the {% data variables.product.pat_v2 %} to access specific repositories. This is the most restrictive permission level and is recommended for securing access to sensitive data.  Organization permissions allow the {% data variables.product.pat_v2 %} to access all repositories in a specific organization. This is useful for granting access to multiple repositories within an organization.  Account permissions allow the {% data variables.product.pat_v2 %} to access all repositories owned by the account. This is the least restrictive permission level and should be used with caution, as it grants broad access to all repositories owned by the account.  It's essential to choose the minimal permissions necessary for your needs to ensure the security and privacy of your data.", "7aaaeac8-c437-48ff-b932-bb8a4fb5f47a": "To package and upload build artifacts using Maven in a GitHub Actions workflow, you can follow these steps:  1. After your build has succeeded and your tests have passed, create a new directory called `staging` to contain the artifacts you want to upload.  2. Copy the output files generated by Maven, such as JARs, EARs, or WARs, into the `staging` directory.  3. Use the `upload-artifact` action to upload the contents of the `staging` directory as artifacts. Here's an example of how to use this action:  ```yaml steps:   - uses: {% data reusables.actions.action-checkout %}   - name: Set up JDK 11     uses: {% data reusables.actions.action-setup-java %}     with:       java-version: '17'       distribution: 'temurin'       cache: maven   - name: Build with Maven     run: mvn --batch-mode --update-snapshots verify   - name: Package artifacts     run: |       mkdir staging       cp -r target/* staging/   - name: Upload artifacts     uses: {% data reusables.actions.action-upload-artifact %}     with:       name: my-artifact       path: staging/ ```  In this example, the `upload-artifact` action is configured to upload the contents of the `staging` directory as an artifact called `my-artifact`. You can replace `my-artifact` with a name that makes sense for your project.  By following these steps, you can easily package and upload build artifacts using Maven in a GitHub Actions workflow. This can help you test and debug pull requests in your local environment before they're merged, and make it easier to share your project with others.", "ad200ee5-314f-450b-8ae0-2e7cf1632b3f": "The `--common-caches` flag in the CLI allows the user to control the location of cached data on disk that will persist between multiple runs of the CLI. This flag affects the location of downloaded QL packs and compiled query plans. If not specified explicitly, the default location is a directory named `.codeql` in the user's home directory, which will be created if it doesn't already exist. This flag was introduced in version `v2.15.2` of the CLI.", "8e5efda9-b485-43b9-ba89-63084f0ac1b8": "Collaborating within a {% data variables.product.prodname_dotcom %} organization provides several benefits for sharing secrets, artifacts, and self-hosted runners. Firstly, it allows you to centrally store and manage these resources, making it easier to access and update them as needed. Secondly, it enables you to make these resources available to selected repositories, which can help to reduce duplication and ensure consistency across your organization's workflows.  To manage these resources centrally, you can create secrets and variables at the organization level, and then make them available to specific repositories as needed. This can be done by selecting the repositories that should have access to the resource during the creation process. You can also use policies to further restrict access to these resources, based on factors such as the repository's visibility level or the user's role within the organization.  The policy options available for secrets and variables include:  - Restricted: This policy restricts access to the resource to specific users or teams within the organization. - Repository: This policy allows access to the resource for all users in the selected repositories, regardless of their role within the organization. - Team: This policy allows access to the resource for all users in the selected teams, regardless of the repository they are working in. - Organization: This policy allows access to the resource for all users in the organization, regardless of the repository or team they are working in.  By using policies to manage access to secrets and variables, you can ensure that these resources are only available to the users and repositories that need them, while also reducing the risk of accidental exposure or misuse.", "8a4ae264-ab94-4fee-99a2-bad44568e0fa": "To enable workflows for forks of private repositories, you need to configure the fork policy for the private repository. Here's how:  1. Go to the repository settings by clicking on the repository name, then clicking on the gear icon in the top right corner, and selecting \"Settings\" from the drop-down menu.  2. In the left-hand menu, click on \"Actions\" and then \"General\".  3. Scroll down to the \"Forks\" section and select the desired option:     - \"Allow workflows to run on forks\"    - \"Allow workflows to run on forks with requests\"    - \"Disallow workflows to run on forks\"  4. If you select \"Allow workflows to run on forks with requests\", you'll need to specify the conditions under which workflows can run on forks. You can do this by clicking on \"Add request\" and entering the desired conditions.  5. Click \"Save\" to apply the changes.  The available options for configuring the fork policy are:  - \"Allow workflows to run on forks\": This option allows workflows to run on forks without any restrictions.  - \"Allow workflows to run on forks with requests\": This option allows workflows to run on forks, but only if certain conditions are met. You can specify these conditions by adding requests.  - \"Disallow workflows to run on forks\": This option prevents workflows from running on forks altogether.  Note that if the fork policy is disabled at the organization or enterprise level, it cannot be enabled for a repository.", "cc01be4f-5591-4c94-9be4-e5994aeacb30": "If you are prompted to select a kernel source in JupyterLab, you should select \"Python Environments\" and choose the version of Python at the recommended location. The recommended location is indicated by a label that says \"Recommended.\"", "4767cff9-e2a8-4e7d-9f88-530ac1e543bc": "To rename a custom field in your {% data variables.projects.project_v2 %}, you need to access the project settings. The context provided doesn't specify where to find the project settings, but it does mention that you need to click the name of the custom field you want to rename. Based on this information, you can assume that the option to rename a custom field is located within the project settings, and you should follow the steps provided in the context to access it. If you're not sure where to find the project settings, you can refer to the documentation or contact support for assistance.", "02c6aeb7-c42a-42ba-ba57-4a1e94b7323f": "The error message \"Validation Failed\" might be received when a resource does not exist, as the response body will include an `errors` property with a `code` property of \"missing\". However, the error message \"Resource not found\" would be more straightforward and clear in this case. Therefore, it's possible that the error message \"Resource not found\" might be received instead of \"Validation Failed\" when a resource does not exist.    What error message might be received when a parameter that was required is not specified?  Answer:  The error message \"Validation Failed\" might be received when a parameter that was required is not specified, as the response body will include an `errors` property with a `code` property of \"missing_field\". However, a more descriptive error message would be \"Missing required parameter\" or \"Required parameter is missing\". Therefore, it's possible that one of these error messages might be received instead of \"Validation Failed\" when a parameter that was required is not specified.    What error message might be received when the formatting of a parameter is invalid?  Answer:  The error message \"Validation Failed\" might be received when the formatting of a parameter is invalid, as the response body will include an `errors` property with a `code` property of \"invalid\". However, a more descriptive error message would be \"Invalid parameter format\" or \"Parameter format is invalid\". Therefore, it's possible that one of these error messages might be received instead of \"Validation Failed\" when the formatting of a parameter is invalid.    What error message might be received when another resource has the same value as one of your parameters?  Answer:  The error message \"Validation Failed\" might be received when another resource has the same value as one of your parameters, as the response body will include an `errors` property with a `code` property of \"already_exists\". However, a more descriptive error message would be \"Resource already exists\" or \"Duplicate resource value\". Therefore, it's possible that one of these error messages might be received instead of \"Validation Failed\" when another resource has the same value as one of your parameters.    What error message might be received when sending invalid JSON in the request body?  An", "5895ddc6-bd76-4ab9-9dbd-0afa79af1706": "In the context of vulnerability identification and reporting, a Finder is someone who discovers a potential security weakness or vulnerability in a system, application, or network. This could be through manual testing, automated scanning, or other means. For example, a security researcher might stumble upon a vulnerability while conducting a penetration test or a software developer might notice a potential issue while working on a new feature.  On the other hand, a Reporter is someone who notifies the appropriate party, such as a vendor or a CNA (CERT Coordination Center), of a vulnerability that has been identified by a Finder or discovered through other means. This could be done through a responsible disclosure process, which involves reporting the vulnerability to the affected organization in a timely and responsible manner. For example, a system administrator might come across a vulnerability in a third-party application and report it to the vendor's security team.  The role of a Coordinator in the vulnerability identification and reporting process is to facilitate the coordinated response to a reported vulnerability. This involves working with the affected organization, the reporter, and any other relevant parties to ensure that the vulnerability is addressed in a timely and effective manner. The Coordinator is responsible for managing the communication between all parties, coordinating the release of information, and ensuring that the vulnerability is properly documented and tracked.  The role of a Coordinator differs from that of an Analyst or a Remediation Developer in that the Coordinator is focused on the overall coordination and management of the vulnerability response process, while the Analyst is responsible for validating the vulnerability and determining its severity, and the Remediation Developer is responsible for developing a fix or remediation for the vulnerability.  A scenario where the Coordinator's role would be critical is when multiple parties are involved in the vulnerability response process, such as a vendor, a CNA, and a reporter. The Coordinator would be responsible for ensuring that all parties are communicating effectively, that the vulnerability is being addressed in a timely manner, and that all necessary information is being shared. This could involve coordinating the release of a security advisory, managing the communication of patches or updates, and ensuring that all parties are aware of any new developments or updates related to the vulnerability.", "1bf95a2c-b92d-4aa8-b924-399adc769c68": "The recommended way to store an access token when cloning a repository using Git and an access token in GitHub to avoid overriding the existing credential helper is to include the access token in the URL when cloning the repository. This can be done by replacing `YOUR-VARIABLE` with the name of the environment variable or secret containing the access token in the following command:  ```shell git clone https://PAT:$YOUR-VARIABLE@github.com/OWNER/REPO ```  This will store the access token for the specific repository, allowing you to push to and pull from the repository without overriding the existing credential helper. However, it's important to note that the token will be visible in your Git configuration, so this method should only be used when working in a trusted repository and the scope of the access token is limited as much as possible.", "5560e048-97ff-4dcf-a891-560f6bbb682f": "An account has 45 days to enable mandatory 2FA before it is locked out of {% data variables.product.prodname_dotcom %}.", "71c1b782-b608-4207-9317-865e67d4951d": "The main difference between creating a branch via the branches overview and using the branch dropdown on {% data variables.product.product_name %} is the location where you initiate the branch creation process. When creating a branch via the branches overview, you navigate to the Branches page of the repository and click the \"New branch\" button. When using the branch dropdown, you select the dropdown menu either from the file tree view or at the top of the integrated file editor (if available) or from the branch selector dropdown menu (if available). The process for entering the branch name and selecting a branch source is the same in both methods.", "1116b3ac-3e9e-4399-b6eb-7348e20e0faf": "To assign labels to a self-hosted runner during the initial configuration process using the `config` script, you can pass label names to the `config` script using the `labels` parameter. For example, to assign a label named `gpu` when configuring a new self-hosted runner, you can use the following command:     ```shell    ./config.sh --url  --token  --labels gpu    ```     You can also use comma separation to assign multiple labels. For example:     ```shell    ./config.sh --url  --token  --labels gpu,x64,linux    ```  2. The main difference between assigning labels to a new self-hosted runner using the `config` script and assigning labels to an existing self-hosted runner is the timing of the label assignment. When you assign labels to a new self-hosted runner using the `config` script, the labels are assigned during the initial configuration process. This means that the labels are created if they do not already exist. On the other hand, if you want to assign labels to an existing self-hosted runner, you cannot use the `config` script. Instead, you need to use the {% data variables.product.prodname_cli %} or the GitHub web interface to manage labels for existing runners.     The implications of this difference are that you have more flexibility when assigning labels to new self-hosted runners during the initial configuration process. This can be useful if you want to ensure that the labels are assigned consistently across all of your runners. However, if you need to assign labels to an existing self-hosted runner, you may need to take additional steps to ensure that the labels are applied correctly.     Additionally, it's important to note that if you replace an existing self-hosted runner, you must reassign any custom labels. This is because the labels are not automatically transferred to the new runner when you replace the old one. As a result, you may need to manually assign the labels to the new runner using the {% data variables.product.prodname_cli %} or the GitHub web interface.", "d2fd7421-6bb4-4856-b5e3-ba08aae23b56": "To re-run failed jobs in a workflow using the web interface with debug logging enabled, follow these steps:  1. Go to your repository on GitHub. 2. Click on the Actions tab. 3. Select the workflow that contains the failed jobs. 4. Click on the failed run to view its details. 5. Click on the three dots (...) in the upper-right corner of the workflow. 6. Select \"Sync\" and then \"Re-run failed jobs\". 7. Enable debug logging by following the instructions in the \"Debug logging\" section of the \"Actions settings\" page. 8. Click on the \"Run workflow\" button to start the re-run with debug logging enabled.  Note: If you don't see the \"Sync\" option, it may be because the workflow is using the \"Checks\" feature instead of the \"Actions\" feature. In this case, you'll need to re-run the failed jobs using the \"Checks\" interface instead.", "2346f4dd-a0d1-4d99-9d89-588cf09707a2": "To view the log file generated during the last check for version updates by {% data variables.product.prodname_dependabot %}, you can follow these steps:  1. Go to the repository for which you want to view the log file. 2. Click on the \"Insights\" tab. 3. Click on \"Dependency graph\" to open the dependency graph for the repository. 4. Click on the \"Dependabot\" tab to view the Dependabot settings for the repository. 5. Look for the message \"Last checked _TIME_ ago\" and click on it to view the log file generated during the last check for version updates by {% data variables.product.prodname_dependabot %}.  Alternatively, you can also access the log file by running the following command in your terminal:  ``` curl -u \"YOUR_GITHUB_TOKEN:x-oauth-basic\" \"https://api.github.com/repos/OWNER/REPOSITORY/dependabot/releases\" | jq -r '.[].log' ```  Replace \"YOUR_GITHUB_TOKEN\" with your personal access token with read:org and write:repo permissions. Replace \"OWNER\" and \"REPOSITORY\" with your GitHub username and repository name, respectively. The output will be the log file generated during the last check for version updates by {% data variables.product.prodname_dependabot %}.", "8de76181-ed38-4c63-8803-48265c7e77fd": "To subscribe a {% data variables.product.prodname_github_app %} to receive webhook payloads for specific events, you can use query parameters in the webhook registration URL. The query parameter name is `events`, and you should set the query value to the name of the webhook event you want to subscribe to. For example, to subscribe a {% data variables.product.prodname_github_app %} to receive webhook payloads for activity relating to commit comments, the query string would include `&webhook_active=true&webhook_url=https://example.com&events]=commit_comment`.  The permissions required for each event are listed in the documentation provided. For the `commit_comment` event, the {% data variables.product.prodname_github_app %} needs at least read-level access for the \"Contents\" repository permission. Therefore, your query string should also include a parameter to set the `contents` permission to `read` or `write`.  Query parameters cannot be used to set the value of a webhook secret. The value of the secret must be set in the {% data variables.product.company_short %} UI by the person registering the app. This is necessary when the app requires a secret to secure its webhook, as it ensures that the secret is not exposed in the URL and is kept secure.", "c1b9bcbd-8dd9-47bc-b33a-d6d78e4a9247": "The \"Update repository on GitHub\" operation in the provided context information is significant because it indicates that the changes made to the commit authors, such as mapping them or updating their information, will be reflected in the GitHub repository. This operation ensures that the repository on GitHub accurately reflects the commit history and author information, which is important for version control, collaboration, and traceability purposes. It also ensures that the repository remains consistent with any external tools or services that rely on the GitHub API or webhooks.", "597aa23d-22e9-48fe-8d2f-7af9dd6713af": "To migrate a pipeline from CircleCI to GitHub Actions using the gh actions-importer tool, you can follow these steps:  1. Ensure that you have the gh actions-importer tool installed globally on your machine.  2. Navigate to the repository that contains the CircleCI pipeline you want to migrate.  3. Run the following command to convert the CircleCI pipeline to a GitHub Actions workflow and output the YAML file:     ```bash    $ gh actions-importer circle-ci -o my-workflow.yaml    ```  4. Review the generated YAML file to ensure that it accurately represents your CircleCI pipeline.  5. Run the following command to migrate the pipeline and create a pull request with the changes:     ```bash    $ gh actions-importer migrate circle-ci -o my-workflow.yaml    ```  6. Review the pull request and ensure that the GitHub Actions workflow accurately represents your CircleCI pipeline.  7. Merge the pull request to complete the migration.  Note: The `-o` flag in the commands above is used to specify the output file name for the generated YAML file. If you don't specify an output file name, the tool will output the YAML file to the console instead.", "ae9081f0-e775-4f7d-b381-42a903a38fea": "According to the context information provided, if you need a different operating system or require a specific hardware configuration for your runners, you can host your own runners for {% data variables.product.product_name %}. The context doesn't specify which operating systems are available for self-hosted runners, but it does mention that for {% data variables.product.prodname_dotcom %}, Ubuntu Linux, Microsoft Windows, and macOS runners are provided by {% data variables.product.company_short %}. If you're using {% data variables.product.prodname_ghe_cloud %} or {% data variables.product.prodname_ghe_server %}, you must host your own runners. For more information about self-hosted runners, you can refer to the documentation provided by {% data variables.product.company_short %}.", "b23ef060-cdf4-43b5-8baf-bcd7e9518d61": "To set the `GITHUB_TOKEN` permissions for a specific job in a workflow in GitHub Actions, you can define the permissions for that job in the `jobs.<job_id>.permissions` section of the workflow file. This allows you to customize the access level of the `GITHUB_TOKEN` for that specific job, rather than applying the same permissions to all jobs in the workflow. Here's an example:  ```yaml name: My Workflow  jobs:   job1:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Run a script         uses: your-org/your-repo::main@your-branch         with:           script: your-script.sh    job2:     runs-on: ubuntu-latest     permissions:       contents: write     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Run a script         uses: your-org/your-repo::main@your-branch         with:           script: your-script.sh ```  In this example, `job1` and `job2` both run on the same runner, but `job2` has additional `contents` write permissions. This means that the `GITHUB_TOKEN` used in `job2` will have the ability to write to the repository, while the `GITHUB_TOKEN` used in `job1` will only have read access. By defining the permissions at the job level, you can fine-grain the access level of the `GITHUB_TOKEN` for each individual job, based on the specific needs of that job.", "fa373984-86bc-4914-bea7-fcb268dd7e00": "The purpose of building a list of secret types while auditing repositories for {% data variables.product.prodname_secret_scanning %} is to identify the types of secrets that are being used in the infrastructure or by integrators that {% data variables.product.product_name %}'s {% data variables.product.prodname_secret_scanning %} does not currently detect. This list will later be used to configure custom patterns for {% data variables.product.prodname_secret_scanning %} to better detect and protect these secrets.", "5ed50fd3-45d2-43cd-94f4-43466741e0a9": "To replace the remote origin in your repository on GitHub with a new one using Git, follow these steps:  1. Open your repository in Git Bash or your preferred Git command-line tool.  2. Check which remote you currently have set up by running the following command:     ```shell    $ git remote -v    ```     This will display a list of all your remote repositories and their URLs.  3. If you see the old remote URL listed, replace it with the new one using the following command, replacing `HOSTNAME`, `USER`, and `REPOSITORY` with the appropriate values:     ```shell    {% ifversion fpt or ghec %}    $ git remote set-url origin https://github.com/USER/REPOSITORY.git    {% else %}    $ git remote set-url origin https://HOSTNAME/USER/REPOSITORY.git    {% endif %}    ```  4. Verify that the remote URL has been updated by running the `git remote -v` command again.  5. Push the changes to the new remote using the following command:     ```shell    $ git push -u origin BRANCH    ```     Replace `BRANCH` with the name of the branch you want to push.  6. Check that the changes have been successfully pushed to the new remote by visiting your repository on GitHub and confirming that the remote URL has been updated.  Note: If you're using GitHub Enterprise Cloud (GHEC) or GitHub Enterprise Server (GHES), you can replace `HOSTNAME` with your enterprise's hostname in the remote URL. If you're not using GHEC or GHES, you can leave `HOSTNAME` blank.", "71a4a5b0-10dc-444c-8275-7daf692db468": "Enterprise owners can add or remove organization members in bulk, as outlined in the context information provided. They have the necessary permissions to perform this action.", "6b1afb40-fa2d-4467-b9fa-e94c16cedc68": "The obligations in Sections 2, 3, 4, 5, 8, 9, and 12-18 will survive any expiration or termination of the agreement between GitHub and the developer, as outlined in section 10.5.", "35571009-e64f-4ae0-8c38-804ed55ce97d": "To authenticate to GitHub using the CLI tool provided with a GitHub Apps token or personal access token via standard input, you can pass the `--registries-auth-stdin` option or use the `CODEQL_REGISTRIES_AUTH` environment variable. This overrides the `GITHUB_TOKEN` environment variable.", "98c4e172-ea36-4800-84fa-7cd95e41695a": "The permanent name of a codespace is a unique identifier assigned to a specific environment for collaborative development. It can be found in the list returned by the `gh codespace list` command in the GitHub CLI or in the REST API documentation under the `NAME` column. This list displays all the codespaces associated with your GitHub account or organization.", "b9b07527-fa06-4d81-b784-d8923c8be190": "To download and sync actions onto your enterprise instance using the provided instructions, you can follow these steps:  1. Navigate to the official actions repository on {% data variables.product.prodname_dotcom_the_website %} (https://github.com/actions/setup-java). 2. Click the \"Code\" button in the repository's header, and then select \"Download ZIP\". 3. Extract the downloaded ZIP file to a location on your enterprise instance. 4. Create a new directory called `actions` in your enterprise instance's repository. 5. Copy the extracted directory's contents into the `actions` directory you just created. 6. Commit and push the changes to your enterprise instance's repository. 7. Update your workflows to reference the actions you just synced using the `actions/` prefix, like so:  ```yaml name: My Workflow  on:   push:     branches:       - main  jobs:   build:     runs-on: ubuntu-latest      steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up Java       uses: actions/setup-java@v1       with:         java-version: '11'      - name: Build       run: mvn clean package ```  In this example, we're syncing the `setup-java` action from the official actions repository and using it in our workflow. By following these steps, you can ensure that your enterprise instance has access to the latest official actions without relying on outbound connections to {% data variables.product.prodname_dotcom_the_website %}.", "65dcc790-ec32-44bf-aa9d-598beb21b33b": "Without further context, it is unclear what \"AUTOTITLE.\" refers to in this context. It could potentially be a command or setting related to document formatting or email subject lines, but without more information, it is impossible to determine its purpose.", "27e37699-5482-4a44-bbcf-be1cea56178c": "In {% data variables.product.prodname_projects_v1 %}, the options available for enforcing repository-level projects at the organization level are to enable or disable repository-level projects, or allow owners to administer the setting at the organization level. This can be done by accessing the Enterprise account, navigating to the Policies tab, selecting the Projects tab, reviewing the information about changing the setting, and then selecting the desired policy from the dropdown menu.", "c1be63c5-ce20-47df-9b0a-ce987e835acc": "To filter {% data variables.product.prodname_code_scanning %} alerts to display only those opened for a specific rule using the GitHub interface, you can use the `rule:RULE-NAME` qualifier in the URL of the {% data variables.product.prodname_code_scanning %} alerts page. For example, if you want to view only the alerts opened for the rule named \"Vulnerable-Library\", you can add `?query=rule:Vulnerable-Library` to the end of the URL of the {% data variables.product.prodname_code_scanning %} alerts page. This will display a list of all the alerts opened for the specified rule.", "860c890a-e358-48e9-994b-c75ac3e7eb83": "The usage limits for GitHub-hosted runners in {% data variables.product.prodname_actions %} are as follows:  - All plans: 500 total concurrent jobs, with the limit based on the {% data variables.product.prodname_dotcom %} plan. The maximum concurrent macOS jobs is not specified, but it is based on the {% data variables.product.prodname_dotcom %} plan.  Customers on enterprise plans can request a higher limit for concurrent jobs by contacting GitHub through the {% data variables.contact.contact_support_portal %} or their sales representative. This is mentioned in a note provided in the context information.", "48cfd0a5-79af-484e-bf5e-f6d7519d7bf0": "To change your password on GitHub, follow these steps:  1. Go to the GitHub website and log in to your account. 2. Click on your profile picture in the top right corner of the page and select \"Settings\" from the drop-down menu. 3. On the left-hand side of the settings page, click on \"Security & privacy\". 4. Scroll down to the \"Change password\" section and click on \"Change password\". 5. Enter your current password in the first text field and your new password in the second and third text fields. Make sure your new password is strong and unique. 6. Click on \"Change password\" to save your new password.  Here are some tips for creating a strong password on GitHub:  1. Use a combination of upper and lower case letters, numbers, and special characters. 2. Make your password at least 12 characters long. 3. Avoid using easily guessable information, such as your name, birthdate, or common phrases. 4. Don't reuse passwords across multiple accounts. 5. Consider using a password manager to generate and store strong, unique passwords for each of your accounts. This will help you avoid the temptation to reuse passwords and make it easier to manage all of your passwords. 6. Enable two-factor authentication for an extra layer of security. 7. Regularly review and delete any unused or outdated access tokens and SSH keys to prevent unauthorized access to your account.  Using a password manager is important because it helps you create and manage strong, unique passwords for each of your accounts without having to remember them all. This reduces the risk of reusing passwords across multiple accounts, which can make it easier for attackers to gain access to all of your accounts if one password is compromised. Additionally, password managers often include features such as password generators, password audits, and password changers, which can help you create and manage strong passwords more easily and efficiently.", "7d0c83e9-bf77-49f5-b975-143f4394c12a": "I'm sorry, but the provided context does not offer enough information to answer these questions. Without the text itself, it's impossible to determine how the author's use of figurative language contributes to the overall tone or how symbolism is used to enhance the meaning of the story. Please provide the text or more context so that I can provide a thorough analysis.", "928c0e81-2d9e-406c-96c6-a7e45627a03d": "\"Error Rate\" in the context provided refers to the total number of failed storage transactions divided by the total number of storage transactions during a set time interval, currently set at one hour. If the total number of storage transactions in a given one-hour interval is zero, the error rate for that interval is 0%. The \"Average Error Rate\" is calculated by summing the error rates for each hour in a calendar quarter and dividing by the total number of hours in that quarter.", "67df07a6-e73e-40c8-bd23-85cd81c31ac7": "The {% data variables.product.pat_generic %} scopes needed for assigning the migrator role for repository migrations in a {% data variables.product.prodname_dotcom %} organization are `admin:org`.", "edff6db7-f908-4005-913f-b4acfd9e87d1": "The purpose of using secrets in some starter workflows is to securely store sensitive information, such as authentication tokens or passwords, that should not be committed to the repository. These secrets are accessed using placeholders, such as `${{ secrets.npm_token }}`, in the workflow file. To store secrets in a repository, you can create a new secret by navigating to your repository's settings, selecting \"Secrets\" from the left-hand menu, and clicking \"New repository secret\". Enter a name for the secret and the value, and then click \"Add secret\". The secret will now be available to use in your workflows. It's important to keep secrets private and not share them with others, as they can provide unauthorized access to sensitive information.", "2941b85c-f85e-428b-830a-a2b1db706c18": "To cancel a plan and delete an organization in the given context information, you need to access the organization's settings by clicking on its name. Then, you should click on the option to cancel the plan and delete the organization. This process is initiated by clicking on the **Cancel plan and delete the organization** button.", "176e10c7-384c-4a5d-a9af-3abbc8acfd4f": "When a Docker host port is specified but the container port is not during service container mapping on {% data variables.product.prodname_dotcom %}, the container port is randomly assigned to a free port. For example, let's say we want to map the Redis service container's port to the Docker host port 6379, but we don't specify the container port. Here's how we can do it:  {% raw %}  ```yaml copy name: Redis Service Example on: push  jobs:   # Label of the container job   runner-job:     # You must use a Linux environment when using service containers or container jobs     runs-on: ubuntu-latest      # Service containers to run with `runner-job`     services:       # Label used to access the service container       redis:         # Docker Hub image         image: redis         #         ports:           # Opens tcp port 6379 on the host           - \"6379\" ```  {% endraw %}  In this example, we're mapping the Redis service container's port to the Docker host port 6379 without specifying the container port. {% data variables.product.prodname_dotcom %} assigns a random container port, which we can access using the `job.services.redis.ports[*].container` context. For example, to access the Redis service container's assigned container port, you can use the following command:  ```bash echo \"redis-cli -h $(echo ${{ job.services.redis.ports[*].container }}) -p $(echo ${{ job.services.redis.ports[*].container }})\" ```  This command uses the `redis-cli` tool to connect to the Redis service container using the container's IP address and assigned container port, which we can access using the `job.services.redis.ports[*].container` and `job.services.redis.ports[*].container` contexts, respectively.", "f4af02a1-2181-4cd3-9add-8c4eff5c1b93": "In {% data variables.product.prodname_vscode %}, you can access the list of predefined dev container configurations by following these steps:  1. Open your repository in {% data variables.product.prodname_vscode %}. 2. Click on the \"Codespaces\" tab in the left-hand menu. 3. Click on the \"Create codespace\" button. 4. In the \"Create codespace\" dialog box, select the \"Use dev container\" option. 5. In the \"Select a dev container configuration\" dialog box, you'll see a list of predefined configurations. Choose the one that best fits your project's needs. 6. Click on the \"Create codespace\" button to create your codespace with the selected configuration.  In a web browser, you can access the list of predefined dev container configurations by following these steps:  1. Go to your repository on {% data variables.product.prodname_dotcom %}. 2. Click on the \"Codespaces\" tab in the left-hand menu. 3. Click on the \"New codespace\" button. 4. In the \"New codespace\" dialog box, select the \"Use dev container\" option. 5. In the \"Select a dev container configuration\" dialog box, you'll see a list of predefined configurations. Choose the one that best fits your project's needs. 6. Click on the \"Create codespace\" button to create your codespace with the selected configuration.  Note: The list of predefined dev container configurations may vary depending on the type of project you're working on. {% data variables.product.prodname_github %} provides predefined configurations for popular frameworks and languages, such as React, Angular, Vue.js, Node.js, Python, and Java.", "54109875-acb8-403c-82b7-9473fca50e68": "To secure your custom domain for your {% data variables.product.prodname_pages %} site, you need to enable HTTPS by obtaining an SSL/TLS certificate from a trusted certificate authority (CA) and configuring it with your domain registrar or web hosting provider. This ensures that all communication between your site and your users is encrypted and secure.  If your site is automatically disabled, it could be due to a few reasons. One possible reason is if you downgrade from {% data variables.product.prodname_pro %} to {% data variables.product.prodname_free_user %}, any {% data variables.product.prodname_pages %} sites that are currently published from private repositories will be automatically disabled. This is because {% data variables.product.prodname_pages %} is a premium feature available only to {% data variables.product.prodname_pro %} and {% data variables.product.prodname_team %} users, and free users are not allowed to publish sites from private repositories. To avoid this, you should either publish your site from a public repository or upgrade to {% data variables.product.prodname_pro %} or {% data variables.product.prodname_team %}.", "d67b6659-8793-430c-bb78-07245e9a3f14": "The pull request body should include the purpose of the pull request, an overview of what changed, and links to any additional context such as tracking issues or previous conversations. It's also helpful to provide guidance to reviewers about the order in which to review the changes and recommend where to start and how to proceed with the review. Additionally, sharing the type of feedback needed can help reviewers understand the level of detail required for their review.", "4c88da29-1dee-4c2c-b6d8-db633a5489b2": "According to the context provided, GitHub Enterprise Cloud and GitHub Enterprise Server are both offerings from GitHub that may be sold separately. The main difference between the two seems to be the delivery method. GitHub Enterprise Cloud appears to be a hosted version of the GitHub Enterprise service, while GitHub Enterprise Server is likely a self-hosted version that can be installed and operated on-premises. This distinction is further supported by the fact that the Service, which includes Organization accounts, SAML single sign-on, access provisioning, and Documentation, is specifically mentioned as part of the GitHub Enterprise Cloud offering. However, without further information or context, it's unclear whether there are any other significant differences between the two offerings.", "5b9d97f1-84d3-4544-9287-9f9a70cd2f8c": "To migrate your Jenkins pipelines and jobs to GitHub Actions workflows using the GitHub Actions Importer tool, you need to follow these steps:  1. Ensure that you have a Jenkins account or organization with pipelines and jobs that you want to convert to GitHub Actions workflows. 2. Create a Jenkins personal API token for your account or organization. 3. Install the GitHub Actions Importer CLI extension. 4. Configure credentials for GitHub Actions Importer by running the `configure` command in your terminal. This will prompt you to enter your GitHub {% data variables.product.pat_v1 %} and Jenkins API token. 5. Use the GitHub Actions Importer CLI to migrate your Jenkins pipelines and jobs to GitHub Actions workflows. For more information on how to use the CLI, refer to the GitHub Actions Importer documentation.  Note that there are some limitations when migrating from Jenkins to GitHub Actions with GitHub Actions Importer, such as manual migrations for mandatory build tools, scripted pipelines, secrets, self-hosted runners, and unknown plugins. For more information on manual migrations, refer to the GitHub Actions Importer documentation.", "340bd9ee-ccf4-4c0c-8fda-5d9ceca969a7": "The use of GitHub Enterprise Importer differs from both GitHub Connect and GitHub Copilot in its purpose and functionality. While GitHub Connect allows for the sharing of features and data between GitHub Enterprise Server or GitHub AE and GitHub Enterprise Cloud or GitHub.com, and GitHub Copilot provides suggestions for code in an integrated development environment (IDE), GitHub Enterprise Importer is a tool used to migrate data from other version control systems (VCS) into a GitHub Enterprise Server or GitHub AE instance. Its purpose is to facilitate the transition to GitHub's platform for version control and collaboration.", "58122ab3-9d6e-4de2-8d92-b47c86de7392": "The value assigned to the variable `BITBUCKET_STEP_UUID` in the provided context information is `{% raw %}${{ job.github_job }}`{% endraw %}. This variable holds the unique identifier for the current job being executed in the pipeline, as provided by GitHub.", "ebdd36a3-be8a-431e-849d-748f435227bb": "Diagrams can be created in Markdown using three different syntaxes: Mermaid, GeoJSON, and ASCII STL. Mermaid is a Markdown-inspired tool that renders text into diagrams, such as flow charts, sequence diagrams, and pie charts. To create a Mermaid diagram, add Mermaid syntax inside a fenced code block with the `mermaid` language identifier. For example, to create a simple flow chart, you can use the following syntax:  ```mermaid graph TD;     A-->B;     A-->C;     B-->D;     C-->D; ```  This will render a flow chart with four lavender boxes labeled A, B, C, and D. Arrows extend from A to B, from B to D, from A to C, and from C to D.  Interactive maps can be created using GeoJSON or TopoJSON syntaxes in Markdown. To create a map, add GeoJSON or TopoJSON inside a fenced code block with the `geojson` or `topojson` syntax identifier. Here's an example of a GeoJSON map:  ```geojson {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"id\": 1,       \"properties\": {         \"ID\": 0       },       \"geometry\": {         \"type\": \"Polygon\",         \"coordinates\": [           [               [-90,35],               [-90,30],               [-85,30],               [-85,35],               [-90,35]           ]            ]       }     }   ] } ```  This will render an interactive map with a polygon representing a specific area. The `ID` property can be used to add additional information to the map.", "cdfa7439-6a17-45dd-abd5-f15ffc36fd72": "The \"forecast\" command in {% data variables.product.prodname_actions_importer %} is used to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in CircleCI. It can be run by executing the command `gh actions-importer forecast circle-ci --output-dir tmp/forecast_reports` in the terminal. The forecast report, which is saved in the specified output directory, contains terms such as job count, pipeline count, and execution time. These terms describe the total number of completed jobs, the number of unique pipelines used, and the amount of time required for job execution, respectively.", "1462a995-64cd-4ed2-a83a-617894d417cf": "To cancel pending changes to your personal account's plan or other subscriptions and usage-based billing on GitHub, follow these steps:  1. Go to your account settings by clicking on your profile picture in the top right corner of the GitHub website and selecting \"Settings\" from the drop-down menu. 2. Click on the \"Billing\" tab in the left-hand menu. 3. If you have any pending changes, you will see a section titled \"Pending changes\" with a list of the changes you have made but have not yet applied. To cancel a pending change, click on the \"Cancel\" button next to the change you want to undo. 4. A confirmation dialog will appear. Click \"Confirm\" to cancel the change.  If you have any pending changes for your organization's plan, the process is similar. Go to your organization's settings by clicking on the organization name in the top left corner of the GitHub website and selecting \"Settings\" from the drop-down menu. Then, click on the \"Billing\" tab and follow steps 3 and 4 above.  Note that if you cancel a pending change, the change will not take effect until your next billing date.", "ab495d3b-c0ef-4df6-9244-7f13d2f2c6aa": "GitHub provides built-in workflows to add automation to a project. These workflows allow you to automatically set fields when items are added or changed, and you can also configure your project to automatically archive items when they meet certain criteria or automatically add items from a repository when they match set criteria. The criteria for automatically archiving items can include factors such as the age of the item, the number of comments it has received, or whether it has been closed for a certain period of time. The criteria for automatically adding items from a repository can include factors such as the presence of a specific label or the inclusion of a specific keyword in the commit message. For more information on how to set up these workflows, see the documentation on \"AUTOTITLE\" and \"AUTOTITLE.\"", "c863b71f-93e8-4292-92c2-b14380c8f6f3": "To manage user access to repositories in your enterprise using the REST API endpoints provided, you can leverage the group management endpoints. By creating and managing groups on your identity provider (IdP), you can control organization and team membership for users, which in turn grants or denies access to repositories in your enterprise. To get started, make requests to the REST API endpoints for group management, such as creating, retrieving, updating, listing, and deleting groups. For more information on supported SCIM user and group attributes, refer to the provided resources.", "833fcb1a-df00-46c7-a7b0-a7db89ceef71": "Section 6 of the agreement between GitHub and the customer outlines the following terms and conditions:  1. The agreement can only be modified through a written amendment signed by an authorized representative of GitHub or through GitHub posting a revised version in accordance with Section R. 2. This agreement represents the complete and exclusive agreement between the parties, superseding any prior agreements or communications related to the subject matter of these terms. 3. In the event of a conflict between the terms of this agreement and any order form or SOW, the terms of the order form or SOW will control with respect to that order form or SOW only.  In summary, this section outlines the process for modifying the agreement, clarifies that this agreement is the complete and exclusive agreement between the parties, and establishes an order of precedence in the event of a conflict between the terms of this agreement and any order form or SOW.", "28003aed-7dde-4a81-ac25-f6845bb22e3b": "The \"Coverage\" view in security overview helps assess the adoption of code security features across repositories in an organization by providing a summary of the percentage of repositories that have enabled specific security features, such as {% data variables.product.prodname_code_scanning %} and {% data variables.secret-scanning.alerts %}. This view allows organizations to identify any gaps in their security posture and prioritize efforts to improve adoption of these features across their repositories.", "25b632f4-cfe4-408d-9bc4-e7f531c3f183": "The GitHub Copilot Pre-release software is a preview, evaluation, demonstration, or trial version of the software that may have bugs and may not operate correctly, potentially deleting or corrupting data. It may not work the same way as the final commercial version of the software, and GitHub may change or discontinue it at any time without notice. The software is provided for non-production use only, and GitHub is not obligated to provide maintenance, technical support, or updates. Third-party components and optional extensions may be included, subject to separate legal notices or agreements.", "4a6101df-28d3-47e1-958d-ed5eeead5611": "Authenticated site administrators can access endpoints that are not available to normal users by providing authentication credentials, such as a personal access token (PAT) with the `admin` scope, when making requests to the GitHub API. This is mentioned in the context information provided. Without this authentication, normal users will receive a `404` response.", "04f6e4f8-a677-4641-8fbe-f80f40d43211": "To import the Git & {% data variables.product.product_name %} starter assignment into your organization using {% data variables.product.prodname_classroom %}, click **Import the assignment** when setting up the basics for an assignment. This will import the starter course into your organization and allow you to customize it for your specific needs.", "97adae44-636d-4e7e-9174-c46d1fff0992": "The workflow needs `permissions:` settings with the `id-token` value set to `write`. This lets you fetch the OIDC token from every job in the workflow. (As mentioned in the context information.)", "51eb2422-ad87-4fb2-a91d-e9118e76fb5d": "To convert a draft task into an issue in GitHub, follow these steps:  1. Next to the draft task you want to convert, click the tracking block item menu (represented by three dots) to the right of the task. 2. In the menu, click \"Convert to issue\". 3. The draft task will be converted into a new issue in the same repository.  Alternatively, you can also convert a draft task into an issue by selecting the draft task and clicking the \"Convert to issue\" button in the top-right corner of the tasklist.  Once the draft task has been converted into an issue, you can then assign yourself to the issue, set labels, and add it to a project, all from the context menu next to the issue in the tasklist.", "df2c0784-230f-441b-b16b-07082eccf1ff": "The \"on\" field in the configuration file specifies the events that trigger the workflow to run. In this case, the workflow will run when a new release is published to the repository. The \"types\" field within the \"on\" block specifies that the workflow should only run when a release is published, and not when other types of events occur, such as a push or a pull request. This ensures that the workflow is only triggered when a new version of the package is released, and not unnecessarily for other events.", "2e12a2d6-0cc1-4f9f-9fc5-16eb431123c6": "The content on the {% data variables.product.prodname_docs %} homepage is organized around {% data variables.product.prodname_dotcom %} concepts and practices, with all top-level doc sets and some categories included. The factors that determine whether a new doc set is added to the homepage include whether it is a new top-level doc set or whether the category serves as the starting point for using a {% data variables.product.prodname_dotcom %} product or feature. The goal is to help people find information about the {% data variables.product.prodname_dotcom %} feature or product that they want to learn about, and every item added to the homepage dilutes the discoverability of every other item, so the number of doc sets included on the homepage is limited.", "7c77686c-0616-4da8-bfeb-000cf4bdf9fd": "While renaming a branch on {% data variables.product.prodname_dotcom %} automatically redirects URLs and updates branch protection policies, there are still some limitations to be aware of. Firstly, raw file URLs are not redirected, so if you share a link to a specific file on the old branch, it will still point to the old location. Secondly, if you perform a `git pull` for the previous branch name, {% data variables.product.prodname_dotcom %} does not perform any redirects. To address these limitations, it's recommended to communicate the branch name change to your collaborators and suggest that they update their local Git environments. Additionally, you may want to consider adding a new branch with the original content plus an additional commit reporting that the branch name is deprecated and suggesting that users migrate to the new branch name, especially if your repository publishes an action. This can help ensure that {% data variables.product.prodname_actions %} workflows continue to function properly after the branch name change.", "62525063-2d4d-4130-9275-e6fa7ffb8df2": "Section 3 of the agreement outlines the restrictions and responsibilities that the developer must adhere to while using the Marketplace and participating in the Program. These restrictions and responsibilities are explicitly conditioned on the developer's compliance with this section of the agreement. However, the specific restrictions and responsibilities are not explicitly stated in the context information provided. Therefore, to answer this question, we would need to refer to Section 3 of the agreement itself to summarize its contents.", "4ecfeaeb-f2f3-4157-a825-e179f58108a2": "If you own the original repository on GitHub or have write permissions, you can sync changes between a local repository and the original repository by pushing your local changes to the remote repository. Here's how:  1. Make sure you have the latest changes from the remote repository by pulling from the original repository:     ```    git pull origin main    ```  2. Make your changes to the local repository.  3. Stage your changes:     ```    git add .    ```  4. Commit your changes:     ```    git commit -m \"Your commit message\"    ```  5. Push your changes to the remote repository:     ```    git push origin main    ```  6. Verify that your changes have been pushed to the remote repository by checking the GitHub web interface or running `git pull` to update your local repository with the latest changes.  If you don't own the original repository or have write permissions, you can't sync changes directly. In this case, you can create a pull request to propose your changes to the original repository's maintainers. They can then review and merge your changes into the original repository.", "5f94e647-a4c6-47e9-b2d2-bff7e2e1a61a": "Users with write permission can click the \"Show more details\" link shown in the annotation to view more information about an alert in the context of {% data variables.product.prodname_code_scanning %} analysis. This will display all of the context and metadata provided by the tool in an alert view, including tags showing the severity, type, and relevant common weakness enumerations (CWEs) for the problem, as well as which commit introduced the problem. Some {% data variables.product.prodname_code_scanning %} tools, like {% data variables.product.prodname_codeql %} analysis, also include a description of the problem and a \"Show more\" link for guidance on how to fix the code.", "82d86d97-cc4e-4c53-b6d3-b8a320b9e045": "The \"Permitted Purposes\" for data processing, as outlined in the context information, are those limited and specific purposes of providing the Online Service as set forth in the Agreement and this DPA, or the purposes for which a Data Subject has authorized the use of Customer Personal Data.", "66629434-3f2b-483c-8b01-0b14725ef223": "To free up storage space in a {% data variables.product.prodname_github_codespaces %} container, you can try running the following commands in the {% data variables.product.prodname_github_codespaces %} terminal:  - Remove packages that are no longer used by using `sudo apt autoremove`. - Clean the apt cache by using `sudo apt clean`. - See the top 10 largest files in the codespace with `sudo find / -printf '%s %p\\n'| sort -nr | head -10`. - Delete unneeded files, such as build artifacts and logs.  Some more destructive options:  - Remove unused Docker images, networks, and containers by using `docker system prune` (append `-a` if you want to remove all images, and `--volumes` if you want to remove all volumes). - Remove untracked files by using `git clean -df`.  Note that some of these commands may be destructive, so be careful when running them. It's always a good idea to back up any important data before running these commands.", "43256289-57fe-4e75-937c-1f4446f79794": "The {% data variables.product.prodname_classroom %} integration pack for {% data variables.product.prodname_vscode_shortname %} includes three extensions: the {% data variables.product.prodname_classroom %} extension, the Visual Studio Live Share extension, and the {% data variables.product.prodname_dotcom %} Pull Request extension. These extensions enhance the learning experience for students by providing custom abstractions for easy navigation and getting started, integrating teaching assistants and classmates for help and collaboration through Live Share, and allowing students to see feedback from their instructors within the editor through Pull Requests. Overall, these extensions make it easier for students to browse, edit, submit, collaborate, and test their Classroom Assignments using their preferred editor, {% data variables.product.prodname_vscode_shortname %}.", "e208452e-9936-4ecc-a7aa-57b451fe0aed": "To track where your sponsorships are coming from, you can use custom URLs with metadata for your {% data variables.product.prodname_sponsors %} profile or checkout page. The metadata should follow the format of `key=value` and must be prefixed with `metadata_`, such as `metadata_campaign`. The metadata will be included in your transaction export in the metadata column. The syntax requirements for metadata are that keys must only contain alphanumeric values, dashes, or underscores, and must be prefixed with `metadata_`. Keys and values must not contain whitespaces, and a maximum of 25 characters per key and 100 characters per value are allowed per request. A maximum of 10 key-value pairs are accepted per request. If more than that are passed, only the first 10 will be saved. If non-accepted characters are passed in either keys or values, a 404 error will be presented.", "d1b24d8c-e042-4bda-b002-c8afa7249512": "The dependency graph, {% data variables.product.prodname_dependabot %}, and the {% data variables.product.prodname_advisory_database %} all play a role in identifying dependencies and vulnerabilities, but they do so in different ways.  The dependency graph parses all known package manifest files in a user's repository and constructs a graph of all of the repository's dependencies and public dependents. It identifies dependencies based on the package manifest files in the repository.  {% data variables.product.prodname_dependabot %} scans any push, to the default branch, that contains a manifest file and generates an alert for each repository that is affected by a new advisory. It identifies vulnerabilities based on the advisories in the {% data variables.product.prodname_advisory_database %}.  The {% data variables.product.prodname_advisory_database %} is a curated database of security advisories for common package ecosystems on {% data variables.product.prodname_dotcom %}. It includes both data reported directly to {% data variables.product.prodname_dotcom %} from {% data variables.product.prodname_security_advisories %}, as well as official feeds and community sources. It identifies vulnerabilities based on the advisories in the database.  In summary, the dependency graph identifies dependencies based on the package manifest files in the repository, {% data variables.product.prodname_dependabot %} identifies vulnerabilities based on the advisories in the {% data variables.product.prodname_advisory_database %}, and the {% data variables.product.prodname_advisory_database %} identifies vulnerabilities based on the advisories in the database.", "2648147f-39ff-415e-8a70-b39df874034c": "To enable SAML SSO and SCIM for {% data variables.product.product_name %} using Azure AD, you need to follow the steps outlined in the Microsoft documentation for configuring SAML SSO and SCIM for custom applications. This will allow you to centrally manage user accounts and access to web applications through Azure AD, simplifying the process of managing identity and access for your enterprise on {% data variables.location.product_location %}.  To assign the {% data variables.product.product_name %} application to a user account on Azure AD, you can follow these steps:  1. Log in to your Azure AD tenant and navigate to the Enterprise applications section. 2. Select the {% data variables.product.product_name %} application. 3. In the left-hand menu, select Users and groups. 4. Select the Add user or group button. 5. In the Add assignment dialog box, select Users and groups, then search for and select the user account you want to assign. 6. Select the Assign button to grant the user access to the {% data variables.product.product_name %} application.  To assign the {% data variables.product.product_name %} application to an IdP group on Azure AD, you can follow these steps:  1. Log in to your Azure AD tenant and navigate to the Enterprise applications section. 2. Select the {% data variables.product.product_name %} application. 3. In the left-hand menu, select Groups. 4. Select the Add group button. 5. In the Add assignment dialog box, select Groups, then search for and select the IdP group you want to assign. 6. Select the Assign button to grant access to the {% data variables.product.product_name %} application for all members of the IdP group.  To unassign the {% data variables.product.product_name %} application from a user account or IdP group on Azure AD, you can follow these steps:  1. Log in to your Azure AD tenant and navigate to the Enterprise applications section. 2. Select the {% data variables.product.product_name %} application. 3. In the left-hand menu, select Users and groups or G", "e6c862f5-06f9-41c7-acad-31bd383d190f": "According to the context information provided, for expenses exceeding $150, approval from Legal in writing is required each time. This process should be followed to ensure compliance with the company's policies and avoid any potential violations of anti-bribery laws. It's always better to err on the side of caution and seek approval before incurring any expenses that could be perceived as bribes or gifts of excessive value.", "5387975f-7f5f-4cd5-84da-0441a28c51bd": "The purpose of the GitHub Actions Importer labs repository is to provide platform-specific learning paths to teach users how to use the {% data variables.product.prodname_actions_importer %} and how to approach migrations to GitHub Actions. This repository can be used to learn how to use the {% data variables.product.prodname_actions_importer %} to help plan, forecast, and automate migrations to GitHub Actions. By following the learning paths provided in this repository, users can gain a better understanding of how to migrate their CI/CD workflows from other platforms to GitHub Actions using the {% data variables.product.prodname_actions_importer %}.", "8cf28514-a56a-4889-ad0b-279b646c6611": "{% data variables.product.prodname_actions_importer %} can be used to limit which actions are allowed in converted workflows by using optional parameters such as `--allowed-actions`, `--allow-verified-actions`, and `--allow-github-created-actions`. The `--allowed-actions` option allows specifying a list of actions to allow in converted workflows, while `--allow-verified-actions` and `--allow-github-created-actions` options allow specifying that all actions from verified creators or actions published from the `github` or `actions` organizations are allowed, respectively. These options can be used in combination to expand the list of allowed actions, and if none of these options are supplied, then all actions are allowed.", "d7508b9d-855d-4f0d-ac5b-1941511e22cc": "The table provided in the context information lists the supported providers and tokens for push protection and validity checks in Secret Scanning. The table is labeled \"High confidence patterns\" and includes a column for push protection and another column for validity checks. The table shows whether push protection and validity checks are supported for each provider and token by displaying a checkmark icon (octicon \"check\") or an X icon (octicon \"x\"). Based on this table, the supported providers and tokens for push protection and validity checks in Secret Scanning are listed in the table. However, it's important to note that the table only lists high confidence patterns, and there may be additional providers and tokens that are supported for push protection and validity checks that are not listed in this table. For more information about validity check support, it's recommended to refer to the {% data variables.product.prodname_ghe_cloud %} documentation.", "a46bf655-6ef2-4da5-a467-07780100826e": "The purpose of using the `template` query parameter to specify a pull request template is to allow users to select a specific template when creating a pull request. This can be useful in large repositories with multiple types of pull requests, as it allows users to quickly and easily select the appropriate template for their proposed changes.  To store multiple pull request templates in a repository, you can create a hidden directory called `.github` and create a subdirectory called `PULL_REQUEST_TEMPLATE`. Within this subdirectory, you can create multiple `.md` files, each containing a different pull request template. Alternatively, you can store the templates in the repository's root directory or `docs/` directory, using a naming convention that includes the template name (e.g., `pull_request_template.md`). By using this naming convention, the templates will be automatically listed in the pull request creation dialog.  Overall, using the `template` query parameter and storing multiple templates in a repository can help improve the consistency and quality of pull requests, as it provides users with a clear and concise starting point for their proposed changes.", "fd99c33e-57be-4e44-aa92-aa858e2a6f50": "Merge box labels in IBE should be formatted in bold and with a descriptive label added to the bottom of the pull request's Conversation tab, as instructed in the context information provided.", "10406ec8-413d-4c96-b441-7f22b1f50a53": "When a public repository is deleted, all of its private forks are also deleted. However, the context doesn't provide information about the public forks. Therefore, we don't know what happens to them when the repository is deleted.  If the context were to provide information about what happens to the public forks when the repository is deleted, then we could answer the question accurately.  If the context were to say that \"When you delete a public repository, all of its forks are also deleted,\" then we could say that all of the repository's forks, including public forks, are deleted when the repository is deleted.  If the context were to say that \"When you delete a public repository, its public forks are moved to a separate network,\" then we could say that the public forks continue to exist in a separate network when the repository is deleted.  If the context were to say that \"When you delete a public repository, its public forks are deleted,\" then we could say that the public forks are deleted when the repository is deleted.  Without the context, we cannot accurately answer the question.", "abe2e09c-8872-4770-99c0-c7a6098b8494": "To retrieve the contents of a file as a secret in your GitHub repository, you can follow these steps:  1. Go to your repository in GitHub. 2. Click on \"Settings\" in the top right corner of the page. 3. Click on \"Secrets\" in the left-hand menu. 4. Click on \"New repository secret\" to create a new secret. 5. Enter a name for the secret and paste the contents of the file you want to store as a secret. 6. Select \"Encrypted\" as the secret type (optional). 7. Click on \"Add secret\" to save the secret.  Alternatively, you can use the GitHub CLI to create a secret:  1. Install the GitHub CLI by following the instructions on the GitHub CLI website. 2. Log in to your GitHub account by running `gh auth login` in your terminal. 3. Navigate to your repository by running `cd path/to/repository`. 4. Run `gh secret set NAME --file=path/to/file` to create a new secret with the contents of the specified file.  Replace \"NAME\" with a name for the secret and \"path/to/file\" with the path to the file you want to store as a secret.  Note: If you want to store sensitive information as a secret, you should encrypt it using the GitHub CLI or the GitHub website to ensure its security.", "a55b8049-a38a-4106-9c27-8badf6a99072": "In this context, the name of the branch that is being merged into the base branch is not explicitly provided. The variables `$TRAVIS_PULL_REQUEST_BRANCH` and `$TRAVIS_PULL_REQUEST` are provided, but they refer to the branch where the pull request was created and the number of the pull request, respectively. To determine the name of the branch being merged, additional context or information would be needed. If this information is not available, it cannot be determined from the provided context.", "459558a2-b1a7-4eff-9730-4796530b36cd": "The {% data variables.product.prodname_docs %} approach to style prioritizes simplicity, user needs, and flexibility over traditional grammar and style guide rules. In scenarios where consistency and grammatical correctness may hinder clarity and meaning, the approach allows for flexibility in decision-making. For example, in a help article, if a user-friendly term is more appropriate than a technically correct term, the approach allows for the use of the former, even if it goes against grammatical rules.  The guidelines for documenting audit log events in the {% data variables.product.prodname_docs %} involve documenting each event that may appear in the audit logs for each type of account: user, organization, and enterprise. The description for an event should be written in past tense and passive voice, and it should describe the event that took place in a way that applies to all versions and contexts. For example, \"The visibility of a repository was changed\" is a correctly written description for an audit log event.  Example:  Event: Repository visibility changed Description: The visibility of a repository was changed from public to private. This action was performed by [user name] on [date and time]. The repository affected by this event is [repository name]. The new visibility setting is [private].  In this example, the description accurately reflects the event that took place, using past tense and passive voice, and it provides enough context for the user to understand the nature of the event. The description also includes the user who performed the action, the date and time, the name of the repository, and the new visibility setting. This level of detail helps users to quickly identify and understand the event in the context of their own usage of the platform.", "65bb8bb5-65d2-41de-b965-deba8db529b0": "The `values` parameter in the second `query` function is used to pass data to the query as placeholders. In this script, the `values` parameter is an array containing the values that will replace the placeholders in the SQL query. The placeholders are represented by question marks (`?`) in the SQL query, and their corresponding values are provided in the `values` array. This allows for dynamic SQL queries with variable data, without the need to concatenate strings or use string interpolation to build the query. By using placeholders and the `values` parameter, the script is able to securely and efficiently pass data to the query without the risk of SQL injection attacks.", "5e32ddbd-c40b-419b-a03f-adfde41fd3e5": "In this case, the context information provided is not enough to give an example of how context information is used in the given text material. The text only mentions \"context information\" and provides a brief explanation of what it means. It then refers to a specific command (\"AUTOTITLE.\") without explaining what it does or how it relates to context information. Without further context, it's unclear how context information would be used in relation to this command or in the text as a whole.", "daf85c19-0631-4d0c-bf38-32535d74199c": "The `alex-page/github-project-automation-plus` action can be used to automatically move issues to a specific project board column when they are assigned. To use this action, follow these steps:  1. Create a workflow file in your repository and add the following YAML configuration:     ```yaml    name: Move Issues to Project Board Column    on:      issues:        types:          assigned:     jobs:      move-issue:        runs-on: ubuntu-latest        steps:          - name: Checkout            uses: actions/checkout@v2           - name: Set up personal access token            uses: actions/setup-personal-access-token@v1            with:              token-name: PERSONAL_ACCESS_TOKEN              token-value: ${{ secrets.PERSONAL_ACCESS_TOKEN }}           - name: Move Issue            uses: alex-page/github-project-automation-plus@v1            with:              project-name: my-project              column-name: To Do    ```     Replace `my-project` and `To Do` with the name of your project and the name of the column you want to move issues to, respectively.  2. Store your personal access token as a secret in your repository.  3. Commit your workflow file.  4. Test the workflow by assigning an issue in your repository. The issue should be moved to the specified project board column.  In user-owned repositories, the `alex-page/github-project-automation-plus` action will act on all projects in your repository or personal account that have the specified project name and column.  In organization-owned repositories, the action will act on all projects in your repository or organization that have the specified project name and column.  Note that the `alex-page/github-project-automation-plus` action is a third-party action, and its usage and functionality may change over time. It's recommended to review the action's documentation for the latest information and requirements.", "94d7a162-9ee2-456a-8349-e0d0a0983437": "To modify your workflows to use a two-step process that includes `pull_request_target` instead of being triggered by {% data variables.product.prodname_dependabot %} on {% data variables.location.product_location %}, you can follow these steps:  1. Create a new branch for the changes you want to make.  2. Create a pull request from the new branch to the main branch.  3. Wait for the pull request to be merged.  4. Create a new workflow that is triggered by the `pull_request_target` event.  5. In the new workflow, add a job that runs after the merge has completed.  6. In the job, add the necessary steps to make the desired changes.  7. Save and activate the new workflow.  By using this two-step process, you can ensure that your workflows are not affected by the limitations imposed on workflows triggered by {% data variables.product.prodname_dependabot %} on {% data variables.location.product_location %}.", "5bcc8741-25d8-4af1-9d7b-7daf29d702fd": "In version `2.15.2` of the software, a teacher/professor can create a new file if it doesn't already exist by following these steps:  1. Open the software and navigate to the location where you want to create the new file. 2. Click on the \"New File\" or \"New Document\" option in the software's menu or toolbar. 3. Choose the type of file you want to create (e.g., text document, spreadsheet, presentation) from the options provided. 4. Give the new file a name and save it in the desired location. 5. Begin working on the new file.  If you're not using version `2.15.2` of the software, the steps to create a new file may be slightly different, but the basic process should be similar. It's always a good idea to consult the software's documentation or help resources for specific instructions on how to create a new file in your particular version.", "1bad3470-7bb7-4725-b5b9-d69281aba1b8": "Any {% data variables.product.prodname_docs %} content type, such as articles, guides, and reference content, can be used to create troubleshooting sections. It's recommended to keep troubleshooting content contained within procedural content or guides whenever possible, but standalone articles can also be created when necessary.", "bf49fa1d-347b-4459-a84d-a5b10c367a9c": "According to the context information, there is potential for the summary of a pull request to include harmful or offensive terms if those terms are present in the content of the pull request. This could lead to unwanted exposure to such content, which could be detrimental to users who are sensitive to such terms. However, the context doesn't provide any specific examples of such terms or the severity of the potential harm.  3. What is the recommended feature mentioned in the context information?  Answer:  The context information doesn't provide enough details to identify the recommended feature. It only mentions that the team recommended the feature during the beta, but it doesn't specify what the feature is or how it differs from the current version of the software.  4. Is there any further reading provided in the context information to learn more about the recommended feature or the potential harm mentioned?  Answer:  Yes, the context information provides two sources for further reading. The first is the {% data variables.product.prodname_copilot %} Trust Center, which likely provides information about the recommended feature and its benefits and limitations. The second source is the \"AUTOTITLE\" documentation for {% data variables.product.prodname_ghe_cloud %}, which could potentially provide more details about the potential harm mentioned in the context information. However, without further context, it's unclear how these sources are related to the recommended feature or the potential harm.", "1c9d3f40-33ae-49de-b92f-dfe6b2d124f2": "{% data variables.large_files.product_name_short %} is a feature that allows for the storage of large files in a Git repository. It can be used in an individual repository by enabling it through the REST API, which is available to people with admin access to the repository. The usage of this feature is subject to billing for certain plans, such as GitHub Enterprise Cloud and GitHub Team. If the repository belongs to an organization, additional access requirements may apply, depending on the organization's plan and ownership structure.", "7ddb0eb3-6749-4d40-903a-f0214eca228b": "To modify the code of the github/github-app-js-sample repository to respond to different webhook events or make different API requests, you can follow these steps:  1. Fork the github/github-app-js-sample repository to create a copy of the repository in your GitHub account.  2. Clone the forked repository to your local machine or codespace.  3. Navigate to the cloned repository in your terminal.  4. Create a new file called `.env` in the root directory of the repository.  5. Add the following lines to the `.env` file, replacing the placeholders with your actual values:  ``` GITHUB_APP_ID=<your app ID> GITHUB_APP_PRIVATE_KEY=<your app private key> GITHUB_APP_WEBHOOK_SECRET=<your app webhook secret> ```  6. Save the `.env` file and commit the changes to your local repository.  7. Push the changes to your forked repository on GitHub.  8. In the GitHub repository settings, go to the \"Webhooks\" tab and click \"Add webhook\".  9. Enter the URL of your local server or codespace where you're running the app.  10. Select the \"Let me select individual events\" option and choose the webhook events that you want to receive.  11. Enter the webhook secret that you added to the `.env` file.  12. Save the webhook settings.  13. In the `index.js` file, find the `handleEvent` function and modify it to handle the specific webhook event that you want to respond to.  14. Add any necessary API requests or logic to the modified `handleEvent` function.  15. Save the changes to the `index.js` file and commit the changes to your local repository.  16. Push the changes to your forked repository on GitHub.  17. Test the modified app by creating a new repository, adding a collaborator, or pushing a new commit to the repository that's associated with your app.  18. Verify that the modified", "d27f8453-a785-4904-9cb7-f46a1f235bb8": "Variables in workflows on {% data variables.product.prodname_dotcom %} can be used to store and reuse non-sensitive configuration information such as compiler flags, usernames, or server names. They can be set in two ways: by defining an environment variable for use in a single workflow using the `env` key in the workflow file, or by defining a configuration variable across multiple workflows at the organization, repository, or environment level. Variables are interpolated on the runner machine that runs the workflow, and commands that run in actions or workflow steps can create, read, and modify variables. By default, variables render unmasked in build outputs, but for greater security for sensitive information, secrets should be used instead.  {% endif %}    Defining configuration variables{% ifversion actions-configuration-variables %} for multiple workflows{  Given the context information and not prior knowledge, answer the following question.   How can variables be defined across multiple workflows on {% data variables.product.prodname_dotcom %}?  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  Variables can be defined across multiple workflows on {% data variables.product.prodname_dotcom %} at the organization, repository, or environment level. This allows for the reuse of non-sensitive configuration information such as compiler flags, usernames, or server names across multiple workflows. These variables are interpolated on the runner machine that runs the workflow, and commands that run in actions or workflow steps can create, read, and modify variables. By default, variables render unmasked in build outputs, but for greater security for sensitive information, secrets should be used instead.  {% endif %}    Default environment variables{% ifversion actions-configuration-variables %}  Given the context information and not prior knowledge, answer the following question.   What are the default environment variables set by {% data variables.product.prodname_dotcom %}?  If the context doesn't give enough information to give an answer,  then say that you weren't given", "01fa84e1-b5f4-455b-aad7-8e47f6650dfa": "To generate an SSH public/private keypair and add the public key to your account on {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %}, follow these steps:  1. Open a terminal or command prompt on your local machine. 2. Run the following command to generate a new SSH public/private keypair:     ```    ssh-keygen -t ed25519 -C \"your_email@example.com\"    ```     Replace `your_email@example.com` with your email address associated with your {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %} account.  3. Follow the prompts to choose a location to save the key pair and set a passphrase (optional).  4. Copy the contents of the newly generated public key (usually located in `~/.ssh/id_ed25519.pub`) to your clipboard.  5. Log in to your {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %} account and navigate to your account settings.  6. Click on \"SSH and GPG keys\" in the left-hand menu.  7. Click the \"New SSH key\" button.  8. Paste the contents of your public key into the \"Key\" field.  9. Give your key a descriptive name and click \"Add SSH key\".  After completing these steps, you can authenticate with {% data variables.product.product_name %} using your SSH key instead of a {% data variables.product.pat_generic %}. This is done by adding the following line to your Git configuration:  ``` [user] email = your_email@example.com name = Your Name ```  Replace `your_email@example.com` and `Your Name` with your email address and name, respectively", "17292473-7833-4c87-99a8-d62403e45ca1": "{% data variables.product.prodname_desktop %} is supported on {% data variables.desktop.mac-osx-versions %} and {% data variables.desktop.windows-versions %}. A 64-bit operating system is required to run {% data variables.product.prodname_desktop %}.", "2354bc37-1703-4ad1-95c6-20a3e9ff01dc": "To authenticate with Azure Blob Storage as your blob storage provider while migrating from {% data variables.product.prodname_ghe_server %} 3.7 or earlier using the gh gei migrate-repo command, you need to use the `--azure-storage-connection-string` flag and provide the value of the `AZURE_STORAGE_CONNECTION_STRING` environment variable, which contains the connection string for your Azure Blob Storage account. This environment variable should be set before running the migration command.", "e0e39f4f-fe4f-4717-b159-e0c2d77f7ae4": "To configure the default `GITHUB_TOKEN` permissions for your organization, follow these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. In the left-hand menu, click on \"Actions\" and then \"Default workflow permissions\". 3. Select the desired permissions for the `GITHUB_TOKEN` by checking the appropriate scopes. 4. Click \"Save\" to apply the changes.  The impact of configuring the default `GITHUB_TOKEN` permissions for your organization is that all new repositories created within the organization will inherit these permissions. This can save time and ensure consistency across your organization's workflows. However, it's important to carefully consider the necessary permissions for your organization's use cases and balance the need for security with the need for functionality.", "39556c5f-b4f7-430a-8b4b-b138c305265f": "In a codespace, the `/tmp` directory is an exception to the rule that directories are not mounted into the container. However, the contents of this directory are not persistent and are cleared each time the codespace stops, such as when a session times out due to inactivity. Therefore, temporary files should not be saved in the `/tmp` directory if they need to be available the next time the codespace is started, as they will be deleted when the codespace stops. It's recommended to save temporary files in a persistent directory to ensure their availability.", "8691dd02-c7c6-4906-971f-68e7d06509c5": "To clone the repositories of all students for a specific assignment using the gh classroom command, you can use the `gh classroom clone student-repos` command. This will clone the student repositories from the given assignment into a directory named after the assignment slug in the current directory by default. If you want to clone into a different directory, you can use the `--directory` flag. The results are paginated by 30 by default, but you can change the number of repositories using the `--per-page NUMBER` flag.", "9705c932-2718-4664-a555-cbab77f9347c": "To enable unified contributions for your enterprise on {% data variables.location.product_location %}, you need to follow these steps:  1. Ensure that {% data variables.product.prodname_github_connect %} is enabled for your enterprise. 2. Log in to {% data variables.location.product_location %} and {% data variables.product.prodname_dotcom_the_website %}. 3. Access the settings for your enterprise on {% data variables.location.product_location %}. 4. Navigate to the Business section. 5. Click on the {% data variables.enterprise.prodname_github_connect %} tab. 6. Enable {% data variables.enterprise.prodname_unified_contributions %}. 7. Inform your end users that they need to connect their user account on {% data variables.product.product_name %} with their personal account on {% data variables.product.prodname_dotcom_the_website %} to send contribution counts from {% data variables.product.product_name %} to their {% data variables.product.prodname_dotcom_the_website %} contribution graph. 8. Set the sync frequency for contribution counts.  Note that if you disable {% data variables.enterprise.prodname_unified_contributions %} or individual users opt out of the connection, the contribution counts from {% data variables.product.product_name %} will be deleted on {% data variables.product.prodname_dotcom_the_website %}. If a user reconnects their profiles after disabling them, the contribution counts for the past 90 days are restored. {% data variables.product.product_name %} only sends the contribution count and source ({% data variables.product.product_name %}) for connected users, and it does not send any information about the contribution or how it was made.", "acdf098e-0043-402c-97ee-1b08c3d3fb02": "The value of the variable `CI_JOB_ID` in the current context can be obtained by using the following expression: `{% raw %}${{ github.job }}`{% endraw %}. This expression will replace the variable with its actual value when the script is executed, providing the unique identifier for the current job being executed in the GitHub Actions workflow.", "5d164a8b-f51c-49d0-8f2f-cee42aa235f0": "The \"License\" drop-down menu in {% data variables.product.prodname_desktop %} allows users to easily add an open-source license to a LICENSE file in their repository. By selecting an available open-source license from the drop-down menu, the license text is automatically added to a new or existing LICENSE file in the repository. This feature simplifies the process of adding a license to a project, making it easier for other developers to understand the terms of use for the code. For more information about available open-source licenses and how to add them to a repository, users can refer to the resources provided in the context information.", "46a7b513-90e7-4667-88d9-85e66193cab7": "To open a codespace in JupyterLab, you can use the following command in your terminal:  ```shell gh codespace jupyter -c CODESPACE-NAME ```  However, before you can use this command, you need to ensure that JupyterLab is installed in your codespace. This can be done by adding the following configuration to your repository's `devcontainer.json` file:  ```json {   \"name\": \"My Container\",   \"image\": \"microsoft/devcontainers:python-3.9-jupyter\",   \"dockerfile\": \"Dockerfile\",   \"features\": {     \"jupyter\": true   } } ```  This configuration will create a new container image based on the `microsoft/devcontainers:python-3.9-jupyter` image, which includes JupyterLab. Once you have committed and pushed this configuration to your repository, you can create a new codespace and use the `gh codespace jupyter` command to open it in JupyterLab.", "94a55337-370a-4fac-8ba6-0ca4e666fb1b": "To invite collaborators to a project board that you own on GitHub, follow these steps:  1. Navigate to the project board where you want to add collaborators. 2. Click on the project board's settings icon (three dots) in the upper-right corner of the page. 3. Select \"Collaborators\" from the drop-down menu. 4. Click the \"Add people\" button. 5. Type the collaborator's name, username, or GitHub email address in the \"Search by username, full name, or email address\" field. 6. Select the collaborator from the search results. 7. Choose the desired permission level for the collaborator from the drop-down menu next to their name. 8. Click the \"Add collaborator\" button to invite the collaborator to the project board.  Alternatively, you can also invite collaborators by clicking the \"Invite members\" button on the project board's main page and following the same steps.", "3d771c01-4f1f-4fc6-bac7-3afe29e74e84": "To download a specific version of a query pack using the {% data variables.product.prodname_codeql %} CLI, you can specify the version to download and unzip as part of the `codeql pack download` command. This can be done by including the version number after the pack name, separated by a colon. For example, to download version 1.0.0 of the `my-pack` query pack, you can run the following command:  ``` codeql pack download my-pack:1.0.0 ```  This will download and unzip the specified version of the query pack to the default location (`~/.codeql/packs` on Unix-based systems and `%APPDATA%\\CodeQL\\packs` on Windows). If you want to specify a different location, you can use the `--pack-dir` flag to specify a custom directory.  Note that if you don't specify a version, the latest version of the pack will be downloaded by default.  Additionally, if you need to authenticate with {% data variables.product.company_short %}'s REST API to download the pack, you can use the `--github-auth-stdin` flag to pass your {% data variables.product.prodname_github_app %} or {% data variables.product.pat_generic %} credentials via standard input. This is not necessary if you have set the `GITHUB_TOKEN` environment variable with the appropriate token.  For more information about query pack compatibility and versioning, see the {% data variables.product.prodname_codeql %} documentation.", "c92f7ab0-de6a-4a93-ba1e-d0dfacea899a": "An organization owner can enable or disable {% data variables.product.prodname_copilot_cli_short %} for their organization by accessing the organization settings, navigating to the \"Code, planning, and automation\" section, clicking on \"{% octicon \"copilot\" aria-hidden=\"true\" %} {% data variables.product.prodname_copilot_short %}\", and then selecting the dropdown menu next to \"{% data variables.product.prodname_copilot_cli %}\" to choose between \"Enabled\" or \"Disabled\". This setting can be configured at the organization level, but may be overridden by an enterprise owner who has set a policy at the enterprise level (for GitHub Enterprise Cloud).", "f1587c67-ede4-471e-a9ca-d64aa64609a9": "The purpose of the GitHub Registered Developer Agreement is to establish a special relationship between GitHub and participating developers, which requires a set of terms that govern their participation in the GitHub Developer Program. These terms are necessary to connect developers with GitHub, share information with them, and support their creative efforts in a better way than ever before. By accepting the terms of this Agreement, developers agree to be bound by all its provisions, which include but are not limited to the requirement to have an account on GitHub.com, and the understanding that GitHub's Terms of Use will also apply to their participation in the Program.", "4de3c097-e422-481f-b2c3-95f0066227ee": "The purpose of the {% data variables.code-scanning.tool_status_page %} is to provide useful information about all of your code scanning tools. It can help in debugging code scanning issues as it serves as a starting point for identifying and resolving problems with code scanning. If code scanning is not working as expected, the {% data variables.code-scanning.tool_status_page %} can provide insights into the status of each tool, any errors or warnings, and any other relevant information that can help in diagnosing and resolving issues.", "58a35a7a-a793-481e-8410-9780cb942841": "Based on the context provided, it seems that the given information is related to GitHub Actions and PowerShell scripts. The example provided shows how to set an environment variable using Bash and PowerShell scripts. This can be useful for passing values between steps in a workflow job or making values available to all steps in a job. However, without further context, it's unclear what specific goal or outcome is being aimed for. If more information is provided, a more specific answer can be given.", "e607f198-76d3-4070-9d32-b908567430c7": "To access the Security Overview dashboard for your organization's repositories, follow these steps:  1. Go to your organization's repository. 2. Click your profile picture in the upper-right corner of the page. 3. Click Settings from the drop-down menu. 4. Click Security & analysis from the left-hand menu. 5. Click Security overview in the Security & analysis section.  If you're an organization owner or administrator, you can also access the Security Overview dashboard from the organization's Settings page.  If you're using {% data variables.product.prodname_GH_advanced_security %}, you can also access the Security Overview dashboard from the Security tab in the organization's Settings page.  If you're using {% data variables.product.prodname_GH_advanced_security %} for public repositories, you can also access the Security Overview dashboard from the Security tab in the repository's Settings page.  If you're using {% data variables.product.prodname_GH_advanced_security %} for enterprise-owned organizations, you can also access the Security Overview dashboard from the Security tab in the organization's Settings page.  If you're using {% data variables.product.prodname_GH_advanced_security %} for public repositories, you can also access the Security Overview dashboard from the Security tab in the repository's Settings page.  If you're using {% data variables.product.prodname_GH_advanced_security %} for enterprise-owned organizations, you can also access the Security Overview dashboard from the Security tab in the organization's Settings page.  If you're using {% data variables.product.prodname_GH_advanced_security %} for public repositories, you can also access the Security Overview dashboard from the Security tab in the repository's Settings page.  If you're using {% data variables.product.prodname_GH_advanced_security %} for enterprise-owned organizations, you can also access the Security Overview dashboard from the Security tab in the organization's Settings page.  If you're using", "5dbcad7f-31c7-472e-858f-a8c4278c5fee": "The \"AUTOTITLE\" cribed in \"AUTOTITLE\" for {% data variables.product.prodname_github_app %} listed on {% data variables.product.prodname_marketplace %} seems to be a naming convention or a specific title format required for {% data variables.product.prodname_github_app %} to be listed on {% data variables.product.prodname_marketplace %}. This suggests that apps that meet this naming convention or title format are more likely to be approved for listing on {% data variables.product.prodname_marketplace %}. However, without further information, it's unclear what specific requirements are included in the \"AUTOTITLE\" format.", "6931c156-027a-4e59-9072-d9da82522302": "The rate limit for {% data variables.product.prodname_github_apps %} authenticating with an installation access token is specific to the installation, while the primary rate limit for authenticated users applies to all requests made by the user, regardless of the app being used. For example, if a user has installed multiple {% data variables.product.prodname_github_apps %} and is making requests using both installation access tokens and a personal access token, the requests made using the installation access tokens will be subject to the installation's rate limit, while the requests made using the personal access token will be subject to the user's primary rate limit.  2. The rate limit for {% data variables.product.prodname_github_apps %} installations on a {% data variables.product.prodname_ghe_cloud %} organization is higher than the rate limit for installations on non-{% data variables.product.prodname_gHE_cloud %} organizations. Specifically, installations on a {% data variables.product.prodname_ghe_cloud %} organization have a rate limit of 15,000 requests per hour, while installations on non-{% data variables.product.prodname_gHE_cloud %} organizations have a rate limit that scales with the number of users and repositories. For installations with more than 20 repositories, an additional 50 requests per hour are allowed for each repository. For installations on organizations with more than 20 users, an additional 50 requests per hour are allowed for each user. The rate limit for installations on non-{% data variables.product.prodname_gHE_cloud %} organizations cannot exceed 12,500 requests per hour.  The factors that determine the rate limit for installations on non-{% data variables.product.prodname_gHE_cloud %} organizations are the number of users and repositories associated with the installation. As the number of users and repositories increases, the rate limit also increases to accommodate the increased usage. This is to ensure that all users and repositories associated with the installation have access to the necessary resources without negatively impacting the overall performance and stability of the GitHub platform.", "1921bf8e-0f97-46af-9355-cb0200409214": "Base permissions set at the organization level determine the default access level for all members of that organization when accessing any of its repositories. This means that new members will automatically have the selected base permission level, and existing members will also have their access level updated to match the new base permission level. However, if an admin grants a member a higher level of access for a specific repository, that higher level of access will override the base permission level for that repository only.   There is an exception to this rule for custom repository roles with inherited roles that have a lower access level than the organization's base permissions. In this case, any members assigned to that role will default to the organization's base permissions rather than the inherited role. This is because the organization's base permissions take precedence over inherited roles.   In summary, base permissions set at the organization level determine the default access level for all members, but they can be overridden by specific repository permissions granted by admins, and custom repository roles with lower access levels than the organization's base permissions will default to the organization's base permissions.", "26125ad3-82ee-452e-ba79-84413172e25a": "To prevent the \"TODOCS\" placeholder from being overlooked and slipping into production, technical writers can use a linting test in the Docs test suite, which will fail if it finds the string \"TODOCS\" anywhere in a Markdown or YAML file. This ensures that any instances of \"TODOCS\" are addressed before the documentation is released. If the placeholder is overlooked and slips into production, the consequence could be incomplete or inaccurate documentation, which could lead to confusion or errors for users.", "eb18ada4-af23-40c3-992d-5b0b68d7aefa": "To update the DNS `CNAME` record to direct traffic to the destination instance after restoring user data from a backup, follow these steps:  1. Log in to your DNS provider's management console. 2. Locate the DNS record for the domain associated with the source instance. 3. Update the `CNAME` record to point to the hostname of the destination instance. 4. Save the changes and wait for the DNS propagation to complete.  Note: If you want to ensure that links to the source instance's old hostname still resolve to the destination instance, you can also configure a redirect using DNS. In addition to the `CNAME` record that points to the destination instance's hostname, create a second `CNAME` record that redirects traffic from the old hostname to the new hostname. Consult your DNS provider's documentation for more information on how to do this.", "0f6f4f67-d343-46fa-a2a5-8b9790e9c397": "In the context of SAML single sign-on for an enterprise account, authorized credentials can be viewed and revoked by following these steps:  1. Access the enterprise account. 2. Go to the People tab. 3. Click on the person whose authorized credentials you want to view or revoke. 4. Click on the SAML tab. 5. To view authorized credentials, click on the View authorized credentials button. 6. To revoke authorized credentials, click on the Revoke authorized credentials button. 7. Confirm the revocation by clicking on the Revoke button in the confirmation dialog box.  Note that if your enterprise uses {% data variables.product.prodname_emus %}, you will not be able to deprovision or remove user accounts from the enterprise on {% data variables.product.product_name %}. Any changes you need to make to your enterprise's managed users should be made through your IdP.", "5e239522-ee1c-4e33-ae96-b7721fe804d1": "According to the guidance provided by the IRS, individuals should not write \"not applicable\" on Line 6 of Form W-8BEN to indicate that a specific document is not legally required. Instead, they should use the phrase \"not legally required\" to accurately reflect their situation.", "684176c1-50a1-46ce-8e75-087277125101": "The \"committed\" event in the context of pull requests signifies that a new commit has been added to the pull request's `HEAD` branch. This event is significant because it indicates that the pull request is progressing and that the developer has made changes to the codebase. This event differs from the \"commented\" event for issues because the \"commented\" event is triggered when a comment is added to an issue, whereas the \"committed\" event is triggered when a commit is added to a pull request.  For example, let's say that a developer has created a pull request to add a new feature to a repository. The \"committed\" event is triggered when the developer adds a new commit to the pull request's `HEAD` branch, which contains the code for the new feature. This event is then recorded in the pull request's timeline, and other developers can view it to see the changes that have been made.  The \"committed\" event for pull requests includes the following information, which can be accessed through the REST API:  - `url`: The REST API URL to retrieve the commit. - `html_url`: The HTML URL of the commit. - `commit`: The SHA-1 hash of the commit. - `author`: The person who committed the changes. - `committer`: The person who committed the changes. - `parents`: An array of SHA-1 hashes of the parent commits. - `message`: The commit message. - `tree`: The SHA-1 hash of the commit's tree. - `url`: The REST API URL to retrieve the commit's tree. - `html_url`: The HTML URL of the commit's tree. - `distinct`: A boolean value indicating whether the commit is distinct from its parents. - `verification`: An object containing information about the commit's verification status. - `commit_url`: The REST API URL to retrieve the commit. - `html_url`: The HTML URL of the commit. - `issue_url`: The HTML URL of the issue associated with the commit, if any. - `id`: The unique identifier of the event. - `node_id`: The", "1b73f0a6-6083-463e-9f99-cbcb732f0f61": "To cancel an app for your personal account on GitHub Marketplace, follow these steps:  1. Go to your GitHub settings by clicking on your profile picture in the top right corner of the page and selecting \"Settings\" from the drop-down menu. 2. Click on \"Billing plans\" in the left-hand menu. 3. Find the app you want to cancel and click on \"Manage subscription\" next to it. 4. Click on \"Cancel subscription\" and confirm the cancellation.  If you're canceling a free trial, follow these steps instead:  1. Go to your GitHub settings by clicking on your profile picture in the top right corner of the page and selecting \"Settings\" from the drop-down menu. 2. Click on \"Billing plans\" in the left-hand menu. 3. Find the app you want to cancel and click on \"Manage subscription\" next to it. 4. Click on \"Cancel free trial\" and confirm the cancellation.  Note that if you're canceling an app for an organization or enterprise, the steps may be slightly different, so be sure to follow the instructions provided in the context information.", "bb141ec2-1b81-4015-b715-6f60eabeacb3": "To protect deploy keys in GitHub, the following security measures should be taken:  1. Use strong and unique passwords for deploy keys, and avoid using default or predictable passwords. 2. Store deploy keys securely, preferably in a secure key management system or a password manager. 3. Limit access to deploy keys to only authorized users who need them. 4. Use two-factor authentication (2FA) to add an extra layer of security to your GitHub account and prevent unauthorized access to your deploy keys. 5. Regularly review and revoke access to deploy keys that are no longer needed. 6. Use SSH keys with passphrases to add an additional layer of protection to your deploy keys. 7. Monitor your GitHub account and deploy logs for any suspicious activity or unauthorized access attempts. 8. Keep your GitHub software and server operating system up-to-date with the latest security patches and updates. 9. Use a firewall to restrict access to your server and prevent unauthorized access to your deploy keys. 10. Consider using GitHub's Deploy Keys feature, which allows you to generate and manage deploy keys directly in GitHub, rather than managing them manually on your server. This can help simplify key management and reduce the risk of errors or misconfigurations.  By following these security measures, you can help ensure the confidentiality, integrity, and availability of your GitHub deploy keys and prevent unauthorized access to your repositories and servers.", "b91a9443-9e27-42c0-b7ea-6e553c2ce8e9": "GitHub determines the date of a commit displayed on a user's profile page using the author date, which is when someone first creates a commit with `git commit`. In a repository, GitHub uses the commit date, which is identical to the author date unless someone changes it using certain Git commands. This can result in the commit sequence being out of order if the commit history is changed. The author date is used to calculate when a commit was made on a user's profile page, while the commit date is used to calculate when a commit was made in the repository.", "42270d21-bf98-4d8f-ae49-72d62db1577b": "To calculate the total number of nodes in a GraphQL query with multiple nested queries, we need to add up the number of nodes returned by each query. Here's an example calculation:  query {   viewer {     repositories(first: 50) {       edges {         repository:node {           name           pullRequests(first: 20) {             edges {               pullRequest:node {                 title                 comments(first: 10) {                   edges {                     comment:node {                       bodyHTML                     }                   }                 }               }             }           }           issues(first: 20) {             totalCount             edges {               issue:node {                 title                 bodyHTML                 comments(first: 10) {                   edges {                     comment:node {                       bodyHTML                     }                   }                 }               }             }           }         }       }     }     followers(first: 10) {       edges {         follower:node {           login         }       }     }   } }  Calculation:  50         = 50 repositories + 50 x 20 = 1000 repository pull requests + 50 x 20 = 1000 repository issues + 10 x 10 = 100 repository comments (for pull requests) + 50 x 20 = 1000 repository comments (for issues) + 10         = 10 followers  Total: 2100 nodes  In this example, we have 50 repositories, each with 20 pull requests and 20 issues. We also have 10 followers. By adding up the number of nodes returned by each query, we get a total of 2100 nodes.", "cbaa8177-fb5b-4df0-ac5a-103593c89467": "The purpose of configuring the {% data variables.product.prodname_ghe_server %} instance, as described in the context information, is to launch and manage instances, specifically by configuring settings in the web-based management console, uploading a license file, and saving settings. This is necessary to secure the instance and prevent potential attacks, as warned in the context information. After completing the configuration, the instance will restart automatically, and the user can visit the instance to continue managing it. Further reading is provided for additional information.", "907a9ff5-24dc-4445-a491-c406a8bca124": "To filter events in the audit log based on specific actions using the `action` qualifier, you can use the syntax `action:<action_name>`. For example, `action:team.create` will filter events to show only those where a team was created.  2. To filter events in the audit log based on the time they occurred using the `created` qualifier, you can use the syntax `created:<date_time>` or `created:<date_range>`. For example, `created:2021-01-01` will show events that occurred on January 1, 2021, and `created:>=2021-01-01` will show events that occurred on or after January 1, 2021.  3. To filter events in the audit log based on the originating country using the `country` qualifier, you can use the syntax `country:<country_code>` or `country:\"<country_name>\"`. For example, `country:US` will show events that originated in the United States, and `country:\"United States\"` will show events that originated in the United States.  4. To export the audit log using the GitHub Enterprise Cloud (GHEC) API, you can use the `GET /audit-log` endpoint with the appropriate query parameters. The exported log will contain the following keys and values:  - `id`: The unique identifier for the event. - `actor`: The user or app that performed the action. - `organization`: The organization where the action occurred. - `repository`: The repository where the action occurred (if applicable). - `event`: The type of event that occurred. - `created_at`: The date and time the event occurred. - `payload`: Additional information about the event, depending on the type of action.  The exported log is limited to the current month and the previous six months. If you need to export a larger time range, you may need to use a third-party tool or contact GitHub Support for assistance.", "447f289f-8936-42da-858c-1ac5435ddd36": "Extractor errors in the context refer to issues that arise during the process of extracting code from source files to create a database for analysis by {% data variables.product.prodname_codeql %}. These errors can impact the health of the database, as they may prevent certain source files from being included in the analysis. The context suggests that a small number of extractor errors is normal and not a cause for concern, but if the majority of source files are affected by errors, further investigation may be necessary to understand the root cause of the issues.", "e3b3ce87-4e79-4712-bcf9-125374e3db6f": "To request the raw format or rendered HTML for repository contents using custom media types, you can use the following custom media types:  - `application/vnd.github.raw` to retrieve the contents of a file in its raw format. - `application/vnd.github.html` to retrieve the rendered HTML for markup files such as Markdown or AsciiDoc.  These media types can be specified in the request header using the `Accept` parameter. For example:  ``` GET /repos/octocat/hello-world/blob/main/README.md Accept: application/vnd.github.raw ```  This will return the raw content of the `README.md` file in the requested repository.  To retrieve the rendered HTML for a markup file, you can use the same `Accept` parameter:  ``` GET /repos/octocat/hello-world/blob/main/README.md Accept: application/vnd.github.html ```  This will return the rendered HTML for the `README.md` file in the requested repository.  For more information about the use of media types in the API, you can refer to the \"AUTOTITLE\" section provided in the context information.", "c1ad8b11-adaf-423a-80ee-09ac2e5a74d5": "If the app is active, it should receive webhook events. The types of events that the app will receive depend on the specific webhook subscriptions that the app has configured. Some examples of events that an app might receive include repository push events, issue comment events, and pull request review events.", "edf4ecfd-8e09-4060-8c8d-2181b2123433": "To search for code within a specific user's or organization's repositories using GitHub's search feature, you can use the `user`, `org`, or `repo` qualifier. For example, `user:defunkt extension:rb` will match code from @defunkt that ends in .rb, `org:github extension:js` will match code from GitHub that ends in .js, and `repo:mozilla/shumway extension:as` will match code from @mozilla's shumway project that ends in .as.  To search for code based on its location within a repository using GitHub's search feature, you can use the `path` qualifier. For example, `path:/` will match _readme_ files with the word \"octocat\" that are located at the root level of a repository, `path:cgi-bin language:perl` will match Perl files with the word \"form\" in the cgi-bin directory, or in any of its subdirectories, and `path:app/public language:javascript` will match JavaScript files with the word \"console\" in the app/public directory, or in any of its subdirectories (even if they reside in app/public/js/form-validators).  To search for code based on its file type using GitHub's search feature, you can use the `extension` qualifier. For example, `extension:rb` will match files with the extension .rb, `extension:js` will match files with the extension .js, and `extension:css` will match files with the extension .css.  To search for code that contains a specific keyword within its file contents or file path using GitHub's search feature, you can use the `in:file` or `in:path` qualifier. For example, `in:file octocat` will match code where \"octocat\" appears in the file contents, and `in:path octocat` will match code where \"octocat\" appears in the file path. To search for code that contains a specific keyword within its file contents or file path, as well as within a specific location, you can combine the `in:file`, `in:path`, `path`, and `repo` qualifiers. For example", "8fa67738-1f04-4111-afd1-04ab83c26252": "According to the context information provided, if an individual is unable to access the enterprise because their IdP is unavailable, they can use a recovery code to bypass SSO. This information is mentioned in a note within the context information. Therefore, if the individual has a recovery code, they can use it to bypass SSO and access the enterprise without relying on their IdP. If the context doesn't provide information about recovery codes, then it's unclear how an individual can bypass SSO in this scenario.", "cf1cb31e-e250-4eef-bb1d-d49c8fcfe966": "Release notes for changes should be written to clarify actors and impact by avoiding passive language when possible. This means using active verbs and specifying who or what is performing the action. For example, instead of saying \"The feature is now available,\" say \"You can now use the feature.\"  The notes should also include a description of the new behavior, a description of the old behavior (if applicable), and any relevant links to GitHub Docs. The audience is often implied, but if necessary, it should be explicitly stated.  Here's an example:  - > As an organization owner, you can now manage your organization's secrets using the new {% data variables.product.prodname_secrets %} feature. Previously, you had to use third-party tools or store secrets in your repository. For more information, see \"Managing secrets.\"  Known issues should also be included in the release notes, with a description of the issue, any recognizable UI elements, and any necessary next steps. Again, the audience and impact should be clarified.  Here's an example:  - > As a repository owner, you may encounter an issue where the \"Compare & pull request\" button is not visible on the repository's main page. This is due to a known issue with the {% data variables.product.prodname_ghe_server %} software. To work around this issue, go to the repository's settings and select \"Pull requests\" from the left-hand menu. From there, you can create a new pull request or view existing ones. We apologize for the inconvenience and are working to resolve this issue in a future release.  In both cases, the release notes should be written in the present tense, as the changes apply to the current release.", "d4d71378-625c-4ff6-8cfe-ef8191cbbaa7": "{% data variables.product.prodname_github_apps %} must have the `administration` permission for repositories or the `organization_self_hosted_runners` permission for organizations to use the endpoints for managing self-hosted runner groups in {% data variables.product.prodname_actions %}. Authenticated users must have admin access to repositories or organizations, or the `manage_runners:enterprise` scope for enterprises to use these endpoints.", "97f5e30c-55c2-4b99-8d60-4040cd86a866": "In the context provided, the difference between PHP, Ruby, and Scala is that PHP and Scala are supported for {% data variables.product.prodname_code_scanning %} by third-party actions, while the support for Ruby is not explicitly mentioned. This means that GitHub does not offer built-in {% data variables.product.prodname_code_scanning %} for these languages, but instead relies on external actions to provide this functionality. The note also mentions that the support for Gradle for dependency graph and {% data variables.product.prodname_dependabot_alerts %} is limited, and that the support for PHP and Scala is limited to the upload of data obtained using the dependency submission API. Without further context, it is unclear whether there are any significant differences between the {% data variables.product.prodname_code_scanning %} capabilities for these languages when using third-party actions.", "c58e4581-3a71-414e-9206-1c253308a732": "To authenticate using your personal access token (PAT) in npm to access a private registry hosted on GitHub, follow these steps:  1. First, make sure you have installed the latest version of npm by running the following command in your terminal:     ```shell    npm install npm@latest -g    ```  2. Next, log in to your GitHub account and navigate to your repository settings.  3. Click on the \"Packages\" tab and select \"Configure and manage packages\" to access your package registry.  4. Click on the \"Settings\" tab and scroll down to the \"NPM\" section.  5. Click on the \"Generate new token\" button and follow the prompts to create a new personal access token (PAT). Make sure to select the \"read:packages\" and \"write:packages\" scopes.  6. Copy the generated PAT and save it in a secure location.  7. In your terminal, run the following command to set the npm registry URL and authentication token:     ```shell    npm config set registry \"https://npm.pkg.github.com\"    npm config set _authToken \"YOUR_PAT\"    ```     Replace \"YOUR_PAT\" with the PAT you generated in step 5.  8. To verify that you are authenticated, run the following command:     ```shell    npm whoami --registry=https://npm.pkg.github.com    ```     This should display your GitHub username.  9. To publish a package using npm to a private registry hosted on GitHub, follow these steps:     a. Make sure your package is properly configured and has a valid `package.json` file.     b. Navigate to your package directory in your terminal.     c. Run the following command to publish your package:     ```shell    npm publish --registry=https://npm.pkg.github.com    ```     This will upload your package to your private registry hosted on GitHub.     Note: Package names and scopes must only use lowercase letters. The tarball for an npm", "8876c0bd-a51f-46f6-972a-b191e81768c8": "To configure credentials for using {% data variables.product.prodname_actions_importer %} with Azure DevOps and {% data variables.product.prodname_dotcom %} as described in the context information, follow these steps:  1. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %} with the \"workflow\" scope. Copy the token and save it in a safe location. 2. Create an Azure DevOps {% data variables.product.pat_generic %} with the following scopes: \"Agents Pool: `Read`\", \"Build: `Read`\", \"Code: `Read`\", \"Release: `Read`\", \"Service Connections: `Read`\", \"Task Groups: `Read`\", and \"Variable Groups: `Read`\". Copy the token and save it in a safe location. 3. In your terminal, run the {% data variables.product.prodname_actions_importer %} `configure` CLI command. 4. When prompted, select \"Azure DevOps\" and press Space to select it, then press Enter. 5. Enter the value of the {% data variables.product.pat_v1 %} you created in step 1, and press Enter. 6. Press Enter to accept the default value for \"Base url of the GitHub instance\" (`https://github.com`) if you're using the default {% data variables.product.product_name %} instance. Otherwise, enter the URL for your {% data variables.product.product_name %} instance and press Enter. 7. Follow the prompts to complete the configuration process.  Note: If you're not using both Azure DevOps and {% data variables.product.prodname_dotcom %}, you can skip the steps for configuring credentials for the provider you're not using.", "18355108-9e79-40b1-872a-a44cafdaf736": "The remedy for a breach of the \"Service Disclaimer\" provided by GitHub is not explicitly stated in the given context. The disclaimer itself disclaims all warranties, including the warranty of non-infringement, and GitHub's only obligation, and Customer's only remedy, for a breach of the warranty regarding Expert Services is either re-performance or termination of the SOW and refund of unused prepaid Fees, as specified in the \"Expert Services Warranty\" section. Therefore, it can be inferred that there is no specific remedy for a breach of the \"Service Disclaimer,\" and the customer assumes all risks associated with using the Service \"AS IS\" and \"AS AVAILABLE\" without any warranties or guarantees from GitHub.", "64276dfa-24bc-45c9-bc85-c7a7561fae8a": "To contribute your query to the {% data variables.product.prodname_codeql %} repository, you can follow these steps:  1. Fork the {% data variables.product.prodname_codeql %} repository on GitHub. 2. Clone your forked repository to your local machine. 3. Create a new branch for your query. 4. Add your query to the `queries` directory in your new branch. 5. Write query help in the `.qhelp` format or include it alongside the corresponding query. 6. Push your changes to your forked repository. 7. Submit a pull request to the {% data variables.product.prodname_codeql %} repository.  For further reading on {% data variables.product.prodname_codeql %} queries, you can refer to the following resources:  - \"{% data variables.product.prodname_codeql %} queries\": This is the official documentation for {% data variables.product.prodname_codeql %} queries. It provides a comprehensive guide to writing queries using the CQL language. - \"Query help files\": This resource explains how to write query help in the `.qhelp` format. It provides examples and best practices for creating helpful and informative query help. - \"AUTOTITLE.\": This resource explains how to automatically generate a title for your query using the `AUTOTITLE` keyword. It provides examples and best practices for creating clear and descriptive query titles.  By following these resources and contributing your queries to the {% data variables.product.prodname_codeql %} repository, you can help build a community of {% data variables.product.prodname_codeql %} users and share your knowledge with others.", "16fba9b4-b7a8-4c01-9ac4-cbab40817aea": "An organization owner can choose whether members can create repositories in their organization by configuring permissions. If allowed, the owner can choose whether members can create both public and private repositories or public repositories only. If the organization is using {% data variables.product.prodname_ghe_cloud %}, the owner can also restrict members to creating private repositories only. Enterprise owners can also restrict the options available for repository creation policies for their organization. If members are allowed to create repositories, the owner can choose whether they can create private or internal repositories to prevent sensitive information from being exposed. However, this setting only restricts the visibility options available when repositories are created and does not restrict the ability to change repository visibility at a later time.", "11448ec3-c314-4a63-bb3b-bb6883419a04": "The purpose of enabling automatic redirects to single sign-on (SSO) instead of the 404 error in an enterprise setting is to provide a seamless and secure login experience for users. When automatic redirects to SSO are enabled, users are automatically redirected to their organization's SSO provider instead of seeing a 404 error page when they try to access a resource on {% data variables.location.product_location %} that requires authentication. This eliminates the need for users to manually enter their credentials and reduces the risk of unauthorized access to sensitive resources. Additionally, it provides a more consistent and streamlined login experience for users across different applications and services within the organization.", "81607386-24e8-47c0-980c-42dfd4a95e73": "Content that should be parsed in code blocks should be wrapped in `` ` tags to indicate that it should be parsed rather than escaped. This is recommended to ensure that the content is interpreted correctly and does not cause any issues with syntax or formatting. For example:  ```shell #!/bin/bash  # Define variables my_var=\"This is a string with spaces\" my_number=123  # Use variables in a command echo \"The value of my_var is: $my_var\" echo \"The value of my_number is: $my_number\"  # Use variables in a command with quotes to preserve whitespace echo \"The value of my_var is: ${my_var}\"  # Use variables in a command with curly braces to expand variables echo \"The value of my_var is: \\${my_var}\"  # Use variables in a command with curly braces and quotes to preserve whitespace echo \"The value of my_var is: \\\"\\${my_var}\\\"\" ```  In this example, the variables `my_var` and `my_number` are defined and then used in various ways in commands. The use of quotes, curly braces, and double quotes is demonstrated to show how to preserve whitespace and expand variables correctly. By wrapping the content in `` ` tags, the shell will interpret the variables correctly and execute the commands as intended.", "100e2d59-954b-47ba-9801-e463fbba0e2b": "To control where jobs are run using groups or labels in GitHub Actions, you can use either runner groups or labels. Both methods allow you to specify which runners your jobs should be executed on.  Using groups to control where jobs are run:  1. Create a runner group in your organization's runner settings. 2. Add runners to the group that meet the criteria you want to use for job execution. 3. In your workflow YAML file, set the `runs-on` key to the name of the runner group.  Example:  ```yaml name: learn-github-actions on: [push] jobs:   check-bats-version:     runs-on: my-runner-group     steps:       - uses: {% data reusables.actions.action-checkout %}       - uses: {% data reusables.actions.action-setup-node %}         with:           node-version: '14'       - run: npm install -g bats       - run: bats -v ```  Using labels to control where jobs are run:  1. Assign labels to your runners that match the criteria you want to use for job execution. 2. In your workflow YAML file, set the `runs-on` key to a label that matches the label assigned to the runners you want to use.  Example:  ```yaml name: learn-github-actions on: [push] jobs:   check-bats-version:     runs-on: ubuntu-20.04-16core     steps:       - uses: {% data reusables.actions.action-checkout %}       - uses: {% data reusables.actions.action-setup-node %}         with:           node-version: '14'       - run: npm install -g bats       - run: bats -v ```  To target macOS hosted runners in a GitHub Actions workflow, you can use the labels associated with macOS {% data variables.actions.hosted_runner %}s", "3775e7bf-814d-4b00-8b60-7a5155624757": "To authenticate to the {% data variables.product.prodname_container_registry %}, you can follow the steps provided in the context information. If you're using a GitHub App or a GitHub Action, you may need to authenticate using a personal access token (PAT) with granular permissions, as explained in the context information. If you're using the command line, you can replace `HOSTNAME` with your GitHub Enterprise Cloud hostname or IP address and follow the steps provided in the context information to authenticate using a PAT. Alternatively, you can use the `docker login` command with your GitHub credentials to authenticate directly from the command line. The specific authentication method required may depend on the context in which you're using the {% data variables.product.prodname_container_registry %}.", "577e48cd-c379-4a79-a928-c09baf9044a4": "Customer ensures compliance with GitHub's Acceptable Use Policies and Community Guidelines while using the Service by following the guidelines outlined in section 2.2.2 of the agreement. This includes avoiding the use of the Service for unlawful, obscene, offensive, or fraudulent Content or activity, as well as avoiding advocating or causing harm, interfering with or violating the integrity or security of a network or system, sending unsolicited, abusive, or deceptive messages, viruses, or harmful code, or violating third-party rights.", "ab6ed101-52a1-464c-a7be-89192c46563a": "To make a one-time payment for a GitHub subscription, the affected customer needs to access their billing settings, review their billing and payment information, and then click the \"Pay now\" button. If payment is not made by the due date, paid features and services will be locked until payment is made. The customer will receive email notifications with a link to their billing settings and two further reminder emails will be sent 7 and 14 days later if payment has not been made.", "d164b27a-9454-4b26-b73e-2d69dd3fb2ce": "Configuring two-factor authentication (2FA) for your {% data variables.product.prodname_dotcom %} account adds an extra layer of security to help protect your account from unauthorized access. This is because, in addition to your password, you'll also need a unique code generated by an app or sent to your phone via text message to log in. This makes it much harder for someone to gain access to your account, even if they have your password.  To configure 2FA for your {% data variables.product.prodname_dotcom %} account, follow these steps:  1. Go to your account settings by clicking your profile picture in the top right corner of the page, then selecting **Settings** from the drop-down menu. 2. In the left-hand menu, click **Security**. 3. Scroll down to the \"Two-factor authentication\" section and click **Set up two-factor authentication**. 4. Choose whether you want to use an authentication app or receive codes via text message. 5. Follow the prompts to complete the setup process.  We strongly recommend that you configure 2FA for your {% data variables.product.prodname_dotcom %} account to help keep your account secure.", "ca555839-4265-4b12-a24f-e9e90b2c1308": "Before you can migrate from {% data variables.product.prodname_ghe_server %} 3.8 or higher for the first time, someone with access to the {% data variables.enterprise.management_console %} needs to set up blob storage for {% data variables.location.product_location_enterprise %}. This is in addition to having sufficient access to both the source and destination.", "5f397cb9-3249-415f-8032-7a78d112089e": "To track which organizations a user access token is authorized for, you should use the `GET /user/installations` REST API endpoint provided by {% data variables.product.company_short %}. This endpoint returns a list of all the organizations that the user access token is authorized for.  During authentication, you should validate organization access by checking whether the organization the user is attempting to access is included in the list of authorized organizations returned by the `GET /user/installations` endpoint. If the organization is not included in the list, you should reject the user's access until they perform SAML SSO. This ensures that users are only granted access to the organizations they are authorized to access.  By following this recommended way to track organization access, you can help prevent unauthorized access to {% data variables.product.company_short %} resources and improve the security of your app.", "4b5d4b67-3db8-47b1-ae4c-647b1777e53a": "{% data variables.product.prodname_desktop %} does not support repositories configured with Folder Redirection. This is the resolution for the \"git clone failed\" error while cloning a repository configured with Folder Redirection using {% data variables.product.prodname_desktop %} on Windows.", "159afd6d-fd07-426c-943f-77b00b399b5a": "To add a secret for an organization using the {% data variables.product.prodname_cli %}, you need to authenticate with the `admin:org` scope in addition to the default `repo` and `read:org` scopes. Then, you can use the `gh secret set` subcommand with the `--org` or `-o` flag followed by the organization name to add a new secret. For example:  ```shell gh auth login --scopes \"admin:org\" gh secret set --org ORG_NAME SECRET_NAME ```  To specify the visibility of the secret, you can use the `--visibility` or `-v` flag. By default, the secret is only available to private repositories. To make it available to all repositories in the organization, use `all`. To make it available to specific repositories, use their names separated by commas. For example:  ```shell gh secret set --org ORG_NAME SECRET_NAME --visibility all gh secret set --org ORG_NAME SECRET_NAME --repos REPO-NAME-1, REPO-NAME-2 ```  To list all secrets for an organization, use the `gh secret list` subcommand with the `--org` or `-o` flag followed by the organization name. For example:  ```shell gh secret list --org ORG_NAME ```  Note that by default, the {% data variables.product.prodname_cli %} only authenticates with the `repo` and `read:org` scopes. To manage organization secrets, you need to additionally authorize the `admin:org` scope.", "251ff5e9-b0a2-474f-8f85-bc83fa135aca": "Unfortunately, the provided context information is not sufficient to provide a detailed implementation for streaming API requests for an enterprise account on GitHub. The context only mentions that the feature is in public beta and subject to change, and it provides instructions on how to enable API request events in the audit log settings. However, it doesn't provide any information on how to actually stream the API requests. To implement this feature, you would need more detailed documentation or guidance from GitHub's support team. I would recommend reaching out to GitHub's support team for further assistance.", "c44251c1-484d-4f64-8092-be45907b687a": "To delete a completed or older workflow run in GitHub, you need to have write permissions for the repository. Then, navigate to the repository's Actions tab, select the workflow run you want to delete, click the three horizontal dots icon to the right of the run, select \"Delete workflow run\" from the drop-down menu, review the confirmation prompt, and click \"Yes, permanently delete this workflow run\" to confirm the deletion. This can be done for completed or older workflow runs that are more than two weeks old.", "ad5148f4-fdfe-434d-a467-c81c0b9a8e06": "Each prebuild that's created consumes storage space that will either incur a billable charge or, for repositories owned by your personal {% data variables.product.prodname_dotcom %} account, will use some of your monthly included storage. This potential cost is mentioned in a note in the context information. However, the exact amount of storage space required and the associated costs are not specified, as this may vary depending on the specific configuration and usage patterns. Therefore, more information is needed to provide a specific answer regarding the potential cost of using prebuilds in {% data variables.product.prodname_github_codespaces %}.", "32c802d5-bbae-4bf0-b64f-49afb1225b78": "The purpose of user GPG key administration in the context provided is to allow users to manage their GPG keys through the GitHub API. However, the data returned in the `public_key` response field is not a GPG formatted key, but rather the cryptographic public key that is extracted and stored when a user uploads their GPG key. This key is not suitable for direct use in programs such as GPG, but can be used by the endpoints in this category.", "f0d2fe95-f66b-4213-b679-115abb84b179": "The \"labeled\" event provides information about a label that was added to an issue or pull request. This event is available for both issues and pull requests, and it includes common properties such as the event ID, actor, and repository. The specific label that was added is also included in the event.", "96c3a302-9015-41dc-9fa5-71ee923a7791": "The SSH authentication agent is a program that securely stores your private SSH keys and automatically forwards them to the SSH client when needed, eliminating the need to enter your private key passphrase for every SSH connection. This improves the security of your SSH connections by reducing the risk of key theft or exposure due to weak passphrases or key reuse. It also makes it more convenient to use SSH keys, as you only need to enter your passphrase once per session instead of for each individual connection.", "f607441e-7351-4b90-b3ec-302524538d33": "To build and push a multi-arch image using {% data variables.product.prodname_actions %} workflow steps, you can follow these steps:  1. First, you need to create a Dockerfile that supports multi-arch builds. This can be done by adding the following lines at the beginning of your Dockerfile:  ```dockerfile FROM node:14-buster-slim as base  FROM --platform=linux/arm64,linux/amd64 base as multi  WORKDIR /app  COPY package*.json ./  RUN npm install  COPY . .  CMD [\"npm\", \"start\"] ```  In this example, we're creating two stages: `base` and `multi`. The `base` stage is used to install dependencies and copy files, while the `multi` stage is used to build the image for both ARM64 and AMD64 architectures.  2. Next, you can create a {% data variables.product.prodname_actions %} workflow step that uses the `docker/build-push-action` action to build and push the image. Here's an example:  ```yaml - name: Build and push Docker image   uses: docker/build-push-action@f2a1d5e99d037542a71f64918e516c093c6f3fc4   with:     context: .     file: ./Dockerfile     platforms: linux/arm64,linux/amd64     push: true     outputs: type=image,name=my-image,annotation-index.org.opencontainers.image.description=My multi-arch image ```  In this example, we're passing the `context`, `file`, and `platforms` parameters to the action. The `push` parameter is set to `true` to automatically push the image to a registry. The `outputs` parameter is used to set the description for the image. The syntax for this parameter is `type=<type>,name=<name>,annotation-index.org.opencontainers.image.description=<description", "d55373f4-2fd9-41a6-8092-cc864f422176": "To reduce the number of queries used for analysis to manageable levels without compromising the accuracy of the results, you can follow these steps:  1. Review the queries used in the analysis and identify which ones are providing the most useful results. 2. Remove queries that are not providing valuable insights or are generating a large number of false positives. 3. Group related queries together and run them as a suite. This can help to reduce the number of queries being run and make it easier to manage the results. 4. Use query parameters to fine-tune the analysis and reduce the number of false positives. For example, you can use query parameters to exclude certain types of code or to focus on specific areas of the codebase. 5. Regularly review the queries being used and adjust them as needed to ensure that they are still providing valuable insights and are not generating too many false positives.  Remember, the goal is to find a balance between the number of queries being run and the accuracy of the results. By carefully selecting the queries being used and fine-tuning their parameters, you can reduce the number of queries being run without compromising the accuracy of the results.", "14403f5b-3326-41c9-b5c3-8763b7e8a6b0": "To edit or remove the contents of a note in the context provided, you can follow these steps:  1. First, you need to access the note you want to edit or remove. The context information provided doesn't specify how to do this, but it's likely that you can find the note in the project management tool being used.  2. Once you've located the note, you should see an option to edit or delete it. The context information provides a hint about where to find these options: \"note that you want to edit or remove. {% data reusables.project-management.project-note-more-options %}\". This suggests that you should look for a menu or icon labeled \"more options\" or something similar, which should contain the editing and deleting functions.  3. To edit the contents of the note, click the \"Edit note\" option. This should open the note in an editing interface, where you can make your changes.  4. To delete the contents of the note, click the \"Delete note\" option. This should prompt you to confirm that you want to delete the note, as deleting a note is a permanent action.  Note that the specific steps and interface may vary depending on the project management tool being used. The context information provided is a general guide, but you may need to consult the documentation or support resources for your specific tool to get more detailed instructions.", "60e6acfa-4f60-4139-aaee-d53d6826c92a": "Enabling private vulnerability reporting for a repository allows developers to receive security vulnerabilities and issues reported privately by trusted security researchers. This feature ensures that sensitive information about the vulnerability is not disclosed publicly until the developer has had a chance to address the issue. This helps to mitigate the risk of malicious actors exploiting the vulnerability before a fix is released. For security researchers, this feature provides a secure and responsible way to report vulnerabilities, as they can be confident that their findings will be handled confidentially and in a timely manner.", "a8d6c7ab-ad5c-43f8-993f-fe3ca0115a56": "To delete the legacy admin team as suggested in the context information, we need to have the necessary permissions and access to the Gacy system. Without this information, it's unclear who has the authority to delete the legacy admin team or how to access the Gacy system to perform this action. Therefore, we cannot proceed with deleting the legacy admin team without further context or clarification.", "6a65980f-131e-407f-8a63-246451ff201e": "If you manage an enterprise account for {% data variables.product.prodname_ghe_cloud %} with {% data variables.product.prodname_emus %} or {% data variables.product.prodname_ghe_server %}, you should use the term \"user account\" instead of \"personal account\" while managing accounts for enterprise products in {% data variables.product.company_short %}'s products. This applies to the following products: {% data variables.product.prodname_ghe_cloud %} with {% data variables.product.prodname_emus %} and {% data variables.product.prodname_ghe_server %}. However, for enterprises on {% data variables.product.prodname_ghe_cloud %} that don't use {% data variables.product.prodname_emus %}, you should use the term \"personal account\" when describing members of organizations owned by the enterprise.", "b85b9ef4-128c-48e5-a993-185a962a70eb": "Private registries that support granular permissions, such as {% data variables.product.prodname_container_registry %}, can be configured to allow packages to be pulled seamlessly into {% data variables.product.prodname_github_codespaces %} during codespace creation, without requiring authentication credentials. This is achieved by enabling the feature in the registry settings, which allows packages to be fetched without providing any authentication credentials. This feature is available for registries that support granular permissions, and the list of such registries can be found by referring to the documentation provided by {% data variables.product.company_short %}.", "94850b6b-d501-4ab7-af6e-d0c0d6f84428": "According to the context provided, {% data variables.actions.hosted_runner %}s are billed differently from standard {% data variables.product.prodname_dotcom %}-hosted runners. While standard runners are eligible for included minutes on private repositories, {% data variables.actions.hosted_runner %}s are not. This means that for both private and public repositories, {% data variables.actions.hosted_runner %}s will always be billed at the per-minute rate, regardless of whether they are in use or not. This difference in billing applies to private repositories because {% data variables.actions.hosted_runner %}s are not eligible for included minutes on private repositories, whereas standard runners are.", "75a575b8-961d-43aa-b3d0-a481bd97f384": "To filter pull requests on GitHub based on reviewers using the \"reviewed-by\" keyword, you can use the following filter in the search bar: `state:open type:pr reviewed-by:octocat`. Replace \"octocat\" with the GitHub username of the reviewer you want to filter by. This will show you only open pull requests that have been reviewed by the specified user.", "30cf2039-5b77-4449-8fee-b7bf47cb56c2": "According to the context information provided, some security and analysis features are enabled by default on {% data variables.product.prodname_dotcom %}. However, the specific features that are enabled by default are not mentioned. To manage the read-only access to these features, you can disable them for all your existing repositories or enable them by default for new repositories that you own. To do this, go to your account settings, select \"Security and analysis,\" and then either disable or enable the features for all your repositories or enable them by default for new repositories. It's important to note that disabling these features may impact the security and analysis capabilities of your repositories, so it's recommended to review the documentation provided by {% data variables.product.prodname_dotcom %} for more information on each feature and its implications.", "0e9fd7a5-8c1a-4480-a07b-2268c4ec1cfa": "According to the context provided, you can validate changes to your {% data variables.product.prodname_ghe_server %} appliance by configuring an IP exception list to allow access from specified IP addresses as a requirement or recommendation. However, the context doesn't provide enough information to explain how to do this. If you need more details, you can refer to the documentation or contact the support team for assistance.", "0d76ed3f-e041-4ca0-86a5-6938dd70e62f": "The identifier for the project containing the source branch of an external pull request in this context is `CI_MERGE_REQUEST_SOURCE_PROJECT_ID` which holds the value of the full name of the repository containing the source branch of the external pull request.", "3c41fb8b-6a5a-4ae4-b23a-ff2fe5be8790": "The context provided explains that there are two default settings for `GITHUB_TOKEN` permissions: restrictive and permissive. The restrictive setting grants read access to the `contents` and `packages` scopes for `GITHUB_TOKEN`, while the permissive setting grants read and write access to these scopes.  To modify this setting for your repository, follow these steps:  1. Go to your repository in GitHub. 2. Click on the settings icon in the top right corner of the page. 3. Select \"Actions\" from the left-hand menu. 4. Click on \"General\" to view the repository's general settings for GitHub Actions. 5. Scroll down to the \"Default `GITHUB_TOKEN` permissions\" section. 6. Select either \"Restrictive\" or \"Permissive\" depending on your preference. 7. Click \"Save changes\" to apply the new setting.  If your repository belongs to an organization and a more restrictive default has been selected at the organization level, you may not be able to select the permissive option. In this case, you will need to contact an organization administrator to modify the organization-level setting.  By default, when you create a new repository in your personal account, `GITHUB_TOKEN` only has read access for the `contents` and `packages` scopes. If you create a new repository in an organization, the setting is inherited from what was selected at the organization level.  If you're not sure whether your repository has the restrictive or permissive default setting, you can check by following the steps above and looking at the current setting.  In summary, the restrictive default setting is more restrictive in terms of `GITHUB_TOKEN` permissions, while the permissive setting is more permissive. You can modify this setting for your repository to suit your specific needs.", "b63497cc-c868-4867-a879-71e0e308d597": "To initiate an SSH key audit in the site admin dashboard, you need to go to the \"All users\" tab and click the \"Start public key audit\" button. This will disable all public keys and require users to verify their public keys to restore SSH access. After clicking the \"Begin audit\" button, all SSH keys will be invalidated, and users will receive a message explaining that a key audit is in progress and providing them with a link to approve their keys. During the audit, users will be unable to push or pull over SSH until their keys are approved.", "17394506-14d6-4fda-8039-5c0a7f761833": "Rate limits for Git operations can be enabled and configured in the management console of {% data variables.product.company_short %}'s enterprise environment. This feature is recommended to be enabled only upon the direct recommendation of a member of {% data variables.product.company_short %}'s staff, as Git operations are rarely the leading driver of CPU and RAM usage. The limits for concurrent operations per minute can be specified for repository networks or user IDs, and are adaptive based on the current CPU load. To enable and configure Git rate limits, follow the steps provided in the context information.", "552d523a-bc07-404f-b8f4-d59cadc9f8cd": "The `matrix` context is available for jobs in a matrix and contains the properties defined in the workflow file that apply to the current job. These properties can be used to customize jobs in a matrix by setting different values for each job based on the matrix combination.  For example, in the provided workflow, the `os` and `node` properties are defined in the matrix, and their values are used to set the runner type and Node.js version for each job. This allows for testing the software on different operating systems and Node.js versions without having to create separate jobs for each combination.  The `matrix` context can also be used to set other properties for each job based on the matrix combination, such as environment variables or command arguments. This can help to further customize the behavior of each job and make the testing more comprehensive.  Overall, the `matrix` context is a powerful feature that allows for more efficient and thorough testing of software in GitHub Actions.", "76148680-edd6-44eb-8a64-77952b559eb5": "To create a collapsed section in {% data variables.product.prodname_dotcom %}, any Markdown within a `` block should be included. This block should also include the `` tag to provide a label for the collapsed section. The label appears to the right of {% octicon \"triangle-right\" aria-label=\"The right triange icon\" %}, which is used to expand and collapse the section. Other elements that can be included within the collapsed section include headers, text, images, and code blocks.", "8afa6289-0b2e-427b-92ba-7ca65b4ca0f9": "In high availability, the failover configuration involves using a DNS record with a low TTL pointed to the primary appliance, or a load balancer. The failover method requires manually promoting the replica appliance in both DNS failover and load balancer configurations.  In clustering, the failover configuration requires a DNS record pointing to a load balancer. If a node behind the load balancer fails, traffic is automatically sent to the remaining nodes in the cluster.  Therefore, the main difference in failover method and configuration between high availability and clustering is the automatic failover in clustering, whereas in high availability, manual promotion of the replica appliance is required.", "32d50ed1-6471-433b-89af-b32fc2c1d53b": "To add a local repository to your project using Windows Explorer in Git, you need to follow these steps:  1. Open Windows Explorer on your computer. 2. Navigate to the location where you want to create the new repository. 3. Right-click on an empty space in the folder and select \"Git Bash Here\" from the context menu. 4. In the Git Bash window, enter the following command to initialize a new repository:     ```    git init    ```  5. Add the files you want to include in the repository using the following command:     ```    git add .    ```  6. Commit the changes to the repository with a message that describes what you've added:     ```    git commit -m \"Initial commit\"    ```  7. To add this repository to your project in Git, open GitHub Desktop or another Git client and follow the instructions to connect to the repository.  8. In Windows Explorer, navigate to the project you want to add the repository to. 9. Right-click on the project and select \"Git Bash Here\" from the context menu. 10. In the Git Bash window, enter the following command to add the remote repository:     ```    git remote add origin [repository URL]    ```  11. Fetch the changes from the remote repository using the following command:     ```    git fetch origin    ```  12. Merge the changes into your local branch using the following command:     ```    git merge origin/main    ```  13. Push the changes to the remote repository using the following command:     ```    git push origin main    ```  14. In GitHub Desktop or another Git client, you should now see the new repository listed under your project.  Answer:  To add a local repository to your project using Windows Explorer in Git, you need to:  1. Open Windows Explorer on your computer. 2. Navigate to the location where you want to create the new repository. 3. Right-click on an empty space in the folder and select \"Git Bash Here\" from the context menu. 4. In", "91a80561-b221-4116-8ad3-aaad26f63b65": "To restore a deleted package, you need to meet certain permission requirements depending on the scope of the package.  For repository-scoped packages, you need admin permissions to the repository in which the deleted package is published.  For user-account scoped packages, the deleted package needs to be scoped to your personal account.  For organization-scoped packages, you need admin permissions to the deleted package in the organization to which the package is scoped.  These permissions are different for each type of package because the scope of the package determines the level of access required to manage it. Repository-scoped packages are managed at the repository level, user-account scoped packages are managed at the personal account level, and organization-scoped packages are managed at the organization level. As a result, different permissions are required to restore a deleted package based on the scope of the package.  If the context doesn't specify the scope of the package, it's unclear which permissions are required to restore it. In this case, you would need to investigate the scope of the package to determine the required permissions.    Deleting a package in an organization  You can delete a package through your organization account settings, as long as the package was in a repository owned by the organization{% ifversion packages-registries-v2 %} or had granular permissions and was scoped to your organization account{% endif %}.  To review who can delete a package in an organization, see \"Required permissions.\"  {% data reusables.organizations.navigate-to-org %} {% data reusables.organizations.org_settings %} 1. On the left, click **Packages**. 1. Under \"Packages\", next to the package you want to delete, click **Delete**. 1. To confirm, type the name of the package and click **I understand the consequences, delete this package**.    Deleting a user-account scoped package  You can delete a package through your personal account settings, if the package was in one of your repositories or scoped to your personal account.  {% data reusables.profile.access_profile %} 1. In the left sidebar, click **Packages**. 1. Under \"Packages\", next", "e5ff3c28-8b25-4cc5-a48c-77bb535b8ea6": "To determine the SHA value for the source branch of an external pull request in this context, you can use the `CI_MERGE_REQUEST_SOURCE_BRANCH_SHA` variable provided in the context information. This variable contains the SHA value of the source branch for the external pull request.", "1c46d674-c0ed-468a-bbb5-388873de82d8": "Dependency caching in {% data variables.product.prodname_actions %} is a feature that allows frequently used dependencies to be cached and reused across multiple workflow runs. This can significantly reduce the time it takes to run workflows, as the dependencies do not need to be downloaded and installed each time.  On the other hand, artifact uploading is the process of uploading files generated during a workflow run to a repository's actions tab. This can be useful for sharing build artifacts, test results, and other output between steps in a workflow, or between workflows.  Dependency caching would be more useful than artifact uploading in situations where the same dependencies are required across multiple workflow runs. For example, if you have a workflow that builds and tests a project, and another workflow that deploys the project, you can use dependency caching to ensure that the same dependencies are used in both workflows, reducing the time it takes to run both workflows. This is because the dependencies will be cached and reused, rather than having to be downloaded and installed again.  In contrast, artifact uploading would be more useful for sharing specific output between steps in a workflow, or between workflows. For example, if you have a workflow that builds a project, and another workflow that runs tests on the built project, you can use artifact uploading to share the built project between the two workflows, rather than having to build the project again in the second workflow. This can save time and resources, as the project does not need to be built twice.  In summary, dependency caching is useful for reducing the time it takes to run workflows by caching frequently used dependencies, while artifact uploading is useful for sharing specific output between steps in a workflow, or between workflows. The choice between the two depends on the specific use case.", "1dfe6636-74ac-4a5c-8153-3d9d59de7004": "The 'github.token' context property is used to authenticate on behalf of the GitHub App installed on the repository. It is functionally equivalent to the 'GITHUB_TOKEN' secret and is set by the Actions runner within the execution steps of a job. This context property is only available within the execution steps of a job and its value will be null otherwise. The purpose of this context property is to provide a secure and convenient way to authenticate with the GitHub API during the execution of GitHub Actions workflows.", "2ac3af6d-7b3f-451b-a4d3-6dec18a71192": "To filter the \"Security coverage\" view to display repositories that have enabled both {% data variables.product.prodname_GH_advanced_security %} and {% data variables.product.prodname_code_scanning %}, you can use the following qualifiers:  - `advanced-security:enabled` - `code-scanning-push:enabled`  Alternatively, you can use the \"Advanced search\" feature to combine these qualifiers using Boolean operators (AND, OR, NOT).  If the context information doesn't include the `code-scanning-push:enabled` qualifier, then you can't filter the view to display repositories that have enabled both {% data variables.product.prodname_GH_advanced_security %} and {% data variables.product.prodname_code_scanning %} using the built-in filters. However, you can still use the \"Advanced search\" feature to filter the view based on your specific requirements.  In summary, the availability of the `code-scanning-push:enabled` qualifier depends on the context information provided. If it's not available, you may need to use the \"Advanced search\" feature instead.", "db5327fa-6f1e-4f45-95f6-5bbec818f47e": "To add your SSH key to the ssh-agent on a Windows machine, follow these steps:  1. Open the PowerShell console. 2. Run the following command to start the ssh-agent service:     ```powershell    Start-Service ssh-agent    ```  3. Run the following command to add your SSH key to the ssh-agent:     ```powershell    ssh-add /c/Users/YOU/.ssh/id_ed25519    ```  Replace `YOU` with your Windows username and `id_ed25519` with the name of your SSH key file.  Note: If you're using a different SSH key algorithm, replace `id_ed25519` with the name of your SSH key file for that algorithm.  After completing these steps, your SSH key should be added to the ssh-agent and automatically loaded when you connect to GitHub using SSH.", "643b20b6-f1a4-4815-bb54-33c59e9ebe72": "To submit an application to the GitHub Education program, the applicant should follow these steps:  1. Go to the GitHub Education website and click on the \"Apply now\" button. 2. Fill out the application form with required information such as name, email, and school details. 3. Upload a resume and a transcript (if required). 4. Click \"Submit Application\". 5. Two weeks after the program applications close, the applicant should check their email for an update on their application status and instructions to submit a video resume.  After submitting the application, the applicant should expect to receive a confirmation email. Two weeks after the program applications close, the applicant will receive an email with an update on their application status and instructions to submit a video resume. The video resume should introduce the applicant as an individual, and a webcam and computer microphone are sufficient for recording. The video should be submitted through a link provided in the email. After the video has been submitted, it will take about a week for the program to review it. If the applicant is accepted, they will receive invitations to start the GitHub Student Leader Program training and join an onboarding call.", "f749c35e-6588-4b22-a41c-54d6dd91494b": "The REST API for managing repositories on {% data variables.product.company_short %} supports all versions, including fpt, ghae, ghec, and ghes, as indicated in the context information provided.", "de76339a-fec0-4a5a-be7d-cfb547c9ce5b": "The guidelines for writing release notes for known issues and deprecations include providing context information such as the product version and the type of issue or deprecation, summarizing the issue or deprecation in a clear and concise manner, providing relevant links to GitHub Docs if necessary, and answering specific questions related to the issue or deprecation, such as whether it applies to the user's role or access, what is being deprecated or replaced, and where to find more information. The release notes should be written in the present or future tense, avoid passive language, and categorize each feature in a section under a feature heading.", "24a16ed6-4b4e-41c2-a4da-87ab77c3659e": "Reviewers and assignees can be specified for all pull requests raised for a package manager in the `dependabot.yml` file by using the `reviewers` and `assignees` options, respectively. The format for specifying a team as a reviewer or assignee is to use the full team name, as if you were @mentioning the team (including the organization). This is demonstrated in the example `dependabot.yml` file provided, where two reviewers (one being a team and the other being a GitHub user) and one assignee are specified for all npm pull requests.", "83bbb441-525e-4f9e-9ea2-d2bcf14a81b8": "To filter notifications on GitHub to only show repository invitation updates, you can use the `is:` query with the value `repository-invitation`. However, if you want to further refine your notification filter to only show repository invitation updates, you can also use the `reason:` query with the value `repository_invitation`. This will show only notifications related to repository invitations, rather than all notifications related to repository invitations and other repository-related activities.  Here's an example of how to use the `reason:` query:  1. Go to your GitHub profile and click on your avatar in the top right corner of the page. 2. Select \"Settings\" from the drop-down menu. 3. Click on \"Notifications\" in the left-hand menu. 4. Scroll down to the \"Custom notifications\" section and click on \"Create new notification\". 5. In the \"Filter\" field, enter `repo:your-repository-name reason:repository_invitation`. 6. In the \"Recipient\" field, select the email address or GitHub username you want to receive the notifications. 7. In the \"Notification delivery\" field, select how you want to receive the notifications (e.g., email, mobile push, or desktop notification). 8. Click on \"Create notification\" to save your custom notification.  Now, you'll only receive notifications related to repository invitations for your specified repository.", "ef1283a6-8218-44db-af1b-0db15e21ab7a": "Yes, according to the context information provided, it is possible to specify `permissions` at the top level of a workflow in GitHub Actions to apply to all jobs. This can be done by including the `permissions` property in the YAML configuration file for the workflow, outside of any specific job definitions. This allows you to set the access level for the `GITHUB_TOKEN` for all jobs in the workflow, rather than having to specify it for each individual job.", "58de8ce9-8e07-4dc7-8f5d-4be0a87b4f5d": "Based on the GitHub Supplemental Terms, GitHub may have access to a customer's private repositories with their consent and knowledge for support reasons or when access is required for security reasons. Additionally, GitHub may have additional access to private repositories if the customer enables certain services or features that require additional rights to the customer content. However, GitHub will continue to treat customer content in private repositories as confidential information. GitHub may also access, review, and remove content from private repositories if it has reason to believe the content is in violation of the law or these GitHub Supplemental Terms, or if required by law. GitHub will provide notice of such actions unless otherwise bound by requirements under law or in response to a security threat or other risk to security. GitHub's HTML/CSS, Javascript, and visual design elements or concepts are copyrighted and cannot be duplicated, copied, or reused without express written permission from GitHub.", "1cb9995d-19f5-4107-acac-5c502af7f383": "The \"--search-path\" option in the \"codeql execute upgrades\" command is used to specify a list of directories under which QL packs may be found. This option affects the search for QL packs by allowing the user to provide a list of directories that will be searched for packs before the ones in the default search path. This is useful when the user has installed additional QL packs in a different location than the default search path. The order of the directories in the \"--search-path\" list defines precedence between them, and if a pack name is found in more than one directory tree, the one given first wins. If the \"--search-path\" option is not given, the default search path will be used, which includes the immediate parent directories of the QL packs. On Windows, the path separator is \";\".", "c2336025-c685-4fce-bf0c-55e3320be849": "By default, {% data variables.product.prodname_code_scanning %} analyzes your code periodically on the default branch and during pull requests.", "d3ed57df-cf30-4ec0-8b61-195784aa0b11": "To maintain a healthy community while managing issues on GitHub, you can report comments that violate GitHub's Community Guidelines {% ifversion fpt or ghec %}as a community moderator{% endif %}. This helps to ensure that the community adheres to the guidelines and promotes a positive and productive environment for all contributors. Additionally, you can use issue forms and templates to help contributors open meaningful issues that provide the necessary information, which can help to reduce the number of low-quality issues and improve the overall quality of the issues in your repository. By maintaining a healthy community, you can foster a collaborative and productive environment for all contributors, which can lead to better software and a more enjoyable experience for everyone involved.", "00de3425-7ef4-4403-98a4-506b0fb96993": "The purpose of leaving individual comments on specific changes while reviewing files in a pull request is to provide feedback and suggestions to the author of the pull request. This helps to ensure that the changes made in the pull request are appropriate and meet the desired standards. After finishing your review of a file, you can mark it as viewed by clicking the \"Mark as viewed\" button, which collapses the file and helps you to identify the files you still need to review. This feature is useful when reviewing large pull requests with multiple files, as it allows you to keep track of which files you have already reviewed.", "ccd40ee5-851b-4043-8b52-b9a62abc4470": "To learn more about \"AUTOTITLE.\" mentioned in the context information, you should refer to the resources \"AUTOTITLE.\", \"AUTOTITLE.\", and \"AUTOTITLE.\" provided in the context information. Without further context, it's unclear what \"AUTOTITLE.\" refers to, so it's recommended to consult these resources for more information.", "8ef415f1-29ad-46fb-aa23-065b79c332ba": "The `jobs` context in reusable workflows is used to access the results and outputs of jobs that have been run in a reusable workflow. This context can be used to set outputs for a reusable workflow by defining the outputs in the job configuration and then accessing them in the `jobs` context.  For example, let's say we have a reusable workflow called \"my-workflow\" that contains a job called \"my-job\". In the job configuration, we define an output called \"my-output\" like this:  ```yaml name: My Workflow  on:   workflow_dispatch:  jobs:   my-job:     runs-on: ubuntu-latest     steps:       - name: Checkout         uses: {% data reusables.actions.action-checkout %}       - name: Run my script         run: |           echo \"My script output\" >> $GITHUB_OUTPUT           echo \"my-output=my-value\" >> $GITHUB_OUTPUT  outputs:   my-output:     value: ${{ steps.Run_my_script.outputs.my-output }} ```  In this example, we're running a script that generates some output and sets an output called \"my-output\" to a value. We then define the \"my-output\" output in the job configuration and set its value to the output of the script.  To access this output in the `jobs` context of a workflow that uses \"my-workflow\", we can define an output with the same name and set its value to the corresponding output in the `jobs` context:  ```yaml name: My Workflow User  on:   workflow_dispatch:  jobs:   my-job:     uses: my-org/my-workflow     steps:       - name: Checkout         uses: {% data reusables.actions.action-checkout %}       - name: Run my script         run: |           echo \"My script output\" >> $GITHUB_OUTPUT           echo \"my-output=my-value\" >>", "94dea5b1-3658-4dbd-9f6f-651089415d37": "To set up a workflow for Java development using Maven in GitHub, follow these steps:  1. Ensure that your repository is either a GitHub Enterprise Server (GHES) or GitHub Advanced Security (GHAS) instance.  2. Create a new file called `maven.yml` in the `.github/workflows` directory of your repository.  3. Copy and paste the following workflow code into the `maven.yml` file:  ```yaml name: Java CI with Maven  on:   push:     branches: [ \"main\" ]   pull_request:     branches: [ \"main\" ]  jobs:   build:     runs-on: ubuntu-latest      steps:     - uses: {% data reusables.actions.action-checkout %}     - name: Set up JDK 17       uses: {% data reusables.actions.action-setup-java %}       with:         java-version: '17'         distribution: 'temurin'         cache: maven     - name: Build with Maven       run: mvn -B package --file pom.xml      # Optional: Uploads the full dependency graph to GitHub to improve the quality of Dependabot alerts this repository can receive     - name: Update dependency graph       uses: advanced-security/maven-dependency-submission-action@571e99aab1055c2e71a1e2309b9691de18d6b7d6 ```  4. Customize the workflow as required, such as changing the Java version.  5. Commit the changes to the `maven.yml` file.  6. The workflow will now run automatically whenever you push changes to the `main` branch or create a pull request.  7. You can use the same commands that you use locally to build and test your code.  8. The workflow will run the `package` target by default, but you can specify different commands or targets as needed.  9. If", "b742aff6-4e91-4889-9561-3820d54e12fb": "Without prior knowledge or context about the text material, it is difficult to determine how the title \"AUTOTITLE\" contributes to the overall meaning and tone of the text. An \"autotitle\" is a term used in publishing to describe a book or other publication that does not have a title assigned to it. In this case, the use of \"AUTOTITLE\" as the title itself is unconventional and may suggest that the text material is experimental or unconventional in nature. However, without further context, it is impossible to say for certain how this title contributes to the overall meaning and tone of the text. More information about the content and style of the text would be necessary to make a more informed analysis.", "81bec0cc-65e3-44e2-b3db-687b3118d269": "Based on the information provided, GitHub is the controller and entity responsible for processing personal data for users in North America, subject to limited exceptions. For individuals outside North America, the data controller is GitHub B.V.", "9bfaa57d-4b06-49be-8396-3b9197848c60": "To enable _security updates_ for {% data variables.product.prodname_dependabot %}, you can add the following line to your `dependabot.yml` configuration file:  ```yaml security_updates: true ```  This will cause {% data variables.product.prodname_dependabot %} to raise pull requests for security updates as soon as they are released, in addition to regular version updates. Note that this setting is optional and is disabled by default. For more information, see the {% data variables.product.prodname_dependabot %} documentation on security updates.", "397c24f8-5b31-4570-b85a-7193af6da86e": "You are prompted to enter a file in which to save the key, and the default location is provided. You can either accept the default location by pressing Enter, or you can enter a custom location. The location will vary based on the operating system being used, as shown in the context information provided.    In summary, to save the key generated during the SSH key setup process, you need to follow the prompts provided during the setup process and either accept the default location or enter a custom location. The exact prompts and locations will depend on the operating system being used.", "65510a18-3653-47ab-84ad-aae259892652": "In {% data variables.product.prodname_desktop %}, to navigate to the \"Changes\" view, you should first open the repository you want to work on. Then, you can either click on the \"Changes\" tab in the sidebar, which is located on the left-hand side of the screen, or you can go to the \"Repository\" menu bar at the top of the screen and select \"Changes\" from the drop-down menu. Once you're in the \"Changes\" view, you should see your _README.md_ file listed in the file list, and a selected checkbox should be highlighted to the left of the file, indicating that the changes you've made to the file will be part of the commit you make.", "2e00eec2-31d4-42a9-bd4c-a70a4c74c217": "To store a secret named 'GKE_SA_KEY' in Google Cloud Platform, you can follow these steps:  1. Go to the Google Cloud Console and navigate to your project. 2. Click on the hamburger menu (three horizontal lines) in the top left corner and select 'Secrets Manager' under 'Security'. 3. Click on the 'Create Secret' button and enter 'GKE_SA_KEY' as the secret ID. 4. Select 'JSON' as the secret type and paste the contents of the JSON keyfile you downloaded earlier. 5. Click on the 'Create' button to create the secret.  To download the JSON keyfile for the service account, you can follow these steps:  1. Go to the Google Cloud Console and navigate to your project. 2. Click on the hamburger menu (three horizontal lines) in the top left corner and select 'IAM & Admin' under 'Tools & Settings'. 3. Click on the 'Service Accounts' tab and select the service account you want to download the keyfile for. 4. Click on the 'Keys' tab and click on the 'Add Key' button. 5. Select 'JSON' as the key type and click on the 'Create' button to download the keyfile. 6. Save the keyfile to your local machine and use it to authenticate with the Google Cloud API. ```", "fd82b0f3-3687-4883-88f6-ed45c6369e48": "The purpose of running `npx smee -u WEBHOOK_PROXY_URL -t http://localhost:3000/api/webhook` in Step 6: Start your server is to receive forwarded webhooks from Smee.io. This command connects Smee.io to the local server running on port 3000, allowing the app to receive webhook events from GitHub and forward them to the server for processing. The `WEBHOOK_PROXY_URL` parameter is replaced with the URL of the webhook proxy, which can be found on the app's settings page. This allows the app to receive webhook events from GitHub and forward them to the server, which can then perform actions based on those events.", "a818281b-1324-4c91-a58a-3fca197ef404": "The error message received if the verification code provided during the OAuth authorization process is incorrect, expired, or does not match the code received in the first request for authorization in GitHub is \"bad_verification_code\". This error message indicates that the code passed is incorrect or expired. The error is described in the context information provided.", "1778b02f-2350-4efe-85a2-79c9fa92461c": "To avoid unintentionally removing team members when connecting an IdP group to a team on {% data variables.product.product_name %}, you should follow these steps:  1. Before connecting the IdP group to the team, confirm that each current team member is also a member of the IdP group. You can do this by logging in to your identity provider and checking the group membership. If you don't have access to your identity provider, you can reach out to your IdP administrator.  2. If you haven't already done so, enable team synchronization for your organization or enterprise account. This can be done by an organization or enterprise owner in the administrative portal for your {% data variables.product.product_name %} account.  3. Authenticate using SAML SSO to connect the IdP group to the team. This ensures that team membership is automatically synchronized based on IdP group membership.  4. If you're using {% data variables.product.product_name %} Enterprise, consider creating new teams to synchronize membership data instead of modifying existing teams. This can help prevent unintentionally removing team members from existing teams.  5. Before synchronizing existing teams, check the IdP group membership to ensure that all team members are still members of the group. This can help prevent unintentionally removing team members from existing teams.  By following these steps, you can help ensure that team membership is synchronized correctly and avoid unintentionally removing team members when connecting an IdP group to a team on {% data variables.product.product_name %}.", "3f7dda09-83f0-44fb-b544-b9da2a0a0f42": "The variables that should be replaced in the provided {% data variables.product.prodname_actions %} workflow file to customize it for your specific organization and webhook are:  - `YOUR_SECRET_NAME`: Replace this with the name of the secret where you stored your {% data variables.product.pat_generic %}. - `YOUR_ORGANIZATION_NAME`: Replace this with the name of the organization where the webhook was created. - `YOUR_HOOK_ID`: Replace this with the ID of the webhook. - `YOUR_LAST_REDELIVERY_VARIABLE_NAME`: Replace this with the name that you want to use for a configuration variable that will be stored in the repository where this workflow is stored. The name can be any string that contains only alphanumeric characters and `_`, and does not start with `GITHUB_` or a number. - `YOUR_HOSTNAME`: Replace this with the name of {% data variables.location.product_location %}.{% ifversion ghes or ghae %}  If you're using GitHub Enterprise Server (GHES) or GitHub Advanced Security (GHAS), replace this with the name of your server.  - `WORKFLOW_REPO_NAME`: Replace this with the name of the repository where this workflow is stored. - `WORKFLOW_REPO_OWNER`: Replace this with the owner of the repository where this workflow is stored.  Note: If you're using GitHub Enterprise Server (GHES) or GitHub Advanced Security (GHAS), you may need to replace `HOSTNAME` with the name of your server.", "438da7f5-f0bb-48a6-ba12-461279acbd62": "The default location for CodeQL's persistence feature is a directory named `.codeql` in the user's home directory. If you want to change this location, you can set the `CODEQL_PERSISTENCE_DIR` environment variable to the desired path. This will override the default location. This feature, which allows for the persistence of downloaded QL packs and compiled query plans between several runs of the CLI, was introduced in version `v2.15.2`.", "8036efb3-e5e8-41ea-a82c-85a26a3721bb": "In a GitHub repository, collaborators can be granted different levels of access based on their roles. These roles are:  1. Read: Collaborators with read access can view the repository's contents, including files, issues, and pull requests. They can't make any changes to the repository.  2. Triage: Collaborators with triage access can perform basic repository management tasks, such as closing issues and pull requests, and labeling issues. They can't add or remove collaborators, or make any changes to the repository's settings.  3. Write: Collaborators with write access can make changes to the repository, including creating and deleting files, and pushing changes to the repository. They can't add or remove collaborators, or make any changes to the repository's settings.  4. Maintain: Collaborators with maintain access can perform more advanced repository management tasks, such as adding and removing collaborators, and managing the repository's settings. They can't make any changes to the organization's settings.  5. Admin: Collaborators with admin access have full control over the repository and the organization. They can make changes to the organization's settings, as well as perform all the tasks that collaborators with maintain access can do.  The table provided in the context information shows which actions can be performed at each level of access. Collaborators with read access can't perform any repository management tasks, while collaborators with admin access have full control over the repository and the organization. Collaborators with write access can make changes to the repository, but can't add or remove collaborators or manage the repository's settings. Collaborators with maintain access can perform more advanced repository management tasks, but can't make changes to the organization's settings. Collaborators with triage access can perform basic repository management tasks, but can't add or remove collaborators or manage the repository's settings.", "60ed9b1e-8c0d-4685-8978-1595f8c88dc6": "The `$(Build.PullRequest.TargetBranch)` variable in the context information represents the branch that a pull request is targeting. This variable is provided by Azure DevOps and can be used in build scripts to perform actions specific to that branch, such as running tests or building artifacts for that branch. In the context of GitHub, this variable would correspond to the branch that a pull request is being merged into, as specified in the pull request settings.", "29bb7c95-7042-4dfb-8932-8215bc8f735c": "To navigate to the action metadata file in your repository and publish your action to the GitHub Marketplace, follow these steps:  1. Go to your repository on GitHub. 2. Click on the \"Settings\" tab in the repository menu. 3. Scroll down to the \"Actions\" section and click on \"Actions\" to open the Actions page. 4. Click on the \"New action\" button to create a new action or select an existing action from the list. 5. Click on the \"Edit\" button to open the action metadata file (`action.yml` or `action.yaml`). 6. Once you have opened the action metadata file, you'll see a banner to publish the action to the GitHub Marketplace. Click \"Draft a release\" to proceed. 7. Select the \"Publish this Action to the GitHub Marketplace\" checkbox under \"Release Action\". 8. If there are any issues with the labels in your metadata file, you will see an error message. Address them by updating your metadata file. Once complete, you will see an \"Everything looks good!\" message. 9. Select the primary category for your action from the dropdown menu. 10. Optionally, select a secondary category from the dropdown menu. 11. Type a version for your action in the tag field. 12. Type a release title in the title field. 13. Complete all other fields and click \"Publish release\". 14. Publishing requires you to use two-factor authentication. For more information, see \"AUTOTITLE.\"  Note: If you own the repository or are an organization owner, and the \"Publish\" checkbox is disabled, it means that the account that owns the repository has not yet accepted the GitHub Marketplace Developer Agreement. In this case, click the link to \"accept the GitHub Marketplace Developer Agreement\", then accept the agreement. If there is no link, send the organization owner a link to this \"Release Action\" page and ask them to accept the agreement.", "a6b660c2-bf89-4342-a371-a0fbbbc6dd1b": "To set a warning message in GitHub Actions using the provided syntax, you can use the `::warning` command followed by the file path, line number, and message. Here's an example:  ```bash echo \"::warning file=app.js,line=1,col=5,endColumn=7::Missing semicolon\" ```  Replace `app.js`, `1`, `5`, and `7` with the appropriate values for your specific use case. The `col` and `endColumn` parameters are optional and can be omitted if you're not sure of the exact column numbers.  This command will print the warning message to the log and display it as a warning in the GitHub Actions UI. The message will include the file path, line number, and column numbers to help you quickly locate the issue.", "e4559617-d40e-4356-993a-f8dbe72c17e1": "If you violate the restrictions mentioned in the text material regarding the Pre-release Materials, you may be subject to legal action for breach of contract. The specific consequences may vary depending on the nature of the violation, but they could include damages for intellectual property infringement, injunctive relief to prevent further unauthorized use, and/or termination of your participation in the Program. Additionally, you may be liable for any costs or expenses incurred by GitHub as a result of your breach, including reasonable attorneys' fees.", "319fb296-3e1c-4423-af63-7606bd52b196": "To remove an external collaborator from a repository in the access overview of your repository settings, follow these steps:  1. Go to your repository on GitHub. 2. Click on the settings icon (the gear icon) in the top right corner of the repository page. 3. Select \"Settings\" from the drop-down menu. 4. Scroll down to the \"Collaborators\" section. 5. Find the collaborator you want to remove and click on their username. 6. Click the \"Remove\" button next to their name. 7. Confirm the removal by clicking \"Remove collaborator\" in the pop-up window.  Alternatively, you can also click the three dots next to the collaborator's name and select \"Remove\" from the drop-down menu.  After completing these steps, the collaborator will no longer have access to the repository.", "0c060c9b-2f1b-4cf2-a39d-4f4782f2803b": "To add a new source for fetching gems in Bundler, you can follow these steps:  1. Open your Gemfile, which is typically located in the root directory of your Ruby project.  2. Add a new `source` block to your Gemfile, specifying the URL for the new source. For example:     ```ruby    source \"https://rubygems.org\"     source \"https://{% ifversion fpt or ghec %}rubygems.pkg.github.com{% else %}REGISTRY_URL{% endif %}/NAMESPACE\" do      gem \"GEM_NAME\"    end    ```     In this example, replace `REGISTRY_URL` with the URL for your {% data variables.product.prodname_registry %} instance, and replace `NAMESPACE` with the personal account or organization that owns the repository containing the gem you want to install. If you're using {% data variables.product.prodname_ghe_server %}, replace `HOSTNAME` with the hostname of your instance.  3. Save the changes to your Gemfile.  4. Install the gem using Bundler:     ```shell    bundle install    ```     This will fetch the gem from the new source you added.  To specify a source for a specific gem in the Gemfile, you can use a nested `source` block, as shown in the example above. This allows you to specify a source for a specific gem, rather than using the same source for all gems.  Inside the nested `source` block, you can specify the gem you want to install, along with any version constraints. For example:  ```ruby source \"https://rubygems.org\"  source \"https://{% ifversion fpt or ghec %}rubygems.pkg.github.com{% else %}REGISTRY_URL{% endif %}/NAMESPACE\" do   gem \"GEM_NAME\", \"0.1.1\" end ```  In this example, replace `REGISTRY_URL` and `NAMESPACE` with the appropriate values for your {% data variables.product", "a853189d-3c48-477f-826e-1903b7e9b05d": "The purpose of sending a review request in a GitHub repository is to seek feedback and input from other collaborators or reviewers with write access to the repository before merging a pull request into the main branch. This helps to ensure that the changes proposed in the pull request are reviewed and approved by other team members, reducing the risk of introducing bugs or conflicts into the main branch. Review requests also facilitate collaboration and knowledge sharing among team members, as they can provide suggestions, feedback, and guidance on the proposed changes.", "75eacc2d-cbb0-484e-b982-6ed727a19c50": "To retrieve a specific number of elements from a list using GraphQL queries, we can use the `first` and `last` arguments in list-based queries.  The `first` argument is used to retrieve the first n elements of a list, while the `last` argument is used to retrieve the last n elements of a list. Both arguments accept an integer value as their parameter.  Here's the syntax for using `first` and `last`:  ```graphql query {   # Retrieve the first 10 elements of a list   list(first: 10) {     nodes {       # Access the fields of each node     }   }    # Retrieve the last 5 elements of a list   list(last: 5) {     nodes {       # Access the fields of each node     }   } } ```  In the `DiscussionCommentConnection` query, the `onlyAnswers` parameter is used to filter discussion comments based on whether they were marked as the answer or not.  When `onlyAnswers` is set to `true`, only discussion comments that were marked as the answer will be returned. When `onlyAnswers` is set to `false` (which is the default value), all discussion comments will be returned.  Here's an example usage of the `onlyAnswers` parameter:  ```graphql query {   discussion(id: \"123\") {     comments(first: 10, onlyAnswers: true) {       nodes {         body         author {           login         }       }     }   } } ```  In this example, we're retrieving the first 10 discussion comments of a specific discussion, and we're filtering them to only include comments that were marked as the answer. We're then accessing the `body` and `author` fields of each comment node.", "8f1a3bbf-8223-45f6-9a5a-0037a83c052f": "Filling out all fields of a counter notice completely is significant because a counter notice is a legal statement, not just to GitHub but also to the complaining party. This means that the counter notice serves as a response to the copyright infringement allegations, and it is necessary to provide all required information to ensure that the counter notice is actionable and valid under the Digital Millennium Copyright Act (DMCA). By filling out all fields completely, the person submitting the counter notice can provide a strong defense against the copyright infringement allegations and prevent the content from being disabled on GitHub.  Publishing all complete and actionable counter notices at a specific location is necessary because it provides transparency and accountability in the DMCA takedown process. After redacting personal information, GitHub publishes all complete and actionable counter notices at a specific location, which allows other parties whose rights would be affected by the counter notice to review it and evaluate the merit of their claims. This also provides a public record of the DMCA takedown process, which can help prevent abuse of the DMCA and ensure that the process is being followed correctly. However, it's important to note that GitHub only publishes redacted notices, but it may provide a complete unredacted copy of any notices it receives directly to any party whose rights would be affected by it.", "8894c572-408b-4a63-89c3-dc51cc51da8f": "Custom labels can be used to route jobs on self-hosted runners by creating and assigning labels to runners based on specific hardware or software requirements. For example, a custom label called \"gpu\" can be created and assigned to runners with the required graphics hardware. This allows jobs to be sent to runners with the necessary hardware, based on the labels assigned to them. Other examples of hardware or software requirements that can be specified using custom labels include specific versions of operating systems, programming languages, or databases. By combining default and custom labels, more specific and complex routing rules can be created to ensure that jobs are executed on the most appropriate runners for their requirements.", "949222b1-4ef3-4219-9596-683ade3b4e9e": "GitHub provides written descriptions of the security controls and other information reasonably requested by its customers regarding its security practices and policies for the Online Service, as outlined in the Security Exhibit. This includes security documents describing its security measures and the relevant procedures and responsibilities of its personnel who have access to customer content and protected data, as well as information about possible consequences of breaching security rules and procedures.", "afe6955e-69f8-48bf-a84d-55d5d166c94a": "A {% data variables.product.prodname_github_app %} is a type of integration that allows developers to interact with and extend the functionality of {% data variables.product.company_short %} without requiring a user to sign in or create a service account. It can be used for various purposes, such as automating tasks, supporting \"Sign in with GitHub,\" providing developer tools, integrating external services, and acting independently of a user. Common use cases for {% data variables.product.prodname_github_apps %} include automating tasks or background processes, supporting \"Sign in with GitHub\" for user authentication, providing developer tools for working with {% data variables.product.company_short %}, integrating external services with {% data variables.product.company_short %}, and acting independently of a user.  Unlike {% data variables.product.prodname_oauth_apps %}, which can only act on behalf of a user, {% data variables.product.prodname_github_apps %} can also act independently of a user. This makes {% data variables.product.prodname_github_apps %} more flexible and powerful, as they can perform tasks without requiring a user to be present.  The {% data variables.product.prodname_app_manager %} is responsible for managing {% data variables.product.prodname_github_apps %}. This includes registering new {% data variables.product.prodname_github_apps %}, configuring their settings, and granting them access to specific repositories. The {% data variables.product.prodname_app_manager %} can also revoke access to {% data variables.product.prodname_github_apps %} if necessary.", "ccb3cf3f-f0b0-45f3-b8db-f9c731525696": "Yes, based on the context information provided, it is possible to grant a GitHub Codespace access to additional repositories beyond the one it was created from. The recommended approach is to list additional permissions in the dev container configuration, which will allow users to authorize the additional permissions when they create a codespace. However, this will not change the permissions of an existing codespace. If access to another repository is required in an existing codespace or if the permissions needed are specific to a particular contributor, a personal access token (PAT) with access to the repository can be created and added to the codespace as an environment variable or as a secret for GitHub Codespaces. It is recommended to limit the token's access by using a PAT v2, selecting only the repositories to which access is needed, and giving the required access to the Contents permission only. When a new secret is added, it should only be accessible to trusted repositories, and the existing codespace should be reloaded to pull in the new secret. The `GH_TOKEN` variable can be used by default in GitHub CLI operations to authenticate in the codespace.", "882f56ce-90ca-4de5-ab33-616412d9193b": "In the case of an organization purchase with per-user access, the customer can be prompted to choose which organization members will have access to the purchased app through the API provided by the marketplace. This can be done by getting all the organization's members via the API and allowing the organization owner to select which members will have paid users on the integrator side. The options for customizing access for these members can vary based on the app's specific requirements, but some suggestions include flat-rate pricing, where all organization members have access to the app, or per-unit pricing, where users can occupy a seat as they log in to the app, and the customer is alerted when they hit the seat count threshold. Ultimately, the app developer has the flexibility to choose the best approach for their app based on the specific needs of their customers.", "f364c098-8993-4e39-a178-451d2c8c6d27": "Unfortunately, the provided context information doesn't offer enough context to determine the meaning or usage of the \"AUTOTITLE\" command. Without further information, it's unclear whether this is a command specific to JetBrains IDEs or a generic command used in a different context. If this command is specific to JetBrains IDEs, it's possible that it's used to automatically set the title of a file or project based on certain criteria, but again, without further context, this is purely speculative. If you have more information, please provide it so that I can better assist you.", "5a7eacff-37b0-4560-8543-54de9e628e39": "According to section A1-5, the requirements for using Protected Data are as follows:  1. Permitted Use: Developer can process Protected Data only for the purposes specified in Section 4(b)(i). 2. No Use in Marketing: Developer cannot use Protected Data for advertising third-party goods or services or sell it to any third party. 3. Automated Decisions: If Developer makes automated decisions affecting GitHub customers' rights, including employment, credit, or health, they must provide notice to the individuals.  These requirements ensure that Protected Data is used only for the intended purpose and that individuals' rights are protected.", "7cfc7ee3-bb69-4fc6-9692-996e58a73068": "To add permissions settings for the token in your YAML file for GitHub Actions, you need to follow these steps:  1. Define the token settings using `actions/github-script` with the `core` toolkit. For example:  ```yaml jobs:   job:     runs-on: ubuntu-latest     steps:     - uses: {% data reusables.actions.action-github-script %}       id: script       timeout-minutes: 10       with:         debug: true         script: |           const token = github.getConfig('token')           const runtimeUrl = github.getConfig('runtimeUrl')           core.setOutput('TOKEN', token.trim())           core.setOutput('IDTOKENURL', runtimeUrl.trim())            // Define the permissions for the token           const permissions = ['read:org', 'write:repo-{% r github.repository.owner %}/{% r github.repository.repo %}']           const tokenRequest = {             'client_id': '{% r github.repository.owner %}.{% r github.repository.repo %}.{% r github.repository.ref %}',             'redirect_uri': 'http://localhost:3000/login/github/callback',             'scope': permissions.join(' '),             'grant_type': 'client_credentials'           }            const response = await github.rest.actions.createTokenForRepo({             owner: '{% r github.repository.owner %}',             repo: '{% r github.repository.repo %}',             token_request: tokenRequest           })            core.setSecret('{% r github.repository.owner %}.{% r github.repository.repo %}.{% r github.repository.ref %}.token', response.token) ```  In this example, we're using `github.getConfig()` to retrieve the token and runtime URL from the repository's configuration. We're then defining the permissions for the token as an array of scopes, and passing them to the `create", "8233ed92-de6f-4229-ae3e-46d1af401ad9": "Developers can mitigate the limitations of autofix suggestions in JavaScript projects by following best practices such as using CI testing of pull requests to verify functional requirements are unaffected, and using dependency management solutions like the dependency review API and action. It is essential to remember that the author of a pull request retains responsibility for how they respond to review comments and suggested code changes, whether proposed by colleagues or automated tools. Developers should always look at suggestions for code changes critically and edit the suggested changes to ensure that the resulting code and application are correct, secure, meet performance criteria, and satisfy all other functional and non-functional requirements for the application.", "595d0f17-3a7b-477d-8666-70ecadf64386": "To configure Nagios to monitor the status of a GitHub Enterprise (GHE) cluster using the `ghe-cluster-status -n` command, follow these step-by-step instructions:  1. Generate an SSH key with a blank passphrase:     ```shell    nagiosuser@nagios:~$ ssh-keygen -t ed25519    > Generating public/private ed25519 key pair.    > Enter file in which to save the key (/home/nagiosuser/.ssh/id_ed25519):    > Enter passphrase (empty for no passphrase): LEAVE BLANK BY PRESSING ENTER    > Enter same passphrase again: PRESS ENTER AGAIN    > Your identification has been saved in /home/nagiosuser/.ssh/id_ed25519.    > Your public key has been saved in /home/nagiosuser/.ssh/id_ed25519.pub.    ```     **Security Warning:** An SSH key without a passphrase can pose a security risk if authorized for full access to a host. Limit this key's authorization to a single read-only command.  2. Copy the private key (`id_ed25519`) to the `nagios` home folder and set the appropriate ownership:     ```shell    nagiosuser@nagios:~$ sudo cp .ssh/id_ed25519 /var/lib/nagios/.ssh/    nagiosuser@nagios:~$ sudo chown nagios:nagios /var/lib/nagios/.ssh/id_ed25519    ```  3. To authorize the public key to run _only_ the `ghe-cluster-status -n` command, use a `command=` prefix in the `/data/user/common/authorized_keys` file. From the administrative shell on any node, modify this file to add the public key generated in step 1. For example: `command=\"/usr/local/bin/ghe-cluster-status -n", "aa3a0ccd-60b0-43de-bfad-d259be077f66": "The purpose of specifying a SARIF category when analyzing multiple {% data variables.product.prodname_codeql %} databases for a single commit using the {% data variables.product.prodname_codeql_cli %} is to allow {% data variables.product.prodname_code_scanning %} to store the results for each language separately when uploading the results to {% data variables.product.product_name %}. If a SARIF category is not specified, each upload overwrites the previous results.", "56b819ac-d7ef-4036-84a7-45a566d34e07": "The currently published {% data variables.product.prodname_pages %} site will be unpublished when a private repository is transferred to a personal account that is using {% data variables.product.prodname_free_user %}. For more information, you can refer to the \"AUTOTITLE\" documentation provided in the context information.", "cdb1308b-f37f-42ab-ad48-7fd999e0efd6": "The potential impact of installing a different version of OpenSSH on a Linux distribution is that the ability of other applications to authenticate via SSH may be affected. This is because OpenSSH is a critical component of the SSH protocol, and any changes to its implementation can potentially break compatibility with other SSH clients and servers. To address this issue, it is recommended to review the documentation for your Linux distribution and ensure that any new version of OpenSSH is compatible with the other applications that you use. If you are unsure about the compatibility of a new version of OpenSSH, it may be best to consult the documentation or seek advice from the developers of the other applications that you use. Additionally, it is always a good practice to test any new version of OpenSSH in a non-production environment before deploying it in a production environment.", "1ce8f13c-0533-4f40-8b2f-83c0ed7487d3": "Environment variables in codespaces on {% data variables.product.prodname_dotcom %} serve as key-value pairs that can be accessed by commands run in codespaces. They provide a way to pass information between commands and scripts, and can be used to store configuration settings, paths to executables, and other environment-specific data. In {% data variables.product.prodname_dotcom %}, default environment variables are set for every codespace, and commands can create, read, and modify these variables. Some of the default environment variables include the codespace name, a flag indicating whether the codespace is active, email and name for future Git commits, and URLs for the GitHub API, GraphQL API, and server. These variables can be used to simplify and automate tasks in codespaces, such as setting up development environments, running tests, and deploying code.", "0d7699aa-d954-411d-b83b-47e525abccd2": "The process of synchronizing changes from an upstream repository to a fork is called pulling requests or fetching. This process provides several benefits, such as keeping the fork up-to-date with the latest changes in the upstream repository, avoiding conflicts when merging changes from both repositories, and making it easier to contribute back to the upstream repository by creating pull requests from the fork. However, the context provided doesn't explicitly mention pulling requests or fetching, so it's unclear which term is being used. If more context is needed to clarify this, it should be provided.", "6f1d5b51-7d3b-4be8-ab42-1c59b0337ff6": "In most cases, prebuilds will automatically be available for branches created from a prebuild-enabled base branch. This is because {% data variables.product.prodname_github_codespaces %} creates a new environment for each new branch based on the environment created during the prebuild process. However, there are some exceptions to this rule. For example, if you have customized the environment creation process for a specific branch, then prebuilds may not be automatically available for branches created from that branch. In such cases, you may need to manually enable prebuilds for the new branch. To do this, go to the \"Prebuild configuration\" section of the repository settings, select the new branch, and enable prebuilds for the desired dev container configuration.", "8506f3cf-1973-466d-9ef3-b5bf285c400f": "{% data variables.contact.github_support %} may request diagnostic files and support bundles to help troubleshoot issues with a GitHub Enterprise Server instance. Diagnostic files are provided in plaintext format and contain information about the instance's settings and environment, while support bundles are provided in archive format and contain a diagnostics file and sanitized log files from the past two days by default. Extended support bundles contain a diagnostics file and sanitized log files from the past eight days. Authentication tokens, keys, and secrets are removed from log files in the support bundle and diagnostic file.", "665e540b-503b-4bc2-8d74-3d611ba98bc2": "To share workflows and other {% data variables.product.prodname_actions %} features with your team within a {% data variables.product.prodname_dotcom %} organization, you can collaborate by centrally storing and managing secrets, artifacts, and self-hosted runners. You can also create starter workflows in the `.github` repository and share them with other users in your organization. Additionally, you can reuse actions and workflows exactly by referencing them in your workflow file. For more information on sharing secrets and variables within an organization, refer to the documentation provided.", "d9353242-32d9-447c-8596-5fa95543969d": "The error \"unknown key type\" occurs when generating a new SSH key using an SSH client that does not support the specified key type. To resolve this issue on a macOS system, you can update your SSH client or install a new SSH client using Homebrew by following these steps:  1. Open Terminal. 2. Enter the command `brew install openssh`. 3. Quit and relaunch Terminal. 4. Try generating a new SSH key again.  Note that if you install OpenSSH using Homebrew, your computer will not be able to retrieve passphrases stored in the Apple keychain. You will need to enter your passphrase or interact with your hardware security key every time you authenticate with SSH to GitHub or another web service. To remove OpenSSH and restore the ability to retrieve passphrases, enter the command `brew uninstall openssh` in Terminal.", "2a16b79a-5195-4e68-84de-232d9ffa11df": "{% data variables.product.prodname_dotcom %} for enterprises supports the entire software development lifecycle by providing a suite of products that enable businesses to store and version control source code, plan and track work using issues and projects, code in a cloud-hosted development environment, review code changes with pull requests, and automate the build, test, and deployment pipeline using {% data variables.product.prodname_actions %} and {% data variables.product.prodname_registry %}. As a result, businesses see significant benefits such as increased development velocity and improved code quality, with developers saving 45 minutes per day and onboarding and training time reduced by 40%. Additionally, enterprise accounts simplify administration by providing a single point of visibility and management for all stages in the software development lifecycle, enabling policy enforcement and audit of access to resources. Optional add-ons such as {% data variables.product.prodname_GH_advanced_security %} and enhanced support options with {% data variables.contact.premium_support %} are also available for businesses with specific needs.", "4c56dcfc-0f64-4547-ac5a-b3a73ce17388": "Subdomain isolation is a security measure that separates different functionalities or applications of a website into distinct subdomains. This technique helps mitigate cross-site scripting (XSS) vulnerabilities by preventing malicious scripts from executing on other parts of the website. By isolating user-supplied content, such as comments or forum posts, into their own subdomains, it becomes much harder for attackers to inject malicious scripts into other parts of the website. This helps prevent XSS attacks, which can steal sensitive information or take control of user accounts. In summary, subdomain isolation is a security best practice that helps protect websites from XSS attacks by isolating user-supplied content into separate subdomains.", "c758dcf9-deff-4676-859f-c175b933dfe0": "The level of access to repositories affects the ability to view {% data variables.product.prodname_code_scanning %} and {% data variables.product.prodname_dependabot %} data in a GitHub organization as follows:  - `admin` access for one or more repositories: View data for those repositories for both {% data variables.product.prodname_code_scanning %} and {% data variables.product.prodname_dependabot %}. - `write` access for one or more repositories: View {% data variables.product.prodname_code_scanning %} and {% data variables.product.prodname_dependabot %} data for those repositories for both {% data variables.product.prodname_code_scanning %} and {% data variables.product.prodname_dependabot %}. - Security alert access for one or more repositories: View all security alert data for those repositories for both {% data variables.product.prodname_code_scanning %} and {% data variables.product.prodname_dependabot %}. - Custom organization role with permission to view one or more types of security alert: View allowed alert data for all repositories for both {% data variables.product.prodname_code_scanning %} and {% data variables.product.prodname_dependabot %} in all views.  If the context doesn't specify the level of access, then it's not possible to determine the ability to view {% data variables.product.prodname_code_scanning %} and {% data variables.product.prodname_dependabot %} data.", "d4de70b5-ad4a-423b-8454-d5b5409606da": "To navigate to a specific reference of a symbol in a repository using GitHub's symbols pane, follow these steps:  1. Select a repository and navigate to a file containing symbols. 2. Click the {% octicon \"code-square\" aria-label=\"The code square icon\" %} above the file content to bring up the symbols pane. 3. Click the symbol you want to find from the symbols pane or within the file itself. 4. To search for a symbol in the repository, click **Search for this symbol in this repository** in the symbols pane. 5. To search for a symbol in all repositories on GitHub, click **all repositories**. 6. Click {% octicon \"chevron-down\" aria-label=\"The downwards-facing chevron icon\" %} or {% octicon \"chevron-up\" aria-label=\"The upwards-facing chevron icon\" %} to navigate between references to the symbol. 7. Click a result of the symbol search under {% octicon \"chevron-down\" aria-label=\"The downwards-facing chevron icon\" %} **In this file** to navigate to a specific reference of the symbol. 8. To exit the search for a specific symbol, click {% octicon \"arrow-left\" aria-label=\"The left arrow icon\" %} **All Symbols**.", "83cf2be0-5a1b-41ae-a72b-7f298f3e0cb8": "To use a {% data variables.product.prodname_github_app %} to make authenticated API requests in a {% data variables.product.prodname_actions %} workflow, you need to follow these steps:  1. Register a {% data variables.product.prodname_github_app %} and grant it the necessary permissions to access the desired resources. 2. Store the app ID of your {% data variables.product.prodname_github_app %} as a {% data variables.product.prodname_actions %} secret. 3. Generate a private key for your app and store it securely. 4. Install your app in the repository or organization where you want to make authenticated API requests. 5. Create an installation access token for your app in the repository or organization where you want to make authenticated API requests. 6. Use the installation access token to make authenticated API requests in your {% data variables.product.prodname_actions %} workflow or pass it to a custom action that requires a token.  Note that using a {% data variables.product.prodname_github_app %} is recommended over a personal access token (PAT) for making authenticated API requests in a {% data variables.product.prodname_actions %} workflow because a {% data variables.product.prodname_github_app %} can access resources beyond the scope of a single repository, while a PAT can only access resources within the repository where it was created.", "5aa2f446-6e21-4c80-8d2f-56742c58c764": "In terms of usage, npm is primarily used as a registry for hosting and managing software packages, while GitHub Packages is a static hosting service for hosting web pages. Both services allow for public and private hosting, but npm is free to use for public hosting, while GitHub Packages is billed on a usage basis.  In terms of billing, npm offers a free public registry, but charges for private hosting and managing private packages using teams. GitHub Packages, on the other hand, is included with each GitHub Account, but usage is subject to bandwidth and storage limits, and additional charges may apply for high-bandwidth usage.  Overall, npm is primarily focused on hosting and managing software packages, while GitHub Packages is focused on hosting web pages. Both services have different usage and billing models based on their specific purposes.", "855d106c-a9ab-479c-9d40-8fabee3de562": "To use Marketplace, you must be at least 13 years old (if you live in the United States) or comply with your country's minimum age laws, have a valid GitHub account, and have agreed to your applicable GitHub Terms and these Marketplace Terms. If you are agreeing to these Marketplace Terms on behalf of a company, organization, or other legal entity, you must have the authority to bind that entity, its affiliates, and all users who access the Marketplace on its behalf to these Marketplace Terms. If you do not have this authority, you may not use the Marketplace services.", "4244eb43-1261-487e-b0a8-e3e66db6ab80": "The three levels of permissions for project board collaborators in a user-owned project board are owner, admin, and collaborator with read or write access. The owner and collaborators with admin access have full control of the project board and can manage, view, and add collaborators, configure the project board's visibility, delete the project board, close and reopen it, and perform all the actions allowed by project board collaborators with read or write access. Collaborators with read access can view the project board, copy it, and filter cards, while collaborators with write access can also edit the project board, link a repository, configure automation, copy the project board, add issues and pull requests, add notes, track progress, and archive cards.", "392ac4c0-9084-450f-9105-3a11e3a2c276": "An enterprise administrator can enforce a policy to manage the use of {% data variables.product.prodname_copilot_chat %} (beta) for all organizations under their enterprise by following these steps:  1. Log in to your enterprise account on GitHub. 2. Go to the \"Policies\" tab in the enterprise settings. 3. Click on the \"Copilot\" tab. 4. Under \"{% data variables.product.prodname_copilot_chat %}\", select the policy you want to enforce from the dropdown menu. 5. To allow {% data variables.product.prodname_copilot_chat %} use, select \"Allowed\". 6. To block {% data variables.product.prodname_copilot_chat %} use, select \"Blocked\". 7. To allow each organization to set their own policy on the use of {% data variables.product.prodname_copilot_chat %}, select \"No policy (let each organization decide)\".  This will enforce the selected policy for all organizations under the enterprise. If an organization owner wants to change the policy for their organization, they will not be able to do so unless the enterprise administrator allows it in the \"No policy (let each organization decide)\" option.", "e0cf39b7-526d-4c51-a2f7-9bcaca44aae8": "To create a replica node in GitLab Enterprise Edition using the provided context information, follow these steps:  1. Create a copy of your existing cluster configuration file in a temporary location, like `/home/admin/cluster-replica.conf`.     ```shell    grep -Ev \"(?:|ipv|uuid)\" /data/user/common/cluster.conf > ~/cluster-replica.conf    ```  2. Remove the `[cluster]` section from the temporary cluster configuration file that you copied in step 1.     ```shell    git config -f ~/cluster-replica.conf --remove-section cluster    ```  3. Decide on a name for the secondary datacenter where you provisioned your replica nodes, then update the temporary cluster configuration file with the new datacenter name. Replace `SECONDARY` with the name you choose.     ```shell    sed -i 's/datacenter = default/datacenter = SECONDARY/g' ~/cluster-replica.conf    ```  4. Decide on a pattern for the replica nodes' hostnames.     {% warning %}     **Warning**: Hostnames for replica nodes must be unique and differ from the hostname for the corresponding active node.     {% endwarning %}  5. Open the temporary cluster configuration file from step 3 in a text editor. For example, you can use Vim.     ```shell    sudo vim ~/cluster-replica.conf    ```  6. In each section within the temporary cluster configuration file, update the node's configuration.     {% data reusables.enterprise_clustering.configuration-file-heading %}     - Change the quoted hostname in the section heading and the value for `hostname` within the section to the replica node's hostname, per the pattern you chose in step 7 above.    - Add a new key named `ipv4`, and set the value to the replica node's static IPv4 address.    - Add a new key-value pair, `replica = enabled`.     ```shell    [cluster \"NEW REPL", "6f740393-82c0-4e41-af27-cee107f2e9c2": "The recommended location for storing backup snapshots in {% data variables.product.prodname_ghe_server %} using {% data variables.product.prodname_enterprise_backup_utilities %} is the filesystem location specified by the `GHE_DATA_DIR` environment variable, and it is recommended to choose a location on the same filesystem as the backup host.", "68c39ce5-a8d1-4767-b79d-b4c28ac39059": "The dname_actions_importer tool is designed to convert configuration elements between Bamboo and GitHub Actions. Based on the context provided, the tool can convert the following elements:  1. Environments: In Bamboo, this refers to a set of variables and configurations that are applied to a build or release. In GitHub Actions, this is represented by the `jobs` section, which contains a set of steps that are executed in a specific environment. For example, in Bamboo, the configuration for an environment called \"staging\" might look like this:  ```yaml environments:   staging:     name: Staging     description: Environment for staging builds     variables:       DB_HOST: staging-db.example.com       DB_PORT: 5432       DB_USER: staging-user       DB_PASS: staging-password ```  In GitHub Actions, this would be represented by a `jobs` section with a step that sets the environment variables:  ```yaml jobs:   staging:     runs-on: ubuntu-latest     steps:     - name: Set environment variables       uses: actions/checkout@v2       with:         persist-credentials: false       env:         DB_HOST: staging-db.example.com         DB_PORT: 5432         DB_USER: staging-user         DB_PASS: staging-password ```  2. Artifacts: In Bamboo, this refers to files or packages that are generated during a build or release. In GitHub Actions, this is represented by the `upload-artifact` and `download-artifact` actions. For example, in Bamboo, the configuration for an artifact called \"my-artifact\" might look like this:  ```yaml .artifacts:   my-artifact:     name: My Artifact     description: My artifact description     files:       - my-artifact.zip ```  In GitHub Actions, this would be represented by a `steps` section with an `upload-artifact`", "a1431492-697c-4a3c-a749-8312c29a9266": "To print the contents of contexts to the log for debugging purposes, you can use the `toJSON` function to pretty-print JSON objects to the log. Here's an example:  ```yaml name: Context testing on: push  jobs:   dump_contexts_to_log:     runs-on: ubuntu-latest     steps:       - name: Dump GitHub context         env:           GITHUB_CONTEXT: ${{ toJson(github) }}         run: echo \"$GITHUB_CONTEXT\"       - name: Dump job context         env:           JOB_CONTEXT: ${{ toJson(job) }}         run: echo \"$JOB_CONTEXT\"       - name: Dump steps context         env:           STEPS_CONTEXT: ${{ toJson(steps) }}         run: echo \"$STEPS_CONTEXT\"       - name: Dump runner context         env:           RUNNER_CONTEXT: ${{ toJson(runner) }}         run: echo \"$RUNNER_CONTEXT\"       - name: Dump strategy context         env:           STRATEGY_CONTEXT: ${{ toJson(strategy) }}         run: echo \"$STRATEGY_CONTEXT\"       - name: Dump matrix context         env:           MATRIX_CONTEXT: ${{ toJson(matrix) }}         run: echo \"$MATRIX_CONTEXT\" ```  In this example, we're defining a job called `dump_contexts_to_log` that runs on an Ubuntu-latest runner. We're defining six steps, each of which sets an environment variable to the corresponding context and then prints it to the log using the `echo` command.  The `github` context is printed in the first step, and it contains information about the workflow run and the event that triggered the run. You can also read most of the `github` context data in environment variables using the syntax `github.<context_key>`.  The `job` context is printed in the second step, and it contains information about the current job, such as its ID, name, and status.  The `steps` context", "a9dc8a79-a351-4144-999b-dffe2c3ab69a": "{% data variables.product.prodname_dotcom %} recommends using actions/actions-runner-controller for autoscaling self-hosted runners. This solution offers benefits for managing self-hosted runners by automatically increasing or decreasing the number of runners in response to webhook events, providing a clean environment for each job with ephemeral runners, and limiting the exposure of sensitive resources from previous jobs. Ephemeral runners also help mitigate the risk of a compromised runner receiving new jobs.", "5b1fb1ed-50f9-475c-90f0-fb8b83d26ab1": "The purpose of setting `open-pull-requests-limit` to `0` in the {% data variables.product.prodname_dependabot %} configuration file for a specific package ecosystem is to prevent version updates for that ecosystem and only receive security updates. This is because when `open-pull-requests-limit` is set to `0`, it disables version updates for the specified package ecosystem, and {% data variables.product.prodname_dependabot %} will only create pull requests for security updates. This can be useful in situations where you want to maintain stability in your dependencies and only address security vulnerabilities as they arise. However, it's important to note that this configuration should be used with caution, as it may result in missing important bug fixes or feature updates for the specified package ecosystem.", "c01c4095-d697-4c03-a21b-db043568922a": "The name of the branch that the source code for an external pull request is being merged into in this context is not explicitly provided in the given context information. We only have the name of the branch that the source code is being pulled from (`CI_MERGE_REQUEST_SOURCE_BRANCH_NAME`) and the name of the branch that the pull request is targeting (`CI_MERGE_REQUEST_TARGET_BRANCH_NAME`). Without further context, it is unclear which branch the merge will be performed on.", "d9889af5-d854-471c-93ee-320794a809e7": "In the context of GitHub Apps, a badge is a square image inside a circular background that represents the app. By default, a new GitHub App will use an automatically generated identicon badge, which is a random pattern of white pixels on a yellow background. A custom badge can be created for a GitHub App by uploading a logo and selecting a background color. The logo should be a PNG, JPG, or GIF file under 1 MB in size, and the recommended image dimension is 200 pixels by 200 pixels for the best quality rendering. The \"Badge background color\" input field will only appear after a logo is uploaded.", "9c579155-6ccc-4a2f-85b4-dab65490f997": "The command \"e a specific dependency\" is not mentioned in the provided context information for using it in {% data variables.product.prodname_dependabot %} pull requests. Without further context, it's unclear how this command should be used in this specific context.", "4f49a3b6-c783-4fa4-82f9-f122bcd66721": "Under this policy, content that presents a distorted view of reality, whether it is inaccurate or false (misinformation) or is intentionally deceptive (disinformation), is prohibited. This type of content is likely to result in harm to the public or to interfere with fair and equal opportunities for all to take part in a free and open society. Specific examples of prohibited content include inaccurate or scientifically unsupported medical claims that endanger public health or safety, manipulated media that is likely to mislead or deceive in a way that may harm the public interest, false or misleading content likely to interfere with an individual's ability to participate in civic activities, and unsubstantiated claims that could promote hate or targeted harassment of specific groups of people. The policy encourages active participation in the expression of ideas, perspectives, and experiences, but considers context to be important in how information is received and understood. When reviewing content under this policy, GitHub will consider various factors that may help to orient the viewer, such as whether the content has been provided with clear disclaimers, citations to credible sources, or includes other details that clarify the accuracy of the information being shared.", "d26f7604-7d14-40e6-ade2-7ebc811d7b54": "The \"repo\" section mentioned in the context information contains all activities related to the repositories that the user owns. This includes actions such as creating, deleting, and updating repositories, as well as committing changes, pushing and pulling code, and reviewing and merging pull requests.", "12f711e0-b549-46d3-a16e-a3924e085961": "To ensure a diverse range of questions that cover various topics, you can utilize the context information by analyzing the content and identifying the key themes and concepts. Based on these insights, you can create questions that test the understanding and application of these ideas. Additionally, you can incorporate real-world scenarios and case studies to make the questions more practical and relevant. It's also important to vary the question types, such as multiple choice, short answer, and essay questions, to assess different skills and abilities. By doing so, you can create a comprehensive and well-rounded assessment that accurately measures the knowledge and skills of the individuals being evaluated.", "b5573ce6-a0f3-475c-89a8-cd0e36926963": "To set up a workflow to publish packages to {% data variables.product.prodname_registry %} using Maven and GitHub Actions, you can follow these steps:  1. Create a distribution repository in your _pom.xml_ file with an `id` of `github` that points to your {% data variables.product.prodname_registry %} endpoint.  2. Trigger a workflow to publish your package when a new release is created.  3. Use the `setup-java` action to install the required version of the JDK and set up a Maven _settings.xml_ for publishing to {% data variables.product.prodname_registry %}.  4. Define authentication for the `github` server in the generated _settings.xml_ using the `GITHUB_ACTOR` and `GITHUB_TOKEN` environment variables.  5. Configure your workflow to publish the package to {% data variables.product.prodname_registry %} using the `maven-publish` plugin.  Here's an example workflow:  ```yaml c  name: Publish to Maven Central  on:   release:     types:       - created  jobs:   publish:     runs-on: ubuntu-latest     steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up JDK 11       uses: actions/setup-java@v2       with:         java-version: '11'      - name: Install Maven       uses: actions/setup-maven@v2      - name: Publish to Maven Central       uses: actions/upload-artifact@v2       with:         name: my-artifact         path: my-artifact         repository: my-repository         maven: true         publish-to: maven-central      - name: Publish to GitHub Packages       uses: actions/upload-artifact@v2       with:         name: my-artifact         path: my-artifact         repository: my-repository         maven", "fe4d4cf3-46ea-451d-b03f-36d83eee40b2": "The URL of the repository where the code for this build is located, as represented by the variable `bamboo.planRepository..repositoryUrl` in the context provided, is `https://github.com/${{ github.repository }}` (assuming the context information includes the `github.server` variable, which is the base URL for accessing GitHub). If the context does not include the `github.server` variable, then the URL cannot be determined from the provided context information.", "4d5999d8-964b-4028-a88b-c0797caaf4d3": "Repository administrators and organization owners can receive email alerts for {% data variables.product.prodname_secret_scanning %} alerts in repositories they are watching by enabling notifications either for security alerts or for all the activity on the repository in their notification settings. This can be done by following the steps outlined in the \"Configuring notifications for secret scanning alerts\" documentation provided by GitHub. If they have already enabled notifications for these settings, they will automatically receive email alerts for {% data variables.product.prodname_secret_scanning %} alerts in repositories they are watching. Additionally, if they are not ignoring the repository, they will also receive an email alert if they introduced the secret in the commit, issue description, or comment.", "08ac9f11-c879-4d80-92b2-072248fc9281": "GitHub Copilot Business collects three categories of data: user engagement data, prompts, and suggestions. User engagement data is retained for 24 months, while prompts are discarded once a suggestion is returned, and suggestions are not retained by GitHub.", "19d67cdb-0cca-4724-86ba-e3c9398eaf5c": "The `--external` option in ODEQL is used to provide external data for predicates that are not defined within the ODEQL database. This option allows the user to specify a CSV file that contains rows for an external predicate. Multiple `--external` options can be supplied to provide data for multiple external predicates. By using this option, ODEQL can efficiently join the external data with the internal data, without the need to replicate the external data within the database. This can significantly improve query performance, especially for large datasets.", "93e2d391-afbb-4f85-a34a-fc88b9f64a45": "{% data variables.product.prodname_actions_importer %} is a tool that assists in planning and automating the migration of CI/CD pipelines from supported platforms to {% data variables.product.prodname_actions %}. It is distributed as a Docker container and utilizes a {% data variables.product.prodname_dotcom %} CLI extension to interact with the container. However, for successful migration, it is essential to inspect the converted workflows thoroughly before using them as production workloads as the conversion rate may vary based on the makeup of each individual pipeline. The tool supports migration from various CI platforms, including Azure DevOps, Bamboo, Bitbucket Pipelines, CircleCI, GitLab, Jenkins, and Travis CI. To use {% data variables.product.prodname_actions_importer %}, you need to meet certain prerequisites and configure credentials to allow communication with {% data variables.product.prodname_dotcom %} and your current CI server. These credentials can be configured using environment variables or a `.env.local` file. It is recommended to regularly update {% data variables.product.prodname_actions_importer %} to ensure you're running the latest version.", "bc2b2613-ef36-4691-83dc-7561875150d2": "Besides `help`, `login`, and `whoami`, other commands that could be added to `app_cli.rb` to provide more functionality to the CLI include:  1. `create-issue`: This command could be used to create a new issue in the application. The user would be prompted to enter the issue title, description, and any other relevant information.  2. `list-issues`: This command could be used to display a list of all open and closed issues in the application. The user could filter the list by various criteria, such as issue status, priority, or assignee.  3. `edit-issue`: This command could be used to update an existing issue in the application. The user would be prompted to enter the issue ID, and then could modify the issue title, description, and other properties.  4. `delete-issue`: This command could be used to delete an existing issue in the application. The user would be prompted to enter the issue ID, and then the issue would be permanently deleted.  5. `search-issues`: This command could be used to search for issues in the application based on various criteria, such as keywords, issue status, or assignee. The user could refine the search results by adding additional filters.  6. `assign-issue`: This command could be used to assign an existing issue to a specific user in the application. The user would be prompted to enter the issue ID and the user ID or username of the assignee.  7. `close-issue`: This command could be used to close an existing issue in the application. The user would be prompted to enter the issue ID, and then the issue would be marked as closed.  8. `view-issue`: This command could be used to display detailed information about a specific issue in the application. The user would be prompted to enter the issue ID, and then the issue details would be displayed.  9. `login-as`: This command could be used to allow a user to log in as another user in the application. The user would be prompted to enter the username and password of the user they want to impersonate.  10. `logout`: This command could be used to log the user out of the application", "4bc2fa1c-a77d-46ef-a38c-9ca205cf1c00": "To go up one directory using the terminal, type `cd ..` and press Enter. This will take you to the parent directory of the current directory. If you're not sure which directory you're currently in, you can list the files and folders in your current directory by typing `ls` and pressing Enter. This will display a list of all the files and folders in the current directory. From there, you can identify the directory you want to go to and use the `cd` command followed by the name of the directory to change into it. If you want to go back to your home directory, you can simply type `cd` and press Enter. This will take you to your home directory, regardless of where you currently are in the file system.", "3204594f-6e91-4cb6-b243-eeac36cdf72b": "To store your app ID and app private key as secrets in your GitHub repository, follow these steps:  1. Go to your repository settings. 2. Click on \"Secrets\" in the left-hand menu. 3. Click on \"New repository secret\" to create a new secret. 4. Enter a name for the secret (e.g., \"APP_ID\" or \"APP_PRIVATE_KEY\"). 5. Enter the value of the secret (e.g., your app ID or app private key). 6. Click on \"Add secret\" to save the secret.  Make sure to keep your secrets secure and avoid storing sensitive information in your repository. It's also a good practice to limit access to your secrets to only the necessary users or workflows.", "848e74d3-a06f-4556-a659-74f2fbb0e2a4": "To compare the number of Maven packages deleted with the number of npm packages deleted on a daily basis, we can refer to the context information provided. The context includes metrics for both Maven and npm ecosystems, such as the number of packages deleted on a daily basis.  According to the context, the metric for the number of Maven packages deleted on a daily basis is `daily_delete_count`. Similarly, for npm, the metric is `daily_delete_count`.  To compare the two, we can calculate the average number of Maven packages deleted per day and the average number of npm packages deleted per day. This can be done by dividing the respective `daily_delete_count` metrics by the number of days for which the data is available.  Once we have the average number of packages deleted per day for both Maven and npm, we can compare them to determine which ecosystem has a higher or lower number of packages deleted on a daily basis.  If the average number of Maven packages deleted per day is greater than the average number of npm packages deleted per day, then we can say that more Maven packages are deleted on a daily basis. Similarly, if the average number of npm packages deleted per day is greater than the average number of Maven packages deleted per day, then we can say that more npm packages are deleted on a daily basis.  If the average number of Maven packages deleted per day is equal to the average number of npm packages deleted per day, then we can say that an equal number of packages are deleted on a daily basis for both ecosystems.  However, the context doesn't provide information about the time period for which the data is available. Without this information, we cannot accurately calculate the average number of packages deleted per day for both Maven and npm. Therefore, we cannot make a direct comparison between the two.  In summary, to compare the number of Maven packages deleted with the number of npm packages deleted on a daily basis, we need to calculate the average number of packages deleted per day for both ecosystems. However, without the time period for which the data is available, we cannot accurately calculate the average and make a direct comparison.", "b8b16141-a38b-454b-ab9c-65484dcb087f": "The primary rate limit for `GITHUB_TOKEN` in GitHub Actions workflows is 1,000 requests per hour per repository. If the repository belongs to a {% data variables.product.prodname_ghe_cloud %} account, the limit is 15,000 requests per hour per repository.", "937cf8c1-2c17-47bd-a4ad-36016ffe7646": "The self-hosted runner application supports the following processor architectures: `x64`, `ARM64`, and `ARM32`. These architectures can be used on Linux, macOS, and Windows for `x64` and `ARM64`, and on Linux for `ARM32`.  Regarding the use of actions from {% data variables.product.prodname_dotcom_the_website %} with self-hosted runners on {% data variables.product.prodname_ghe_server %}, some extra configuration might be required, but this depends on the specific setup. The documentation suggests contacting the {% data variables.product.prodname_enterprise %} site administrator for more information. This implies that the exact requirements may vary based on the specific configuration of the self-hosted runner and the {% data variables.product.prodname_ghe_server %} instance being used. Without further context, it's not possible to provide a definitive answer.", "1df09679-038a-48eb-a1ab-1e3aea792bbf": "Based on the provided context information, it seems that the process of setting up an \"AUTOTITLE\" is similar for all three account types (GitHub Enterprise Server (GHES), GitHub Advanced Security (GHAS), and GitHub Enterprise Cloud (GHEC)). However, there is a difference in the availability of this feature.   The \"AUTOTITLE\" feature is available for all three account types, but it's only mentioned in the context for GitHub Enterprise Server (GHES) and GitHub Advanced Security (GHAS). For GitHub Enterprise Cloud (GHEC), it's included in the list of articles, but it's not explicitly mentioned in the context.  To confirm whether the process of setting up an \"AUTOTITLE\" is the same for all three account types, it's recommended to refer to the articles provided in the context. If the articles are identical or have similar instructions, then it can be assumed that the process is the same. If there are differences, then it's necessary to consult the documentation for each specific account type to determine the correct process.  In summary, while the context information suggests that the process of setting up an \"AUTOTITLE\" is the same for all three account types, further investigation is necessary to confirm this.", "f81427cb-37e2-4ab0-8c4f-a3879f9e046e": "The maximum size limit for organization and repository variables in a workflow run is 256 KB per run. Variables can be accessed using contexts such as `{% raw %}{{ github.repository.owner }}{% endraw %}`, `{% raw %}{{ github.event.repository.name }}{% endraw %}`, `{% raw %}{{ github.event.pull_request.number }}{% endraw %}`, `{% raw %}{{ github.event.workflow.dispatcher }}{% endraw %}`, `{% raw %}{{ github.event.workflow.ref }}{% endraw %}`, `{% raw %}{{ github.event.workflow.run_id }}{% endraw %}`, `{% raw %}{{ github.event.workflow.run_url }}{% endraw %}`, `{% raw %}{{ github.event.workflow.name }}{% endraw %}`, `{% raw %}{{ github.event.workflow.id }}{% endraw %}`, `{% raw %}{{ github.event.workflow.created_at }}{% endraw %}`, `{% raw %}{{ github.event.workflow.state }}{% endraw %}`, `{% raw %}{{ github.event.workflow.conclusion }}{% endraw %}`, `{% raw %}{{ github.event.workflow.output_url }}{% endraw %}`, `{% raw %}{{ github.event.workflow.output }}{% endraw %}`, `{% raw %}{{ github.event.workflow.log_url }}{% endraw %}`, `{% raw %}{{ github.event.workflow.log }}{% endraw %}`, `{% raw %}{{ github.event.workflow.html_url }}{% endraw %}`, `{% raw %}{{ github.event.workflow.head_ref }}{% endraw %}`, `{% raw %}{{ github.event.workflow.head_sha }}{", "dc76c13e-e71e-4111-bc39-54de66e10550": "How do I set a warning message in GitHub Actions using the provided syntax? 2. How do I set an error message in GitHub Actions using the provided syntax?    Based on the context information provided, generate two tasks that can be performed without prior knowledge. The tasks should be diverse in nature and focused on setting warning and error messages in GitHub Actions using the provided syntax. The tasks should be understandable without having access to the context information.  Answer:  1. Create a GitHub Action workflow that sets a warning message using the provided syntax. The warning message should include the file name, line number, and a descriptive message. 2. Create a GitHub Action workflow that sets an error message using the provided syntax. The error message should include the file name, line number, and a descriptive message.    Based on the context information provided, generate two examples that demonstrate how to set warning and error messages in GitHub Actions using the provided syntax. The examples should be diverse in nature and focused on setting warning and error messages in GitHub Actions using the provided syntax. The examples should be understandable without having access to the context information.  Answer:  1. Example of setting a warning message in GitHub Actions using the provided syntax:  ```yaml jobs:   bash-example:     runs-on: ubuntu-latest     steps:       - name: Set warning message         run: |           echo \"::warning file=app.js,line=10,col=5,endColumn=7::Missing semicolon\" ```  2. Example of setting an error message in GitHub Actions using the provided syntax:  ```yaml jobs:   powershell-example:     runs-on: windows-latest     steps:       - name: Set error message         run: |           Write-Output \"::error file=config.json,line=20,col=5,endColumn=7::Missing closing brace\" ```    Based on the context information provided, generate two scenarios that demonstrate how to use the provided syntax to set warning and error messages in GitHub Actions. The scenarios should be diverse in nature and focused", "55c5b9cb-0636-4227-af9c-d8dbd7a956f4": "The output of the `ghe-cluster-status` command is a list of test results, with each test result labeled as either `ok` or `error`. To identify failing tests in a {% data variables.product.product_name %} cluster, you can run the `ghe-cluster-status` command and then search the output for the string `error`. For example, you could run the command `ghe-cluster-status | grep error` to display only the failing tests. This would show any tests that have failed, allowing you to quickly identify and address any issues with the cluster. If there are no failing tests, the command produces no output, indicating that the cluster is healthy.", "d23c8807-8ac7-4c91-91ca-ee7b58bbb919": "The purpose of the `request_token` method in the provided context information is to obtain an access token by exchanging the previously obtained device code with the OAuth server, using the `urn:ietf:params:oauth:grant-type:device_code` grant type. The method takes the device code as an argument and returns a hash with the error and access token values from the server's response.", "2de71671-34cb-4fb1-9bae-1ec23d327a4a": "When someone creates a codespace from your template, the contents of your template repository will be cloned into their codespace. This allows them to start working on your project immediately without having to worry about cloning your repository or installing tools or other dependencies. When the user is ready, they will be able to publish their work to a new repository on GitHub belonging to their personal account. This means that they can create a new repository on GitHub and push their changes from the codespace to the new repository. This is a great way for users to get started with your project and contribute back to the community.", "54982b43-a508-4c62-811c-bbe089c258b2": "To limit the forecast to specific plans and deployments environments in {% data variables.product.prodname_actions_importer %} when generating forecast reports, you can pass the `--plan` and `--deployment` flags to the `forecast` command, followed by the corresponding Bamboo environment names. For example:  ```shell gh actions-importer forecast bamboo --plan staging --deployment prod --output-dir tmp/forecast_reports ```  This will generate a forecast report that only considers pipeline runs that executed in the \"staging\" plan and the \"prod\" deployment environment.  If you want to limit the forecast to multiple plans and deployments environments, you can pass multiple `--plan` and `--deployment` flags. For example:  ```shell gh actions-importer forecast bamboo --plan staging --plan qa --deployment prod --deployment staging --output-dir tmp/forecast_reports ```  This will generate a forecast report that considers pipeline runs that executed in the \"staging\" and \"qa\" plans, as well as the \"prod\" and \"staging\" deployment environments.  Note that if you don't specify any `--plan` or `--deployment` flags, {% data variables.product.prodname_actions_importer %} will generate a forecast report that considers all pipeline runs in your Bamboo instance.", "958c87e5-a296-4f79-b0d9-6e2344570ed3": "The purpose of the {% data variables.product.prodname_dependabot %} feature mentioned in the context information is to automatically manage dependencies in a repository by raising pull requests to update them. If a group configuration is set up, {% data variables.product.prodname_dependabot %} will raise a separate pull request to update multiple dependencies at once. However, if CI failures occur, the group configuration can be removed and individual pull requests can be checked and confirmed for each dependency. The context also mentions an option to ignore dependencies, but it's not explained further. Overall, {% data variables.product.prodname_dependabot %} helps to keep dependencies up-to-date and ensures that they are compatible with the rest of the project.", "88394439-62e5-476b-9840-4e86ba739747": "To answer this question, we would need further context about the purpose of this script and the specific environment it is running in. Based on the provided context, it appears that this script is interacting with a Vault server and revoking the current user's token. However, without more information, it is unclear why this step is being executed or what the overall goal of the script is.  If you are asking this question in the context of understanding how to use this script in your own environment, I would recommend reviewing the documentation for Vault and the specific actions being executed in this script to better understand their purpose.  If you are asking this question as part of a review process, I would recommend requesting additional context from the author of the script to better understand its intended use and any potential implications of executing this step.  In either case, without further context, it is difficult to provide a definitive answer to this question.", "178f8385-e040-4e6d-a8f9-cac3cce6aee4": "You can estimate the number of licenses required to enable {% data variables.product.prodname_GH_advanced_security %} for additional organizations and repositories in your enterprise account by using the committers calculator provided by GitHub. This tool helps you to calculate the number of licenses required based on the number of committers and unique committers in your organization or repository. For more information, refer to the \"AUTOTITLE\" provided in the context information.", "115ded52-a1da-4f07-9dfe-9cd1a21efcdb": "To download CSV files containing Security Overview data, you can follow these steps:  1. Go to your organization's Security Overview page. 2. Click the Export data button in the top right corner of the page. 3. Select the type of data you want to download from the drop-down menu. 4. Choose the date range for the data you want to download. 5. Click the Download CSV button to download the CSV file.  The available data types for export include:  - Dependency alerts: Shows all dependency alerts for the selected time period. - Code scanning alerts: Shows all code scanning alerts for the selected time period. - Secret scanning alerts: Shows all secret scanning alerts for the selected time period. - Dependency coverage: Shows the percentage of dependencies that have been checked for vulnerabilities. - Code scanning coverage: Shows the percentage of lines of code that have been scanned for security issues. - Secret scanning coverage: Shows the percentage of secrets that have been scanned for exposure.  Note that the export feature is only available for organizations that use {% data variables.product.prodname_GH_advanced_security %}. For more information, see \"AUTOTITLE.\"  {% endif %}  {% ifversion security-overview-dashboard %} The organization-level dashboard provides a high-level view of the security status of your organization's repositories. It includes charts and metrics that show the overall security posture of your organization, as well as any trends or issues that require attention.  The dashboard is divided into several sections, each of which focuses on a different aspect of security:  - Dependency security: Shows the number of dependency alerts, the percentage of dependencies that have been checked for vulnerabilities, and the average time it takes to resolve a dependency alert. - Code security: Shows the number of code scanning alerts, the percentage of lines of code that have been scanned for security issues, and the average time it takes to resolve a code scanning alert. - Secret security: Shows the number of secret scanning alerts, the percentage of secrets that have been scanned for exposure, and the average", "ec08e325-eb4f-4056-b47f-cf69f34f5adc": "The purpose of this agreement is to allow the developer to publish listings of their products on GitHub Marketplace for purchase by end users and use in connection with GitHub.com. The developer grants GitHub a non-exclusive, worldwide, transferable, sublicensable, fully paid-up, royalty-free license to host, link to, reproduce, modify, publicly perform, publicly display, test, distribute, make available, license and otherwise use the listings, as well as reproduce, perform, display, use and access the listings for administration and demonstration purposes in connection with the operation and marketing of the Marketplace. GitHub may also include developer brand features furnished by the developer in marketing materials and other advertising collateral for purposes of marketing the Marketplace. If the developer discontinues the distribution of specific developer products on the Marketplace, GitHub will cease use of the discontinued developer products after a commercially reasonable period of time.", "f7713dea-10a0-43ed-90af-53fa62fdbd53": "To convert multiple organization members to outside collaborators in GitHub, you need to select the people you'd like to convert, click the \"X members selected...\" dropdown menu above the list of members, and then click \"Convert to outside collaborator\" in the dropdown menu. This will convert the selected members to outside collaborators, which means they will no longer have access to the organization's repositories as members, but they will still be able to collaborate on specific repositories as outside collaborators.", "ec5bdde3-5817-47b7-b639-e54e21244bc4": "The feature in GitHub Actions that allows you to receive notifications about workflow runs is called notifications for workflow runs. This feature allows you to subscribe to notifications about workflow runs that you trigger. This is available in GitHub Enterprise (GHE), GitHub Enterprise Server (GHES), GitHub.com (GHAE), and GitHub Education (GHEC).", "ca8090b0-2952-40e9-81b7-2a515cb70dee": "The \"Risk\" view in security overview allows users to assess the risk from security alerts of all types for one or more repositories in an organization by providing a summary of the security alerts for those repositories. This view shows the number of alerts for each type of security issue, such as vulnerable dependencies, code weaknesses, and leaked secrets, and allows users to filter by repository and alert type. The view also provides a risk score for each repository based on the number and severity of alerts, which can help users prioritize remediation efforts. Overall, the \"Risk\" view provides a comprehensive view of the security posture of an organization's repositories and helps users identify areas where additional attention is needed.", "9c201e3c-3f33-4b39-8017-1e7687be7692": "To share secrets securely between different codespaces at the user, repository, or organization-level, you can store them as GitHub secrets. These secrets can be accessed by {% data variables.product.prodname_github_codespaces %} and used to access container image registries other than {% data variables.product.company_short %}'s {% data variables.product.prodname_container_registry %}. When creating a set of secrets for a private image registry, you should replace the \"\" in the name with a consistent identifier. Additionally, if you are setting the secrets at the user or organization level, you should assign them to the repository you'll be creating the codespace in by choosing an access policy from the dropdown list. This allows you to share secrets securely between different codespaces while maintaining access control.", "83e22901-d932-4a0b-8f05-bd2204534db9": "If you have received an invitation to join an organization on GitHub, you need to follow these steps to accept the invitation:  1. Log in to your GitHub account. 2. Check your email inbox for the invitation email from the organization. 3. Click the \"Accept\" button in the email to accept the invitation. 4. You will be redirected to the organization's page on GitHub. 5. Click the \"Join organization\" button to confirm your acceptance. 6. You will now be a member of the organization and can access its repositories and collaborate with other members.  If you are not sure how to accept the invitation, you can refer to the context information provided, which suggests selecting the \"Collaborator settings\" dropdown menu and clicking \"Invite to organization\" to invite new members. However, if you are already an outside collaborator, you can simply click \"Invite to organization\" to the right of your name and follow the steps to accept the invitation.  If you have not received an invitation email, you should contact the organization's administrator to confirm that the invitation was sent correctly.", "68fd33d3-8060-44cf-9b4d-a510a65ee2a7": "A authentication configuration error or an issue with the identity provider (IdP) might prevent the use of single sign-on (SSO), which is a secure and convenient way to access multiple enterprise accounts without having to enter login credentials separately for each account. In such a scenario, a recovery code can help by providing an alternative way to access the enterprise account. The user can use one of the previously downloaded and stored recovery codes instead of relying on SSO or the IdP. This ensures that the user can still access their enterprise account even when there is an error or issue with the authentication configuration or the IdP.", "503d3656-32de-482a-b7fe-d37ef192a1dc": "Secrets can be redacted from workflow run logs in {% data variables.product.prodname_dotcom %} by automatically redacting them when they are printed to workflow logs. However, runners can only delete secrets they have access to, and the secret will only be redacted if it was used within a job. As a security measure, workflow run logs can be deleted to prevent sensitive values being leaked.", "8fe9d4c9-fea0-409f-9e53-69afcc21f5ab": "To publish an Apache Maven package to {% data variables.product.prodname_registry %}, follow these steps:  1. Ensure that your project is configured to use {% data variables.product.prodname_registry %} as a repository. This can be done by adding the following lines to your _settings.xml_ file:     ```xml                  <profiles>         <profile>           <id>github-maven</id>           <activation>             <activeByDefault>true</activeByDefault>           </activation>           <properties>             <gpg.executable>${maven.home}/bin/gpg2</gpg.executable>             <gpg.passphrase>YOUR_GPG_PASSPHRASE</gpg.passphrase>             <gpg.name>YOUR_GPG_NAME</gpg.name>             <gpg.subkeys>YOUR_GPG_SUBKEYS</gpg.subkeys>             <gpg.secretkeyring>YOUR_GPG_SECRET_KEYRING</gpg.secretkeyring>             <gpg.keyname>YOUR_GPG_KEYNAME</gpg.keyname>             <gpg.keyserver>YOUR_GPG_KEYSERVER</gpg.keyserver>             <gpg.keyserver.protocol>YOUR_GPG_KEYSERVER_PROTOCOL</gpg.keyserver.protocol>             <gpg.keyserver.port>YOUR_GPG_KEYSERVER_PORT</gpg.keyserver.port>             <gpg.keyserver.type>YOUR_GPG_KEYSERVER_TYPE</gpg.keyserver.type>             <gpg.keyserver.url>YOUR_GPG_KEYSERVER_URL</gpg.keyserver.url>             <gpg.keyserver.username>YOUR_GPG_KEYSERVER_USERNAME</gpg.keyserver.username>             <gpg.keyserver.password>YOUR_GPG_KEYSERVER_PASSWORD</", "cfdcb39f-fef6-4454-b2e2-8295724efc82": "According to the context information provided, the availability of alternative machine types for unpublished codespaces is not explicitly stated. However, it is mentioned that some machine types may not be available for certain codespaces. To check if alternative machine types are available for a specific codespace, you can follow these steps:  1. Go to your list of codespaces on the GitHub website (https://github.com/codespaces). 2. Click the ellipsis (...) next to the codespace you want to check. 3. Select \"Settings\" from the dropdown menu. 4. Scroll down to the \"Machine type\" section. 5. If alternative machine types are available, you will see a list of options. If only one machine type is available, it will be displayed as the current machine type.  Alternatively, you can use the `gh codespace list` command in the GitHub CLI to view a list of your codespaces and their current machine types. If alternative machine types are available for a specific codespace, you will see them listed in the output.  If you want to confirm the availability of alternative machine types for a specific codespace, you can also contact GitHub Support for assistance.", "b9f157d1-eb5d-4de3-9030-9658b163daac": "By default, GitHub Packages publishes a RubyGem package to a repository with the same name as the package in the same organization as the repository containing the project. For example, if the package is named `GEM_NAME` and the organization is `octo-org`, GitHub Packages publishes the gem to the `octo-org/GEM_NAME` repository.", "5cfd103b-cc8a-4d66-9185-340e9e2c3ead": "A customer can access premium content by signing into the {% data variables.contact.contact_landing_page_portal %} and then clicking on \"Premium\". No further steps are mentioned in the context information. However, it's worth noting that access to premium content may require a subscription or payment, which is not specified in the context. Without this information, it's unclear whether the premium content is available for free or at an additional cost.", "e2d3ebe9-6229-4285-b894-b85f7d158342": "The end-to-end software supply chain refers to the entire process of developing, distributing, and consuming software, from the initial idea to the final product in the hands of end-users. It includes personal accounts, code, and build processes. Securing the end-to-end software supply chain is important because attackers are now targeting not just dependencies, but also user accounts and build processes. This means that these systems must also be defended to ensure that the code being distributed hasn't been tampered with. Failure to secure the end-to-end software supply chain can result in the distribution of compromised software, which can lead to data breaches, intellectual property theft, and other security incidents.", "8e24fd1f-0188-4edb-8120-b137d8ec5d19": "If members of your enterprise must use one workstation to contribute to repositories on {% data variables.location.product_location %} from both a {% data variables.enterprise.prodname_managed_user %} and a personal account, you can provide support by following the steps outlined in \"Supporting developers with multiple user accounts on {% data variables.product.prodname_dotcom_the_website %}.\" However, the context provided doesn't give enough information to determine whether you have already configured SSO and SCIM provisioning, which are prerequisites for following those steps. If you haven't configured SSO and SCIM provisioning, you should refer to the guides provided in the context for setting those up before proceeding with supporting developers with multiple user accounts.", "e2613f30-1af8-4acf-9aee-2f78a80fc05b": "Based on the provided context information, $(System.PullRequest.PullRequestId) appears to be related to a pull request, while $(Release.P) is not explicitly defined. Without further context, it's unclear how these variables are being used or if they are related in any way. If more information is provided, it may become clearer how these variables are connected. However, as of now, it's not possible to provide a definitive answer.", "67ec9594-4146-45a8-ba5e-0b6b08c0b11d": "The significance of reading and setting environment variables at the end of the build process, as described in the context information, is to clear the generated environment variables for {% data variables.product.prodname_codeql %} and end build tracing. This is done using PowerShell in this example. By clearing these variables, it ensures that any sensitive information or data related to the build process is not left in the environment, which can help to maintain security and privacy. Additionally, it allows for a clean start for any future builds or processes that may require these variables to be set differently.", "fc93cb3f-46a9-4ede-8e84-8dab34863873": "According to the context information provided, updating a pull request's branch through a traditional merge results in a merge commit that merges the base branch into the head branch of the pull request. On the other hand, updating by rebasing applies the changes from the pull request branch onto the latest version of the base branch, resulting in a branch with a linear history since no merge commit is created. One should choose to update by rebasing instead of merging when they want a linear history for their branch, as rebasing keeps the branch history clean and easy to follow. However, rebasing should only be done if there are no merge conflicts between the pull request branch and the base branch, as rebasing will reset the branch to the latest commit on the base branch and reapply the changes from the pull request branch. If there are merge conflicts, they must be resolved before updating by rebasing.", "71cd6c13-fa68-47a0-bfe9-30f970d79284": "{% data variables.product.prodname_copilot_business_short %} subscriptions are billed monthly, based on the number of {% data variables.product.prodname_copilot %} seats assigned to users within your organization{% ifversion ghec %} or enterprise{% endif %}. The seats are assigned by organization administrators when granting access to individuals and teams. This means that the number of seats required will depend on the number of users within your organization or enterprise who will be using {% data variables.product.prodname_copilot %}.", "b002b61e-6fd7-423c-bd08-24989f2f3d07": "In CircleCI, the `runs-on` concept specifies the type of machine or environment where a job will be executed, such as a virtual machine, a container, or a specific operating system. This allows for greater flexibility and control over the execution environment.  The `jobs` concept in CircleCI refers to a group of related steps or tasks that are executed as part of a larger workflow. Each job can have its own set of steps, parameters, and dependencies.  In {% data variables.product.prodname_actions %}, both the `runs-on` and `jobs` concepts are supported, but with some limitations. The `runs-on` concept is partially supported, as only certain types of executors are currently available, such as container and service executors. Self-hosted runners are not yet supported.  The `jobs` concept is fully supported, and jobs can be nested and have dependencies on other jobs. However, the `jobs` concept in {% data variables.product.prodname_actions %} is called `steps`, and it is nested within a workflow file rather than being a separate concept.  In summary, while the concepts of `runs-on` and `jobs` are similar in both CircleCI and {% data variables.product.prodname_actions %}, there are some differences in terms of supported executors and the naming convention used for jobs.", "c3f03b96-1ca2-4216-9313-cfa4d18b4bbb": "Clustering in {% data variables.product.product_name %} provides horizontal scaling for companies with a large number of developers by distributing services and data across multiple virtual machines (VMs) that run the {% data variables.product.product_name %} software. Each VM is called a node. This allows for the addition of resources as the number of developers and usage increases, without the need for a single primary node to handle all the workload.  However, setting up a redundant and scalable cluster involves additional complexities compared to other topologies like high availability. These complexities include careful planning, higher installation, configuration, disaster recovery, and upgrade requirements. While high availability configurations are simpler to configure and maintain, they may not accommodate all environments, especially those with tens of thousands of developers. Clustering is not intended for redundancy across geographic locations, and low latency between nodes is required. Additionally, clustering is not intended to replace a high-availability configuration, which should still be considered for most environments.", "7a49aa35-1984-49af-acfe-5e66b6fdfa51": "When referring to {% data variables.product.prodname_github_apps %}, use \"App\" since this is a product. Use \"app\" when referring to {% data variables.product.prodname_oauth_apps %} since these are not a product.  Example:  - To register a new {% data variables.product.prodname_github_app %}, go to your {% data variables.product.prodname_github_apps %} settings. - To register a new {% data variables.product.prodname_oauth_app %}, go to your developer settings.    Authentication  Use \"authentication\" in general content. - **Use:** Enable two-factor authentication - **Use:** Set up authentication for GitHub Pages  Use \"authorization\" when referring to OAuth authorization. - **Use:** Authorize an app  Use \"sign in\" or \"sign in with\" when referring to authentication with a third-party service. - **Use:** Sign in with Google  Use \"sign out\" or \"sign out of\" when referring to signing out of a service. - **Use:** Sign out of GitHub    Backups  Use \"backup\" in general content. - **Use:** Create a backup of your repository    Beta  Use \"beta\" in general content. - **Use:** Join the GitHub Beta program    Branches  Use \"branch\" in general content. - **Use:** Create a branch - **Use:** Switch to a branch    Cards  Use \"card\" in general content. - **Use:** Create a card - **Use:** Add a card to a project    Commits  Use \"commit\" in general content. - **Use:** Make a commit - **Use:** View commit activity    Community  Use \"community\" in general content. - **Use:** Join the GitHub Community    Contributors  Use \"contributor\" in general content. - **Use:** Add a contributor to your organization    Developers  Use \"developer\" in general content. - **Use:** Add a developer", "a5251d23-a875-43c3-a316-5940b50f9a2e": "{% data variables.product.prodname_ghe_cloud %} is a set of advanced functionality on {% data variables.product.prodname_dotcom_the_website %}, while {% data variables.product.prodname_ghe_server %} is a self-hosted platform. This means that {% data variables.product.prodname_ghe_cloud %} is a cloud-based version of GitHub Enterprise that provides additional features, while {% data variables.product.prodname_ghe_server %} is a version of GitHub Enterprise that can be installed and run on a company's own servers. The specific differences between the two will depend on the specific features and functionality included in each version, which can be found in the documentation for each product.", "ec979cd6-5be7-4a72-b7a1-f1d868095fb1": "To determine whether a diagram is necessary for a particular topic in {% data variables.product.prodname_docs %}, the following steps should be taken:  1. Identify the audience for the diagram. Who will be using it, and what do they need to understand?  2. Determine the scope of the diagram. What information needs to be included, and what can be left out?  3. Evaluate how the diagram will complement the accompanying text. Will it provide additional context, clarify a concept, or illustrate a process?  4. Determine how the diagram's effectiveness will be evaluated. Will it be measured by user feedback, engagement metrics, or other means?  Acceptance criteria for diagrams should include:  - Clear and concise labeling of all elements in the diagram - Consistent use of symbols and icons - Use of color to differentiate elements and highlight important information - Use of whitespace to improve readability and reduce clutter - Use of a consistent style and format throughout the diagram - Use of a legend or key to explain any unfamiliar symbols or icons - Use of a title and caption to provide context and explain the diagram's purpose - Use of a clear and legible font - Use of a consistent scale and measurement units - Use of a consistent orientation (e.g., landscape or portrait) - Use of a consistent level of detail (e.g., high-level overview or detailed step-by-step process) - Use of a consistent layout and structure (e.g., left-to-right or top-to-bottom) - Use of a consistent color palette and contrast ratio to ensure accessibility for users with visual impairments - Use of a consistent level of annotation and explanation to ensure that the diagram is self-contained and can be understood without referring to the accompanying text  By following these acceptance criteria, diagrams in {% data variables.product.prodname_docs %} can be effective tools for communicating complex information and concepts to users.", "a48cfa8f-528a-402c-8bb5-581ea740e5b4": "Without further context, it is unclear how \"AUTOTITLE\" is utilized in the provided context. Please provide additional information or clarify the context to assist in answering this question.", "c9557305-ec49-4ed7-b35d-5f507103368b": "To create CodeQL databases for multiple languages using the {% data variables.product.prodname_codeql_cli %}, you can use the `codeql database create` command with the `--language` flag to specify the languages you want to analyze. Here's an example command:  ```shell codeql database create my-databases --source-root=src \\     --db-cluster --language=java,python --command=./myBuildScript ```  In this command, replace `my-databases` with the name you want to give your databases, `src` with the path to your source code, and `./myBuildScript` with the path to your build script (if you're using one).  To upload the results of a CodeQL analysis to {% data variables.product.product_name %} using the {% data variables.product.prodname_codeql_cli %}, you can use the `codeql github upload-results` command with the necessary parameters. Here's an example command:  ```shell codeql github upload-results \\     --repository=my-org/example-repo \\     --ref=refs/heads/main --commit=deb275d2d5fe9a522a0b7bd8b6b6a1c939552718 \\     --sarif=java-results.sarif ```  In this command, replace `my-org/example-repo` with the name of your repository on {% data variables.product.product_name %}, `deb275d2d5fe9a522a0b7bd8b6b6a1c939552718` with the SHA of the commit you want to upload the results to, and `java-results.sarif` with the name of the SARIF file containing the results of your CodeQL analysis. If you're uploading results for multiple languages, you can repeat this command with a different SARIF file for each language.", "fb63b8fa-93cd-496c-b32d-f7bc0e05ff52": "The default working directory for steps in a GitHub Actions workflow is `GITHUB_WORKSPACE`, which is `/home/runner/work/my-repo-name/my-repo-name` in this case. When using the `checkout` action, your repository is located in the same directory, `GITHUB_WORKSPACE`.", "e8fdc65b-dada-4d96-976e-b9ef24284599": "The REST API can be utilized to manage protected branches by using the API endpoints provided for GitHub Enterprise Server (ghes), GitHub Enterprise Cloud (ghec), GitHub Enterprise (ghae), and GitHub.com (gha). These endpoints allow for the creation, deletion, and modification of protected branches, as well as the ability to view and manage branch protection settings. By using the REST API, developers can automate branch protection workflows, integrate branch protection into their CI/CD pipelines, and streamline branch management across their organization.", "04cd1319-fde3-4f0e-8250-e0dd2bd01153": "The duration of the agreement between GitHub and the developer is not explicitly stated, but it mentions that the agreement will continue to apply until terminated by either party. The developer can terminate the agreement by providing written notice to GitHub 45 days prior to the immediately following calendar month, and during this 45-day period, the terms in effect at the time of the notice of termination will govern. GitHub can also terminate the agreement immediately if the developer breaches any provision of the agreement or if GitHub is required to do so by law. The obligations in certain sections, such as intellectual property, confidentiality, indemnification, and dispute resolution, will survive any expiration or termination of the agreement. If the agreement is terminated as a result of a GitHub modification to the terms, the terms in effect immediately prior to such modification will govern during the 45-day notice period until the date of actual termination.", "93defaca-a425-47d1-950f-98c20012953d": "During the GitHub Enterprise migration, you can filter the records using the `ghe-migrator audit` command by specifying the record types and states.  Record type filters:  - To filter by user records, use the `user` filter name. - To filter by organization records, use the `organization` filter name. - To filter by repository records, use the `repository` filter name. - To filter by team records, use the `team` filter name. - To filter by milestone records, use the `milestone` filter name. - To filter by project board records, use the `project` filter name. - To filter by issue records, use the `issue` filter name. - To filter by issue comment records, use the `issue_comment` filter name. - To filter by pull request records, use the `pull_request` filter name. - To filter by pull request review records, use the `pull_request_review` filter name. - To filter by commit comment records, use the `commit_comment` filter name. - To filter by pull request review comment records, use the `pull_request_review_comment` filter name. - To filter by release records, use the `release` filter name. - To filter by actions taken on pull requests or issues records, use the `issue_event` filter name. - To filter by protected branch records, use the `protected_branch` filter name.  Record state filters:  - To filter by records that will be exported, use the `export` filter name. - To filter by records that will be imported, use the `import` filter name. - To filter by records that will be mapped, use the `map` filter name. - To filter by records that will be renamed, use the `rename` filter name. - To filter by records that will be merged, use the `merge` filter name. - To filter by records that were successfully exported, use the `exported` filter name. - To filter by records that were successfully imported, use the `imported` filter name. - To filter by records that were successfully mapped, use the `mapped` filter name. - To filter by records that were successfully renamed, use the `renamed", "e1b6a335-b844-4b0b-9b0d-65dc4595ccf3": "To keep your fork synced with the upstream repository using the terminal, you can use the following Git commands:  1. Fetch changes from both the remote repositories (origin and upstream) by running `git fetch --all`.  2. Merge the changes from the upstream repository into your local branch by running `git merge upstream/main`.  3. Push the changes to your remote repository (origin) by running `git push origin main`.  Here's an example command:  ```shell git fetch --all && git merge upstream/main && git push origin main ```  You can automate this process by creating a Git alias or a shell script. For more information, see \"AUTOTITLE.\"  {% endcli %}  {% webui %}  {% data reusables.webui.learn-more %}  To configure a remote repository for the forked repository, follow these steps:  1. Go to your forked repository.  2. Click the Settings tab.  3. In the \"Remote\" section, click the Add button.  4. Enter a name for the remote repository (e.g., \"upstream\").  5. Enter the URL for the original repository (e.g., \"https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY\").  6. Click the Add remote button.  Now, you can keep your fork synced with the upstream repository by following these steps:  1. Go to your forked repository.  2. Click the Pull requests tab.  3. Click the New pull request button.  4. Select the branch you want to merge from the upstream repository.  5. Select the branch you want to merge into your forked repository.  6. Click the Create pull request button.  7. Review the changes and click the Merge pull request button.  Alternatively, you can use Git to keep your fork synced with the upstream repository. For more information, see \"AUTOTITLE.\"  {% endwebui %}  {% cli %}  {% data reusables.cli.cli-", "10130ca4-07fe-4da9-a49d-6999db7c5205": "Based on the provided context, the purpose of the \"AUTOTITLE\" command is unclear as it is enclosed in a conditional statement ({% if %}) and no further information is provided about its functionality. Without additional context, it is impossible to summarize the purpose of this command.", "25e10a54-1595-4970-86ea-21940c1c8a5c": "To add a new autograding test in a GitHub Classroom assignment, follow these steps:  a. Go to your GitHub Classroom and select the assignment for which you want to add a new autograding test.  b. Click on the \"Grading and feedback\" tab in the left-hand menu.  c. Click on the \"Add autograding tests\" dropdown menu and select the grading method you want to use.  d. Configure the test according to the instructions provided by the grading method.  e. Click \"Save test case\" to add the new autograding test.  Answer:  2. To edit an existing autograding test in a GitHub Classroom assignment, follow these steps:  a. Go to your GitHub Classroom and select the assignment for which you want to edit an autograding test.  b. Click on the \"Grading and feedback\" tab in the left-hand menu.  c. Locate the autograding test you want to edit and click on the pencil icon to the right of its name.  d. Make the necessary changes to the test configuration.  e. Click \"Save test case\" to save the changes.  Answer:  3. To delete an autograding test in a GitHub Classroom assignment, follow these steps:  a. Go to your GitHub Classroom and select the assignment for which you want to delete an autograding test.  b. Click on the \"Grading and feedback\" tab in the left-hand menu.  c. Locate the autograding test you want to delete and click on the trash icon to the right of its name.  d. Confirm that you want to delete the test by clicking \"Delete\" in the confirmation dialog box.  Answer:  4. To view individual logs for a student's submission in a GitHub Classroom assignment, follow these steps:  a. Go to your GitHub Classroom and select the assignment for which you want to view individual logs.  b. Locate the student's submission for which you want to view individual logs.  c. Click on the checklist icon to the right of the", "5ec813de-4aab-4f61-9d77-a43967c69f80": "How do I unpin a discussion from a category in a repository on GitHub? 2. How do I transfer a discussion from one repository to another on GitHub? What are the requirements for transferring a discussion between repositories?", "4f816e8f-8cf7-4dd2-ac57-9f9a7cff23e5": "Unfortunately, based on the provided context, it seems that {% data variables.product.prodname_mobile %} may not work with your enterprise if you're required to access your enterprise over VPN. It's recommended to check with your enterprise owner or IT department to confirm if this is the case for your specific setup. If you're able to access your enterprise without VPN, then you can follow the prerequisites and instructions provided to use {% data variables.product.prodname_mobile %} with {% data variables.product.prodname_ghe_server %}.", "806703d6-e225-4de9-ab26-564b02d1afff": "To create a branch using the branch dropdown on {% data variables.product.product_name %}, follow these steps:  1. Navigate to the repository you want to create the branch in. 2. Select the {% octicon \"git-branch\" aria-hidden=\"true\" %} branch dropdown menu, either in the file tree view or at the top of the integrated file editor. 3. If you want to create the new branch from a branch other than the default branch of the repository, click another branch, then select the branch dropdown menu again. 4. In the \"Find or create a branch...\" text field, type a unique name for your new branch, then click **Create branch**.  Note: You can only create a branch in a repository to which you have push access.", "d62eccbc-870f-482b-8fa1-bb6008218b29": "In a counter notice, the content that was disabled and its location should be identified by providing the URL(s) where it appeared.", "b99ff4e7-1756-49f8-b42c-e8d5dcd15f15": "The context information provided mentions that to learn more about different events that can trigger a workflow, we can refer to \"AUTOTITLE.\" However, the context doesn't specify what \"AUTOTITLE\" is. Without this information, we cannot determine what events can trigger a workflow in GitHub.  To find examples of workflows using the GitHub CLI, we can search GitHub for \"workflow\" and \"GitHub CLI\" in the repository's codebase or documentation. This will help us understand how others have implemented workflows using the GitHub CLI and potentially provide us with ideas for our own workflows. Alternatively, we can refer to the GitHub CLI manual provided in the context information for more information on how to use the CLI to create and manage workflows.", "470d9e67-c4a8-433b-bf2c-d995f15154d4": "There are several tools available for programmatically creating and importing repositories, including {% data variables.product.prodname_importer_proper_name %} and the Git CLI.  To use {% data variables.product.prodname_importer_proper_name %}, you must have admin access to the source repository and be using a source repository hosted on {% data variables.product.prodname_dotcom_the_website %}, {% data variables.product.prodname_ghe_server %}, Azure DevOps Services, Bitbucket Server, or Bitbucket Data Center. You can also programmatically import repositories using the GraphQL API.  To use the Git CLI, you must have access to install Git on the system that hosts your application. You can programmatically create a repository using {% data variables.product.prodname_dotcom %}'s REST API, then use commands like `git clone` and `git push` to import the repository to {% data variables.product.prodname_dotcom_the_website %}. The specifics of how to call the Git CLI will depend on the language you're using.  In both cases, you'll need to have read access to an existing repository that's a template, or access to create a template, in order to use repository templates.", "a47802a4-2bd6-4db9-b7f9-ff2f202689e8": "Developers in Iran will be able to access all public and private services on GitHub, including individual Free and Pro plans, as well as organization accounts, both free and paid, under the license obtained by GitHub from OFAC. However, trade-restricted users' access to private repository data, such as downloading or deletion, is currently restricted until authorized by the U.S. government.", "bee20362-2833-49b9-9512-86717e1dacbb": "High confidence tokens related to supported patterns and specified custom patterns, as well as non-provider tokens such as private keys, which often result in false positives, are included in the \"leaks\" reported to users on {% data variables.product.prodname_dotcom %} for repositories with {% data variables.product.prodname_secret_scanning %} and push protection enabled.", "48553f96-46c3-44b1-bc9f-547fc5839ae2": "Setting `debug: true` in the `init` step of the {% data variables.product.prodname_codeql %} workflow file allows us to upload the database as an actions artifact that can be downloaded to a local machine. This can help us identify which types of files are not being analyzed by {% data variables.product.prodname_codeql %}, making it easier to understand how we may need to change the workflow for analysis. This feature is useful for debugging and troubleshooting issues with {% data variables.product.prodname_codeql %} analysis. Without this context, we wouldn't know that setting `debug: true` can provide us with additional information about the analysis process.  2. The `init` step and the `debug` option  The `init` step in the {% data variables.product.prodname_codeql %} workflow file is used to initialize the analysis process. By setting `debug: true` in this step, we can upload the database as an actions artifact, which can be downloaded to a local machine for further analysis. This feature is useful for debugging and troubleshooting issues with {% data variables.product.prodname_codeql %} analysis. Without this context, we wouldn't know that the `init` step has a `debug` option that can provide us with additional information about the analysis process.  Answer:  The `init` step in the {% data variables.product.prodname_codeql %} workflow file is a crucial part of the analysis process. By setting `debug: true` in this step, we can upload the database as an actions artifact, which can be downloaded to a local machine for further analysis. This feature is useful for debugging and troubleshooting issues with {% data variables.product.prodname_codeql %} analysis. By analyzing the database on a local machine, we can identify which types of files are not being analyzed by {% data variables.product.prodname_codeql %}, making it easier to understand how we may need to change the workflow for analysis. Without this context, we wouldn't know that the `init` step has a `debug` option that can provide us with additional information about the analysis process.    Based on the", "533516f0-ed4b-476e-8a21-f19dafe359b6": "To create a ruleset for branch and tag protections, metadata restrictions, and targeting specific repositories or branches/tags, follow these steps:  1. Go to your organization's repository settings. 2. Click on \"Branches\" or \"Tags\" to select the type of protection you want to set up. 3. Click on \"Create a ruleset\" to create a new ruleset. 4. Choose the type of ruleset you want to create: \"Branch protection\" or \"Tag protection\". 5. Select the repository or branch/tag you want to protect. 6. Choose the protection level you want to apply: \"Required reviews\", \"Restricted\", or \"Draft\". 7. If you selected \"Restricted\" or \"Draft\", you can also choose who can push to the protected branch/tag: \"Only admins\", \"Only selected users\", or \"Only selected teams\". 8. To add metadata restrictions, click on \"Add metadata restrictions\" and enter the metadata you want to restrict. 9. To target specific repositories or branches/tags, click on \"Target repositories\" and select \"Target: REPOSITORIES\" or \"Target: BRANCHES/TAGS\". 10. If you selected \"Target: REPOSITORIES\", you can choose to target a dynamic list of repositories by properties or select repositories manually. 11. If you selected \"Target: BRANCHES/TAGS\", you can choose to target a specific branch/tag or use `fnmatch` patterns to match branches/tags. 12. Click on \"Save\" to save your ruleset. 13. To view insights about your ruleset, click on \"View insights\" in the ruleset settings. 14. To edit or delete your ruleset, click on the three dots next to the ruleset name and select \"Edit\" or \"Delete\".  Note: `fnmatch` patterns are a way to match branches/tags using wildcard characters. For example, `*` matches any sequence of characters, and `?` matches any single character. You can use `fnmatch` patterns to target specific branches/tags that match a certain pattern.  Regular expressions can also be used to match commit metadata, such as commit messages", "59d588aa-fbd6-495a-8652-e36abdb1d1c3": "The \"converted_to_discussion\" event for issues provides information about the issue being closed and converted to a discussion. The event includes common properties such as the event ID, URL, and resource path, as well as project card properties such as the project and number. No further information is provided in the context given.", "656d9d30-50b7-4727-a470-b609bd12f91c": "To review the results file and determine if a single query is dominating the number of results reported, you can use a SARIF viewer to see the distribution of results. If you notice that a large number of results are being reported by a single query, you can try excluding that query from analysis. This can be done by modifying the query configuration for the analysis tool being used, such as {% data variables.product.prodname_codeql %}. By excluding the query, you can reintroduce it later when other alerts have been fixed. This can help to reduce the size of the results file and make it easier to manage and address the remaining issues. For more information about query configuration for {% data variables.product.prodname_codeql %}, you can refer to the documentation on excluding a query from analysis.", "61b8375f-4168-4b94-acbd-5a84174d523b": "To suggest changes directly from a comment on a specific file or section in a pull request's Files changed tab in GitHub, you can add a line or file comment with the suggested changes enclosed in double backticks (````) and a label starting with `!`. For example, to suggest changing the text \"Hello, world!\" to \"Hello, GitHub!\" in the file `example.md`, you can leave a comment like this:  ``` ! [BREAKING CHANGE] Change \"Hello, world!\" to \"Hello, GitHub!\" Hello, GitHub! ```  When the author of the pull request reviews your comment, they can click the label to apply your suggested changes directly to the file. This feature is available for GitHub-hosted repositories with the pull request comment feature enabled.  If the context doesn't provide enough information to determine whether the repository has this feature enabled, you can suggest checking the repository's settings to confirm.    Adding comments to a pull request's Conversation tab  {% data reusables.repositories.sidebar-pr %} 1. In the list of pull requests, click the pull request where you'd like to leave a comment. 2. Click the **Conversation** tab. 3. Type your comment in the text box. 4. Click **Comment** to leave your comment.    Adding comments to a specific file or section in a pull request's Files changed tab  {% data reusables.repositories.sidebar-pr %} 1. In the list of pull requests, click the pull request where you'd like to leave a comment. 2. Click the **Files changed** tab. 3. Find the file or section where you'd like to leave a comment. 4. Click the line or section where you'd like to leave a comment. 5. Type your comment in the text box that appears. 6. Click **Comment** to leave your comment.    Adding comments to a specific line in a pull request's Files changed tab  {% data reusables.repositories.sidebar-pr %} 1. In the list of pull requests, click the pull request where you'd like to leave", "0afbff49-ac31-4fef-b1cd-be086348ffba": "Each self-hosted runner will require one of the following procedures:  - In the self-hosted runner application directory, edit the `.runner` and `.credentials` files to replace all mentions of the old hostname with the new hostname, then restart the self-hosted runner application. - Remove the runner from {% data variables.product.prodname_ghe_server %} using the UI, and re-add it.  Note: If the old hostname no longer resolves to your instance, self-hosted runners will be unable to connect to the old hostname, and will not execute any jobs. You will need to update the configuration of your self-hosted runners to use the new hostname for {% data variables.location.product_location %}.", "ea92213d-a064-43b0-a8fc-a5173fe25279": "Internal GitHub Apps are a type of GitHub App that are owned and operated by GitHub's parent company, GitHub, Inc. (also known as {% data variables.product.company_short %}). These apps are granted special capabilities, such as the ability to access data from an organization without requiring approval from the organization's administrators. Unlike regular GitHub Apps, which require user authorization, some internal apps are automatically included with GitHub and do not appear in the list of authorized or installed apps. These apps will still appear in the user security log, but will not appear in organization or enterprise audit logs. Examples of internal apps include GitHub Classroom, VSCode Auth Provider, Git Src Migrator, MS Teams, Slack, GitHub Codespaces, and GitHub Copilot plugin.", "cbb72000-eb8f-4abc-9a2b-3690ad898131": "The `showMiniToc` property determines whether an article should display a mini table of contents (TOC) above the main content. This property is set to `true` by default for articles, but can be set to `false` if the mini TOC is not desired.  The `allowTitleToDifferFromFilename` property determines whether a page is allowed to have a title that differs from its filename. This property is set to `false` by default, meaning that pages with different titles and filenames will be flagged in tests and updated by a script. If a page needs to have a different title than its filename, this property should be set to `true`.  In summary, `showMiniToc` is used to control the display of a mini TOC, while `allowTitleToDifferFromFilename` is used to allow a page to have a different title than its filename.", "c3b50796-52f1-495a-b59f-37db402a8bbd": "The purpose of using starter workflows for code scanning, as outlined in the context information, is to provide suggested workflows for implementing {% data variables.product.prodname_code_scanning %} in {% data variables.product.product_name %}, instead of starting from scratch. These starter workflows are provided by {% data variables.product.company_short%} and third-party providers, and can help users construct their own {% data variables.product.prodname_code_scanning %} workflows more easily. The {% data variables.product.company_short%}'s workflow, the {% data variables.code-scanning.codeql_workflow %}, is powered by {% data variables.product.prodname_codeql %}.", "ef9baf5e-31fc-4b26-8f09-c79d4ad45ca9": "To manage whether team sync can re-invite non-members to your organization in GitHub, you need to follow these steps:  1. Access your organization settings by clicking on your organization's name in the top left corner of the GitHub page, then selecting \"Settings\" from the drop-down menu. 2. Click on \"Security\" in the left-hand menu. 3. Scroll down to the \"Team synchronization\" section. 4. Select or deselect the option \"Do not allow Team Sync to re-invite past members to this organization that were removed by an organization owner.\"  Note that this feature is only available for GitHub Enterprise Cloud and GitHub Enterprise Server instances that have Team Sync enabled. If you're not sure whether your organization has this feature, you can contact your GitHub administrator or support team for more information.", "e5c536cd-9581-4288-a4ae-7452177ed6a3": "GitHub must ensure that any onward subprocessor agrees in writing to process data in a country with adequate protection or on terms equivalent to the Standard Contractual Clauses, Binding Corporate Rules, or compliant and valid EU-US Privacy Shield and Swiss-U.S. Privacy Shield certifications. Additionally, GitHub must restrict the subprocessor's access to Customer Personal Data to what is strictly necessary to perform its services and prohibit it from processing the data for any other purpose. These requirements are outlined in Section 6.3 of the DPA.", "00b5342e-5508-4a74-a767-81e555db702a": "To configure Yarn Berry to only access private registries, you can use two different configuration methods:  1. Defining the private registry configuration in a `dependabot.yml` file:     a. Create a `dependabot.yml` file in the root directory of your repository.     b. Add the following configuration to the file:     ```yaml    version: 2    updates:      encoding:        from: \"0.1.11\"        to: \"0.1.13\"    ```     c. Replace `encoding` with the name of the package you want to update, and replace the version numbers with the desired versions.     d. Save the file and commit the changes.     This configuration tells {% data variables.product.prodname_dependabot %} to update the `encoding` package to version `0.1.13` and save the changes to the `dependabot.yml` file.     To ensure the private registry is listed as the dependency source in the project's `yarn.lock` file, you need to run `yarn install` on a machine with private registry access. Yarn should update the `resolved` field to include the private registry URL.  2. Adding the private registry to a `.yarnrc.yml` file in the project root:     a. Create a `.yarnrc.yml` file in the root directory of your repository.     b. Add the following configuration to the file:     ```yaml    npmRegistryServer: \"https://private_registry_url\"    ```     c. Replace `private_registry_url` with the URL of your private registry.     d. Save the file and commit the changes.     This configuration tells Yarn Berry to use the specified private registry for all package installations.     If you want to ensure the private registry is listed as the dependency source in the project's `yarn.lock` file, you should define the private registry configuration in a `dependabot.yml` file and run `yarn install` on a machine with private registry access.     To implement this", "ebf32bde-56d3-44c6-a1f5-6e09539dcf61": "Content that falls under any of the following categories is excluded from the definition of \"Confidential Information\":  1. Content that is in the public domain or publicly available through sources other than the disclosing party. 2. Content that is independently developed by the receiving party without access to or use of the other party's Confidential Information. 3. Content that is approved by the disclosing party for disclosure without restriction in a written document signed by a duly authorized officer of the disclosing party. 4. Content that is stored or posted on the Service and outside of the customer's private repositories.  Note: The definition of \"Content\" includes various types of materials, such as code, text, data, articles, images, packages, photographs, graphics, software, applications, designs, features, and other materials that are featured, displayed, or otherwise made available through the Service.", "c11114d1-4d13-4223-8cdf-0be6dafcdb53": "The advantages of using installation access tokens over personal access tokens in GitHub are:  1. Decoupled from user identities: Installation access tokens are not tied to a specific user's GitHub account, which means they do not consume any licensed seats. This is in contrast to personal access tokens, which are tied to a specific user's account and can consume licensed seats if the user is part of a paid organization.  2. Dedicated rate limits: Installation access tokens have dedicated rate limits that grow with your organization. This means that as your organization grows, the rate limits for your installation access tokens will also increase, providing more flexibility and scalability. Personal access tokens, on the other hand, have a fixed rate limit that does not grow with your organization.  3. Longer expiration time: Installation access tokens expire after 1 hour, but they can be re-generated on-demand using code. This is in contrast to personal access tokens, which can be set to expire after a specific time period (such as 1 week, 1 month, or 1 year) and cannot be re-generated without the user's input.  To set up a private GitHub app and generate an installation access token for it, follow these steps:  1. Determine if your GitHub app should be public or private. If your app will only act on repositories within your organization, you likely want it private.  2. Determine the permissions your app requires, such as read-only access to repository contents.  3. Create your GitHub app via your organization's settings page. For more information, see Creating a GitHub app.  4. Note your GitHub app's `id`.  5. Generate and download your GitHub app's private key, and store this safely. For more information, see Generating a private key.  6. Install your GitHub app on the repositories it needs to act upon, optionally you may install the app on all repositories in your organization.  7. Identify the `installation_id` that represents the connection between your GitHub app and the organization repositories it can access. Each GitHub app and organization pair have at most a single `installation_id`. You can identify this `installation_id", "a9bf64cd-46df-45ed-b860-85c5c0467a19": "Without further context, it is unclear how the concept of \"AUTOTITLE\" differs from \"AUTOTITLE\" in the given context. Please provide additional information to help clarify the difference between these two concepts.", "27704590-c8e1-44e4-8a32-198e5796fdf2": "To determine whether the Docker engine installed on your system was installed using `snap`, you can use the `which` command to check the location of the `docker` binary. If the location of the binary starts with `/snap/bin/`, then the Docker engine was installed using `snap`. For example:  ```shell $ which docker /snap/bin/docker ```  In this case, the Docker engine was installed using `snap`. If the location of the binary does not start with `/snap/bin/`, then the Docker engine was likely installed using a different method, such as from a package manager or a binary download.", "94ec2434-2611-47e3-ae57-d17f2f0a76c5": "The `swift-actions/setup-swift` action can be used to find a specific version of Swift from the tools cache on a {% data variables.product.prodname_dotcom %}-hosted runner and add the necessary binaries to PATH.", "f39794e9-8129-4b4e-a3d1-74b115d346d2": "The two forms of {% data variables.product.prodname_secret_scanning %} available are {% data variables.secret-scanning.partner_alerts_caps %} and {% data variables.secret-scanning.user_alerts_caps %}. The main difference between them is that {% data variables.secret-scanning.partner_alerts_caps %} are enabled by default and automatically run on all public repositories and public npm packages, while {% data variables.secret-scanning.user_alerts %} requires explicit enablement and provides additional scanning capabilities for organizations. When enabled, {% data variables.secret-scanning.user_alerts %} can detect secrets on public repositories owned by personal accounts on {% data variables.product.prodname_dotcom_the_website %}, public repositories owned by organizations, and private and internal repositories owned by organizations using {% data variables.product.prodname_GHE_cloud %} (when a license for {% data variables.product.prodname_GH_advanced_security %} is available).", "3e9544a0-ee9b-4572-ba64-0275748590df": "Context properties like `GITHUB_REF` can be accessed during workflow processing by using the curly braces syntax to reference them, as shown in the example provided in the context information. For example, `{% raw %}${{ github.ref }}`{% endraw %}.  The recommended way to access filesystem in actions is to use variables instead of hardcoding file paths. {% data variables.product.prodname_dotcom %} sets variables for actions to use in all runner environments, which can be accessed using the syntax `{% raw %}${{ variable_name }}`{% endraw %}. This approach provides greater flexibility and portability, as the same action can be used in different repositories and workflows without the need for manual adjustments to file paths.", "357c5316-8f98-4a85-bcb3-c8c03ebbb5ee": "To add a new organization secret using the GitHub CLI, you can follow these steps:  1. Ensure that you have authorized the `admin:org` scope for your GitHub CLI authentication.  2. Navigate to the organization for which you want to add the secret.  3. Run the following command:     ```shell    gh secret set --org ORGANIZATION_NAME SECRET_NAME    ```     Replace `ORGANIZATION_NAME` with the name of your organization, and `SECRET_NAME` with the name you want to give the new secret.  4. If you want the secret to be available to all repositories in the organization, add the `--visibility` or `-v` flag, followed by `all`.     For example:     ```shell    gh secret set --org ORGANIZATION_NAME SECRET_NAME --visibility all    ```  5. If you want the secret to be available to specific repositories in the organization, add the `--repos` or `-r` flag, followed by the names of the repositories.     For example:     ```shell    gh secret set --org ORGANIZATION_NAME SECRET_NAME --repos REPO_NAME_1, REPO_NAME_2    ```  6. Confirm the creation of the new secret by checking the list of organization secrets using the `gh secret list` command.     For example:     ```shell    gh secret list --org ORGANIZATION_NAME    ```  7. Review the access policies for the new secret by checking the organization settings.     For more information on managing organization secrets using the GitHub CLI, refer to the GitHub CLI documentation.", "16f162b1-476f-47d8-8499-f742bc7540c0": "A user with the \"Yes\" setting for the \"Manage branch protection rules\" feature can perform the following actions:  a) Create and edit repository social cards - No, this is not a feature related to branch protection rules. b) View rulesets for a repository - Yes. c) Push to protected branches - Yes. d) Merge pull requests on protected branches, even if there are no approving reviews - Yes. e) Delete tags that match a tag protection rule - No, this is related to tag protection rules. f) Create tags that match a tag protection rule - No, this is related to tag protection rules.  The \"Limit interactions in a repository\" feature can restrict the following actions:  a) Push to protected branches - Yes. b) Merge pull requests on protected branches, even if there are no approving reviewsc - Yes. c) Create tags that match a tag protection ruled - No, this is related to tag protection rules. d) Delete tags that match a tag protection rule - No, this is related to tag protection rules. e) Create and edit repository social cards - No, this is not related to repository interactions. f) View rulesets for - No, this is not related to repository interactions.  If the context doesn't give enough information to determine whether a feature is available or not, then it's not possible to say whether a user can perform a specific action or whether a feature can restrict certain actions. In such cases, it's best to say that you weren't given enough information to give an answer.", "07614ada-18a2-4ee7-8b46-7e398546c4ad": "The `x-ratelimit-remaining` header in the response of a rate-limited request indicates the number of remaining requests that can be made before hitting the rate limit again. If the value of this header is `0`, it means that the developer has exceeded the rate limit and should not make any more requests until the time specified by the `x-ratelimit-reset` header. The `x-ratelimit-reset` header specifies the time, in UTC epoch seconds, when the developer's rate limit will be reset, allowing them to make more requests. If a developer exceeds a secondary rate limit, they should not retry their request until after the time specified by the `retry-after` response header, if present, or until after the time specified by the `x-ratelimit-reset` header, if the `retry-after` header is not present and the `x-ratelimit-remaining` header is `0`.", "7d971a9b-60d2-4ea5-bdf5-14d01523439c": "The purpose of the `language` matrix in the {% data variables.product.prodname_codeql %} workflow is to specify multiple compiled languages that should be analyzed in the workflow. Without a matrix, the workflow will attempt to analyze the compiled language with the most source files in the repository. This feature allows for more flexibility in analyzing multiple languages within a single workflow. However, analysis of other compiled languages in the repository will fail unless explicit build commands are supplied. This is because {% data variables.product.prodname_codeql %} requires explicit build commands for compiled languages other than Go.", "39bf18f8-433c-405e-b5c9-3142b4a6ec8b": "To ensure that GitHub Actions use SHA pinning, you can add the following comment at the beginning of your workflow file:  {% data reusables.actions.actions-use-sha-pinning-comment %}  This comment tells GitHub Actions to use SHA pinning for all actions in the workflow. SHA pinning ensures that the exact same version of an action is used every time it is run, preventing unexpected behavior caused by updates or version changes. It's recommended to use SHA pinning to ensure consistency and reliability in your workflows.", "51647945-27a5-4a06-9158-1121ce27207a": "The '--learn' option in the 'codeql test run' command is an advanced feature that allows unexpected test output to be updated as expected output, instead of failing the test. This means that if a test produces unexpected output, the command will update the '.expected' file to match the actual output, making the test pass. However, tests can still fail in this mode, for example, if the creation of a test database fails. This feature is useful for learning the expected output of tests that produce unexpected results, without having to manually update the '.expected' file.", "ace96ab0-5067-489f-986d-866fca5fe358": "Variables in {% data variables.product.prodname_actions %} serve as placeholders for dynamic values that can be used in workflows. They allow you to store and reuse data across multiple steps in a workflow, making it easier to manage and maintain your automation tasks. By using variables, you can also make your workflows more flexible and customizable, as you can easily change the values of variables without having to modify the entire workflow. Overall, variables in {% data variables.product.prodname_actions %} help to improve the efficiency and maintainability of your automation processes.", "1463e1d4-9eb5-4cb8-aa2c-42968320466b": "To decrease the number of progress messages printed by the CLI tool, you can use the `--quiet` option. This option incrementally decreases the number of progress messages printed.", "9d5eafed-09e1-4a83-aa10-bd7a24a4f1d6": "To solve issues with suspended applications, please contact GitHub support by visiting the following URL and submitting a support request: https://help.github.com/contact/support-request. Alternatively, you can send an email to support@github.com with a detailed explanation of the issue you're facing.", "c09450d3-891d-4f39-ab34-ec81a6bdc328": "To find out how many jobs are currently being executed in the current context, we can use the `CI_NODE_TOTAL` and `CI_NODE_INDEX` variables.   The `CI_NODE_INDEX` variable represents the index of the current job within the current strategy. The `CI_NODE_TOTAL` variable represents the total number of jobs being executed within the current strategy.  If we don't have enough information about the current strategy, we can't determine the total number of jobs being executed. However, if we know that all jobs are being executed within a single strategy, we can use the following expression:  ```yaml - name: Count Jobs   run: |     echo \"Number of jobs: $((${CI_NODE_TOTAL} - ${CI_NODE_INDEX} + 1))\" ```  This expression calculates the number of jobs by subtracting the current job index from the total number of jobs and adding 1 to account for the current job.  If we don't have enough information about the current strategy, we can't use this expression, and we'll have to wait for more context to determine the total number of jobs being executed.", "66f724d8-3a92-4f05-b81c-a7de7b061202": "To determine if a specific SSH key has already been used on a GitHub account or repository, follow these steps using the command line:  1. Open a terminal and type the following command, replacing `id_rsa` with the name of the key you want to check:     ```shell    $ ssh -T -i ~/.ssh/id_rsa git@{% data variables.command_line.codeblock %}    ```  2. If the key has already been used on a GitHub account or repository, you will see a message similar to \"Hi USERNAME! You've successfully authenticated, but GitHub does not provide shell access.\" Replace `USERNAME` with the actual username associated with the account or repository where the key is currently attached.  3. If the key has been attached to a repository as a deploy key, the message will include the repository name, such as \"username/repo\".  4. To force SSH to use only the key provided on the command line, add the `IdentitiesOnly=yes` option using the `-o` flag:     ```shell    ssh -v -o \"IdentitiesOnly=yes\" -i ~/.ssh/id_rsa git@{% data variables.command_line.codeblock %}    ```  If you encounter an error message while trying to use a specific SSH key on GitHub, indicating that the key has already been used on another account or repository, here are some possible solutions to resolve this issue:  1. Remove the key from the other account or repository and then add it to your account.  2. If you don't have permissions to transfer the key, and can't contact a user who does, remove the keypair and generate a brand new one.  3. If you're running into this error while setting up deploy keys, see the GitHub documentation on \"Deploy keys\" for more information and solutions.", "2dc4de3a-4444-4dd1-af99-3ddb01f7edb4": "Repository administrators can add constraints on merging a pull request into a specific branch using branch protection rules. These restrictions may include requirements for passing status checks, review approvals, or certain label or keyword usage. These rules can be set up in the repository settings under the \"Branches\" tab. By default, any pull request can be merged at any time, unless the head branch is in conflict with the base branch. However, these branch protection rules can be used to add additional restrictions and ensure that pull requests meet certain criteria before they are merged into the specified branch.", "9f26bdd7-e0f7-4b9f-a788-e29efa2d6e13": "The purpose of the `always_reject.sh` script in the context provided is to reject all pushes, which can be used to lock a repository. This script can be used to prevent any changes from being pushed to the repository until the lock is released. By creating a local Docker environment and testing this script locally, developers can ensure that the pre-receive hook is functioning correctly before deploying it to the remote repository. This can help prevent accidental pushes or unwanted changes from being made to the repository.", "be726d9a-3b9a-4d47-b989-08ab25dab7c0": "To ensure that your self-hosted runners without internet access can use the included `actions/setup-LANGUAGE` actions, such as `setup-node`, to set up their development environments, you need to manually populate the tool cache on the runner and transfer and extract the tool cache from a {% data variables.product.prodname_dotcom %}-hosted runner's artifact.  Here's how to do it:  1. Determine which development environments your self-hosted runners will need. In this example, we'll use Node.js versions 14 and 16.  2. Access a repository on {% data variables.product.prodname_dotcom_the_website %} that you can use to run a workflow. You can create a new repository or use an existing one.  3. Create a new workflow file named `tool-cache.yml` in the repository's `.github/workflows` directory.  4. Add the following YAML configuration to the `tool-cache.yml` file:  ```yaml name: Populate Tool Cache  on:   push:     branches:       - main  jobs:   populate-tool-cache:     runs-on: ubuntu-latest      steps:     - name: Checkout       uses: actions/checkout@v3      - name: Set up Node.js       uses: actions/setup-node@v3       with:         node-version: '14'         registry-url: https://registry.npmjs.org      - name: Install Node.js dependencies       run: npm install --quiet      - name: Set up Node.js       uses: actions/setup-node@v3       with:         node-version: '16'         registry-url: https://registry.npmjs.org      - name: Install Node.js dependencies       run: npm install --quiet      - name: Archive the tool cache       uses: actions/upload-artifact@v3       with:         name: tool-cache", "22fda4d5-8496-48d5-b711-d16944eacc65": "Known issues in troubleshooting content are a subset of troubleshooting content that specifically address bugs, UX/UI issues, and other product quirks that generate a high volume of support tickets. They explain problems that people will encounter, rather than errors that they might encounter. The goal of documenting known issues is to help people confirm that the issue exists and save them time searching for a solution that doesn't exist yet after {% data variables.product.prodname_dotcom %} has already determined there isn't a workaround. Product and feature owners should help plan and review known issue content. Known issues should be used to explain product behavior that regularly contradicts people's expectations, behavior that regularly prevents the use of the product or feature for a common purpose, and rare or severe bugs that {% data variables.product.prodname_dotcom %} has not yet prioritized fixing, and that are not explained in the product or by existing content on {% data variables.product.prodname_docs %}.", "582117a8-8500-480a-ad73-72f01e05333f": "By querying the `rateLimit` object in a GraphQL request, you can obtain the following information:  1. The maximum number of points that you can use per hour (`limit`) 2. The number of points remaining in the current rate limit window (`remaining`) 3. The number of points you have used in the current rate limit window (`used`) 4. The time at which the current rate limit window resets (`resetAt`)  This information can help you monitor your usage of the GraphQL API and avoid exceeding your rate limit.", "cb7b7163-bad0-4d18-929f-6734269e7ef4": "To migrate a repository from Bitbucket Server to GitHub using {% data variables.product.prodname_importer_proper_name %}, an organization owner for the destination organization in GitHub, or an organization owner who grants the migrator role, is responsible for running the migration process. The migrator must have admin or super admin permissions and SFTP access to the Bitbucket Server instance. If the Bitbucket Server instance runs Windows, file sharing (SMB) access is also required.", "c72af641-a5d5-4039-8de7-89f69296ea47": "To ensure that a RubyGem is linked to a specific repository on GitHub as soon as it is published, you need to include the URL of the GitHub repository in the `github_repo` field of the `gem.metadata` hash in your RubyGem's `gemspec` file. Here's an example:  ```ruby gem.metadata = {   \"github_repo\" => \"ssh://{% ifversion fpt or ghec %}github.com{% else %}HOSTNAME{% endif %}/OWNER/REPOSITORY\" } ```  Replace `OWNER` and `REPOSITORY` with the appropriate values for your GitHub repository, and replace `HOSTNAME` with the hostname of your GitHub instance if you're using a self-hosted GitHub Enterprise Server (GHES).  To publish multiple RubyGems to the same repository on GitHub using {% data variables.product.prodname_registry %}, you can include the URL of the GitHub repository in the `github_repo` field of the `gem.metadata` hash for each RubyGem. {% data variables.product.prodname_registry %} will match the repository based on this URL, instead of using the gem name.  Here's an example:  ```ruby gem.metadata = {   \"github_repo\" => \"ssh://{% ifversion fpt or ghec %}github.com{% else %}HOSTNAME{% endif %}/OWNER/REPOSITORY\" } ```  Replace `OWNER` and `REPOSITORY` with the appropriate values for your GitHub repository, and replace `HOSTNAME` with the hostname of your GitHub instance if you're using a self-hosted GitHub Enterprise Server (GHES).  When you publish your RubyGems, they will be linked to the specified repository on GitHub. This allows you to manage the source code and releases for your RubyGems in a centralized location, making it easier to collaborate with others and manage dependencies.", "5600e91f-9be2-4285-9d68-44427e267871": "Key pieces of information that can be found in the \"User reports\" section include:    - `created_at`: When the user account was created    - `id`: Account ID for the user or organization    - `login`: Account's login name    - `role`: Whether the account is an admin or an ordinary user    - `suspended?`: Whether the account has been suspended  2. The \"Organization reports\" section provides the following details:    - `id`: Organization ID    - `created_at`: When the organization was created    - `login`: Organization's login name    - `email`: Organization's primary email address    - `owners`: Number of organization owners    - `members`: Number of organization members    - `teams`: Number of organization teams    - `repos`: Number of organization repositories    - `2fa_required?`: Whether the organization requires two-factor authentication.", "8ff5c582-b5c3-44c6-bb16-c132b831bcc6": "According to section 14 of the provided context, the confidentiality obligations for using the pre-release software are subject to the confidentiality obligations between the user and GitHub in the Agreement. This means that the user must adhere to the confidentiality provisions outlined in their agreement with GitHub. The section also specifies that the pre-release software is non-public, confidential information of GitHub, and prohibits the user from disclosing or sharing the software with anyone who is not subject to these terms and a non-disclosure agreement. Additionally, the user is not allowed to post or allow others to post any photos or videos of the pre-release software on or via any online platform, including personal social media websites, or describe or discuss any part of the software on or via any online platform, unless given advance and express permission by GitHub to do so.", "baec92b4-9d95-4bbb-90bf-89fe4a41ef03": "The `repository_secret_scanning` activity stream in the provided context is related to the {% data variables.product.prodname_secret_scanning %} feature. It contains repository-level activities related to this feature, which is a security tool that scans a repository for secrets such as passwords, tokens, and encryption keys. This activity stream allows users to monitor and track the usage of this feature for their repositories.", "45a8698b-af34-48e7-9689-12d25c9d7337": "To change the visibility settings of your {% data variables.product.prodname_github_app %}, follow these steps:  1. Go to your account settings on GitHub. 2. Click on \"GitHub Apps\" under the \"Developer settings\" section. 3. Find the {% data variables.product.prodname_github_app %} you want to change the visibility settings for. 4. Click on the {% data variables.product.prodname_github_app %} to open its settings page. 5. Click on \"Advanced settings\" in the left-hand menu. 6. Under \"Danger zone,\" click either \"Make public\" or \"Make private\" depending on the current visibility of your {% data variables.product.prodname_github_app %}. 7. Click \"Save changes\" to apply the new visibility settings.  Note that public apps cannot be made private if they're installed on other accounts.", "77283828-e07e-41fc-886e-c5052aa56291": "In GitHub, the permission level required to configure pre-receive hooks for a repository is repository owner. The organization owner must also have selected the \"Configurable\" hook permission for the pre-receive hook in order for repository owners to configure it. The site administrator can select the \"Administrators can enable or disable this hook\" option when creating the pre-receive hook, but this is not a requirement for repository owners to configure it.", "3b853bda-6e17-4063-b47b-7ff0785320bc": "On April 14, 2021, GitHub announced several plans and pricing changes. Firstly, GitHub Free for organizations is now available and includes private repositories for an unlimited number of users. Secondly, all organizations previously using Team for Open Source now have GitHub Free. Thirdly, GitHub Free for individual developers now includes an unlimited number of collaborators. Fourthly, organizations and individuals using GitHub Free will receive GitHub Community Support. Fifthly, GitHub Pro now includes 2GB of Packages storage and 10GB of data transfer, and its monthly price has been reduced to $4. Sixthly, GitHub Pro now has a reduced monthly price of $4 per user. Seventhly, GitHub Team now has a reduced monthly price of $4 per user, and it no longer requires a minimum of five seats. Lastly, GitHub Team includes 3,000 Actions minutes per month for private repositories. These changes are permanent.", "7c3cdf16-d6b5-4f16-98f6-e2509915a199": "The purpose of creating a webhook secret for a GitHub App is to secure incoming webhook requests by verifying that they originate from GitHub. This is achieved by including the webhook secret in the request headers, which the server can then validate to ensure the request is legitimate. To secure and validate the webhook secret on the server, it should be stored in a secure location, such as an environment variable or a configuration file with restricted access. The server should also validate the webhook secret in each incoming request to ensure it matches the expected value. This helps prevent malicious actors from spoofing webhook requests and executing unauthorized actions on your GitHub App.", "5b99888d-c469-4747-84ca-920a55f085a1": "The context information provided mentions \"AUTOTITLE\" in the {% data variables.product.prodname_ghe_cloud %} documentation, but doesn't explicitly list the different license statuses for an enterprise account on {% data variables.product.prodname_dotcom_the_website %}. However, the context does direct you to the \"License\" tab in the enterprise account settings, where you can view your total licenses, consumed licenses, and subscription expiration date. It also mentions the option to view details for license usage or download a CSV file with license details, as well as the option to view usage details for other features in the left sidebar under \"Billing\". Based on this information, it can be inferred that the different license statuses for an enterprise account on {% data variables.product.prodname_dotcom_the_website %} include total licenses, consumed licenses, and subscription expiration date. However, without access to the \"AUTOTITLE\" documentation, it's unclear if there are any other license statuses that may be relevant.", "6302cc76-62cf-4b26-84b4-4a24e8b91d96": "To create a plan using {% data variables.product.prodname_projects_v2 %}, follow these steps:  1. Go to your repository in GitHub and click on the \"Projects\" tab. 2. Click on the \"New project\" button and select \"Set up a new project\". 3. Choose a project template or create a custom one. 4. Name your project and select the appropriate project board (e.g., Kanban, Scrum). 5. Add columns to your board to represent the stages of your project (e.g., \"To do\", \"In progress\", \"Done\"). 6. Add tasks or issues to your board by dragging them from the \"Issues\" tab or creating new ones directly on the board. 7. Assign tasks to team members, set due dates, and add labels to help categorize and prioritize tasks. 8. Use the \"Iteration\" feature to group tasks by timeframe or release cycle. 9. Use the \"By Status\" feature to group tasks by their current status (e.g., \"Backlog\", \"In Progress\", \"Done\"). 10. To add linked pull requests as a visible field in the board view, click on the \"Customize columns\" button and select the \"By Status\" tab. 11. Click on the \"Add column\" button and select \"Custom\". 12. Name the column (e.g., \"Linked PRs\") and select the \"Linked pull requests\" field from the \"Hidden\" section. 13. Save the column and drag tasks to it as needed.  To use the board view in {% data variables.product.prodname_projects_v2 %} to understand the state of your issues at a glance, follow these steps:  1. Go to your project in GitHub and click on the \"Board\" tab. 2. Use the columns you created to represent the stages of your project. 3. Drag tasks between columns as they progress through the project. 4. Use the \"Iteration\" and \"By Status\" features to group tasks by timeframe or status. 5. Click on a task to view its details and make updates as needed. 6. Use the \"Linked PRs\" column (if you added it) to quickly access related pull requests. 7", "5324c85c-9834-40b4-99f5-87aebf1678b2": "To add a topic to your repository's metadata, follow these steps:  1. Go to your repository on GitHub. 2. Click the gear icon in the top right corner of the page, to the right of \"About\". 3. Under \"Topics\", start typing the topic you want to add to your repository to display a dropdown menu of any matching topics. Click the topic you want to add or continue typing to create a new topic. 4. Optional, if there are \"Suggested\" topics displayed under the \"Topics\" field, click {% octicon \"plus\" aria-label=\"Add this topic\" %} to add or {% octicon \"dash\" aria-label=\"Decline this topic\" %} to decline the suggested topic. 5. After you've finished adding topics, click \"Save changes\".  Note: The {% octicon \"gear\" aria-label=\"Edit repository metadata\" %} icon and the \"Topics\" field may have different labels or icons depending on the GitHub interface you're using.", "83aeb3f4-22cb-4532-a3ef-a3d8ac3665ce": "To check the status of your {% data variables.product.product_name %} cluster using the `gh es` extension for {% data variables.product.prodname_cli %}, you can follow these steps:  1. Install the `gh es` extension for {% data variables.product.prodname_cli %} by running the following command:     ```shell    gh extension install ghe-enterprise-services/cli-extension    ```  2. Log in to your {% data variables.product.prodname_ghe_server %} instance using the `gh auth login` command.  3. Run the `gh es cluster status` command to check the status of your {% data variables.product.product_name %} cluster. This command displays a JSON object containing information about the cluster, including the status of each node and service.  4. To view a summary of the cluster status, run the `gh es cluster summary` command. This command displays a table summarizing the status of each node and service.  5. To view detailed information about a specific node or service, use the `gh es node` or `gh es service` commands, respectively. For example, to view detailed information about the Git service on the first node, run:     ```shell    gh es node 1 --service git    ```  6. To receive notifications when the status of a node or service changes, use the `gh es node` or `gh es service` commands with the `--watch` flag. For example, to receive notifications when the status of the Git service on the first node changes, run:     ```shell    gh es node 1 --service git --watch    ```  7. To stop receiving notifications for a specific node or service, use the `gh es node` or `gh es service` commands with the `--unwatch` flag. For example, to stop receiving notifications for the Git service on the first node, run:     ```shell    gh es node 1 --service git --unwatch    ```  8. To view the configuration of your {% data variables.product.product_name %} cluster, use the `gh es cluster config` command. This command displays a JSON", "7371f60d-53be-4cd9-b758-893105622899": "To retrieve a list of pull requests with their corresponding numbers, titles, and mergeability statuses in JSON format using the `gh pr list` command, you can use the following command:  ```shell gh pr list --json number,title,mergeStateStatus ```  This command will output the list of pull requests in JSON format, with the `number`, `title`, and `mergeStateStatus` fields included. You can then parse this JSON output using a tool like `jq` to extract the desired information.", "b6835838-fee6-4729-a2b0-7a7a4740867d": "To create a default label for a repository in an organization on GitHub, follow these steps:  1. Access the organization on GitHub. 2. Go to the organization settings. 3. Click on \"Repository defaults\". 4. Under \"Repository labels\", click \"New label\". 5. Enter a name and description for the label. 6. Choose a color for the label (optional). 7. Click \"Create label\".  This label will now be applied to all new repositories created in the organization, and can be edited or deleted from the organization settings.", "b8d75c19-017c-4443-b54c-176851d1470b": "To pull a private Docker image inside your codespace at runtime using Docker-in-Docker, you need to ensure that the secrets required for login to Docker are automatically added to the `~/.docker/config.json` file within your codespace. This is done automatically by {% data variables.product.prodname_github_codespaces %} after the `onCreateCommand` lifecycle hook but before `postCreateCommand`, `postStartCommand`, and `preStopCommand`. Therefore, you don't need to take any additional steps to pull a private Docker image inside your codespace at runtime using Docker-in-Docker, as long as the required secrets are properly set up. You can store these secrets at the user, repository, or organization-level, and you can share them securely between different codespaces. When creating a set of secrets for a private image registry, make sure to replace the \"\" in the name with a consistent identifier. If you are setting the secrets at the user or organization level, remember to assign them to the repository you'll be creating the codespace in by choosing an access policy from the dropdown list.", "a215d671-0a26-4592-9ad1-54a0d2af598a": "The Program mentioned in the text material involves participating in a program with potential fees associated. The terms and conditions of the Program include restrictions on the use and distribution of pre-release materials, the possibility of updates, enhancements, or corrections to products and services, and the non-refundability of payments made for the Program. The agreement lasts for one year and can be automatically renewed for successive one-year terms unless terminated by either party with proper notice. The Program Fees are due in U.S. Dollars and are exclusive of taxes. The text material also mentions that GitHub may terminate the agreement for any reason without cause, but it doesn't specify whether the same applies to the participant. Overall, the terms and conditions seem to favor GitHub and impose restrictions on the participant's use of the pre-release materials.", "8682b8a8-3515-4b8f-86f0-969c75b605ea": "GitHub uses cookies on their websites to provide and secure their services, as well as to analyze the usage of their websites in order to offer a great user experience. If you'd like more information about cookies and how and why GitHub uses them, you can refer to their Privacy Statement. To view the current list of cookies on GitHub, you can visit https://github.com/privacy/cookies and sign up to receive updates on any changes to this list. If you have any questions or concerns about a new subprocessor, you can contact GitHub via {% data variables.contact.contact_privacy %}.", "ad79aef0-17b0-4a48-9c57-bec469f1d61a": "To check if you already have a GPG key, you can follow these steps:  1. Open a terminal or command prompt on your computer. 2. Type the following command to list your existing GPG keys:     ```    gpg --list-keys    ```  3. If you have already created a GPG key, you will see a list of your keys, including their IDs and expiration dates.  4. If you do not see any keys listed, it means that you do not have a GPG key set up on your computer. In this case, you will need to follow the steps to generate a new GPG key.  Note: The exact steps to check for existing GPG keys may vary depending on the operating system you are using. If you are unsure how to check for existing GPG keys on your specific operating system, you can search for \"how to check for existing gpg keys [your operating system]\" in your preferred search engine for more information.", "8892606b-3501-472a-8a50-8bcf75bc6162": "Yes, based on the context information provided, I can explain the concept of rate limits in the GraphQL API and how it helps prevent abuse and denial-of-service attacks.  The GraphQL API assigns points to each query and limits the points that can be used within a specific amount of time. This limit helps prevent abuse and denial-of-service attacks by ensuring that the API remains available for all users.  For users, the primary rate limit for the GraphQL API is 5,000 points per hour per user. This includes requests made with a personal access token (PAT) as well as requests made by a GitHub app or OAuth app on behalf of a user that authorized the app. Requests made on a user's behalf by a GitHub app that is owned by a GitHub Enterprise Cloud (GHEC) organization have a higher rate limit of 10,000 points per hour.  By limiting the number of requests that can be made within a certain time period, the GraphQL API can prevent abuse and denial-of-service attacks by malicious actors who might try to overwhelm the API with a large number of requests. This helps ensure that the API remains available and responsive for all users, even during periods of high traffic or usage.  In summary, rate limits in the GraphQL API help prevent abuse and denial-of-service attacks by limiting the number of requests that can be made within a certain time period, and ensuring that the API remains available and responsive for all users.", "7bfadd0a-fa1e-4681-bb4b-98f9520ad72b": "The significance of including the `administrator` attribute in the SAML assertion for a user account on an IdP is that it indicates that the user has administrative privileges within the organization's directory or system. This attribute can be used by service providers (SPs) to make access control decisions based on the user's role within the organization. For example, an SP may grant additional permissions or access to certain resources to users with administrative privileges. Including this attribute in the SAML claim from Azure AD allows for more granular access control and can improve the overall security and functionality of the system.", "13cfbb98-cf76-4c4d-9b1c-55e35fb7f123": "The MIT license granted by COPYRIGHT-HOLDER allows anyone to use, copy, modify, merge, publish, distribute, sublicense, and sell copies of the software, as long as the copyright notice and permission notice are included in all copies or substantial portions of the software. The software is provided \"as is\" without any warranty of any kind, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement. The authors or copyright holders will not be liable for any claim, damages, or other liability arising from, out of, or in connection with the software or the use or other dealings in the software.", "6919e1f6-878d-4f53-b14c-8ec81a542cd4": "The \"Configuration file\" dropdown menu in the context provided allows the user to select the `devcontainer.json` configuration file that they want to use for their prebuilds. This affects the prebuild updates by determining which configuration file will be used to build the dev container image that will be used in the prebuild. Choosing a different configuration file may result in a different set of dependencies being installed, which could affect the behavior of the prebuild. The user can select a different configuration file to test different versions of their application or to ensure that their prebuilds are using the correct set of dependencies.", "1f8214f5-7065-4018-8005-2e0363ce5e99": "The name of the current job can be obtained using the variable `CI_JOB_NAME`. This variable holds the name of the current job being executed in the GitHub Actions workflow.", "f33e71d6-fbd0-4930-bd4b-4a4f108127a4": "To create teams in {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to your destination organization in {% data variables.product.prodname_dotcom %}. 2. Click on the organization settings icon (the gear icon) in the upper-right corner of the page. 3. Select \"Teams\" from the drop-down menu. 4. Click the \"New team\" button in the upper-right corner of the page. 5. Enter a name for the team and click \"Create\". 6. (Optional) Add a description for the team. 7. (Optional) Add members to the team by searching for their usernames or email addresses and clicking the \"Add members\" button. 8. (Optional) Set the team's permissions by selecting the appropriate repository and permission level from the drop-down menus. 9. Click \"Save\" to create the team.  Alternatively, you can also create teams while adding members to a repository. For more information, see \"AUTOTITLE.\"", "a4aa89ce-f047-491f-932d-022982d5500c": "Without further context, it's unclear why product.prodname_github_app would be owned by an individual account. However, in some cases, an individual developer or contributor may have created and managed the GitHub app for a specific product or feature. This could be due to the app's specific functionality or the developer's role in the project. Alternatively, the app may have been created for personal use or testing purposes, in which case it would also be owned by an individual account. Without more information, it's difficult to provide a specific scenario.", "3cdfe45d-e142-4ab5-b35b-605d22b2e8ed": "To manually delete a codespace in {% data variables.product.prodname_vscode %}, follow these steps:  1. Open {% data variables.product.prodname_vscode %} and connect to your GitHub account. 2. Open the GitHub Codespaces tab in the left-hand menu. 3. Find the codespace you want to delete and right-click on it. 4. Select \"Delete\" from the drop-down menu. 5. Confirm the deletion by clicking \"Delete\" in the confirmation dialog box.  Alternatively, you can use the {% data variables.product.prodname_cli %} to delete a codespace from the command line. For more information, see the \"Deleting a codespace\" section in the context information provided.", "8923f70e-555b-41cb-bcc0-8bf667e0edfb": "The features that can be managed by an organization owner on GitHub for {% data variables.product.prodname_pages %} are:  - Configure a publishing source for {% data variables.product.prodname_pages %} (g) - Enable project boards (h)  Note: The context information provided doesn't mention anything about managing {% data variables.product.prodname_pages %} specifically, so it's unclear whether the other features listed in the context can also be managed for {% data variables.product.prodname_pages %}. However, based on the documentation, it seems that only the features listed above are specifically related to managing {% data variables.product.prodname_pages %} for an organization. For more information, you can refer to the official GitHub documentation on managing organization settings for {% data variables.product.prodname_pages %}: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site/configuring-a-publishing-source-for-your-github-pages-site#configuring-a-publishing-source-for-your-github-pages-site-using-an-organization-repository.", "09df19ec-8749-402c-af56-1fc4b325632d": "To test your webhook locally using the provided JavaScript example, follow these steps:  1. Clone the repository containing the JavaScript example to your local machine or codespace. 2. Navigate to the directory containing the JavaScript file. 3. Install the required dependencies by running the following command:  ```shell copy npm install express ```  4. Start the server by running the following command:  ```shell copy node index.js ```  5. In a separate terminal window or tab, send a test event to your local server by running the following command:  ```shell copy curl -X POST -H \"Content-Type: application/json\" -d '{\"name\": \"test\"}' http://localhost:3000/webhook ```  6. Verify that the server received the test event by checking the console output. The server should respond with a message indicating that the delivery was successfully received.  Note: Make sure to replace `http://localhost:3000` with the URL of your server if you're running the example on a different machine or codespace.  This test should help you confirm that your webhook is properly configured and that your server can handle webhook deliveries.", "ffbd25b6-c09a-4b52-ad0a-7dc06cea5f5e": "{% data variables.product.prodname_dotcom %} casts data types to a string for comparison purposes using the following conversions:  - Null: `''` (empty string) - Boolean: `'true'` or `'false'` - Number: Decimal format for smaller numbers, exponential format for larger numbers - Array: Arrays are not converted to a string - Object: Objects are not converted to a string  For example, `contains('123', '12')` returns `true`, and `contains(fromJSON('[\"push\", \"pull_request\"]'), \"pull_request\")` returns `true`. If `search` is an array, `contains()` returns `true` if the `item` is an element in the array. If `search` is a string, `contains()` returns `true` if the `item` is a substring of `search`. This function is not case sensitive.", "2dda91e0-e3c0-4df8-9152-a8ad7292e278": "To edit a global webhook using the REST API, you can follow these steps:  1. Make a GET request to the `/repos/{owner}/{repo}/settings/webhooks` endpoint to retrieve the list of webhooks for the specified repository. 2. Find the webhook with the desired ID or name. 3. Make a PUT request to the `/repos/{owner}/{repo}/settings/webhooks/{webhook_id}` endpoint with the updated webhook configuration in the request body. 4. Make a GET request to the `/repos/{owner}/{repo}/settings/webhooks/{webhook_id}` endpoint to confirm that the webhook was updated successfully.  Additional information about using the REST API to edit a global webhook can be found in the \"AUTOTITLE\" documentation, which provides detailed instructions and examples for working with webhooks using the GitHub API. This documentation covers topics such as creating, updating, and deleting webhooks, as well as handling webhook events and configuring webhook settings.", "5e28718e-2d14-4436-86ac-6bc1cc4f56da": "To specify the timeout for a specific job in a workflow using GitHub Actions, you can use the `timeout-minutes` field under the `jobs` and `steps` sections. For example, to set a timeout of 30 minutes for a job named \"my_job\", you can define it as follows:  ```yaml name: My Workflow  jobs:   my_job:     timeout-minutes: 30     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Run tests         uses: actions/setup-node@v2         with:           node-version: 14         run: npm test ```  In this example, the `timeout-minutes` field is set to 30 under the `my_job` section, and it applies to all steps within that job. If you want to set a different timeout for a specific step, you can also use the `timeout-minutes` field under the `step` section. For example:  ```yaml name: My Workflow  jobs:   my_job:     steps:       - name: Checkout         uses: actions/checkout@v2        - name: Run tests         uses: actions/setup-node@v2         with:           node-version: 14         run: npm test         timeout-minutes: 20        - name: Deploy to production         uses: my-org/my-action@v1         with:           environment: production         timeout-minutes: 60 ```  In this example, the `timeout-minutes` field is set to 20 for the second step, and it applies only to that step. The third step has a different timeout of 60 minutes.", "963af657-c724-4177-bb64-b723844a9b8e": "{% ifversion fpt or ghec %}  {% data variables.secret-scanning.user_alerts %} in {% data variables.product.prodname_dotcom %} generate alerts for secrets issued by a large variety of service providers as well as for secrets that match specified custom patterns. These alerts can be of two types: high confidence alerts, which relate to supported patterns, and non-provider alerts, which correspond to secrets such as private keys and have a higher ratio of false positives. Users can view these alerts on the **Security** tab of the repository in {% data variables.product.prodname_dotcom %}. {% endif %}  If the context doesn't provide information about whether {% data variables.product.prodname_fpt %} or {% data variables.product.prodname_ghec %} is being used, then say that {% data variables.secret-scanning.user_alerts %} in {% data variables.product.prodname_dotcom %} generate alerts for secrets issued by a large variety of service providers as well as for secrets that match specified custom patterns. These alerts can be of two types: high confidence alerts, which relate to supported patterns, and non-provider alerts, which correspond to secrets such as private keys and have a higher ratio of false positives. Users can view these alerts on the **Security** tab of the repository in {% data variables.product.prodname_dotcom %}. However, it's not specified whether {% data variables.product.prodname_fpt %} or {% data variables.product.prodname_ghec %} is being used, so it's unclear whether user alerts are being reported or if this refers to {% data variables.product.prodname_secret_scanning %} alerts in general. In this case, more context is needed to provide a complete answer.", "d2a4b739-1330-4daa-97ab-2f1d7f4bf5ac": "The relationship between the Company and the Government, as stated in \"The Parties are independent entities and nothing in this Amendment or ToS creates an agency, partnership, joint venture, or employer/employee relationship,\" is that they are separate and distinct entities with no legal or financial ties beyond the terms of the Amendment and ToS.", "147c5e13-14ed-40bd-b80b-6bb7d42f64e9": "The purpose of the `startRepositoryMigration` mutation in the context of the {% data variables.product.prodname_ghe_cloud %} Enterprise Migration Tool is to initiate the migration process for a specific repository from an on-premises {% data variables.product.prodname_ghe_server %} instance to a {% data variables.product.prodname_ghe_cloud %} organization. This mutation takes various input parameters such as the source repository ID, owner ID, repository name, access token, GitHub PAT, Git archive URL, metadata archive URL, source repository URL, and target repository visibility to initiate the migration process. The mutation returns the ID and migration source details of the repository migration. After initiating the migration, the status of the migration can be checked, and the error log can be validated to ensure the success of the migration.", "2f2f9df6-2160-4ba8-9f35-2ba9a50760e2": "Without further context, it's unclear what software or application the \"AUTOTITLE\" feature is referring to. However, in general, the \"AUTOTITLE\" feature in software or applications can automatically generate a title for a document, presentation, or video based on its content or metadata. This feature can be useful in situations where a user forgets to add a title or wants to quickly generate a title for a large number of documents. For example, in a legal setting, a document management system may use the \"AUTOTITLE\" feature to automatically generate a title based on the case number and document type, making it easier to search and organize documents. In a video editing software, the \"AUTOTITLE\" feature can automatically generate a title based on the video's content, such as the name of the location or the main topic being discussed. Overall, the \"AUTOTITLE\" feature can save time and improve organization in various applications.", "fe8eca99-4134-4b21-a967-b3683425faa8": "To determine the `unit_count` for a specific user's account, we need to access the `unit_count` key in the `plan` object returned by the `GET /user/subscriptions` endpoint. This endpoint requires the authenticated user's access token, which can be obtained using the `GET /user/authorizations` endpoint. If the user's account has multiple subscriptions, we need to identify the specific subscription for which we want to retrieve the `unit_count`. If the user's account has a free GitHub plan and has purchased a free {% data variables.product.prodname_marketplace %} plan, the `unit_count` will be `nil`.  In summary, to answer the question, we need to make a request to the `GET /user/subscriptions` endpoint, parse the response, and extract the `unit_count` for the specific subscription we're interested in. If the `unit_count` is `nil`, it means the user hasn't purchased any units for that subscription.", "fc415d2f-ec4f-4936-a390-79b5f1b512dd": "To request publisher verification on GitHub, follow these steps:  1. Go to your organization's settings by clicking on your profile picture in the top right corner of the page, selecting \"Settings\" from the drop-down menu, and then clicking on \"Organizations\" from the left-hand menu. 2. Select your organization from the list of organizations you manage. 3. Click on \"Developer settings\" at the bottom of the left-hand menu. 4. Under \"Developer settings,\" click on \"Publisher Verification.\" 5. Complete the checklist by ensuring that your basic profile information is accurate, Two-factor authentication is enabled for your organization, and a verified domain is submitted. 6. Click \"Request Verification.\" 7. GitHub will review your details and let you know once your publisher verification is complete.  For more information about publishing apps, refer to the related resources provided in the context information.", "25504f01-f03e-4b0b-ac0f-1f436327c8f2": "For {% data variables.product.prodname_marketplace %} webhooks, the owner of the app or an app manager for the organization that owns the app can edit the webhook.  For {% data variables.product.prodname_sponsors %} webhooks, only the owner of the sponsored account can edit sponsorship webhooks for that account.  For {% data variables.product.prodname_github_app %} webhooks, the webhook for the app cannot be deleted, but it can be activated or deactivated. There is no information provided about who has permission to edit the webhook for a {% data variables.product.prodname_github_app %}.  If the context doesn't provide enough information to determine who has permission to edit a webhook, then say that you weren't given enough information to answer.", "8caf70bf-8e8e-49fb-a7d7-514ac666eabf": "In the context provided, an interpreted extension is an extension that uses a scripting language like Bash to execute commands, while a precompiled extension is an extension that is compiled into binary executables. The choice between the two depends on the specific use case and the user's environment. Interpreted extensions are more flexible and easier to develop, as they allow for dynamic code execution and easier debugging. However, they may require the user to have the necessary interpreter installed, which could be a limitation in some environments. Precompiled extensions, on the other hand, are more portable and do not require the user to have the interpreter installed, but they may be more difficult to develop and debug due to the need for compilation. In general, interpreted extensions are recommended for simpler use cases, while precompiled extensions are recommended for more complex or performance-critical use cases.", "a4f31bf5-c854-4142-9b3d-3871165c7e90": "To validate a restore using the `ghe-restore` command, you can configure an IP exception list to allow access to a specified list of IP addresses. This step is optional, but it can help you ensure that only authorized users can access the restored instance. After running the `ghe-restore` command, you should visit the `https://<REPLACEMENT_INSTANCE_IP>/setup/settings` URL to review the appliance configuration and verify that the restore was successful. This step is required to ensure that the restored instance is properly configured and ready for use.", "70382fb6-3aed-4383-9c5d-3e4abcb6a025": "If you forget your password and lose access to your two-factor authentication credentials on {% data variables.location.product_location %}, you can start account recovery by following these steps:  1. Click \"Forgot password?\". 2. Enter a primary or backup email address associated with your account on {% data variables.location.product_location %}, then click \"Send password reset email.\". 3. Check your email for a link to reset your password. You must click on this link within three hours of receiving the email. If you don't see an email from us, make sure to check your spam folder. 4. Click on the link in the email, then under \"Having problems?\", click \"Start a 2FA recovery request\". 5. To complete your recovery request, you'll need to verify an alternative authentication factor. {% data reusables.accounts.alternative-authentication %} 6. {% data reusables.accounts.support-request-recovery %}  If no recovery methods are available, you can choose to unlink your email address from your account. The email address is then available for you to link it to a new or existing account, maintaining your commit history. For more information, see \"AUTOTITLE\".  Note: The context information provided does not specify what alternative authentication factors are available for recovery. It's best to refer to the documentation for {% data variables.location.product_location %} for a list of supported recovery methods.", "1ceea300-2148-4443-bb1f-5ec84725c277": "To delete a custom organization role in a GitHub organization as an organization owner or a user with the \"Manage custom organization roles\" permission, follow these steps:  1. Go to the organization's settings by clicking on the organization's name in the top left corner of the GitHub page, then clicking on \"Settings\" in the drop-down menu. 2. In the left-hand menu, click on \"Members\" to view the list of users and teams in the organization. 3. Click on \"Custom roles\" in the left-hand menu to view the list of custom roles in the organization. 4. Find the custom role you want to delete and click on its name to view its details. 5. Click on the \"Delete role\" button in the right-hand menu. 6. Confirm that you want to delete the custom role by clicking on the \"Delete role\" button in the confirmation dialog.  Note that deleting a custom role will also delete all users and teams assigned to that role, as well as any permissions associated with that role. Make sure you understand the implications of deleting a custom role before proceeding.", "6f331ebc-d78f-46c5-a688-5666204e269c": "To enable SAML for an enterprise account, you need to follow the instructions provided. These instructions will outline the necessary steps and required information for the setup process. Specifically, you will need information related to your enterprise's SAML configuration, such as the SAML metadata, identity provider (IDP) URL, and service provider (SP) URL. The exact details will vary depending on your specific enterprise's SAML implementation, but the setup instructions should provide clear guidance on what is needed.", "ae879065-8159-4db7-8fd9-ec740aeb82e4": "Reducing the number of open pull requests for {% data variables.product.prodname_dependabot %} can help prevent timeouts during security updates by freeing up resources and allowing {% data variables.product.prodname_dependabot %} to review and generate pull requests for security updates in a timely manner. When the limit on the number of open pull requests is reached, {% data variables.product.prodname_dependabot %} cannot open any more pull requests, which can result in timeouts during security updates. By reviewing and merging some of the open pull requests, you can create space for new security update pull requests and reduce the likelihood of timeouts. Additionally, you can adjust the limit for version update pull requests using the `open-pull-requests-limit` parameter in the configuration file to prevent version update pull requests from blocking the creation of security update pull requests.", "5380f8d4-316c-40ba-b36e-cf4b498d47ac": "The purpose of using environment variables in actions is to provide a way to pass values to the action's execution environment, allowing it to access and manipulate data without hardcoding it into the action's code. This makes the action more flexible and reusable, as the values can be easily changed without modifying the action itself.  To set environment variables for a single workflow, you can define them in the workflow file using YAML syntax. Here's an example:  ```yaml name: My Workflow  on:   push:     branches:       - main  jobs:   my-job:     runs-on: ubuntu-latest      steps:     - name: Checkout       uses: actions/checkout@v2      - name: Set up JDT (Java Development Toolkit)       uses: actions/setup-java@v2       with:         java-version: '11'      - name: Build and Test       uses: my-org/my-action@v1       with:         my-env-var: ${{ secrets.MY_ENV_VAR }} ```  In this example, we're defining a workflow that runs on the `main` branch push events. The workflow consists of three steps: `Checkout`, `Set up JDT (Java Development Toolkit)`, and `Build and Test`.  In the `Set up JDT (Java Development Toolkit)` step, we're using the `actions/setup-java` action to install the Java Development Toolkit (JDT) with version `11`. We're also passing a `java-version` parameter to the action, which is used to specify the version of Java to be installed.  In the `Build and Test` step, we're using our own action, `my-org/my-action`, with version `v1`. We're also passing an environment variable, `my-env-var`, to the action using the `with` keyword. This variable is defined as a secret in the repository, and its value is accessed using the `${{ secrets.MY_ENV_VAR }}` syntax.  By using environment variables in actions, we can make our", "2aa5b8db-8dcb-43a8-a3d4-ec3aa339a245": "Git's ability to modify previous commits and changes is considered a difference from SVN's approach to project history because SVN assumes that the history of a project never changes. This means that once a commit is made in SVN, it cannot be altered or deleted. In contrast, Git allows you to manipulate previous commits and changes using tools like `git rebase`. This feature can be used in Git to make corrections to previous commits, such as fixing typos, updating commit messages, or removing sensitive data. It can also be used to reorganize commits, such as squashing multiple commits into one or splitting a commit into multiple commits. However, it's important to note that modifying previous commits can have consequences, such as changing the SHA-1 hash of the commit and potentially causing conflicts with other branches or forks that rely on that commit. Therefore, it's recommended to use this feature with caution and only when necessary.", "1129c0f2-5ebe-47f7-ba46-0316c56a7818": "The provided workflow configures Maven for publishing packages to both Maven Central Repository and GitHub Packages by calling the `setup-java` action twice, once for each repository. During the first call, the `ossrh` repository is configured with authentication options set to environment variables defined in the next step. During the second call, the `setup-java` action automatically configures the Maven _settings.xml_ file for GitHub Packages. The workflow then performs the actual publishing steps using the `publish-to-maven` and `publish-to-packages` actions, respectively. The authentication options for GitHub Packages are also set as environment variables.", "5763e368-940b-4ccd-b0d9-3ae5cd2b35df": "Making a {% data variables.product.prodname_github_app %} public or private affects who can install it. Public apps can be installed by anyone with a GitHub account, while private apps can only be installed by the owner of the repository or organization where the app is installed. Private apps are also hidden from search results and cannot be discovered by other users. Public apps, on the other hand, can be discovered and installed by other users, which can lead to more installations and potentially more users for your app. However, private apps may be more appropriate for apps that contain sensitive data or are intended for use by a specific group of users.", "6921f1da-c396-4829-9267-3c375054688b": "The list of people with access to a repository can be used for security checkups and compliance purposes by helping to off-board people, gather data for compliance, and perform general security checkups. This information can be obtained by viewing the combined overview of teams and people with access to the repository in the repository settings or by exporting a CSV list of people with access to the repository (for organizations using GitHub Enterprise Cloud). This list can assist in ensuring that only authorized individuals have access to sensitive information and can help to identify any potential security risks or compliance issues.", "079122c7-3960-4149-a706-12e214889224": "To invite a new member to join your organization on GitHub, follow these steps:  1. Go to your organization's settings by clicking on the organization's name in the top left corner of the GitHub page, and then selecting \"Settings\" from the drop-down menu. 2. Click on the \"People\" tab in the left-hand menu. 3. Click on the \"Add people\" button in the upper right-hand corner of the page. 4. Enter the GitHub username or email address of the person you want to invite. You can also enter multiple email addresses or usernames separated by commas. 5. Select the role you want to assign the new member from the \"Role\" drop-down menu. 6. Click the \"Send invitations\" button to send the invitations. 7. The new members will receive an email with a link to accept the invitation. Once they accept, they will be added to your organization.  Note: If you're using a paid subscription, make sure you have an unused license available before inviting new members. Also, if your organization requires two-factor authentication, new members will need to enable it before accepting the invitation.", "3c372c3f-96eb-4d55-b60a-3d0492e34d29": "The built-in `GITHUB_TOKEN` is suitable for making authenticated API requests within the repository where the workflow is running. However, if you need to access resources outside the repository, such as resources in an organization or in another repository, you should use a {% data variables.product.prodname_github_app %}. A {% data variables.product.prodname_github_app %} provides more granular permissions and can access resources across multiple repositories and organizations. In contrast, a {% data variables.product.pat_generic %} is limited to a single repository and has more restricted permissions. Therefore, it's more appropriate to use a {% data variables.product.prodname_github_app %} over a {% data variables.product.pat_generic %} when you need to access resources outside the repository where the workflow is running.", "e31865a2-b755-458d-9c20-5ee957efe6fe": "To enable {% data variables.product.prodname_actions %} for your repository, follow these steps:  1. Go to your repository on GitHub. 2. Click on the settings icon in the top right corner of the page. 3. Select \"Settings\" from the drop-down menu. 4. Click on \"Actions\" in the left-hand menu. 5. Under \"Actions permissions,\" select \"Enable actions.\" 6. Click \"Save\" to apply the changes.  Enabling {% data variables.product.prodname_actions %} allows you to automate tasks and workflows in your repository using GitHub Actions. This can help you save time and improve efficiency by automating repetitive tasks, such as building and testing your code, or deploying your application. However, it's important to note that enabling {% data variables.product.prodname_actions %} also gives other users with push access to your repository the ability to create and run workflows, which could potentially modify your code or trigger unwanted actions. To mitigate this risk, you may want to consider setting up a policy to limit the actions and workflows that can be used in your repository.", "4c751c48-860c-47a7-84ec-4ea4b51ce9a3": "The path to the project containing the merge request being tested in this context is `CI_MERGE_REQUEST_PROJECT_PATH` which is equal to `{% raw %}${{ github.repository }}`{% endraw %}.", "6c7b2f83-bb4e-4d73-9582-01c80bdbf26e": "The prerequisites required to use {% data variables.product.prodname_actions_importer %} for migrating Bitbucket Pipelines to GitHub Actions are:  1. Node.js version 12 or higher installed on your local machine. 2. The {% data variables.product.prodname_actions_importer %} CLI extension installed globally. 3. A GitHub account with a repository containing Bitbucket Pipelines. 4. A Bitbucket Cloud account with a repository containing Bitbucket Pipelines. 5. The repository containing Bitbucket Pipelines must be linked to your GitHub account. 6. The repository containing Bitbucket Pipelines must have the {% data variables.product.prodname_actions %} feature enabled. 7. The repository containing Bitbucket Pipelines must have the {% data variables.product.prodname_actions_importer %} feature enabled. 8. The repository containing Bitbucket Pipelines must have the {% data variables.product.prodname_actions_importer %} CLI extension installed locally. 9. The repository containing Bitbucket Pipelines must have the {% data variables.product.prodname_actions_importer %} CLI extension version 1.x.x or higher installed locally. 10. The repository containing Bitbucket Pipelines must have the {% data variables.product.prodname_actions_importer %} CLI extension configured with your GitHub and Bitbucket Cloud credentials.  If any of these prerequisites are not met, then {% data variables.product.prodname_actions_importer %} cannot be used to migrate Bitbucket Pipelines to GitHub Actions.", "1b41325f-4193-4b79-a445-228c4ce6591d": "To modify the title of a tasklist in a {% data variables.product.product_name %} issue using the UI, follow these steps:  1. Go to the issue containing the tasklist you want to rename. 2. Click the three dots (kebab icon) in the top-right corner of the issue. 3. Select \"Edit\" from the drop-down menu. 4. Locate the tasklist you want to rename and click the three dots (kebab icon) next to it. 5. Select \"Rename\" from the drop-down menu. 6. Type the new title for the tasklist and press Enter. 7. Click \"Update comment\" to save the changes.  Alternatively, you can edit the issue's Markdown directly to modify the tasklist title. Locate the tasklist in the issue's body, and replace the existing title with the new one. Remember to save the changes by clicking \"Update comment\" at the bottom of the page.", "ee200a57-b098-4c36-bc3d-2ba923c0d6ce": "Before enforcing SAML single sign-on in an organization, you should review organization membership, enable SAML SSO, and ensure that organization members have signed in and linked their accounts with the IdP, according to the provided context information.", "fce209ef-af53-4ba3-8153-2e4a65161d7d": "To configure your project's built-in workflows to automatically archive items, you need to follow these steps:  1. Access your project's workflows by clicking on \"Settings\" in the project's menu, then selecting \"Workflows\" from the left-hand sidebar. 2. In the \"Default workflows\" list, click \"Auto-archive items\". 3. If you're using the new workflows UI, click \"Edit\" in the top right corner. 4. Check the item type(s) you want to automatically archive. 5. In the \"Filters\" field, type the filter criteria you want to use to automatically archive items. You can only use the `is`, `reason`, and `updated` filters. 6. Save your changes and turn on the workflow by clicking \"Save and turn on workflow\" (if you're using the new workflows UI).  The `is` filter can have the values `open`, `closed`, `merged`, `draft`, `issue`, or `pr`. The `reason` filter can have the value `completed` or `\"not planned\"`. The `updated` filter can have the values `14d` (the last 14 days), `3w` (the last 3 weeks), or `1m` (the last month).  Note that items will be marked as updated when field values in your project are changed, and that items in your project that already meet your criteria will also be archived when you enable automatic archiving. Additionally, projects have a limit on the number of archived items they can contain, which is {% data variables.projects.archived_item_limit %}. For more information on permanently deleting items, see the documentation provided.", "8000c017-f850-4ec5-a472-2ffdf5d4f60d": "{% data variables.product.prodname_dependabot_security_updates %} is a feature provided by {% data variables.product.prodname_dotcom %} that simplifies the process of fixing vulnerable dependencies in a repository. When a security vulnerability is detected in a dependency, {% data variables.product.prodname_dependabot %} automatically creates a pull request to update the dependency to a fixed version, without disrupting the dependency graph. This feature is available for repositories that have enabled dependency graphs and {% data variables.product.prodname_dependabot_alerts %}. It only triggers security updates for dependencies that are specified in a manifest or lock file. This helps developers to quickly and easily address security issues in their dependencies, without having to manually update them.", "bcba58d0-80dd-4af0-98bc-bfa1bc88f984": "To format text using Kramdown's Markdown syntax in {% data variables.product.prodname_dotcom %} Flavored Markdown Spec, you can follow these basic formatting rules:  1. Headings: Use one or more # symbols at the beginning of a line to create headings of different sizes. For example, # Heading 1 is the largest heading, and ## Heading 2 is a smaller heading.  2. Bold and italic text: Enclose text in two asterisks (**) or underscores (_) to make it bold or italic, respectively. For example, **bold text** and _italic text_.  3. Lists: Use asterisks (*), plus signs (+), or hyphens (-) followed by a space to create unordered lists. For example, * item 1, * item 2, and * item 3. Use numbers followed by periods (.) to create ordered lists. For example, 1. Item 1, 2. Item 2, and 3. Item 3.  4. Links: Enclose the URL in square brackets [ ] and the text you want to display in round brackets ( ). For example, [GitHub](https://github.com).  5. Images: Enclose the image filename in square brackets [ ] and the image URL in parentheses ( ). For example, ![GitHub Logo](https://github.githubusercontent.com/images/modules/logos/marketing/GitHub-Logo.png).  6. Block quotes: Use a > symbol at the beginning of a line to create a block quote. For example, > This is a block quote.  7. Horizontal rule: Use three or more asterisks (*** ), underscores (_ _ _), or hyphens (- - -) to create a horizontal rule. For example, *** or ___ or - - -.  These are just the basics, and Kramdown supports many more formatting options. For a complete list, refer to the Kramdown documentation.", "dd788b4d-0741-457d-9559-4a6d36c08a9c": "According to the context provided, it seems that the feature to show or hide specific fields on the roadmap layout when the side panel opens is not mentioned. Therefore, I'm afraid I'm unable to provide an answer to your question based on the given context. You may want to check the documentation or contact GitHub support for further assistance.", "663bec76-8066-43ae-a567-7517a45b9717": "To remove the \"Deadline extended\" label from a student or group's entry in the classroom roster, you should follow these steps:  1. Go to the classroom roster. 2. Locate the student or group whose deadline you want to revoke. 3. Click on the three dots (kebab menu) next to their name. 4. Select \"Revoke extension\" from the dropdown menu. 5. In the modal that appears, select \"Revoke STUDENT OR GROUP'S assignment extension\". 6. Click \"I understand, revoke the extension\". 7. The \"Deadline extended\" label will be removed from the student or group's entry in the classroom roster.  Note: These steps are provided in the context information, so if you didn't have access to that information, you wouldn't be able to answer this question.", "46d9e143-a238-4d74-84ca-80b07f775f2e": "Context information such as the user's access token and the available scopes can be used to detect changes in token scopes. By making authenticated requests with the access token and checking the response, the application can determine which scopes are currently active. If a previously granted scope is no longer present, the application can inform the user of the change and explain the impact on available application functionality. This can be done through a user interface element such as a notification or a message displayed during the login process. Additionally, the application can provide a way for the user to re-grant the missing scope if desired. By keeping the user informed of changes to their token scopes, the application can ensure a consistent and seamless user experience.", "f3bd495c-7f35-42e4-8795-18dbd4f4d137": "The context information suggests setting up a wildcard Domain Name System (DNS) record or individual DNS records for the subdomains listed above. It recommends creating an A record for `*.HOSTNAME` that points to your server's IP address to avoid creating multiple records for each subdomain. This would be a wildcard DNS record.", "fcb646a4-2d78-4e35-b63a-b56f08f18f08": "According to the information provided in the text material, if one party wishes to enter into a contractual agreement with the other party, they should first establish a similar relationship between the Parties, as the current relationship is described as having a similar relationship but neither Party can bind the other contractually. The text suggests contacting the other party to clarify the next steps in establishing a contractual relationship. Therefore, the party wishing to enter into a contractual agreement should contact the other party to discuss the necessary steps to establish a contractual relationship.", "83f4bb8f-ebf3-488f-8b6f-95cb7d17306f": "If your enterprise is connected to an Azure VNET, you can give runner groups access to the virtual network by configuring private network access for larger runners. For more information, see \"AUTOTITLE.\"    Changing the name of an enterprise runner group  {% data reusables.actions.runner-groups-enterprise-navigation %} {% data reusables.actions.changing-name-for-a-runner-group %}    Changing the name of an organization runner group  {% data reusables.actions.runner-groups-org-navigation %} {% data reusables.actions.changing-name-for-a-runner-group %}    Deleting a runner group  {% ifversion ghec %} You can delete runner groups at the enterprise and organization levels.  - Deleting an organization runner group - Deleting an enterprise runner group    Deleting an organization runner group  {% data reusables.actions.runner-groups-org-navigation %} {% data reusables.actions.deleting-a-runner-group %}    Deleting an enterprise runner group  {% data reusables.actions.runner-groups-enterprise-navigation %} {% data reusables.actions.deleting-a-runner-group %}    Adding a self-hosted runner to a runner group  {% data reusables.actions.runner-groups-org-navigation %} {% data reusables.actions.adding-a-self-hosted-runner-to-a-runner-group %}    Adding a self-hosted runner to an enterprise runner group  {% data reusables.actions.runner-groups-enterprise-navigation %} {% data reusables.actions.adding-a-self-hosted-runner-to-a-runner-group %}    Removing a self-hosted runner from a runner group  {% data reusables.actions.runner-groups-org-navigation %} {% data reusables.actions.removing-a-", "3aa46ac3-caf9-4c8f-abf2-2dd661b6814a": "In this context, the `gradle/gradle-build-action` action is being used to publish a package. The action being used is `gradle/gradle-build-action` with the `publish` argument. The `GITHUB_TOKEN` environment variable will be set with the content of the `GITHUB_TOKEN` secret, and the `permissions` key will specify the access that the `GITHUB_TOKEN` secret will allow. This action can be used to publish packages to both the Maven Central Repository and {% data variables.product.prodname_registry %}. The configuration for publishing to both repositories is included in the _build.gradle_ file, with each repository specified in a distribution management repository with a unique `name`. In this example, the organization is named \"octocat\" and the repository is named \"hello-world\". The configuration for the Maven Central Repository provider is specified with a `name` of \"OSSRH\", and the configuration for {% data variables.product.prodname_registry %} is specified with a `name` of \"GitHubPackages\".", "60730cc9-3332-4d1b-be8c-164923303453": "To automatically open specific files in the VS Code web client when a codespace is created from a template, you can configure your `devcontainer.json` file with the `openFiles` property under the `customizations` object for the `codespaces` section. This property should be an array containing the relative paths to the files you want to open automatically. For example:  ```json {   \"customizations\": {     \"codespaces\": {       \"openFiles\": [         \"src/App.js\",         \"package.json\"       ]     }   } } ```  In this example, the `App.js` and `package.json` files will be automatically opened in the VS Code web client when a codespace is created from this template.", "0e9216bc-9352-4c69-97e7-4e75bf9114a2": "To confirm the transfer of an OAuth app to a new owner in {% data variables.product.prodname_dotcom %}, you need to type the name of the OAuth app you want to transfer in the text field provided and then enter the name of the new owner's {% data variables.product.prodname_dotcom %} username or organization name. After clicking \"Transfer this application\", the new owner will receive a transfer request that they need to complete by navigating to their OAuth apps page, clicking on the app name, and then clicking \"Complete transfer\".", "a79b2f0a-b073-4847-ae84-ee74f9ba55d2": "According to the \"Recipients\" column in the table, GitHub shares sensitive data with service providers and user-directed entities for the purposes of transacting commerce, processing transactions, fulfilling orders, providing Services, helping, securing, and troubleshooting, and detecting and preventing fraud. Specific examples of sensitive data that may be shared with service providers and user-directed entities include account login information, financial account numbers, debit or credit card numbers, and security or access codes. These entities are likely involved in tasks such as payment processing, fraud detection, and customer support. However, it's important to note that GitHub only shares sensitive data with these entities to the extent necessary and proportionate for the specific purpose at hand.", "a63c0507-8213-4e74-894a-2f13605d3a15": "Without further context, it is unclear what \"rebase\" options are and what the significance of the \"AUTOTITLE\" command is in this context. However, based on the repetition of the command, it is possible that \"AUTOTITLE\" is a command used in a specific software or tool, and it may be related to setting a title or label for a specific task or operation. Without more information, it is difficult to provide a definitive answer.", "b430f2c4-1ea8-4ca8-b380-de9867ad5877": "On {% data variables.product.prodname_dotcom %}, there are three types of accounts: personal accounts, organization accounts, and enterprise accounts (for enterprise plans only). Access permissions differ for each type as follows:  1. Personal accounts:    - Repository owner: Has complete administrative access to the repository, including the ability to add and remove collaborators, manage repository settings, and push code.    - Collaborators: Can push code, create and manage branches, and pull requests.  2. Organization accounts:    - Owners (for FPTP or GHEC plans): Have complete administrative access to the organization, including the ability to add and remove members, manage billing settings (for FPTP or GHEC plans), and manage organization settings.    - Billing managers (for FPTP or GHEC plans): Can manage billing settings (for FPTP or GHEC plans) but cannot manage organization settings or add and remove members.    - Members: Can access and contribute to repositories owned by the organization, but cannot manage organization settings or add and remove members.  3. Enterprise accounts (for FPTP plans only):    - Enterprise owners: Have ultimate power over the enterprise account and can take every action in the enterprise account.    - Billing managers (for GHEC or GHES plans): Can manage billing settings (for GHEC or GHES plans) but cannot manage enterprise settings or add and remove members.    - Members: Are automatically members of the enterprise account but have no access to the enterprise account itself or its settings. They can only access and contribute to repositories owned by organizations within the enterprise account.  Note that for enterprise accounts using {% data variables.product.prodname_emus %}, members are provisioned as new personal accounts on {% data variables.product.prodname_dotcom %} and are fully managed by the identity provider. They have read-only access to repositories that are not a part of their enterprise and cannot interact with users that are not members of their enterprise.", "8a5b3057-cb99-4cfd-9bc9-5210fc69b988": "According to the context information provided, a {% data variables.secret-scanning.alerts %} can be closed as \"used in tests\" if it is found in a testing environment and is not intended to be used in production. This means that the secret was accidentally or intentionally included in a test environment, but it is not a security concern in the production environment. The \"used in tests\" resolution indicates that the secret is not a real security issue and can be safely ignored in the production environment.", "5eea516f-7039-4071-9701-a0b5df500725": "Custom patterns for {% data variables.product.prodname_secret_scanning %} can be defined at the repository, organization, and enterprise levels in {% data variables.product.prodname_dotcom %}. For more information, refer to the documentation provided by GitHub.", "f9bd2b97-68a4-4086-a367-31f373c207cf": "To exclude deep paths from the output of dataflow queries and reduce the number of results reported, you can modify the analysis configuration for {% data variables.product.prodname_codeql %}. Specifically, you can set the `max-depth` option in the `dataflow` section of the `.codeql.yml` file. This option limits the maximum depth of the dataflow paths that are reported. By default, `max-depth` is set to `Infinity`, which means that all dataflow paths are reported.  Here's an example of how to set `max-depth`:  ```yaml dataflow:   max-depth: 3 ```  In this example, `max-depth` is set to 3, which means that only dataflow paths up to a depth of 3 are reported. This can significantly reduce the number of results reported by dataflow queries, especially for large codebases with complex dataflow graphs.  Note that setting `max-depth` may cause some valid dataflow paths to be omitted from the results, so you should test the impact of this option on your codebase before using it in production.", "cc990488-fb0f-4d71-80e2-661fe3b5ad00": "In the context provided, it is not explicitly stated what specific features can be enabled or disabled at the organization or repository level for an enterprise account on GitHub. However, based on the language used, it can be inferred that these features are related to GitHub's advanced security and workflow automation capabilities. By enabling or disabling these features at the organization or repository level, an administrator can customize the security and automation settings for their specific needs within their enterprise account. This could include features such as code scanning, secret scanning, dependency graph, and workflow templates, among others. For more information on these features, the context suggests consulting the relevant documentation pages.", "06bdfab5-3881-49c8-abce-b64e94929892": "To define the token settings using `actions/github-script` with the `core` toolkit in your GitHub Actions workflow, you can follow the example provided in the context information. In your deployment job, you'll need to use `actions/github-script` with the `core` toolkit and define the token settings. For example:  ```yaml jobs:   job:     runs-on: ubuntu-latest     steps:     - uses: {% data reusables.actions.action-github-script %}       id: script       timeout-minutes: 10       with:         debug: true         script: |           const token = process.env['ACTIONS_RUNTIME_TOKEN']           const runtimeUrl = process.env['ACTIONS_ID_TOKEN_REQUEST_URL']           core.setOutput('TOKEN', token.trim())           core.setOutput('IDTOKENURL', runtimeUrl.trim()) ```  In this example, the `actions/github-script` action is used with the `core` toolkit, and the token settings are defined by setting environment variables `ACTIONS_RUNTIME_TOKEN` and `ACTIONS_ID_TOKEN_REQUEST_URL`. These variables can be used in the script to retrieve the JWT and the URL to request the JWT from the {% data variables.product.prodname_dotcom %} OIDC provider.", "6de631ce-4fd8-4467-88a3-98ccf5114446": "The purpose of using cookies and similar technologies on Enterprise Marketing Pages is to gather information about enterprise users' interests and online activities to personalize their experiences, including by making the ads, content, recommendations, and marketing seen or received more relevant. These cookies also serve and measure the effectiveness of targeted advertising and other marketing efforts. The length of time a cookie will stay on a user's device depends on whether it is a \"persistent\" or \"session\" cookie. Session cookies will only stay on the user's device until they stop browsing, while persistent cookies stay until they expire or are deleted. The expiration time or retention period applicable to persistent cookies depends on the purpose of the cookie collection and tool used.", "abf41e55-15a4-412e-9ec8-921f71938dca": "To change the default branch name used by {% data variables.product.product_name %} for new repositories created on {% data variables.location.product_location %}, follow these steps:  1. Go to your {% data variables.product.product_name %} account on {% data variables.location.product_location %}. 2. Click on your profile icon in the top right corner of the page and select \"Settings\" from the drop-down menu. 3. Click on the \"Repositories\" tab in the left-hand menu. 4. Scroll down to the \"Repository default branch\" section and click \"Change default branch name now\". 5. Type the new default branch name you want to use in the text field provided. 6. Click \"Update\" to save the changes.  After completing these steps, the new default branch name will be applied to all new repositories you create on {% data variables.location.product_location %}.", "3a747e57-abd0-46a9-a20b-ca14407aa83b": "The name of the repository being used in this build process is `${{ github.repository }}`, and it can be found on GitHub at `${{ github.server.url }}/${{ github.repository }}`.", "8d1dc8d2-6265-45ba-a9f8-3966d3e7b9c6": "When creating a token for HTTPS cloning, the access scopes that should be considered are \"repo\" and \"admin:repo_hook\". These scopes grant the token the necessary permissions to read and write to the repository, as well as to create and manage repository hooks. To manage these scopes, you can use the OAuth API to create, update, and revoke tokens with specific access scopes. It's important to ensure that the access scopes are appropriate for the intended use of the token and that they are reviewed and updated as needed to maintain security.", "109ac424-5a68-4d5b-b3ce-067fa0241c6f": "Based on the context provided, a reciprocal relationship is depicted in the diagram by two-way lines with arrows on both ends, indicating that there is a mutual interaction or influence between the elements connected by the lines. Non-reciprocal relationships, on the other hand, are depicted by one-way lines or arrows, indicating that there is a unidirectional interaction or influence between the elements connected by the lines.", "636e899d-bdc2-492d-8bfa-02e47547c457": "After the 5-minute cancellation timeout period, all jobs and steps that have not finished running or failed to complete the cancellation process will be forcefully terminated by the server.", "8f8a8b6f-5730-499e-8554-b03f383a96e3": "To add a repository secret using the GitHub CLI, you can use the `gh secret set` subcommand. Replace `secret-name` with the name of your secret and `REPOSITORY_NAME` with the name of your repository.  ```shell gh secret set REPOSITORY_NAME/secret-name ```  The CLI will prompt you to enter a secret value. Alternatively, you can read the value of the secret from a file.  ```shell gh secret set REPOSITORY_NAME/secret-name < secret.txt ```  To list all secrets for a repository, use the `gh secret list` subcommand.  ```shell gh secret list REPOSITORY_NAME ```  If you want to add a secret for an environment, you can use the `--env` or `-e` flag followed by the environment name.  ```shell gh secret set --env ENV_NAME REPOSITORY_NAME/secret-name ```  To list all secrets for an environment, use the `gh secret list` subcommand with the `--env` or `-e` flag followed by the environment name.  ```shell gh secret list --env ENV_NAME REPOSITORY_NAME ```  If you want to add a secret for the organization, you can use the `--org` or `-o` flag followed by the organization name.  ```shell gh secret set --org ORGANIZATION_NAME SECRET_NAME ```  To list all secrets for the organization, use the `gh secret list` subcommand with the `--org` or `-o` flag followed by the organization name.  ```shell gh secret list --org ORGANIZATION_NAME ```    How can repository secrets be added in GitHub using the CLI?", "fa80f0fa-2993-4b24-9e73-ce92489f76c8": "Unfortunately, without the title and author of the novel, it is impossible to provide a specific answer to this question. The author's use of symbolism and the overall theme of identity will vary from novel to novel. However, in general, symbolism can be a powerful tool for exploring the complexities of identity. For example, in Toni Morrison's Beloved, the character Sethe's scar is a powerful symbol of her past trauma and the way it has shaped her identity. Similarly, in F. Scott Fitzgerald's The Great Gatsby, the green light at the end of Daisy's dock represents Gatsby's longing for a past that he can never fully recapture.  As for the role of technology in the novel, again, without specifics, it is difficult to provide a detailed answer. However, in many contemporary novels, technology is used to explore themes of connection, disconnection, and the impact of technology on human relationships. For example, in Donna Tartt's The Secret History, the characters' use of technology (such as email and instant messaging) highlights the ways in which they are both connected and disconnected from one another, as well as the ways in which technology can both facilitate and hinder communication. The author's use of language and imagery in these scenes can help to convey the complexities of these themes.  In summary, without the specifics of the novel in question, it is impossible to provide a detailed answer to this question. However, in general, symbolism and technology can both be powerful tools for exploring themes of identity and human relationships, and authors often use language and imagery to convey these themes in a nuanced and impactful way.", "b40240ed-3f66-4dd7-9e4d-2a64c59a8894": "You can specify optional arguments in {% data variables.product.prodname_actions_importer %} when running the forecast, dry-run, or migration subcommands by passing them as arguments after the command. For example, to run the forecast subcommand with a specific source file path, you would use the following command:  ``` {% data variables.product.prodname_actions_importer %} forecast --source-file-path=path/to/source/file ```  If you want to supply multiple source files when running the forecast subcommand, you can use pattern match by specifying a glob pattern instead of a specific file path. For example:  ``` {% data variables.product.prodname_actions_importer %} forecast --source-file-path=path/to/source/*.groovy ```  This will match all files with the `.groovy` extension in the specified directory.", "b2a202e0-7699-4214-b6bb-d35dee384221": "{% data variables.product.prodname_dotcom %} links a personal account to a SAML identity in an organization by recording a link between the two when the user authenticates with their IdP account and returns to {% data variables.product.prodname_dotcom %}. This linked identity is significant because it is used to validate the user's membership in the organization and determine which organizations and teams they are a member of, depending on the organization or enterprise setup. Each {% data variables.product.prodname_dotcom %} account can be linked to exactly one SAML identity per organization, and each SAML identity can be linked to exactly one {% data variables.product.prodname_dotcom %} account in an organization. This link ensures that the user's access to organization resources is properly managed and controlled.", "dcd178d1-793b-4dd7-b1d8-22a8104cc6bf": "The statement \"greement with GitHub\" refers to a legal agreement between the customer and GitHub, which likely outlines the terms and conditions of the services provided by GitHub. This agreement may include the Service Level Agreement (SLA) mentioned in the context information, which outlines the performance and availability guarantees for GitHub's services. The statement \"greement with GitHub\" highlights the fact that the Service Credits mentioned in the context information are a part of this agreement and are the sole and exclusive remedy for any failures by GitHub to meet their obligations under the SLA.", "acf4f92c-80b2-4a95-abd9-221c8f304752": "In {% data variables.product.prodname_secret_scanning %}, {% data variables.secret-scanning.partner_alerts %} are notifications sent to service providers when they detect potential secrets in public repositories or public packages on the npm registry. These alerts are automatically triggered and the service provider decides on the appropriate action based on the associated risks. On the other hand, {% data variables.secret-scanning.user_alerts %} are notifications sent to repository owners or collaborators when they detect potential secrets in their private repositories. These alerts can be configured by the repository owner or collaborator, and they can choose to take action based on the detected secrets. In summary, {% data variables.secret-scanning.partner_alerts %} are triggered automatically for public repositories and packages, while {% data variables.secret-scanning.user_alerts %} are triggered manually for private repositories.", "43eed19d-138d-43b5-9e97-9f7c9df1e303": "To mitigate data leaks in GitHub, you can use the `git filter-repo` tool or the BFG Repo-Cleaner open source tool to remove sensitive data that has been accidentally pushed. If the user who pushed the sensitive data is unable to remove it, you can contact GitHub Support with the concerning commit SHA or fill out a DMCA takedown notice form and inform GitHub Support. Additionally, it's possible to revert almost anything in Git, and you can refer to the GitHub blog for more information. At the organization level, you can also set up branch protection rules to enforce certain workflows or requirements before a contributor can push changes, and you can leverage your organization's audit log and the GraphQL Audit Log API to proactively secure IP and maintain compliance. If you're using GitHub Enterprise Cloud (GHEC) or GitHub Enterprise Server (GHES), you can also extend the detection of custom secret string structures.", "62f7fa69-1702-4fdd-8ce8-dd36bc0d153f": "The purpose of the `org.opencontainers.image.licenses` key when adding a label to a Docker image is to provide the license under which the image is distributed. The license identifier provided should be an SPDX license identifier, limited to 256 characters, such as \"MIT\". This information will appear on the package page, in the \"Details\" sidebar, for the image.", "5a33065c-0374-40df-a55c-61bc3672dde8": "The prefix used to indicate the type of token in the context information provided is `ghp_` for {% data variables.product.pat_v1_caps %}, `github_pat_` for {% data variables.product.pat_v2_caps %}, `gho_` for OAuth access token, `ghu_` for User access token for a {% data variables.product.prodname_github_app %}, and `ghs_` for Installation access token for a {% data variables.product.prodname_github_app %}. If the context didn't provide this information, then it wasn't given enough information to give an answer.", "11e01aba-eb51-4907-8956-27dce78a6c45": "If you choose to save your SSH key with a passphrase during the setup process, you will be prompted to enter a passphrase when you connect to a GitHub repository using SSH. This adds an extra layer of security to your key, as the passphrase is required in addition to the private key to access your repository. If you choose not to save your key with a passphrase, you will not be prompted to enter a passphrase when connecting to a repository using SSH, as the private key alone is sufficient to access your repository.", "2caf16e1-38db-4f73-a692-fe09a8bf80cc": "To download a CSV/JSON file with license details for your enterprise account on {% data variables.product.prodname_dotcom_the_website %}, go to your enterprise account settings, click on the \"License\" tab, and then click on either \"View details\" (if you're using {% ifversion ghec %}GitHub Enterprise Cloud{% elsif ghes %}GitHub Enterprise Server{% endif %}) or \"Export license usage\" (if you're using {% ifversion ghes %}GitHub Enterprise Server{% endif %}). This will display the license usage details, and you can then click on the {% ifversion ghec %}{% octicon \"download\" aria-hidden=\"true\" %} CSV report{% elsif ghes %}Export{% endif %} button to download the CSV/JSON file. If you want to view usage details for other features, you can also click on \"Billing\" in the left sidebar (if you're using {% ifversion ghec %}GitHub Enterprise Cloud{% endif %}).", "8eb8b67e-a887-4c25-b4ea-4cc71e5120e1": "In the given context, \"Members\" and \"Owners\" have the ability to invite new members to join an organization. \"Moderators\", \"Billing managers\", and \"Security managers\" do not have this ability.", "442ba633-3d45-4735-ac35-048c3685f922": "The \"codeql database export-diagnostics\" command is used to export diagnostic information from a CodeQL database for a failed analysis. Its purpose is to provide detailed information about the errors and issues encountered during the analysis, which can help developers to understand and resolve those issues. The available options include the path to the CodeQL database, the format in which to write the results (raw, SARIF, or text), the option to process multiple databases in a cluster, the output path, and options specific to the SARIF format (exit code, exit code description, and category).", "a0c11d6b-5e9d-4894-a22e-c137b7eaf648": "The unique identifier for an external pull request in this context is `CI_EXTERNAL_PULL_REQUEST_IID`.", "0fdfa3c8-71a3-4064-80a5-da87d4040eac": "The Amendment and ToS address the issue of financial obligations between the federal government and the Company by stating that \"Nothing in this Amendment or ToS obligates You to expend appropriations or incur financial obligations\" and that \"At the Company\u2019s discretion, GitHub may offer a free account under a free usage plan, and in that case this Amendment will apply to the Government\u2019s usage under the free account/plan.\" Additionally, the Amendment specifies that \"Company will provide You with at least 30 days advance notice\" before beginning to charge for a previously free service. This indicates that the federal government will not be obligated to pay any fees to the Company for the use of its services, unless the Company chooses to offer a paid usage plan, in which case the federal government will be given advance notice of any charges.", "69f67b4e-ddca-4d2d-86b4-4dcc2793a720": "To delete an OAuth app in {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to your user settings by clicking on your profile picture in the top right corner of the page and selecting \"Settings\" from the drop-down menu. 2. Click on \"Developer settings\" in the left-hand menu. 3. Click on \"OAuth apps\" in the left-hand menu. 4. Select the OAuth app you want to delete. 5. In the left sidebar, click \"Advanced\". 6. Click \"Delete application\". 7. Click \"Delete this OAuth Application\".  Note: These steps may vary slightly depending on the version of {% data variables.product.prodname_dotcom %} you are using.", "19fddb0f-8dd6-4fe8-aaaf-90880955818e": "Context information such as the topic of the quiz/examination, the level of difficulty, the type of questions (multiple choice, true/false, short answer, essay), and the learning objectives can be utilized to generate diverse questions for an upcoming quiz/examination. By analyzing the context, educators can create questions that assess students' understanding of the concepts covered in the course, as well as their ability to apply them in real-world scenarios. They can also incorporate different types of questions to assess students' critical thinking, problem-solving, and analytical skills. Additionally, they can ensure that the questions align with the learning objectives and provide feedback to students on their performance.", "0ec93261-be74-41bd-a6b3-04834f8e8c84": "To write detailed logs to a specific directory during the execution of the CLI, you can use the `--logdir` option provided in the context information. This option allows you to specify a directory where detailed logs will be written with generated names that include timestamps and the name of the running subcommand. This option overrides the `--log-to-stderr` option, which writes logs to standard error stream. If you want to write a log file with a name you have full control over, you can instead use the `--log-to-stderr` option and redirect standard error stream as desired.", "bb896a7a-a9ae-4b37-8453-da60cfb3a88d": "To share a list of starred repositories on GitHub with others, you can create a public list and add the repositories you want to include. Other users can then view and follow the list to see the same repositories. This is a great way to curate and organize a collection of resources for yourself or others. To create a public list, go to your profile, click on Lists, and then click the green New button. Give your list a name and description, and then add the repositories you want to include by clicking the Add items button and selecting the repositories from your starred list. Once you've added all the repositories you want, click Save to create the list. Your list will now be visible to others and can be found by going to your profile and clicking on Lists. Other users can then follow your list to see the same repositories.", "c8882097-5f01-42da-85e7-e26976235480": "According to the context information provided, the \"symbol:\" qualifier in code search can be used to search for symbol definitions, such as function or class definitions. However, it's important to note that this feature only searches for definitions and not references, and not all symbol types or languages are fully supported yet. The context doesn't specify which languages are currently supported, so unfortunately, we don't have enough information to provide a complete answer. For a list of supported languages, you can refer to the documentation mentioned in the context, \"AUTOTITLE.\"", "bc2ac898-b8c2-42c3-8e22-90aa8ec5e95f": "The purpose of the `redeliverWebhook` function is to resend a failed webhook delivery. It is called by passing the delivery ID, organization name, and hook ID to the function, and then making a POST request to the `/orgs/{org}/hooks/{hook_id}/deliveries/{delivery_id}/attempts` endpoint using the `octokit.request` method. This request will attempt to redeliver the failed webhook delivery.", "20663f99-2dd2-4baa-8860-9dcdea37d8f0": "To disable a feature or setting in our software, follow these procedural steps:  1. Open the software and navigate to the settings or preferences menu. 2. Locate the feature or setting you want to disable and select it. 3. Click on the disable or turn off button. 4. Confirm the action by clicking on the OK or Apply button. 5. Close the settings or preferences menu and save your changes.  If you encounter any issues while disabling a feature or setting, refer to our troubleshooting information for assistance.", "8bfa4f59-95b4-4d31-a270-27162ebbcf2b": "To set a custom environment variable for a single workflow in GitHub Actions, you can define it using the `env` key in the workflow file. The scope of a custom variable set by this method is limited to the element in which it is defined. You can define variables that are scoped for:  - The entire workflow, by using `env` at the top level of the workflow file. - The contents of a job within a workflow, by using `jobs..env`. - A specific step within a job, by using `jobs..steps[*].env`.  For example, the following YAML configuration sets a custom variable `DAY_OF_WEEK` for the entire workflow, a custom variable `Greeting` for the job \"greeting_job\", and a custom variable `First_Name` for a specific step within the job:  ```yaml name: Greeting on variable day  on:   workflow_dispatch  env:   DAY_OF_WEEK: Monday  jobs:   greeting_job:     runs-on: ubuntu-latest     env:       Greeting: Hello     steps:       - name: \"Say Hello Mona it's Monday\"         run: echo \"$Greeting $First_Name. Today is $DAY_OF_WEEK!\"         env:           First_Name: Mona ```  To access these variables in the `run` steps of a workflow or a referenced action, you can use runner environment variables or contexts. The example above shows three custom variables being used as runner environment variables in an `echo` command: `$DAY_OF_WEEK`, `$Greeting`, and `$First_Name`. The values for these variables are set, and scoped, at the workflow, job, and step level respectively.  In the parts of a workflow that are not sent to the runner, such as the `name` and `on` keys, you must use contexts to access variable values. For example, to access the `DAY_OF_WEEK` variable in the `name` key, you can use the `{{}}` syntax to access the context:  ``", "35222fae-4f14-446e-b72f-0268b13a734c": "In code blocks, style any placeholders that people need to replace with their own values in all caps. For example, `git checkout -b BRANCH-NAME` should be used instead of `git checkout -b `. This makes it clear to users that they should replace the placeholder with their own branch name.", "14662ddc-402e-448d-9435-f681b5b73513": "The variable `$(Build.Reason)` represents the reason why the build was triggered, such as a push event, pull request, or manual trigger. On the other hand, the variable `$(Build.SourceVersion)` represents the specific version of the source code that is being built, which is identified by its SHA or commit ID. Therefore, while both variables provide information about the build process, they serve different purposes and represent different aspects of the build.", "e74abc97-9941-4a12-80ef-8f160fed1852": "According to Section 5.1 of the agreement, the Developer represents and warrants that they have informed all users of their Developer Applications that their account data will be transmitted outside the Service and that Developer terms will control the privacy, security, and integrity of such account data. The Developer also represents and warrants that to the extent their Applications store, process, or transmit account data, neither they nor their Applications will modify the content of account data in a manner that adversely affects the integrity of account data, disclose account data to any third party without appropriate prior user consent or except to the extent required by applicable law, or use account data for any purpose other than providing the Developer Application functionality to users of such Developer Application, without prior user consent. The Developer is also responsible for maintaining and handling all account data in accordance with privacy and security measures reasonably adequate to preserve the confidentiality and security of all account data and all applicable privacy laws and regulations.", "e2326ea1-955f-4122-a6e7-48eeb5c56701": "Entering the {% data variables.product.pat_generic_caps %} for GitHub and CircleCI, as well as the base URLs for GitHub and CircleCI, during the {% data variables.product.prodname_actions_importer %} `configure` CLI command is significant because it allows the {% data variables.product.prodname_actions_importer %} to authenticate and connect to both GitHub and CircleCI, which are the CI providers being configured. The personal access tokens (PATs) for GitHub and CircleCI provide authentication, while the base URLs specify the locations of the GitHub and CircleCI instances being used. This information is necessary for the {% data variables.product.prodname_actions_importer %} to import workflows and jobs from GitHub to CircleCI. Without this information, the {% data variables.product.prodname_actions_importer %} would not be able to access the necessary resources and data to complete the import process.", "9971daf4-dad3-483f-80ea-6452ccf2353c": "The purpose of setting the `_DIR` variable in the `backup.config` file for {% data variables.product.prodname_enterprise_backup_utilities %} is to specify the directory where the snapshots created by {% data variables.product.prodname_enterprise_backup_utilities %} will be stored. This directory should be located on a filesystem that supports symbolic and hard links. It is recommended to avoid storing snapshots in a subdirectory of the {% data variables.product.prodname_enterprise_backup_utilities %} installation directory to prevent accidentally overwriting data during upgrades.", "421b122e-ae93-4314-9f67-7ed812ac728e": "Based on the context information provided, it suggests that all conversations related to {% data variables.product.prodname_code_scanning %} alerts, as well as any other conversations in the pull request, should be resolved before merging. This is implied by the statement \"se to require all conversations in a pull request, including those on {% data variables.product.prodname_code_scanning %} alerts, to be resolved before a pull request can be merged.\" Therefore, it's essential to ensure that all issues and discussions related to the pull request are addressed and resolved before merging it into the main branch.", "32bc5274-c64a-454a-8aa0-557d197d288a": "A pull request is a proposed change to a shared codebase that is reviewed and discussed by collaborators before being integrated into the main codebase. It allows multiple developers to work on the same project simultaneously without conflicting changes, as each pull request is reviewed and merged separately. Pull requests display the differences between the proposed changes and the current codebase, making it easy to see what has been added, removed, or modified. They are an essential part of collaborative software development as they facilitate code review, improve code quality, and ensure that changes are thoroughly tested before being merged into the main codebase.", "c83bf21c-9171-434e-b323-76b7c39f993f": "The purpose of generating private keys for {% data variables.product.prodname_github_apps %} is to make secure requests to the {% data variables.product.product_name %} API on behalf of the application. These private keys are used to sign JSON Web Tokens (JWTs) that are required to request installation access tokens. Private keys help prevent unauthorized access to the application's data by ensuring that only authorized requests are made to the API. It is essential to keep these private keys secure and to rotate them periodically to prevent downtime in case of loss or compromise.", "988999b5-6c53-4dfc-b12f-91354e647e52": "The recommended method for securing your account on GitHub is \"Two-factor authentication (2FA)\" (recommended). This involves enabling 2FA and providing a code that's generated by a time-based one time password (TOTP) application on your mobile device after entering your username and password. This is recommended over \"Username and password only\" because it provides an additional layer of security to prevent unauthorized access to your account. If you have not enabled 2FA, GitHub will ask for additional verification when you first sign in from an unrecognized device, but this is not as secure as 2FA.", "75e9cd63-7f23-4765-a6ca-f1de6c5be04b": "The recommended approach for managing access to a package linked to a repository is to let the package inherit its permissions from the linked repository. This simplifies the process of managing access to the package because you only need to configure the permissions of the linked repository to grant or remove access to the package. It also ensures consistency between the package and the repository, as any changes made to the repository's permissions will automatically apply to the package as well. However, if you prefer to configure the access settings for the package separately from the linked repository, you can remove the inherited permissions from the package. It's important to note that changing how a package gets its access permissions will overwrite any existing permissions for the package.", "0a8e0dd9-8792-4893-a4c7-457e01e96d57": "How can I add a new secret to an organization in GitHub?  2. How can I manage the access policies for organization-level secrets in GitHub?", "04ccf912-8baf-4e7d-a9cc-b0972291743c": "To search for code containing a quotation mark using Code Search in Google Cloud Platform, you can escape the quotation mark using a backslash. For example, to find the exact string `name = \"tensorflow\"`, you can search:  ```text \"name = \\\"tensorflow\\\"\" ```  2. To refine your search results using boolean operations in Code Search, you can use the operators `AND`, `OR`, and `NOT`. For example, to search for documents containing either the terms `sparse` or `index`, or both, you can use the `OR` operator:  ```text sparse OR index ```  To exclude files from your search results, you can use the `NOT` operator. For example, to exclude files in the `__testing__` directory, you can search:  ```text \"fatal error\" NOT path:__testing__ ```  You can also use parentheses to express more complicated boolean expressions. For example:  ```text (language:ruby OR language:python) AND NOT path:\"/tests/\" ```  This query will match documents containing either Ruby or Python code, but not files located in the `/tests/` directory.", "522ececd-83f3-48a3-a816-8f30547ed41a": "{% data variables.product.prodname_classroom %} manages permissions, administration, and security for each classroom created using organization accounts on {% data variables.product.product_name %}. Admins, who can be teachers, TAs, or any other course administrator, can be invited to the classroom and have control over the classrooms on {% data variables.product.prodname_classroom %}. Organization owners can administer any classroom for the organization. When a classroom is no longer needed, it can be archived or deleted.", "08c8326c-fba8-4466-8a4a-3eac21c69a92": "In the given text material, making a package private means restricting access to it to only the selected people in the organization. On the other hand, making a package internal means making it visible to all organization members, including those in an enterprise if the organization belongs to one. Therefore, the main difference between making a package private and making it internal is the level of visibility it has within the organization and its enterprise (if any). Private packages are only accessible to specific individuals, while internal packages are accessible to all organization members, including those in the enterprise.", "17b6e36f-250a-474c-961a-95e42167591b": "Team synchronization fetches group information from an IdP at least once every hour in {% ifversion ghec %}{% data variables.product.product_name %}{% endif %}.", "4011999a-0be7-458a-a544-0507dc648893": "Developers can use productivity and collaboration tools in {% data variables.product.product_name %} such as Git version control, powerful APIs, and integrations. Developers familiar with {% data variables.product.prodname_dotcom_the_website %} can onboard and contribute seamlessly using familiar features and workflows.", "83c00232-7b6e-4d54-a83a-bbafa9cd5bcf": "Based on the context provided, it is not clear how the recommended feature during the beta differs from the current version of the software. More information is needed to make a comparison.", "b9a6f237-5fb8-483c-b33d-afec98a26361": "To push changes from your local repository to {% data variables.location.product_location %} using the provided context (assuming the default branch is not named \"main\"), you need to replace \"main\" with the name of your default branch in the following command:  ```shell git push origin YOUR_DEFAULT_BRANCH_NAME ```  Before pushing, you should verify that you set the remote URL correctly by running the following command:  ```shell git remote -v ```  This will display a list of your remote repositories and their URLs. If the URL for {% data variables.location.product_location %} is correct, you should see it listed under the \"origin\" remote.  If you're not sure what your default branch is named, you can check it by running the following command:  ```shell git rev-parse --abbrev-ref --symbolic-ref HEAD ```  This will display the name of the branch that you're currently on, which should be your default branch.  Note that if you're using GitHub.com or GitHub Enterprise Cloud (GHEC), the default branch is named \"main\" instead of \"master.\" If you're using GitHub Enterprise Server (GHES), you can set the default branch name by following the instructions in \"AUTOTITLE.\"", "f37c3001-e730-4548-9323-7ad00c5b70e1": "To use GitHub Codespaces to review pull requests and test changes, follow these steps:  1. Open the pull request in a codespace, as described in \"AUTOTITLE.\"  2. In the Activity Bar, click the **GitHub Pull Request** view. This view only appears when you open a pull request in a codespace.  3. To review a specific file, click the **Open File** icon in the Side Bar.  4. To add review comments, click the **+** icon next to the line number. Type your review comment and then click **Start Review**.  5. When you are finished adding review comments, from the Side Bar you can choose to either submit the comments, approve the changes, or request changes.  6. To review changes to dependencies, go to the **Dependabot** tab in the pull request. Here you can see a summary of the dependency changes and review them.  7. To test the changes, run the tests in the codespace. You can do this by running the test command in the terminal or by using a test runner like Jest or Mocha.  8. To submit your review, click the **Submit review** button in the pull request. Here you can choose to approve or request changes.  9. If you want to cancel a pending review, including all of its pending comments, click **Review changes** above the changed code, then click **Cancel review**.  10. When you are done reviewing and testing, close the codespace.  Overall, GitHub Codespaces provides a convenient and efficient way to review pull requests and test changes, as it allows you to work directly in the codebase and provides a range of tools and features to help you review and test changes.", "a38f0f47-903d-4b94-93b5-16ce9cb3e6c2": "The repository maintainer should consider adding a comment explaining the reasoning behind why they don't consider the report a security risk before closing the vulnerability advisory as not being a security risk. This helps to provide transparency and justification for the decision, which can be useful for other stakeholders, such as users and developers, who may have different opinions about the severity of the issue. Additionally, it can help to prevent misunderstandings or disputes in the future, as the reasoning behind the decision is clearly documented.", "b2f57499-de1e-4e85-9986-b8de601bc817": "The purpose of browsing existing custom deployment protection rules in the given context is to help developers find suitable protection rules for their {% data variables.product.company_short %} repositories. This can be done by allowing developers to discover and install these rules or by browsing through existing custom deployment protection rules that have already been created. This can save developers time and effort in creating their own protection rules and ensure that their repositories are protected according to best practices and industry standards.", "983726ad-13a8-4ca6-b87c-3f1160c51c2d": "To sort items by a specific field value on the roadmap layout, you can follow these steps:  1. Go to your project's roadmap layout. 2. Click on the **Customize** button in the top right corner of the page. 3. Click on the **Sort** tab. 4. Select the field you want to sort by from the drop-down menu. 5. Choose whether you want to sort in ascending or descending order. 6. Click on the **Save** button to apply the changes.  Alternatively, you can also sort items by a specific field value directly on the roadmap layout by clicking on the field header. This will sort the items in ascending order by default. To sort in descending order, click on the field header again.  Note that this feature is only available in GitHub Projects version 2. If you're using an older version of Projects, you may not have access to this feature.", "4e675a70-51c1-4996-b688-8952a7252389": "The \"ct.prodname_dependabot %}\" section in the context information provided seems to be a placeholder or variable that will be replaced with the actual name of the product related to Dependabot. Dependabot is a feature that automatically updates dependencies in a project's package manager configuration files to the latest versions that are compatible with the project's requirements. The purpose of the \"ct.prodname_dependabot %}\" section is likely to provide a customizable label or title for the Dependabot feature in the context of the given instructions, which involve disabling or enabling all Dependabot alerts for an organization's repositories. However, without knowing the actual value of \"ct.prodname_dependabot %}\", it's difficult to provide a definitive answer.", "da72c9f8-954e-4add-8d42-51c7a475baaf": "Yes, based on the provided context information, I can summarize the input fields and return type fields for the \"deleteDiscussionComment\" mutation.  Input fields:  - `id: ID!` The node ID of the discussion comment to delete. - `clientMutationId: String` A unique identifier for the client performing the mutation.  Return type fields:  - `clientMutationId: String` The unique identifier provided as an input. - `comment: DiscussionComment` The discussion comment that was deleted.  The \"deleteDiscussionComment\" mutation allows the user to delete a specific discussion comment with the provided node ID. The client performing the mutation should provide a unique identifier for tracking purposes. The mutation returns the deleted discussion comment and the unique identifier provided as an input.", "c696812a-a8da-47ac-9442-58fb9f621c2c": "The \"Prebuild ready\" label indicates that prebuilds are available for the selected branch, dev container configuration file, and region, while the \"Prebuild in progress\" label indicates that prebuilds are currently being created for those same criteria. This information helps users choose whether to wait for a prebuild to finish or select a different machine type option.", "c61a8eae-936c-4c33-9345-a66753677bfd": "The purpose of the `defaultPlatform` frontmatter in GitHub documentation pages is to override the initial platform selection for a page. This frontmatter allows the author to specify a default platform for the page, which can be useful in cases where the platform-specific content is independent of the reader's operating system. The available options for this frontmatter are `mac`, `windows`, and `linux`. If this frontmatter is omitted, then the platform-specific content matching the reader's operating system is shown by default. This behavior can be changed for individual pages, for which a manual selection is more reasonable.", "ec6893cc-c45d-4240-a806-9fd680141378": "Jump to your chosen location in the command palette.    If the context gives enough information to give an answer,  then give an answer.  Answer:  Use Enter to jump to your chosen location in the command palette.    If the context gives enough information to give an answer, but requires some explanation,  then give an explanation.  Answer:  Use Enter to jump to your chosen location in the command palette. This will take you directly to the selected location without having to navigate through the menus or search results.    If the context gives enough information to give an answer, but requires some examples,  then give examples.  Answer:  Use Enter to jump to your chosen location in the command palette. For example, if you want to navigate to a specific repository, start typing the repository name in the command palette, and then use Enter to jump directly to that repository.    If the context gives enough information to give an answer, but requires some warnings,  then give warnings.  Answer:  Use Enter to jump to your chosen location in the command palette, but be aware that this will close the command palette. If you want to keep the command palette open, use Ctrl+Enter (Windows and Linux) or Command+Enter (Mac) to open the location in a new browser tab.    If the context gives enough information to give an answer, but requires some alternatives,  then give alternatives.  Answer:  Use Enter to jump to your chosen location in the command palette, or use Ctrl+Enter (Windows and Linux) or Command+Enter (Mac) to open the location in a new browser tab.    If the context gives enough information to give an answer, but requires some limitations,  then give limitations.  Answer:  Use Enter to jump to your chosen location in the command palette, but be aware that this feature is only available for certain types of locations. For more information, see the documentation.    If the context gives enough information to give an answer, but requires some troubleshooting,  then give troubleshooting.  Answer:  Use Enter to jump to your chosen location in the command pal", "933a8452-6b0c-410f-8721-69bac34162d4": "To collect log files for {% data variables.product.prodname_copilot%} in IntelliJ IDEA, follow these steps:  1. Open the IntelliJ IDEA application. 2. Go to the \"Help\" menu. 3. Select \"Show Log in Finder\". 4. Open the \"idea.log\" file in your preferred text editor. 5. Look for any errors related to {% data variables.product.prodname_dotcom%} or {% data variables.product.prodname_copilot%}.  Note: This process is the same for other JetBrains IDEs such as Android Studio, GoLand, PhpStorm, PyCharm, RubyMine, and WebStorm. For more information, refer to the documentation for the specific IDE you are using.", "c6ee3793-65e6-4856-96e4-aeab6a6d7958": "In the event of a dispute between the data subject and the data importer, the data subject has the option to refer the dispute to mediation, either by an independent person or by the supervisory authority, as agreed upon by the data importer. Alternatively, the data subject can choose to refer the dispute to the courts in the Member State in which the data exporter is established. This choice will not prejudice the data subject's substantive or procedural rights to seek remedies in accordance with other provisions of national or international law. The impact of this on the data subject's rights will depend on the specific circumstances of the dispute and the applicable laws.", "bfc7489b-df3a-4102-94e2-8b573ea74ee2": "The significance of including the statement \"I have read and understood GitHub's Guide to Filing a DMCA Counter Notice\" in a counter notice is that it indicates that the person submitting the counter notice has familiarized themselves with the guidelines provided by GitHub for filing a DMCA counter notice. This statement helps to ensure that the counter notice is complete and meets the requirements of the DMCA, as GitHub may ask the person to go back and read the guidelines if this statement is not included. It also demonstrates that the person submitting the counter notice has a basic understanding of the DMCA and the process for challenging a takedown notice.", "e837577b-8852-459c-a346-0240cae1e08f": "If the organization is already using {% data variables.product.prodname_dotcom %}, they should reconfigure their integrations to point to their new organizations and repositories. This involves generating new tokens and keys, as well as contacting vendors for instructions if using third-party integrations. If the integration was developed in-house, they should reconfigure it in their new organization.  2. To link user activity to their new identities during a migration process that involves collaboration history and metadata, as well as code, the organization should follow these steps:  a. If the user's identity remains the same, they should ensure that the user's account is linked between the old and new hosting providers. This can be done by syncing user accounts or using a migration tool that automatically links accounts.  b. If the user's identity changes, they should create a new user account on the new hosting provider and migrate the user's activity to the new account. This can be done manually or using a migration tool that automatically migrates activity.  For example, suppose @octocat created an issue on {% data variables.location.product_location_enterprise %}, and they're moving to {% data variables.product.prodname_ghe_cloud %}. If @octocat's identity remains the same, the organization should ensure that @octocat's account is linked between the old and new hosting providers. If @octocat's identity changes, the organization should create a new user account on {% data variables.product.prodname_ghe_cloud %} for the new user, and then migrate @octocat's activity to the new account. This can be done manually or using a migration tool that automatically migrates activity.  In both scenarios, the organization should also ensure that any permissions or access levels are correctly transferred to the new hosting provider. This can be done by syncing permissions or using a migration tool that automatically transfers permissions.  Overall, the key to a smooth migration process is to plan ahead, test the migration process thoroughly, and communicate clearly with users about the changes. By following these steps, the organization can ensure that user activity is correctly linked to their new identities, and that collaboration history and metadata are preserved during the migration process.", "2d9e4a3a-6688-4472-8c17-1c02d6087c85": "To ensure that all necessary context information is provided when writing technical documentation to eliminate ambiguity, follow these guidelines:  1. Be specific: Use clear and concise language to avoid ambiguity. Avoid using vague nouns and pronouns, and provide more context to eliminate ambiguity.  2. Avoid nominalization: Instead of using complex nouns, use active verbs to make your writing more direct and clear.  3. Avoid modal auxiliary verbs: Be more specific when using auxiliary verbs like \"may\" and \"might.\"  4. Avoid prepositional phrases: Instead of using prepositional phrases, use more direct language to make your writing clearer.  5. Keep inline links to a minimum: Clearly introduce links following the style guide to avoid ambiguity.  By following these guidelines, you can ensure that your technical documentation is clear, concise, and unambiguous, making it easier for users to understand and follow.", "924aba4d-0a57-4d32-84cb-64128edac45e": "{% data variables.product.prodname_github_codespaces %} detects localhost URL patterns and automatically forwards ports by identifying the pattern in the URLs generated when running a local server in a development environment. When a local server is started in a codespace, {% data variables.product.prodname_github_codespaces %} detects the localhost URL pattern and automatically forwards the corresponding port to a publicly accessible URL, allowing the server to be accessed from a web browser without the need for manual configuration or port forwarding. This feature simplifies the development process by enabling developers to easily test and debug their applications without the need for external tools or services.", "422d3b98-c21b-44fc-9f83-aad2549449c1": "To request a fix for a bug in the file using the {% data variables.product.prodname_copilot_chat %} feature in JetBrains, you can type a request like, `Fix this bug` in the file where the bug is located. This will prompt the {% data variables.product.prodname_copilot_chat %} feature to suggest possible solutions or workarounds for the bug. If you're not satisfied with the suggestions, you can request a fix by providing more details about the bug, such as the steps to reproduce it, the expected behavior, and any error messages you're seeing. You can also contact GitHub Support for further assistance. However, if the context doesn't provide any specific instructions on how to use the {% data variables.product.prodname_copilot_chat %} feature for requesting a fix, then you'll need to refer to the further reading resources provided, such as the {% data variables.product.prodname_copilot %} FAQ, to learn more about how to use this feature effectively.", "b7b1c9d0-3b4b-41b8-afd3-acb2cb5a3d47": "The purpose of adding a query reference file (`.qlref`) to a sub-directory in a test {% data variables.product.prodname_codeql %} pack is to define the location of the query to test relative to the root of the {% data variables.product.prodname_codeql %} pack that contains the query. This is useful when the query is stored separately from the tests, as it allows the tests to be run against different versions of the query without having to modify the test code. The location is defined in the `.qlref` file using a relative path from the root of the {% data variables.product.prodname_codeql %} pack. For example, if the query is stored in a sub-directory called `queries` in the {% data variables.product.prodname_codeql %} pack, the path in the `.qlref` file would be `queries/my_query.ql`.", "37e21dfe-51a7-4bd1-b0e9-827c93600a95": "If the SSH configuration file does not exist in the default location, you should create a new file. Here are the specific instructions:  1. Open a terminal window. 2. Run the following command to create a new file:     ```shell    touch ~/.ssh/config    ```  3. Open the newly created file using a text editor:     ```shell    code ~/.ssh/config    ```  4. Add the following lines to the file, replacing `HOSTNAME` with the hostname of your GitHub.com organization (if you're using GitHub Enterprise Cloud, replace `github.com` with your organization's hostname):     ```text    Host HOSTNAME      AddKeysToAgent yes      UseKeychain yes      IdentityFile ~/.ssh/id_ed25519    ```     If you chose not to add a passphrase to your key, you should omit the `UseKeychain` line.  5. Save and close the file.  If you see a `Bad configuration option: usekeychain` error, add an additional line to the configuration's `Host *.HOSTNAME` section:     ```text    Host *.HOSTNAME      IgnoreUnknown UseKeychain    ```  6. Add your SSH private key to the ssh-agent and store your passphrase in the keychain:     ```shell    ssh-add ~/.ssh/id_ed25519    ```     Follow the prompts to store your passphrase in the keychain.  7. Add your public key to GitHub:     ```shell    ssh-copy-id -i ~/.ssh/id_ed25519 git@HOSTNAME:    ```     Replace `HOSTNAME` with the hostname of your GitHub.com organization (if you're using GitHub Enterprise Cloud, replace `github.com` with your organization's hostname).  8. Test your connection:     ```shell    ssh -T git@HOSTNAME    ```     Replace `HOSTNAME` with the hostname of your GitHub.com organization", "93ef3978-70c1-4ec9-9cff-672b004c1d89": "The context information states that the IdP groups and memberships listed in the view are based on information sent from the IdP to GitHub via SCIM. This suggests that the specific protocol used to transmit this information is SCIM (System for Cross-domain Identity Management). SCIM is an open standard for exchanging user and group management data between identity providers and service providers. When an IdP sends this data to GitHub via SCIM, it creates a list of IdP groups and their memberships that can be viewed and managed within GitHub.", "204fb2cd-19ea-43c0-b169-3e89058ee3ae": "GitHub Copilot may collect snippets of Your Code and additional usage information through the IDE or editor tied to your Account, which may include personal data. This data is governed by the GitHub Privacy Statement.", "ef7cdc26-2b55-40ac-8a12-db0388b476e4": "Developers located in North Korea are restricted from using GitHub services altogether, as stated in the context information. This restriction is likely due to U.S. trade control restrictions and economic sanctions against North Korea.", "423123d2-91e5-4961-bd1a-8868b7597010": "The alt text for screenshots should follow the format provided, which includes starting with the type of graphic, describing the most relevant elements, and using consistent language for UI elements. It should also include the product name and any highlighted areas, as well as their states and keyboard shortcuts. The format is as follows: \"Screenshot of the [product name] [UI element] shown. The [UI element] [state of the element/controls], and its [keyboard shortcut XYZ], are outlined in [color].\" For example, \"Screenshot of the {% data variables.product.company_short %} repository settings tab shown. The 'Branches' [UI element] [is/are] selected, and its [keyboard shortcut XYZ] is 'B'.\"", "e322df0e-a927-4f68-bc06-8745bb628374": "To obtain technical support from {% data variables.product.company_short %} for issues related to replication on your GitHub Enterprise instance, you should follow these steps:  1. Collect the necessary information:     a. On each affected node, run the following command to check the status of replication:     ```shell    ghe-repl-status -vv    ```     b. Copy the output of this command to a text file or save it to your clipboard.     c. Create a support bundle for each affected node using the following steps:     1. Log in to your GitHub Enterprise instance as an administrator.    2. Go to your instance's settings by clicking your profile picture in the top-right corner of the page, then selecting \"Settings\" from the drop-down menu.    3. Click \"Support\" in the left-hand menu.    4. Click \"Create support bundle\" and follow the prompts to create a bundle for each affected node.    5. Attach the support bundles to your support request.  2. Interpret the output of the `ghe-repl-status` command:     The output of this command will provide detailed information about the status of replication on the affected node. Here's a breakdown of some of the key fields:     a. `Last heartbeat`: This field shows the time at which the node last communicated with the primary node. If this field is blank or shows a time in the distant past, it may indicate a connectivity issue.     b. `Last sync`: This field shows the time at which the node last synchronized data with the primary node. If this field is blank or shows a time in the distant past, it may indicate a synchronization issue.     c. `Sync status`: This field shows the current status of synchronization. Possible values include \"synced\", \"syncing\", \"syncing (catching up)\", and \"syncing (behind)\". If this field shows \"syncing (behind)\", it may indicate a synchronization lag.     d. `Sync lag`: This field shows the amount of time that has elapsed since the node last synchronized data with the primary node. If this field shows a large number", "dad482ee-8482-4bc4-993f-07d4b59bf0f5": "Someone with write access to a repository can edit comments on issues, pull requests, and commits by expanding the dropdown menu next to the comment author's name and selecting \"Edit\". Appropriate content to edit in such comments includes removing content that doesn't contribute to the conversation and violates the community's code of conduct or GitHub's Community Guidelines. It's also appropriate to clearly indicate edits and their justification, but anyone with read access to the repository can view the comment's edit history. Additionally, sensitive information can be deleted from the edit history by replacing it with \"[REDACTED]\" and adding a note explaining the edit.", "091196ab-86cf-40de-a966-c517d720c70c": "Choose a dedicated server that meets the requirements for handling webhook traffic from {% data variables.product.company_short %}. 2. Install the necessary software and dependencies on the server. 3. Configure the server to listen for webhook traffic on the appropriate port. 4. Transfer your webhook-handling code to the server using a secure file transfer protocol like SSH or SFTP. 5. Update the webhook URL in your webhook settings to reflect the new server's URL. 6. Test the webhook delivery and handling on the new server to ensure everything is working correctly. 7. Monitor the server's performance and logs to ensure it is handling webhook traffic efficiently and without errors. 8. Follow best practices for webhook handling, such as using HTTPS, validating the webhook signature, and handling errors gracefully. 9. Implement a backup and recovery plan in case of server failure or data loss. 10. Regularly update the server's software and dependencies to ensure security and performance.", "763f038f-b442-4861-95a6-e210da05a12a": "To approve or reject a job that requires review in a workflow, you need to navigate to the workflow run that requires review and click on the review request notification. Then, select the job environment(s) to approve or reject, and either approve and deploy the job by clicking \"Approve and deploy\" or reject the job by clicking \"Reject\". If a job is not approved within 30 days, it will automatically fail.", "44e7c7b7-7a39-4e7a-b5ae-a4384c2ee44d": "Smee.io can be utilized to provide a webhook proxy URL and forward webhooks by following these steps:  1. Go to smee.io and start a new channel to get a webhook proxy URL. 2. Configure your webhook to use the webhook proxy URL. 3. Start a local server on your computer or codespace. 4. Use the smee-client to receive forwarded webhooks from smee.io by running a command in your terminal, replacing the placeholders with your specific values.  This allows {% data variables.product.company_short %} to send webhook deliveries to the webhook proxy URL, which forwards them to your local server. This can be useful for testing or development purposes, as it allows you to run your code locally while still receiving webhook deliveries.", "abd5cbaf-884b-44b9-b8aa-aa199d9664c8": "To view a table displaying the list of individuals who have made commits to a specific repository on {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to the repository in question on {% data variables.product.prodname_dotcom %}. 2. Click on the \"Settings\" tab in the repository's navigation menu. 3. Click on \"Collaborators\" in the left-hand sidebar. 4. This will display a table listing all of the individuals who have made commits to the repository, along with various details about their contributions.  Alternatively, you can also access this information by clicking on the \"Commits\" tab in the repository's navigation menu, and then clicking on the \"Authors\" tab within the \"Commits\" page. This will display a list of all of the individuals who have made commits to the repository, along with various details about their contributions.    Based on the context information provided, generate the following quiz/examination questions:2. How can I download a CSV report containing a list of all of the individuals who have made commits to a specific repository on {% data variables.product.prodname_dotcom %}?  Answer:  To download a CSV report containing a list of all of the individuals who have made commits to a specific repository on {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to the repository in question on {% data variables.product.prodname_dotcom %}. 2. Click on the \"Settings\" tab in the repository's navigation menu. 3. Click on \"Collaborators\" in the left-hand sidebar. 4. Click on the \"CSV\" button located in the upper-right corner of the \"Collaborators\" page. 5. This will prompt you to download a CSV file containing a list of all of the individuals who have made commits to the repository, along with various details about their contributions.  Alternatively, you can also access this feature by clicking on the \"Commits\" tab in the repository's navigation menu, and then clicking on the \"Authors\" tab within the \"Commits\" page. From there, you can click on the \"CS", "77ee3f3b-a217-4fb9-a39d-b4d20cfcdb64": "Yes, each member of the organization will need to authorize the app before the app can act on their behalf. After installation, the app will have access to the organization and repository resources that it requested during the installation process, for all members of the organization who have authorized the app.", "0c215e3d-3da4-47f2-adac-6a1445d3bfd2": "According to the context information provided, if your dotfiles are not being cloned in your GitHub Codespaces workspace, you should first check if the \"Automatically install dotfiles\" option is enabled in your personal GitHub Codespaces settings. If it is enabled, you can then check the `/workspaces/.codespaces/.persistedshare/dotfiles` directory to see if your dotfiles were cloned. If your dotfiles were not cloned, you can check the `/workspaces/.codespaces/.persistedshare/EnvironmentLog.txt` file to see if there was a problem cloning them. This file contains detailed logs about the environment setup process, which can help you identify any issues related to dotfile cloning. If you encounter any errors or issues during the dotfile cloning process, you can refer to the logs in this file to troubleshoot the problem.", "a5200a52-de67-4ead-86e4-2e149d011fbb": "To check the network connectivity of a self-hosted runner, you can use the `run` script with the `--check` parameter provided by the self-hosted runner application. Along with this parameter, you need to provide two arguments: `--url` with the URL to your GitHub repository, organization, or enterprise, and `--pat` with the value of a personal access token (PAT) that has the `workflow` scope (for GitHub.com) or workflows read and write access (for GitHub Enterprise Cloud) {% ifversion pat-v2%}. This helps to ensure that the self-hosted runner can access all required network services on GitHub.com or GitHub Enterprise Cloud. For example, `run --check --url https://github.com/octo-org/octo-repo --pat ghp_abcd1234`.", "96ea3692-5916-45da-8e77-d719c4ec31f3": "An app should authenticate as an app installation when it needs to access resources that are owned by the user or organization that installed the app, and it doesn't require user input for automation workflows. In this context, the app can access resources related to the user or organization that installed it, such as repositories, issues, and pull requests. This is ideal for automation workflows that don't involve user input, as it allows the app to perform actions on behalf of the user or organization without requiring them to authenticate separately.", "d36ebf76-6de5-43b6-a279-e78cba1aef64": "The `from` field is used to specify a different {% data variables.product.prodname_codeql %} pack to resolve queries, while the `qlpack` instruction is used to resolve queries in the default suite of a named {% data variables.product.prodname_codeql %} pack.  For example, let's say we have two {% data variables.product.prodname_codeql %} packs, `pack1` and `pack2`, and we want to use queries from both packs in our suite definition. We can add a `from` field to specify the pack we want to resolve queries from:  ```yaml - queries:   from: pack2   - name: query1     description: This is a query from pack2  - queries:   - name: query2     description: This is a query from the current pack ```  In this example, the first set of queries will be resolved from the `pack2` pack, while the second set of queries will be resolved from the current pack.  On the other hand, let's say we want to use the default suite of queries from the `pack1` pack in our suite definition:  ```yaml - qlpack: pack1   - name: query3     description: This is a query from the default suite of pack1 ```  In this example, the `qlpack` instruction tells {% data variables.product.prodname_codeql %} to resolve queries from the default suite of the `pack1` pack. If the `pack1` pack doesn't have a default suite, all queries in the pack will be selected instead.", "6622a7a2-54fd-421d-9e93-8137b681e666": "Based on the \"Sensitive Data Type\" and \"Purposes of Processing\" sections provided, GitHub collects and processes sensitive data such as account log-in, financial account, debit or credit card number, and security or access codes for purposes of transacting commerce, processing transactions, fulfilling orders, providing Services, helping, securing, and troubleshooting, and detecting and preventing fraud. It also collects sensitive data related to racial or ethnic origin, religious or philosophical beliefs, or union membership for purposes of providing and personalizing products, product development, helping, securing, and troubleshooting, and marketing. Medical or mental health, sex life, or sexual orientation data is also collected and processed for purposes of providing and personalizing products, product development, helping, securing, and troubleshooting, and marketing. GitHub asks for consent or direction to collect and process sensitive data, and it does not use or disclose sensitive data for purposes other than those listed, except to perform services to operate its business, to help ensure security and integrity, to combat malicious, deceptive, fraudulent, or illegal acts, and to protect physical safety, as long as the processing is reasonably necessary and proportionate. GitHub shares sensitive data with service providers and user-directed entities.", "abedd080-7edd-4a1b-af4c-1a9de30d43f9": "To explicitly install dependencies managed via CocoaPods or Carthage before generating the {% data variables.product.prodname_codeql %} database for Swift analysis, you should add a build step to your workflow file that runs the appropriate CocoaPods or Carthage commands. For example, if your project uses CocoaPods, you could add the following build step:  ```yaml - run: |     pod install     pod update ```  If your project uses Carthage, you could add the following build step:  ```yaml - run: |     script/bootstrap.sh     script/build.sh ```  Make sure to add these build steps before the {% data variables.product.prodname_codeql %} analysis step in your workflow file.", "faa5fbd5-814e-40b5-8445-f858a21c72e6": "To approve a domain for your organization on GitHub, follow these steps:  1. Access your organization's settings by clicking on your organization's name in the top left corner of the GitHub page, then selecting \"Settings\" from the drop-down menu. 2. Click on \"Organization settings\" in the left-hand menu. 3. Click on \"Verified & approved domains\" under the \"Organization settings\" section. 4. Click on \"Add a domain\" and enter the domain you want to approve. 5. Follow the instructions provided to verify ownership of the domain. 6. Once the domain is verified, it will be approved for your organization. 7. If you want to remove an approved or verified domain, select the \"kebab-horizontal\" dropdown menu to the right of the domain and click \"Delete\".  Note: This feature is currently in beta, so it may not be available for all organizations.", "3131f164-f36c-4569-99e9-dcb162d77a22": "The JVM running the command has options `-v` (incrementally increase the number of progress messages printed), `-q` (incrementally decrease the number of progress messages printed), and `--verbosity=<level>` (explicitly set the verbosity level to one of errors, warnings, progress, progress+, progress++, progress+++) to control the verbosity level of progress messages printed.", "c6c4b848-8e15-468c-b506-52eac5065037": "To resolve conflicts during a migration process using the provided tool, you can review the _conflicts.csv_ file generated by the tool. This file lists any conflicts that were identified during the migration process, such as when two users have the same username on the source and target instances.  To make corrections to incorrect changes, you can modify the data in the _conflicts.csv_ file. For example, if the tool recommends mapping a user from the source to a user with the same name on the target, but you know that the user should be mapped to a different user on the target, you can change the `target_url` column in the _conflicts.csv_ file to reflect the correct user.  Here's an example of how to change the `target_url` for a user in the _conflicts.csv_ file:  | `model_name`   | `source_url`   | `target_url` | `recommended_action` | |--------------|--------------|------------|--------------------| | `user`         | `https://example-gh.source/octocat` | `https://example-gh.target/octocat` | `map`  To change the `target_url` for the user `octocat` to `monalisa`, you would modify the `target_url` column as follows:  | `model_name`   | `source_url`   | `target_url` | `recommended_action` | |--------------|--------------|------------|--------------------| | `user`         | `https://example-gh.source/octocat` | `https://example-gh.target/monalisa` | `map`  The recommended action for renaming a repository from its source URL to a new URL on the target instance is to update the `target_url` column in the _conflicts.csv_ file and set the `recommended_action` to `rename`.  Here's an example of how to update the _conflicts.csv_ file for this scenario:  | `model_name`   | `source_url`   | `target_url` | `recommended_action` | |--------------|--------------|------------|----------------", "26a1ecdb-874b-48eb-8621-9e7e9c3ab472": "To enable the \"Unified contributions\" feature on your GitHub Connect page, follow these steps:  1. Go to your organization's settings on GitHub.com (tcom_the_website %}.{% data reusables.enterprise-accounts.access-enterprise %}{% data reusables.enterprise-accounts.github-connect-tab %}{% endif %}) 2. Scroll down to the \"GitHub Connect\" section and click on the \"Manage\" tab. 3. To the right of \"Unified contributions\", click the \"Enable\" button (see screenshot below). 4. You will be prompted to sign in to your {% data variables.product.prodname_ghe_server %} site to receive further instructions.  ! [Screenshot of the \"Unified contributions\" option on the GitHub Connect page](https://docs.github.com/assets/github-docs/topics/github-connect/github-connect-unified-contributions-161122216.png)  If you are using {% data variables.product.prodname_ghe_server %}, you may be redirected to your {% data variables.product.prodname_ghe_server %} site to confirm your terms of service before enabling this feature.  Note: These steps may vary slightly depending on the version of GitHub Connect you are using.", "e57e3b9c-9732-4691-b6c1-3d09b34f75d5": "A screenshot should be used to complement text instructions in {% data variables.product.prodname_docs %} when the user interface (UI) element is hard to find, such as when it is small, visually subtle, or not immediately visible. This is because screenshots can make articles more visually scannable and easier to understand, especially for people who have difficulty reading.  For example, in the case of cloning a gist, the options to clone a gist are not immediately visible. The user must first navigate to the gist they want to clone, then click the \"Settings\" button, and then click the \"Clone or embed\" tab. A screenshot can help guide the user through this process and make it clear where to find the cloning options.  !Screenshot of an article showing text instructions and a UI screenshot for cloning a gist.  On the other hand, screenshots should not be used for procedural steps where text alone is clear, or to show code commands or outputs. This is because screenshots can add length and load time to articles, privilege sighted users, and increase the volume of content that needs to be maintained.  For example, in the case of running a command in the terminal, text instructions are clear and concise. A screenshot is not necessary, as the user can simply follow the text instructions.  !Screenshot of an article showing text instructions for running a command in the terminal.  In summary, screenshots should be used to complement text instructions when they are necessary to clarify the user interface, but should not be overused as they can add unnecessary length and load time to articles.", "36fe8189-3a58-4942-aa87-aaf15aadee38": "In order to set up an \"AUTOTITLE\" for an account in GitHub Enterprise Cloud (GHEC), GitHub Enterprise Server (GHES), or GitHub Advanced Security (GHAE), you can refer to the articles provided in the context information. However, if you don't have access to these articles or if you're not sure which account type you're working with, you'll need to gather more context information before proceeding with setting up an \"AUTOTITLE\". Without this information, it's not possible to provide a specific answer.", "7fd13323-422c-46cc-8362-05be672576cd": "The `--verbosity` option with a level of progress+++ will print detailed progress messages, which is the highest level of verbosity available. This will result in the CLI printing a large number of progress messages during the execution of commands.", "5edff1c2-483d-4dd0-a6e0-21dce928eb1b": "Custom labels can be set for all pull requests raised for a package manager using the `labels` key in the `dependabot.yml` file. This key allows you to override the default labels and specify alternative labels for all pull requests raised for a package manager. The alternative labels must already exist in the repository, and you cannot create new labels in the `dependabot.yml` file. Here's an example `dependabot.yml` file that sets custom labels for all npm pull requests:  ```yaml version: 2 updates:   - package-ecosystem: \"npm\"     directory: \"/\"     schedule:       interval: \"weekly\"     labels:       - \"npm dependencies\"       - \"triage-board\" ```  In this example, all npm pull requests will have the custom labels \"npm dependencies\" and \"triage-board\" assigned to them. This can help with organization and prioritization of pull requests.", "bd581184-076b-4fd8-85fd-ae78efcd3a6b": "Based on the context information provided, it is not clear whether a TLS certificate needs to be updated or uploaded for the packages host URL. The context only mentions that connections to `registry.npmjs.com` traverse through the Cloudflare network, and subsequently do not connect to a single static IP address. To determine whether a TLS certificate needs to be updated or uploaded, you should check if there are any issues with SSL/TLS connections to `registry.npmjs.com` or consult the documentation for your specific use case. If you are unsure, it may be best to consult with a network or security specialist for guidance.", "aa17f5de-2931-4d8f-9811-ba7014fbee5e": "To update the credit card information for your organization's account on GitHub, follow these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of GitHub and selecting \"Settings\" from the drop-down menu. 2. Click on the \"Billing\" tab in the left-hand menu. 3. Under \"Payment method\", click \"New Card\". 4. Enter your new credit card information and click \"Save card\".  Alternatively, you can also update your credit card information by going to your personal account settings and adding a new payment method. Then, go back to your organization's billing settings and select the new payment method as the primary payment method for your organization.  Note: If you're not sure where to find your organization's settings or the \"Billing\" tab, refer to the context information provided or search for \"GitHub organization settings\" in a search engine for more guidance.", "4fed2def-82a4-4595-9014-a5245c81fb1e": "The \"DEPLOYMENT_NAME\" variable in the \"setup-build-publish-deploy\" job is significant because it is used to name the deployment that will be created in the Google Kubernetes Engine (GKE) cluster. This variable is used in the configuration to specify the name of the deployment that will be created from the Docker image that was built and published in the previous steps. The deployment will be created in the specified GKE cluster and zone, and will be accessible through the cluster's load balancer. The deployment name should be unique within the cluster and should accurately reflect the purpose of the deployment.", "a8251f98-de96-49bf-b0b3-480c63c622e9": "Before creating a workflow using {% data variables.product.prodname_actions %} to automate the process of building, pushing, and deploying a containerized application to Google Kubernetes Engine (GKE), you need to complete the following prerequisites:  1. Authenticate using the `gcloud` CLI with `gcloud auth login`. 2. Create a GKE cluster using `gcloud container clusters create $GKE_CLUSTER --project=$GKE_PROJECT --zone=$GKE_ZONE`. 3. Enable the Kubernetes Engine and Container Registry APIs using `gcloud services enable containerregistry.googleapis.com container.googleapis.com`. 4. Create a service account using `gcloud iam service-accounts create $SA_NAME`. 5. Retrieve the email address of the service account using `gcloud iam service-accounts list`. 6. Add roles to the service account. 7. Retrieve the keys of the service account and store them as a base64-encoded (encrypted for {% ifversion fpt or ghec %}) repository secret named `GKE_SA_KEY`.  Note: {% data reusables.actions.about-oidc-short-overview %} (if applicable).", "5f682257-b99e-47a9-8d10-bbaefeb59a1d": "Anyone watching the pull request or repository will receive a notification of your comment, as mentioned in the context information. To ensure that others are notified, you can also consider mentioning their GitHub username in the comment using the \"@\" symbol. This will send them a direct notification, in addition to the pull request notification.", "b337307c-aa41-4465-bae8-8906220ae789": "The purpose of adding {% data variables.product.prodname_dotcom %}'s IP ranges to an IP allowlist during a migration from {% data variables.product.prodname_ghe_server %} to {% data variables.product.prodname_dotcom %} is to ensure that communication between the source repository and {% data variables.product.prodname_dotcom %} is not blocked by firewall restrictions. This is necessary because during the migration process, {% data variables.product.prodname_dotcom %} will access the source repository to retrieve data, and if the IP ranges used by {% data variables.product.prodname_dotcom %} are not allowed, the migration may fail or be delayed. By adding these IP ranges to the IP allowlist, you can ensure that the migration proceeds smoothly and without interruption.", "efcde9bb-20a1-435c-a223-459e725a3cf4": "To add members to an organization in {% data variables.location.product_location %}, follow these steps:  1. Go to your organization's settings by clicking on the organization's name from the organization selection drop-down menu in the top left corner of the page, and then clicking on the Settings tab.  2. In the left-hand menu, click on Membership.  3. Click the green Add people button in the upper right-hand corner of the page.  4. Enter the email addresses of the users you want to add, or paste a list of email addresses separated by commas or newlines.  5. Select the role you want to assign to the new members from the drop-down menu next to each email address.  6. Click the green Add selected people button to send invitations to the new members.  Alternatively, you can also add members to an organization by clicking on the Invitations tab in the organization's settings and following the same steps as above. The invited members will receive an email with instructions on how to accept the invitation and join the organization.", "34767d82-a2b2-4940-a572-e230abdb0732": "To download artifacts using {% data variables.product.prodname_actions %}, you can use the `download-artifact` action. This action downloads an artifact from a previous workflow run and saves it to a directory on the runner machine. The downloaded artifact can then be accessed by subsequent steps in the same workflow.  The limitations of downloading artifacts using {% data variables.product.prodname_actions %} are:  1. You can only download artifacts that were uploaded during the same workflow run. 2. The artifact must have been uploaded using the `upload-artifact` action. 3. The artifact must have a name specified during the upload process. 4. The artifact must be available for download during the time the `download-artifact` action is executed. 5. The `download-artifact` action can only download one artifact at a time. If you need to download multiple artifacts, you will need to use the action multiple times.  For more information on the `download-artifact` action, see the {% ifversion fpt or ghec %}actions/download-artifact action{% else %} `actions/download-artifact` action on {% data variables.location.product_location %}{% endif %}.", "b71736da-283a-4b6c-81ee-cd21b44d926e": "Without further context, it is unclear what \"AUTOTITLE\" is or what its purpose might be in a document. It could potentially be a formatting command or a placeholder for a title that will be automatically generated by the software or program being used. However, without more information, it is impossible to determine the exact purpose of \"AUTOTITLE\" in this context.", "7c18eac7-16a3-46bb-bce6-254cd777ab85": "In the context of {% data variables.product.prodname_GH_advanced_security %} and secret scanning, the difference between organization-owned repositories and user-owned repositories is that the feature is available for all organization-owned repositories as part of {% data variables.product.prodname_GH_advanced_security %}, while it is not available for user-owned repositories. This means that if a user wants to enable secret scanning for their repositories, they need to have an organization-owned repository instead of a user-owned repository.", "acbe0102-3695-41fe-bb04-0cba67436150": "The most commonly supported media types by the {% data variables.product.prodname_dotcom %} REST API are `application/vnd.github+json` and `application/json`. These media types specify the format of the data being consumed as JSON (JavaScript Object Notation).  To specify a media type in a request, you can add it to the `Accept` header of the request. For example, to request JSON data from the {% data variables.product.prodname_dotcom %} REST API, you can set the `Accept` header to `application/vnd.github+json` or `application/json`.  Here's an example of a request that uses media types:  ```shell GET /users/octocat Accept: application/vnd.github+json ```  This request asks the {% data variables.product.prodname_dotcom %} REST API to return data about the user \"octocat\" in the `application/vnd.github+json` format. If you omit the `Accept` header, the {% data variables.product.prodname_dotcom %} REST API will return data in the default format, which is `application/json`.", "ea1e52e3-1eba-4058-a8e7-72f049cf4082": "To download a repository from {% data variables.product.prodname_dotcom %} to your computer and create a new branch using {% data variables.product.prodname_desktop %}, follow these steps:  1. Open {% data variables.product.prodname_desktop %} on your computer. 2. Sign in to your {% data variables.product.prodname_dotcom %} account. 3. Click on the \"Clone or create a repository\" button on the repository you want to download. 4. Select \"Open with GitHub Desktop\" to automatically clone the repository and create a new branch. 5. Alternatively, you can manually clone the repository by clicking on the \"Code\" button and copying the URL, then in {% data variables.product.prodname_desktop %}, click on the \"Clone repository\" button, paste the URL, and select the location on your computer where you want to save the repository. 6. Once the repository is cloned, you can create a new branch by clicking on the \"Branch\" button in the top left corner of {% data variables.product.prodname_desktop %}, entering the name of the new branch, and clicking on the \"Create branch\" button. 7. Make your changes to the code using an editor such as {% data variables.product.prodname_vscode %}. 8. Commit your changes by clicking on the \"Commit changes\" button in the bottom left corner of {% data variables.product.prodname_desktop %}, entering a commit message, and clicking on the \"Commit changes\" button again. 9. Push your changes to {% data variables.product.prodname_dotcom %} by clicking on the \"Push\" button in the top left corner of {% data variables.product.prodname_desktop %}.    If you're not familiar with using {% data variables.product.prodname_desktop %}, you can refer to the \"AUTOTITLE\" documentation for more information on how to use the tool. Additionally, you can visit the `desktop/desktop` repository to learn more about the open source {% data variables.product.prodname_desktop %} project, contribute to the project, or open an issue to provide feedback.", "d3bae138-d7ad-48c6-9e4b-17eb8571eb35": "The options for resolving QL packs outside of the package registry using the \"codeql pack ci\" command are provided through the \"--search-path\" option. This option takes a list of directories where QL packs may be found. Each directory can either be a QL pack (or bundle of packs containing a \".codeqlmanifest.json\" file at the root) or the immediate parent of one or more packs. This allows for the use of locally developed or custom packs that are not part of the official package registry.", "a76cd8f0-10a7-4408-9894-d316a3e797b4": "GPG (Gnu Privacy Guard) is an open-source implementation of the OpenPGP standard for data encryption and digital signatures. It allows users to sign and encrypt their commits and tags in Git, which can be verified by others using their public key. With GitHub, users can add their GPG key to their account and have their locally signed commits and tags cryptographically verifiable against that key on the platform. This adds an extra layer of security and authenticity to Git workflows, ensuring that the commits and tags are indeed from the claimed author.", "2a8814c9-9c6e-43a6-b080-b7897439eb77": "Based on the context information provided, the recommended approach for opening network ports in Google Cloud Platform is to selectively open network ports based on the network services required for administrative and user purposes, as mentioned in the context information. The context provides a table listing the required ports and their respective uses, which can be used as a guide for creating firewall rules using the gcloud compute firewall-rules command. It's also recommended to reserve a static external IP address for the {% data variables.product.prodname_ghe_server %} VM in production environments to ensure the public IP address is retained after restarts.", "0a72f2e3-8c68-496e-bf68-7c93abd27434": "The organization-level activities related to enabling and disabling automatic validity checks for GitHub Secret Scanning are contained in the `org_secret_scanning_automatic_validity_checks` category mentioned in the audit log context information.", "c9897dc1-1ca8-4988-9dfb-f7e01a6fd56b": "The syntax for caching dependencies differs between Travis CI and {% data variables.product.prodname_actions %}. In Travis CI, you can simply specify the type of cache to use, such as \"npm\" or \"yarn\", in the \"cache\" section of your YAML configuration file. In {% data variables.product.prodname_actions %}, you use the {% data reusables.actions.action-cache %} action to cache dependencies. This action takes several parameters, including the path to the directory containing the dependencies, a key to identify the cache, and a prefix for restoring cached keys. The syntax for specifying the cache key is also different, with {% raw %}{% endraw %} used to embed shell variables in the key in {% data variables.product.prodname_actions %}.", "dda0b70b-de84-496a-8bba-a233116070cc": "Rule insights provide a way to view and analyze the actions taken by rulesets in a repository. By filtering the insights by ruleset, branch, actor, and time period, you can understand the impact of rulesets on specific areas of the repository. This information can help you identify any issues or inconsistencies with the rulesets and make informed decisions about how to improve your code review process. Overall, rule insights provide a valuable tool for monitoring and optimizing the effectiveness of rulesets in your repository.", "084c2837-472c-4267-9cc6-3c43ecc7e3e4": "To configure authentication with Azure Maps API in your GitHub Enterprise Server instance using the provided context information, you need to follow these steps:  1. Access the GitHub Enterprise Server settings by going to {% data reusables.enterprise_site_admin_settings.access-settings %}. 2. Click on \"Management console\" in the left-hand menu. 3. In the \"Settings\" sidebar, click \"Privacy\". 4. To enable interactive maps, select \"Enable GeoJSON rendering\". 5. Under \"Azure Maps API Token\", paste the authentication token provided by Azure. 6. Save the settings by clicking \"Save settings\" in the bottom right-hand corner of the page.  Note that this process assumes that you already have an Azure Maps account and have obtained an authentication token. If you need help with this, you can refer to the Microsoft Docs for more information on authentication with Azure Maps.", "2873c3c0-f82e-4505-8e54-96999a6995db": "To set up Git and {% data variables.product.prodname_dotcom %} for version control and collaboration on your projects, follow these step-by-step instructions:  1. Generating SSH keys:  a. Open a terminal or command prompt on your computer.  b. Type the following command to generate a new SSH key:     ```    ssh-keygen -t ed25519 -C \"your_email@example.com\"    ```     Replace \"your_email@example.com\" with your GitHub email address.  c. Follow the prompts to save the key in the default location and enter a secure passphrase (optional).  d. Copy the SSH key to your clipboard by running the following command:     ```    pbcopy < ~/.ssh/id_ed25519.pub    ```     (Mac)     ```    xclip -sel clip < ~/.ssh/id_ed25519.pub    ```     (Linux)  e. Go to your {% data variables.product.prodname_dotcom %} account settings and click on \"SSH and GPG keys\" in the left-hand menu.  f. Click on the \"New SSH key\" button and paste your SSH key into the text box.  g. Click \"Add SSH key\" to save the key.  2. Configuring a credential helper:  a. Install the `git-credential-manager` package by running the following command:     ```    brew install git-credential-manager-core    ```     (Mac)     ```    sudo apt-get update    sudo apt-get install git-credential-manager-core    ```     (Linux)  b. Open your Git configuration file by running the following command:     ```    nano ~/.gitconfig    ```  c. Add the following lines to the file:     ```    [credential]    helper = manager    ```  d. Save and close the file.  e. Test the cred", "5d607579-f5d1-493c-9c5b-1ba3d58598ec": "Yes, based on the context information provided, here's a summary of the specifications for creating diagrams:  - Use the PNG file format for static images only (no GIFs) - Keep file sizes at 250 KB or less - Use descriptive file names for easy identification - Follow the shape conventions and use colors defined in the Primer Design System for consistency - Ensure proper contrast and add alt text for accessibility - If a diagram applies to all {% data variables.product.prodname_dotcom %} plans, no versioning is required. If a diagram is only relevant to specific plans or versions of {% data variables.product.prodname_ghe_server %}, versioning with Liquid conditional statements is necessary.", "51163c1a-bbaf-478d-8246-f8ec6f8a2c80": "The \"Reduce prebuild available to only specific regions\" option in the \"Prebuild triggers\" settings allows you to create prebuilds only in selected regions instead of all available regions. This option can help reduce storage charges per prebuild as prebuilds in each region incur individual storage charges. By enabling prebuilds only in specific regions, you can avoid unnecessary storage charges for regions where prebuilds are not frequently used. This option can also help improve the performance of prebuilds as they are created in regions closer to the developer's location, reducing latency and improving the overall user experience.", "9ef47fbb-cacc-470c-98fe-30ebc61612c9": "{% data variables.product.prodname_copilot_for_business %} utilizes User Engagement Data, including pseudonymous identifiers and general usage data, for several purposes. Firstly, it is used to evaluate the impact of {% data variables.product.prodname_copilot %} on users by measuring its positive effects and benefits. Secondly, it helps in fine-tuning ranking and sorting algorithms used for suggestions, thereby enhancing the overall user experience. Thirdly, it is examined to detect abuse and policy violations associated with {% data variables.product.prodname_copilot %}. Lastly, it is used for conducting experiments and research related to developers and their utilization of developer tools and services, which aids in gaining valuable insights into user behavior and preferences. This data collection process is aimed at improving the service and is necessary for the use of {% data variables.product.prodname_copilot %}.", "78aad96c-0fa8-4155-9d81-471b6fd12c93": "An enterprise account is a type of account that allows an organization to manage and enforce policies for multiple organizations under a single umbrella. This differs from a single organization setup, where each organization has its own separate account. With an enterprise account, each member of the organization can sign in to their personal account, but they are all managed and billed under the enterprise account.  In an enterprise account, owners have complete administrative access to the account, including the ability to add and remove members, manage billing settings, and access enterprise resources. Billing managers, on the other hand, can only manage billing settings and cannot access enterprise resources. This role is useful for organizations that want to delegate billing responsibilities to a specific person.  Payments for an enterprise account can be made by credit card or PayPal, and the account is billed in US dollars. The company does not offer discounts for procurement companies or for renewal orders. However, setting up yearly billing for enterprise accounts can make purchasing easier and more efficient for procurement companies. This is because it allows for a single payment to be made for the entire year, rather than multiple payments throughout the year. This can also help to reduce administrative costs associated with managing multiple accounts.", "2d88d8ca-e5b0-45ff-98f1-94292f93dc8e": "There are two ways to set variables in workflows on {% data variables.product.prodname_dotcom %}:  1. To define an environment variable for use in a single workflow, you can use the `env` key in the workflow file.  2. To define a configuration variable across multiple workflows, you can define it at the organization, repository, or environment level.  {% endif %}    Defining configuration variables{% ifversion actions-configuration-variables %} for multiple workflows{  Given the context information and not prior knowledge, answer the following question.   What are the different ways to set variables in workflows on {% data variables.product.prodname_dotcom %}  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  There are three ways to set variables in workflows on {% data variables.product.prodname_dotcom %}:  1. To define an environment variable for use in a single workflow, you can use the `env` key in the workflow file.  2. To define a configuration variable across multiple workflows, you can define it at the organization, repository, or environment level.  3. You can also use the default variables that {% data variables.product.prodname_dotcom %} sets automatically.  {% endif %}    Default environment variables{% ifversion actions-configuration-variables %}  Given the context information and not prior knowledge, answer the following question.   What are the default environment variables that {% data variables.product.prodname_dotcom %} sets automatically?  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  {% data variables.product.prodname_dotcom %} sets several default environment variables automatically. Some of these variables are specific to the operating system of the runner machine, while others are available on all operating systems. Here are some examples:  - `GITHUB_SHA`: The SHA-1 hash of the", "683d3eff-9eee-487e-884d-05ee1e441c91": "The syntax for setting up a scheduled workflow to run at a specific time each week is `on.schedule` followed by a cron-like expression. In the provided context, the workflow is set to run every Monday at 7:20 UTC, which is expressed as `0 20 * * 1`.  To change the day of the week, replace the last digit (1 in this case) with the number representing the day of the week. Monday is represented by 1, Tuesday is represented by 2, and so on.  To change the time, replace the second and fourth digits (20 in this case) with the desired hour and minute values, respectively. The first digit (0 in this case) is always set to 0 for scheduled workflows.  Here's an example of how to change the day and time:  - To run the workflow every Tuesday at 9:00 UTC, use `0 09 * * 2`. - To run the workflow every Friday at 11:30 UTC, use `30 11 * * 5`.  For more information about cron expressions, see the [Cron Job documentation](https://docs.github.com/en/actions/learn-github-actions/events/cron-jobs) on the GitHub documentation website.", "39354fd3-4401-487e-96c1-7be6af6cbb8c": "To change the visibility of your repository on GitHub Pages, follow these steps:  1. Go to your repository on GitHub. 2. Click on the Settings tab in the repository menu. 3. Scroll down to the GitHub Pages section. 4. Click on the source dropdown menu and select the branch you want to use for your GitHub Pages site. 5. Choose the visibility you want for your site: either Public or Private. 6. Click Save to apply the changes.  Note: If you're using a custom domain, you'll also need to update your DNS settings to reflect the new visibility.  For more information, see the GitHub documentation on configuring GitHub Pages.", "114bcccd-cb03-4fbe-9b44-782fce1ad607": "The purpose of using the {% data variables.product.prodname_actions_importer %} CLI extension in the provided context is to set required credentials and options for working with Azure DevOps and {% data variables.product.prodname_dotcom %} when using post-deployment gates, post-deployment approvals, and some resource triggers in GitHub Actions. This is achieved by running the `configure` CLI command to provide the necessary credentials and options, such as GitHub {% data variables.product.pat_v1 %} and Azure DevOps {% data variables.product.pat_generic %} tokens, and the base URL of the GitHub instance.", "cf61bd23-9425-476b-8747-9658c24ab411": "Workflows triggered from a forked repository using the `pull_request` event have read-only permissions and have no access to secrets. However, for various event triggers such as `issue_comment`, `issues`, `push` and `pull_request` from a branch within the repository, the accessibility of secrets can vary. In these cases, the attacker could attempt to steal repository secrets or use the write permission of the job's `GITHUB_TOKEN`. Specifically, if the secret or token is set to an environment variable, it can be directly accessed through the environment using `printenv`. If the secret is used directly in an expression, the generated shell script is stored on-disk and is accessible. The risk for a custom action can vary depending on how a program is using the secret it obtained from the argument.", "4a366a83-157d-409a-9e4a-86535c4452a0": "To access the {% data variables.enterprise.management_console %} for {% data variables.location.product_location %} for the first time, you need to upload your license file. Follow these steps:  1. Access the {% data variables.enterprise.management_console %} by visiting this URL in your browser, replacing `hostname` with your actual {% data variables.product.prodname_ghe_server %} hostname or IP address:     ```shell    http(s)://HOSTNAME/setup    ```  2. Type in the password for the {% data variables.enterprise.management_console %} and click \"Continue authentication\".  3. Follow the prompts to upload your license file and complete the setup process.  If you don't have the license file or are unsure how to obtain it, you should contact your {% data variables.product.prodname_ghe_server %} administrator or GitHub support for assistance.", "7676c71a-fb60-4db1-8f90-7bc198230d7c": "To unblock a user from their profile page on GitHub, follow these steps:  1. Go to the user's profile page. 2. In the left sidebar, below the user's profile information, click \"Unblock or report user\". 3. Click \"Unblock user\".  Note that settings that were removed when you blocked the user, such as collaborator status, stars, and follows, are not restored when you unblock the user.", "36bf066b-8f4d-4a85-88ff-9a240d8e5a1a": "According to the context provided, an organization owner or user can request a change in the permissions of a GitHub app by modifying the permissions under the sections \"Repository permissions,\" \"Organization permissions,\" and \"Account permissions\" in the app's settings. When these changes are requested, {% data variables.product.prodname_dotcom %} will send an email to each organization owner or user notifying them of the request. The new permissions won't take effect on an installation or user authorization until the new permissions are approved. The process for approving these changes is not explicitly stated in the context, but it can be inferred that the organization owner or user who requested the changes needs to approve them before they take effect. However, it's possible that additional approval steps are required, depending on the specific permissions being requested. For more information, it's recommended to refer to the GitHub documentation on GitHub apps and permissions.", "0346a0a2-2df6-4673-80d7-e5a2277e6f02": "The format for specifying tags when pushing a Docker image to Docker Hub using GitHub Actions in the `docker/build-push-action` is `DOCKER-HUB-NAMESPACE/DOCKER-HUB-REPOSITORY:VERSION`. This can be set as a single tag or specified in a list. The tags should be passed as the value of the `tags` parameter in the `docker/build-push-action` step. The tags can be extracted using the `docker/metadata-action` step, as shown in the example provided.", "2dc0c773-b52a-468c-893d-75ccedab8231": "The pre-release software mentioned in the text material is provided by GitHub for testing purposes only, and users are required to sign a separate agreement to participate in the technical preview. The software may collect usage information, which is used to improve the software and related products. GitHub is the data controller for this personal data. Users may also collect data from users of their applications using certain features, but they must follow applicable laws and provide appropriate notices to users. Users can revoke their consent to data collection by contacting GitHub and requesting removal from the technical preview. Updates to the software are only available from GitHub or authorized sources and may change or disable any part of the software, including potentially removing features and services or revoking support for certain platforms or hardware.", "ff7d5831-b4be-4afb-b40d-24fccaea59f9": "According to the context information provided, if you want to link your SAML identity to your GitHub account for authentication instead of your current account, you'll need to ask your admin to unlink your SAML identity from your old account, so that you can link it to your new account. Depending on the setup of your organization or enterprise, your admin may also need to reassign your identity within your SAML provider. This is necessary because your SAML identity is used to govern access and team membership, and continuing with a new SAML identity can cause you to lose access to teams and organizations inside of GitHub. Only continue with the new SAML identity if you know that you're supposed to use that new SAML identity for authentication in the future.", "07366307-233a-4c15-a3ee-b72bb25213e9": "Organization owners can invite users to join an organization in GitHub (if the organization is on a paid per-user subscription and an unused license is available) or add users to an organization (if the organization doesn't require members to use two-factor authentication).  Explanation:  The context information provided in the text mentions that the article is about converting an outside collaborator into an organization member in GitHub. It also mentions that organization owners can invite users to join an organization (if the organization is on a paid per-user subscription and an unused license is available) or add users to an organization (if the organization doesn't require members to use two-factor authentication). Based on this information, we can infer that organization owners have the necessary permissions to convert an outside collaborator into an organization member in GitHub.  If the context didn't provide this information, we wouldn't be able to answer the question accurately. For example, if the context only mentioned that the article is about converting an outside collaborator into an organization member in GitHub, but didn't provide any information about the required permissions, we wouldn't know whether organization owners, collaborators, or anyone else could perform this action. In this case, we would say that we weren't given enough information to give an answer.", "72bb106e-ad3f-4f56-8da4-144588958a91": "Organizations can choose whether members can create repositories, and if so, which types of repositories they can create. Enterprise owners can restrict the options available for repository creation policies for their organization. This can include restricting members to creating private or internal repositories only, or allowing only certain types of repositories to be created. Enterprise owners can also restrict the visibility options available when repositories are created, but this does not restrict the ability to change repository visibility at a later time. For more information about these options and how to restrict them, refer to the documentation provided by GitHub (if using GitHub Enterprise Cloud or GitHub Enterprise Server) or GitHub.com (if using GitHub.com).", "bb605de9-0405-48be-bf73-4af6163208c2": "To upgrade an app for your personal account on GitHub Marketplace, follow these steps:  1. Go to your GitHub settings by clicking on your profile picture in the top right corner of the page and selecting \"Settings\" from the drop-down menu. 2. Click on \"Billing & plans\" in the left-hand menu. 3. Click on \"GitHub Marketplace\" to view your current apps and their plans. 4. Find the app you want to upgrade and click on \"Manage subscription\" next to it. 5. Click on \"Change plan\" and select the new plan you want to switch to. 6. Choose the new quantity of the app you want to use. 7. Click \"Confirm changes\" to complete the upgrade process.  Note: If you don't see the \"Change plan\" option, it may be because your app is free or already on the highest plan available. In this case, you won't be able to upgrade further.", "5272ce37-1ffd-4796-a3bc-4a7399ad9d77": "To replace the default identicon image with a custom image for your organization's profile on {% data variables.product.product_name %}, you can either upload a new image or use a Gravatar image. To upload a new image, follow these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of the page, then selecting \"Settings\" from the drop-down menu. 2. Click on your organization's profile picture in the top left corner of the page. 3. Click \"Upload new picture\" and select the image you want to use.  Alternatively, you can use a Gravatar image by following these steps:  1. Go to your organization's settings by clicking on your organization's name in the top left corner of the page, then selecting \"Settings\" from the drop-down menu. 2. In the \"Gravatar email (Private)\" field, enter the email address associated with your Gravatar image. 3. Click \"Update profile\".  Note that if you're using a Gravatar image, it will be used for your organization's profile picture as well as for your personal profile picture across all {% data variables.product.product_name %} properties where you've used the same email address.", "201ad515-bdad-46e3-a54b-602ab3177212": "The pattern matching for the `hashFiles` function in GitHub Actions follows glob pattern matching, which is a commonly used pattern for matching filenames and wildcard characters. This pattern is supported by various operating systems and tools, including GitHub Actions.  On Windows, the pattern matching for `hashFiles` is case-insensitive, which means that it doesn't distinguish between uppercase and lowercase characters. This behavior is consistent with the case-insensitive nature of Windows file systems.  In summary, the pattern matching for `hashFiles` follows glob pattern matching and is case-insensitive on Windows.", "e6f8e5aa-bfec-427b-9e76-9443bb2d0686": "To delete an entire private package on {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to your repository that contains the private package. 2. Click on the \"Packages\" tab in the left-hand menu. 3. Click on the package you want to delete. 4. Click on the \"Settings\" tab in the package's menu. 5. Click on the \"Delete this package\" button. 6. Confirm the deletion by clicking on the \"Delete\" button in the pop-up window.  There are no restrictions associated with deleting a private package, as long as you have the required access.  However, if you want to delete an entire public package on {% data variables.product.prodname_dotcom %}, there are some restrictions:  1. The package must not have more than 5,000 downloads of any version. 2. If the package has more than 5,000 downloads, you need to contact {% data variables.contact.contact_support_portal %} for further assistance. 3. Be aware that deleting a public package may break projects that depend on your package.  {% endif %}    How can one restore an entire package or package version on {% data variables.product.prodname_dotcom %}? What are the conditions for restoring a package or package version?  Answer:  To restore an entire package or package version on {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to your repository that contains the deleted package or package version. 2. Click on the \"Packages\" tab in the left-hand menu. 3. Click on the \"Deleted packages\" or \"Deleted versions\" tab, depending on what you want to restore. 4. Find the package or package version you want to restore and click on the \"Restore\" button. 5. Confirm the restoration by clicking on the \"Restore\" button in the pop-up window.  There are some conditions for restoring a package or package version:  1. You must restore the package or package version within 30 days of its deletion. 2. The same package namespace must still be", "828e8bd2-1266-4bf1-a22b-5d8bfbc7b461": "To identify and resolve conflicts during the migration process, you can follow these steps:  1. Prepare the migrated data by copying the migration archive to the target instance and generating a new Migration GUID using the `ghe-migrator prepare` command. 2. Generate a list of migration conflicts using the `ghe-migrator conflicts` command and review the _conflicts.csv_ file to understand the nature of the conflicts. 3. If conflicts are identified, you can copy the _conflicts.csv_ file to your local computer using the `scp` command. 4. Review the _conflicts.csv_ file using a text editor or CSV-compatible spreadsheet software to understand the recommended actions for each conflict. 5. Resolve the conflicts by taking the recommended actions or customizing the mappings as needed. 6. Import the data using the `ghe-migrator import` command with the new Migration GUID.  The _conflicts.csv_ file provides a migration map that lists the source URL, target URL, and recommended action for each conflict. By reviewing this file, you can understand the nature of the conflicts and take appropriate actions to resolve them.  Some common types of conflicts that may be identified during the migration process include:  - User conflicts: This occurs when a user with the same username exists on both the source and target instances. In this case, the recommended action is to merge the user accounts or choose which user to keep. - Repository conflicts: This occurs when a repository with the same name exists on both the source and target instances. In this case, the recommended action is to merge the repositories or choose which repository to keep. - Branch conflicts: This occurs when a branch with the same name exists on both the source and target instances. In this case, the recommended action is to merge the branches or choose which branch to keep.  By understanding the nature of the conflicts and taking appropriate actions, you can ensure that the migration process is successful and that all data is accurately and consistently applied to the target instance.", "cd2cf9ca-75b3-4b65-8a41-34d69bdf16da": "The \"last_active\" key in the \"User reports\" section indicates when the user account was last actively used. This information can be useful for identifying inactive or dormant accounts, which may need to be reviewed or deleted to maintain the security and efficiency of the system. It can also be helpful for understanding user behavior and engagement with the platform.", "86d80a02-f05b-41af-bdb9-32685a397f7e": "The X-Github-Next-Global-ID header can be used to facilitate migration to the new global ID format in the {% data variables.product.product_name %} GraphQL API by forcing the response payload to always use the new ID format for any object that was requested for the `id` field. This can be achieved by setting the value of the header to `1`. If the value is set to `0`, the default behavior of showing either the legacy ID or the new ID depending on the object creation date will continue.  Treating global node IDs as opaque strings instead of decoding them for type information can have potential consequences. For example, if the first two characters of a legacy ID are used to determine if the object is a pull request, this service will break since the format of the IDs has changed. In this case, it's recommended to migrate the service to treat these IDs as opaque strings, as they will still be unique and can be relied on directly as references.", "680e67ba-2477-46a5-9c57-1b967bb4c208": "{% data variables.product.prodname_mobile %} is available in English, Japanese, Brazilian Portuguese, Simplified Chinese, Spanish, German, and Korean. To change the language used by {% data variables.product.prodname_mobile %}, you can access the **Settings** menu in the app and make the desired language selection. If your device is configured to use one of the supported languages, {% data variables.product.prodname_mobile %} will automatically default to that language.", "3af769d0-3a87-422d-9dd4-87b5476b57b5": "In the context of GitHub commits, if an email address associated with a commit has a Gravatar, it will be displayed next to the commit instead of the default gray Octocat icon. This allows for a more personalized and recognizable representation of the author of the commit. It is a feature provided by Gravatar, a service that associates an avatar or profile picture with an email address, and is not required for GitHub commits.", "5a8a7c90-a999-4bd9-b61d-bb23343854c6": "To forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in your Travis CI server, you can use the {% data variables.product.prodname_actions_importer %} `forecast` command. This command analyzes the pipeline runs in your Travis CI server during the past seven days by default and generates a report that provides insights into potential {% data variables.product.prodname_actions %} usage. To run the `forecast` command, open your terminal and execute the following command:  ```shell gh actions-importer forecast travis-ci --output-dir tmp/forecast ```  The results of the forecast will be saved in the specified output directory in a file named `forecast_report.md`. This report provides valuable information that can help you plan and optimize your {% data variables.product.prodname_actions %} usage based on your Travis CI server's historical data.", "43982711-bf0f-43f0-97bb-9ac136522812": "To configure the {% data variables.product.prodname_actions_importer %} CLI command for Bamboo integration with GitHub, the following information is required:  1. The {% data variables.product.pat_generic_caps %} for GitHub, which is the personal access token (PAT) created in your GitHub account settings. 2. The URL for your GitHub instance (either {% data variables.product.product_name %} or GitHub.com). 3. The {% data variables.product.pat_generic_caps %} for Bamboo, which is the Bamboo {% data variables.product.pat_generic %} created in your Bamboo Server or Bamboo Data Center instance. 4. The URL for your Bamboo Server or Bamboo Data Center instance.  Note that the \"Which CI providers are you configuring?\" question is optional, as the default value is already set to Bamboo.", "aa341b9f-95e7-462a-94a6-74c999e2dbd7": "The recipient isn't allowed to assign or transfer this Agreement, or any of their rights under it, in whole or in part, without the prior written consent of the other party. This restriction applies both by operation of law and through the recipient's own choice. Prior written consent is required for any assignment or transfer.", "597c9b23-3944-4782-8a13-199184d61157": "According to the context information provided, there is no clear indication of which billing cycle is recommended. The context only mentions that users can choose between monthly and yearly billing cycles for a GitHub business plan, and that they should finish the upgrade process. To determine which billing cycle is more suitable for your needs, you may want to consider factors such as your budget, the number of users in your organization, and the length of time you plan to use the GitHub business plan. Monthly billing may be more flexible for organizations with fluctuating user counts or shorter-term needs, while yearly billing may offer a discounted rate for organizations with a longer-term commitment. It's recommended to review the pricing details and consult with your organization's finance team to make an informed decision.", "919d3256-1885-420c-af53-47e920de63f1": "In order to mitigate the risk of script injection when working with potentially untrusted input in the `github` context, it's important to follow defensive programming practices and treat these values as privileged application code. This means ensuring that they do not flow directly into workflows, actions, API calls, or anywhere else where they could be interpreted as executable code.  For example, in the context provided, when working with the `github.event.issue.title` context, it's recommended to avoid using it directly in shell commands or scripts. Instead, you should store it in a variable and sanitize it before using it in any executable code.  Here's an example of how to do this:  ```yaml       - name: Check issue title         run: |           title=\"${{ github.event.issue.title }}\"           title=$(echo \"$title\" | tr -d '\\n') # Remove newlines           title=$(echo \"$title\" | sed 's/[[:space:]]//g') # Remove spaces           if [[ $title =~ ^octocat ]]; then           echo \"Issue title starts with 'octocat'\"           exit 0           else           echo \"Issue title did not start with 'octocat'\"           exit 1           fi ```  In this example, we first store the `github.event.issue.title` context in a variable called `title`. We then sanitize the variable by removing newlines and spaces using `tr` and `sed`, respectively. This helps prevent script injection attacks by ensuring that the variable only contains the expected characters and does not contain any malicious code.  By following these defensive programming practices, you can help mitigate the risk of script injection and make your use of {% data variables.product.prodname_actions %} more secure.", "582f2f76-71f2-473b-8738-d494c02f90f2": "To test the workflow in your repository, follow these steps:  1. Open an issue in your repository. 2. Label the issue with the specified label in your workflow file. 3. Wait for the workflow to run. You can view the history of your workflow runs to see when it completed. 4. After the workflow completes, the issue that you labeled should have a comment added.  You can learn more about additional things you can do with the GitHub CLI, like editing existing comments, by visiting the GitHub CLI Manual.", "87229202-78c7-4e9a-8cce-b232812f7b65": "To configure a policy in HashiCorp Vault to grant access to specific paths for your workflows to retrieve secrets, follow these steps:  1. Log in to your HashiCorp Vault instance and navigate to the Policies tab. 2. Click the Create button to create a new policy. 3. Enter a name for the policy, such as `myproject-production`. 4. In the policy editor, define the path and capabilities for the specific paths your workflows will use to retrieve secrets. For example:  ```yaml path \"secret/data/production/*\" {   capabilities = [ \"read\" ] } ```  This configuration grants read-only access to the `secret/data/production/*` path.  5. Save the policy and exit the policy editor.  To create a role in HashiCorp Vault to group policies together and attach them to an access token generated by an OIDC authentication, follow these steps:  1. Log in to your HashiCorp Vault instance and navigate to the Auth tab. 2. Click the Create button to create a new role. 3. Enter a name for the role, such as `myproject-production`. 4. In the role editor, define the role type as `jwt`, the user claim as `actor`, and the bound claims as required by your security requirements. For example:  ```yaml {   \"role_type\": \"jwt\",   \"user_claim\": \"actor\",   \"bound_claims\": {     \"repository\": \"user-or-org-name/repo-name\"   },   \"policies\": [\"myproject-production\"],   \"ttl\": \"10m\" } ```  This configuration defines a role with a JWT token, a user claim of `actor`, a bound claim of `repository` with a value of `user-or-org-name/repo-name`, the `myproject-production` policy, and a token time-to-live (TTL) of 10 minutes.  5. Save the role and exit the role editor.  These steps will create a policy and a role that can be used to grant access to specific paths for your work", "b89356d8-0c4e-44d6-9199-f54ae2a14804": "To ensure that CodeQL's persistence feature is enabled during the execution of your CLI commands, you need to set the `codeql.persistence.directory` configuration property. This property specifies the directory where CodeQL will persist data such as downloaded QL packs and compiled query plans. If you don't set this property explicitly, it will default to a directory named `.codeql` in your home directory, which will be created if it doesn't already exist. By enabling persistence, you can avoid having to download and compile QL packs repeatedly, which can significantly improve the performance and efficiency of your CodeQL workflow.", "2d419b4f-d678-43cd-8029-d5c8ceea4c43": "\"Manage organization ref update rules and rulesets\" allows access to manage rulesets and view ruleset insights at the organization level, while \"View organization audit log\" provides access to the audit log for the organization, which may contain metadata for repositories in the organization. These permissions are distinct as managing rulesets involves making changes to the organization's ref update behavior, while viewing the audit log is for monitoring and reviewing past actions and events within the organization's Git repositories.\"", "277dbc26-8ec5-41c8-b648-cc46f60ffb16": "To filter issues and pull requests based on their labels in a GitHub repository, follow these steps:  1. Go to the repository in question. 2. Click on the \"Issues\" or \"Pull requests\" tab in the left-hand menu. 3. Click on the \"Labels\" tab in the right-hand menu. 4. Click on the label you want to filter by. 5. All issues and pull requests with that label will now be displayed.  Alternatively, you can also filter by multiple labels at once by selecting them all and clicking \"Apply\". This will display all issues and pull requests with any of the selected labels.  To clear the label filter, click the \"Clear current search query, filters, and sorts\" button in the right-hand menu.", "6a20abc9-75cc-49b9-a365-5bc7ef047772": "To install a specific extension in a codespace using the provided context information, you can follow these steps:  1. Locate the `devcontainer.json` file in your repository. 2. Add the following JSON object to the `devcontainer.json` file:     ```jsonc    \"extensions\": [      \"davidanich.vscode-dotnet\"    ]    ```     Replace `davidanich.vscode-dotnet` with the ID of the extension you want to install. You can find the ID of an extension by searching for it in the Visual Studio Code Marketplace.  3. Save the changes to the `devcontainer.json` file.  4. Run the following command in your terminal to create a new codespace:     ```sh    git codespace new    ```     This will create a new codespace with the specified extension installed.  Note: If you want to install multiple extensions, you can add their IDs to the `extensions` array.  If the context information doesn't provide the ID of the extension you want to install, then you'll need to provide that information yourself. In that case, you can follow the steps above, but replace `davidanich.vscode-dotnet` with the ID of the extension you want to install.", "fb4648e6-a8ef-45a0-8c8c-bf9f1ab5a431": "To permanently remove files from a repository's history on both the local computer and GitHub, you can use either the BFG Repo-Cleaner or the `git filter-repo` command. Here's how to remove a file that was added in an earlier commit using either method:  Using BFG Repo-Cleaner:  1. Download the BFG Repo-Cleaner from the official website (https://rtyley.github.io/bfg-repo-cleaner/) and extract the contents to a directory on your local computer.  2. Open a terminal or command prompt and navigate to the directory where you extracted the BFG Repo-Cleaner.  3. Run the following command to remove the file from the repository's history:     ```shell    $ java -jar bfg.jar --delete-files GIANT_FILE repo.git    # Remove the specified file from the repository's history    ```  4. Replace `repo.git` with the name of your repository and `GIANT_FILE` with the name of the file you want to remove.  5. After the command completes, run the following command to repack the repository:     ```shell    $ git gc --aggressive --prune=now    # Repack the repository to remove the deleted files    ```  6. Push the changes to GitHub:     ```shell    $ git push --force    # Push the changes to GitHub    ```  Using `git filter-repo`:  1. Install `git filter-repo` by running the following command:     ```shell    $ brew install git-filter-repo    # Install `git filter-repo` on macOS using Homebrew    ```  2. Open a terminal or command prompt and navigate to the directory where your repository is located.  3. Run the following command to remove the file from the repository's history:     ```shell    $ git filter-repo --force --index-filter 'path:GIANT_FILE $ prune' --tree-filter 'path:GIANT_FILE $ prune' HEAD    # Remove the specified file from the repository's history", "129b4734-6043-4cde-a387-df7137233123": "Adding policies in codespaces makes the choice of image more restrictive by defining a set of allowed images. When setting organization-wide policies, it's recommended to set the largest choice of images that will be available for any repository in the organization, and then add repository-specific policies to further restrict the choice. This is because if an organization-wide policy is set, any policies set for individual repositories must fall within the restriction set at the organization level. Specifying additional images for a repository will have no effect if they are not specified in the organization-level policy. Therefore, it's essential to consider the images that will be required for all repositories in the organization while setting organization-wide policies.", "0db80d36-eacd-441e-804d-a6eebb1e6531": "To search the security log for specific events, you can use the search function provided in the \"Security log\" section of your account settings. You can search based on the action performed by entering keywords related to the event you're looking for. The events listed in the security log are triggered by various actions, and they are grouped into different categories. Some of the categories included in this log are \"billing,\" which contains all activities related to your billing information, \"codespaces,\" which contains all activities related to GitHub Codespaces, \"marketplace_agreement_signature,\" which contains all activities related to signing the GitHub Marketplace Developer Agreement, \"marketplace_listing,\" which contains all activities related to listing apps in the GitHub Marketplace, \"oauth_access,\" which contains all activities related to OAuth access tokens, \"oauth_authorization,\" which contains all activities related to authorizing GitHub OAuth apps, \"passkey,\" which contains activities related to passkeys, \"payment_method,\" which contains all activities related to paying for your GitHub subscription, and \"personal_access_token,\" which contains activities related to GitHub personal access tokens (PATs). Note that some of these categories may not be available depending on the GitHub plan you're using.", "ffd58f58-e59c-4025-a7a8-4d341a962a03": "To update your local repository to match the changes made in the remote repository without affecting your current work, you can use the `git fetch` command. This command safely copies information from the remote repository to your local clone without making any changes to the files you have checked out. You can use `git fetch upstream` to get information from a repository you've forked, or `git fetch origin` to get information from a repository you've only cloned. This ensures that your local repository is up-to-date with the latest changes made in the remote repository without affecting your current work.", "a996c566-26a7-4a43-966c-8178d9c13543": "To remediate previously committed secrets using GitHub Advanced Security, an organization should follow these steps:  1. Define a process for monitoring, notifying, and remediating newly published secrets. This process should include educating developers on securely managing secrets and creating an escalation process for repeated offenses.  2. Identify the most critical secret types for the organization and define a process for remediating each type. This process should be documented and given to the team maintaining the repository.  3. Implement the process for remediating the most critical secrets. This may involve revoking access to the secret, updating configurations, or migrating to a new solution.  4. Repeat the process for any remaining secret types until all previously committed secrets have been remediated.  5. Continuously monitor for new secrets and repeat the process for newly committed secrets.  6. Consider implementing auto-remediation for certain types of secrets using tools like the GitHub Secret Scanner Auto Remediator.  7. Regularly review and update the process for monitoring, notifying, and remediating secrets to ensure it remains effective.  8. Educate developers on securely managing secrets and encourage them to take responsibility for managing secrets used in their code securely.  9. Measure the reduction in newly committed secrets and adjust the process as necessary to further reduce the number of secrets committed.  10. Regularly review and update the list of most critical secret types as the organization's processes and integrations change.", "0dfa31d6-5759-4452-9566-b6d32ea8b45a": "To view the discussions you have created or participated in on {% data variables.product.product_name %}, follow these steps:  1. In the top-left corner of {% data variables.location.product_location %}, select {% octicon \"three-bars\" aria-label=\"Open global navigation menu\" %}, then click {% octicon \"comment-discussion\" aria-hidden=\"true\" %} **Discussions**.     !Screenshot of the navigation bar on {% data variables.product.product_name %}. The \"Open global navigation menu\" icon is outlined in dark orange.  2. Toggle between **Created** and **Commented** to see the discussions you've created or participated in.     !Screenshot of the \"Discussions\" page on {% data variables.product.product_name %}. The \"Created\" and \"Commented\" tabs are outlined in dark orange.  Alternatively, if you're already signed in to {% data variables.product.product_name %}, you can also click your profile photo in the top-right corner of the page and select **Your discussions** to view your discussions.     !Screenshot of the account dropdown on {% data variables.product.product_name %}. The \"Your discussions\" option is outlined in dark orange.  I hope this helps! Let me know if you have any further questions.    Further reading:  - \"AUTOTITLE\" - \"AUTOTITLE\" - \"AUTOTITLE\"  These resources provide more information on using discussions on {% data variables.product.product_name %}. I recommend checking them out if you want to learn more about this feature.    I hope this helps! Let me know if you have any further questions.    Further reading:  - \"AUTOTITLE\" - \"AUTOTITLE\" - \"AUTOTITLE\"  These resources provide more information on using discussions on {% data variables.product.product_name %}. I recommend checking them out if you want to learn more about this feature.    I hope this helps! Let me know if you have any further questions.", "399e4f8f-07e4-43d5-aaad-8f9209596bf2": "To utilize Azure Virtual Network (VNET) for private networking with {% data variables.product.prodname_dotcom %}-hosted runners, you can follow the steps outlined in the \"Private networking with Azure Virtual Network (VNET)\" documentation provided by GitHub. This feature is currently in beta and requires an Azure subscription. With Azure Virtual Network (VNET), you can create a network overlay that allows your {% data variables.product.prodname_dotcom %}-hosted runners to securely access resources in your Azure environment without exposing them to the public internet. This can improve security and performance by reducing the need for public IP addresses and minimizing network latency.  [Optional] 4. When comparing and contrasting the implementation process and benefits of using an API Gateway with OIDC versus using WireGuard to create a network overlay for private networking with {% data variables.product.prodname_dotcom %}-hosted runners, there are a few key differences to consider.  API Gateway with OIDC: - Implementation process: To use API Gateway with OIDC, you'll need to create an API Gateway in your cloud environment (such as AWS or Azure) and configure it to authenticate and authorize requests from your {% data variables.product.prodname_dotcom %}-hosted runners. You'll also need to create an OIDC provider (such as Okta or Auth0) and configure it to issue tokens that can be used to authenticate requests. - Benefits: API Gateway with OIDC provides a centralized authentication and authorization mechanism that can be used to secure access to multiple services and resources. It also supports a wide range of cloud environments and programming languages, making it a flexible option for many organizations.  WireGuard: - Implementation process: To use WireGuard for private networking, you'll need to configure your {% data variables.product.prodname_dotcom %}-hosted runners to connect to a WireGuard server (such as a VPN appliance or a cloud-based service) using a WireGuard configuration file. You'll also need to configure your server to allow connections from your runners and to route traffic to the resources you want to access. - Benefits: WireGuard provides a", "ff532dfd-79c5-457e-817d-bb6f66cabbd1": "The value assigned to the variable `BITBUCKET_PR_ID` in the provided context information is `{% raw %}${{ github.event.pull_request.number }}`{% endraw %}. This variable will contain the ID number of the pull request that triggered the build in Bitbucket.", "89e1d058-05eb-4602-bfc6-4aeb9b867e25": "Organization administrators can view {% ifversion actions-cache-admin-ui %}and manage {% endif %}{% data variables.product.prodname_actions %} cache storage for all repositories in the organization by following these steps:  1. Access the organization settings by clicking on the organization name in the top left corner of the GitHub interface, then clicking on the Settings tab. 2. In the left sidebar, click {% octicon \"play\" aria-hidden=\"true\" %} **Actions**, then click **Caches**. 3. Review the list of repositories for information about their {% data variables.product.prodname_actions %} caches. You can click on a repository name to see more detail about the repository's caches.  If the context mentions the availability of the Actions cache admin UI, then organization administrators can also configure the size limit for {% data variables.product.prodname_actions %} caches that will apply to each repository in the organization by following these steps:  1. Access the organization settings by clicking on the organization name in the top left corner of the GitHub interface, then clicking on the Settings tab. 2. In the left sidebar, click {% octicon \"play\" aria-hidden=\"true\" %} **Actions**, then click **Caches**. 3. Click the **Configure cache defaults** button. 4. Enter the desired cache size limit in megabytes (MB). 5. Click the **Save** button to apply the settings.  Note that the cache size limit for an organization cannot exceed the cache size limit set in the enterprise policy. Repository admins will be able to set a smaller limit in their repositories.", "e27a43f0-db79-4a89-851b-3e7f781791ea": "To determine the repository that converted reusable workflows are migrated to using the YAML file provided to the `--config-file-path` argument with {% data variables.product.prodname_actions_importer %} during a `migrate` command, you can refer to the `target_url` property in the `reusable_workflows` section of the YAML file. This property specifies the URL of the repository where the converted reusable workflow or composite action should be added. For example, if the YAML file contains the following `reusable_workflows` section:  ```yaml reusable_workflows:   - name: my-reusable-workflow.yml     target_url: https://github.com/my-org/my-repo     ref: main ```  Then, during the `migrate` command, {% data variables.product.prodname_actions_importer %} will add the converted reusable workflow or composite action to the repository specified by the `target_url` property.", "d15a40cc-203c-4012-b949-c92d2b4813dc": "To ensure that a self-hosted runner does not automatically perform software updates and instead updates the runner version on your own schedule, you can specify the `--disableupdate` flag when registering your runner using `config.sh`. This will turn off automatic software updates and allow you to update the runner version on the container image directly on your own schedule. However, it's important to note that you must still update your runner version regularly to ensure compatibility with new functionality in {% data variables.product.prodname_actions %}. If you disable automatic updates, you will be required to update your runner version within 30 days of a new version being made available.", "480025f2-977d-4854-9678-79c6345efc0d": "In order to answer the question about the specific research software mentioned in the context information, we would need more details such as the title of the research or the name of the study. Without this information, it is not possible to provide a specific answer. Please provide additional context or clarify your question for further assistance.", "21ca7912-8080-4c1a-a467-acbd02dfc456": "According to the context provided, an app can revoke installation access tokens and user access tokens using the respective REST API endpoints \"GET /installations/{installation_id}/access_tokens\" and \"GET /user/access_tokens\". This feature allows the app to revoke tokens that are no longer needed, which provides several benefits in terms of security and resource management.  Firstly, revoking tokens that are no longer needed helps to mitigate security risks by preventing unauthorized access to sensitive data. If a user's access token is compromised, the app can revoke it to prevent further unauthorized access. Similarly, if an installation's access token is compromised, the app can revoke it to prevent unauthorized access to the entire organization's data.  Secondly, revoking tokens that are no longer needed helps to manage resources more efficiently. By revoking unused tokens, the app can reduce the number of tokens that need to be managed and cached, which can improve performance and reduce the risk of token-related errors. Additionally, by revoking tokens that are no longer needed, the app can reduce the number of tokens that need to be rotated, which can further improve security and resource management.  In summary, revoking tokens that are no longer needed is an important feature for managing security and resource management in an app. By providing REST API endpoints for revoking installation access tokens and user access tokens, {% data variables.product.company_short %} is providing developers with the tools they need to manage tokens more efficiently and securely.", "af25ab73-3014-48f9-a900-86eaa1832767": "After configuring high availability replication for nodes in a cluster, the following steps are required to verify that cluster replication is working correctly:  1. Run the command `ghe-cluster-repl-status` to check the status of cluster replication.  2. Verify that the output of the command shows that replication is correctly set up and working for all configured replica nodes.  3. If any issues are found during the verification process, troubleshoot and resolve them before proceeding with failover testing.  4. Regularly monitor the status of cluster replication to ensure that it remains healthy and functioning correctly.  The command `ghe-cluster-repl-status` should be used to check the status of cluster replication.", "866cac3d-7904-41bf-ab2a-332819c2037f": "If a pull request has merge conflicts or if the head branch is in conflict with the base branch during the merging process, you'll need to resolve these conflicts before you can merge the pull request. To do this, you can check out the pull request locally and merge it using the command line. Here's how:  1. Go to the repository in GitHub and click the pull request you'd like to merge. 2. Click the green \"Code\" button to open the pull request in your default text editor. 3. In your text editor, you'll see that some files have conflicts. These files will have a `<<<<<<< HEAD`, `=======`, and `>>>` marker in the conflicted sections. 4. Resolve the conflicts by choosing the correct version of the code for each conflicted section. 5. Save and close the files. 6. Add and commit the changes:     ```    git add .    git commit -m \"Resolved merge conflicts\"    ```  7. Push the changes to the remote repository:     ```    git push origin HEAD:<branch-name>    ```  8. Go back to GitHub and refresh the pull request. The conflicts should now be resolved, and you can merge the pull request as usual.  Note: If you're using GitHub Desktop, you can also resolve conflicts and merge the pull request directly in the app. For more information, see \"AUTOTITLE.\"", "0e3f1964-2aeb-41a5-83fb-accca9421235": "To sign commits using GPG and have them verified on GitHub, follow these steps:  1. Check for existing GPG keys 2. Generate a new GPG key 3. Add a GPG key to your GitHub account 4. Tell Git about your signing key 5. Sign commits 6. Sign tags  Note: These steps are provided in the context information given. If you're not sure how to perform any of these steps, you may need to consult additional resources or seek assistance from a technical expert.", "05c32dc1-438c-43a6-ae15-b89d275fbc54": "Based on the context information provided, it is not clear what type of scheduled reminders can be set up using the instructions given. Without further information, it is unclear what specific tasks or events the reminders are for. Therefore, an example of a scheduled reminder cannot be provided.", "51f5a031-1c7e-407f-83c4-8a691e3cd10e": "To contribute to interesting projects and repositories on GitHub by creating a copy of the repository, follow these steps:  1. Go to the GitHub website and log in to your account. 2. Search for the repository you want to contribute to using the search bar at the top of the page. 3. Once you find the repository, click on it to open it. 4. Click on the \"Fork\" button located on the right-hand side of the page, just above the repository's name. 5. This will create a copy of the repository in your GitHub account. 6. After the fork is created, you will be redirected to the newly created repository. 7. To make changes to the repository, clone it to your local machine using the command `git clone https://github.com/your-username/repository-name.git`. 8. Make the necessary changes to the code and commit them using the command `git add .` followed by `git commit -m \"commit message\"`. 9. Push the changes to the remote repository using the command `git push origin main`. 10. Go back to the GitHub website and create a pull request by clicking on the \"Pull requests\" tab and then clicking on the \"New pull request\" button. 11. Select the branch you pushed your changes to and the branch you want to merge your changes into. 12. Write a clear and concise description of your changes and click on the \"Create pull request\" button. 13. The project maintainers will review your pull request and either merge it into the main branch or provide feedback on how to improve it.  By following these steps, you can contribute to interesting projects and repositories on GitHub by creating a copy of the repository and making changes to it.", "53383156-5f99-4460-8876-d4dc32582b49": "When adding a date to a project in GitHub, you should select the \"Date\" field type.", "e04249f4-3a79-4501-95d0-7102dbfe2ad7": "{% data variables.product.prodname_dotcom_the_website %} automatically creates a username for each person in an enterprise setting by normalizing an identifier provided by the IdP. This means that the unique parts of the identifier provided by the IdP are removed during normalization. If a conflict occurs during provisioning due to the same username being created for multiple people, the username provided by the IdP should be modified to avoid the conflict.", "21b4eb1f-bf56-4abd-86cc-5248f82f418d": "To minimize storage charges for your image cache in Docker, you can consider the following options:  1. Use a smaller base image: Choose a base image that is as small as possible to reduce the size of your final image. This can significantly reduce the amount of storage required for your image cache.  2. Remove unnecessary files: Remove any unnecessary files from your image to reduce its size. This can be done by using a tool like `tree-sitter` to analyze your codebase and remove unused dependencies.  3. Use multi-stage builds: Multi-stage builds allow you to create a smaller final image by using a smaller intermediate image during the build process. This can significantly reduce the size of your final image and, in turn, the amount of storage required for your image cache.  4. Use a registry with storage optimization: Some Docker registries offer storage optimization features that can significantly reduce the amount of storage required for your image cache. For example, Docker Hub offers a feature called \"Docker Hub Storage Optimization\" that compresses images and removes duplicate layers to reduce storage requirements.  5. Use a registry with tiered storage: Some Docker registries offer tiered storage plans that charge less for frequently accessed images and more for less frequently accessed images. This can help you save money on storage charges by ensuring that your most frequently accessed images are stored in a lower-cost tier.  6. Use a registry with image pruning: Some Docker registries offer image pruning features that automatically remove unused images from your registry, freeing up storage space and reducing storage charges.  7. Use a registry with image versioning: Some Docker registries offer image versioning features that allow you to keep multiple versions of your images in your registry, without having to store multiple copies of the same image. This can help you save storage space and reduce storage charges by ensuring that you only store the versions of your images that you actually need.  By implementing these strategies, you can significantly reduce the amount of storage required for your image cache and, in turn, minimize storage charges for your Docker infrastructure.", "9514f4df-6388-4f46-945c-671dd65434fa": "According to the context information provided, the attribution process differs between `ghe-migrator`, `gl-exporter`, or `bbs-exporter` on one hand, and {% data variables.product.prodname_importer_proper_name %} on the other hand. With the former tools, the attribution process involves deciding how to attribute data ahead of time and including a mapping file when importing data. In contrast, with {% data variables.product.prodname_importer_proper_name %}, data is initially linked to placeholder identities called \"mannequins\", and real users can be assigned this history after migration.", "e926feff-6413-4321-bcb4-201b2b450c76": "On Windows, running `ssh-add -t` is used to configure the SSH agent to forget the private key after a certain time. This means that you will be prompted for your passphrase again after the specified time. On a Mac OS X Leopard through OS X El Capitan, if you use the default private key files (_.ssh/id_rsa_ or _.ssh/identity_), you will be prompted to enter your passphrase the first time you use the key. If you choose to save the passphrase with your keychain, you won't have to enter it again. If you want to store your passphrase in the keychain when adding your key to the ssh-agent, you can do so, but this is not explicitly mentioned in the context information provided. Therefore, it's unclear whether this is possible on a Mac OS X Leopard through OS X El Capitan without further information.", "c5769cbc-a997-4efa-8739-b7221c6437d8": "Based on the provided context information, it is unclear what document is being referred to. Therefore, it is not possible to summarize the main topic discussed in the document. Please provide more context or specify which document is being referred to.", "29d06ddb-9974-4e9c-92a9-ae7e240bbcc2": "The current number of user accounts in the GitHub Enterprise instance can be found in the context information using the variable `AN` which represents `ghe_stats.users.total_users`. The value of this variable should be retrieved from the GitHub Enterprise instance's statistics API to obtain the current number of user accounts.", "1ecad351-9632-4e11-a5bc-f2ecb0d8103c": "To filter the list of repositories in the security overview to display only those that have no detected level of risk, you can use the `risk:clear` qualifier. This qualifier is available in the main summary views, as well as in the enterprise-level \"Overview\" and organization-level \"Security risk\" view, if the organization has enabled the security-overview-org-risk-coverage feature. Simply add `risk:clear` to your filter criteria to display only the repositories that have no detected level of risk.", "90b0350b-dea2-44cc-b467-6e89314bf5ed": "The `JSON.parse` method in the provided Ruby code snippet converts the request body, which is in JSON format, into a Ruby hash or array. This is achieved by passing the result of `read` method of the request body object to the `JSON.parse` method. The resulting hash or array can then be accessed and manipulated in the Ruby code. In this specific code snippet, the `data` variable is assigned the parsed JSON object, which is then used to extract the event type, action, and issue details.", "65e5b69c-3391-4f31-939a-9a7b8c2ae6c3": "To add features in {% data variables.product.prodname_vscode_shortname %} while working locally, without being connected to a codespace, you must have the \"Dev Containers\" extension installed and enabled.", "20742092-b0de-4002-814a-de481cf91f47": "Labels can be used to route workflow jobs to specific types of self-hosted runners based on their shared characteristics by assigning custom or default labels to runners based on their characteristics. For example, a label could be assigned to a runner with a specific hardware component, such as \"gpu\" or \"arm\", to ensure that jobs requiring those components are executed on that runner.  Runner groups for self-hosted runners defined at the organization or enterprise level allow for the grouping of runners with shared characteristics, such as a specific operating system or software package, into a single runner group. This can simplify the configuration of workflows by allowing jobs to be targeted to the runner group instead of individual runners. For example, a runner group could be created for runners with Ubuntu 18.04 installed, and jobs requiring that operating system could be configured to run on that runner group.  Overall, labels and runner groups provide a flexible and scalable way to manage self-hosted runners and ensure that workflows are executed efficiently and effectively.", "2e352ff0-14c9-4523-85e7-a1345b734d9d": "To enable log forwarding on your {% data variables.enterprise.management_console %}, follow these steps:  1. Go to the {% data variables.enterprise.management_console %} settings page and click on \"Monitoring\" in the left sidebar. 2. Select \"Enable log forwarding\". 3. Enter the address of the server you want to forward logs to in the \"Server address\" field. You can enter multiple addresses separated by commas. 4. Select the protocol you want to use to communicate with the log server from the \"Protocol\" dropdown menu. 5. If you want to encrypt communication between syslog endpoints, select \"Enable TLS\". We recommend enabling TLS according to your local security policies, especially if there are untrusted networks between the appliance and any remote log servers. 6. To encrypt communication between syslog endpoints, click \"Choose File\" and select a CA certificate for the remote syslog server. You should upload a CA bundle containing a concatenation of the certificates of the CAs involved in signing the certificate of the remote log server. The entire certificate chain will be validated, and must terminate in a root certificate.  Note: These steps are for GitHub Enterprise Server (GHES). If you're using GitHub Enterprise Cloud (GHEC), follow the steps provided in the context information.", "dc7bd942-33a9-4940-84aa-74ff69e57b08": "As an organization owner on GitHub, to set an end date for an existing sponsorship, you need to follow these steps:  1. Go to your organization's page on GitHub. 2. Click on \"Settings\" in the organization's menu. 3. Select \"Sponsoring\" from the left-hand menu. 4. Find the sponsorship you want to set an end date for and click \"Manage\" next to it. 5. Click \"Manage sponsorship\" from the drop-down menu. 6. Scroll down to the \"End date\" section and select the date you want the sponsorship to end. 7. Click \"Update sponsorship\" to save the changes.  Note: These steps assume that you are already signed in to your GitHub account as an organization owner. If you are not signed in, you will be prompted to do so before proceeding.", "3f116e17-f4a0-41a4-93b2-ba1016a69de1": "To configure Node.JS to use the certificate for a self-hosted runner application, you need to set the `NODE_EXTRA_CA_CERTS` environment variable. This can be done by either setting it as a system environment variable or declaring it in a file called `.env` in the self-hosted runner application directory. The value of the variable should be the path to the certificate file. This variable should be set before configuring or starting the self-hosted runner application, and if the certificate configuration changes, the self-hosted runner application should be restarted.", "e67c0b0b-2036-42f9-8a94-0190c44b9b1e": "To remediate newly committed secrets when enabling {% data variables.product.prodname_secret_scanning %}, you should follow a three-step process: notify, follow up, and educate. Firstly, use webhooks to ensure that any new secret alerts are seen by the right teams as quickly as possible. Then, create a high-level remediation process that works for all secret types, which could involve contacting the developer who committed the secret and their technical lead on that project, highlighting the dangers of committing secrets to GitHub, and asking them to revoke and update the detected secret. Finally, create an internal training document assigned to the developer who committed the secret, explaining the risks created by committing secrets and directing them to resources that can help them avoid making similar mistakes in the future. Automation can also be incorporated into the webhook process to make the follow-up step more efficient for larger organizations with many repositories.", "b91953e9-04eb-4e5f-9eab-44939130de2b": "The duration of temporary interaction limits for all public repositories owned by a personal account when interaction limits are enabled for that account is {% data reusables.community.interaction-limits-duration %}.", "bde09723-0ecc-48ee-97a3-2f654ccc8593": "The difference between committing dependencies to the main branch and committing them only to the tagged release commits in an automated release management strategy is that in the former approach, dependencies are updated and committed to the main branch regularly, while in the latter approach, dependencies are updated and committed only to the tagged release commits during the release process. The latter approach is preferred for ensuring security of third party pull requests because it helps in avoiding any potential security vulnerabilities that may arise due to outdated dependencies in the main branch. By committing dependencies only to the tagged release commits, the user can ensure that the dependencies used in the release are tested and verified, reducing the risk of any security issues. Additionally, it encourages users to reference named tags or `sha`s, making it easier to track and manage dependencies across different releases.", "7d8963f4-0db8-4a16-8e27-4a82d6fc63e4": "Without further context, it is unclear what \"AUTOTITLE\" refers to. If \"AUTOTITLE\" is referring to a specific document feature, it could potentially differ from other document features in terms of functionality based on the specific software or document type being used. More information is needed to accurately answer this question.", "71c93889-aac2-485f-9df7-fa381d6ad918": "According to section 4 of the GitHub Terms of Service, if a copyright owner believes that content on the GitHub platform violates their copyright, they can report the infringement to GitHub in accordance with GitHub's Digital Millennium Copyright Act Policy. This can be done by filling out the DMCA form on GitHub's website or by emailing copyright@github.com.", "c9f0d5d2-e613-4b9d-ae9a-cbc069b8a0e2": "To quickly create a {% data variables.product.prodname_github_app %} that adds a comment to a pull request when it is opened in a specific repository, you can follow these steps:  1. Clone the github/github-app-js-sample repository using the command `git clone https://github.com/github/github-app-js-sample.git` or by using {% data variables.product.prodname_github_codespaces %}. 2. Navigate to the directory where your clone is stored in a terminal window. 3. Run `npm install` to install the dependencies. 4. Get a webhook proxy URL by following these steps:    a. In your browser, navigate to https://smee.io/.    b. Click **Start a new channel**.    c. Copy the full URL under \"Webhook Proxy URL\". 5. Register a {% data variables.product.prodname_github_app %} by following these steps:    a. Go to your {% data variables.product.company_short %} account settings.    b. Click **Developer settings**.    c. Click **GitHub Apps**.    d. Click **New GitHub App**. 6. Fill in the required information for your app, including:    a. App name: Enter a name for your app.    b. Homepage URL: Enter the URL for your app's homepage.    c. Webhook secret: Leave this blank for now.    d. Subscription events: Select \"Pull requests\" and \"Pull request review comments\".    e. Webhook URL: Paste the webhook proxy URL you copied from Smee.io.    f. Permissions: Select the repository where you want your app to add comments to pull requests.    g. Click **Register app**. 7. In your terminal window, run `node index.js` to start your app. 8. Open a pull request in the repository you registered your app for. Your app should add a comment to the pull request.  Note: If you want to learn how to write the code yourself, see \"AUTOTITLE.\"", "67dec1de-d868-44a4-9b87-93deac6bcb92": "Automatic license synchronization is a feature that allows the automatic synchronization of license usage between {% data variables.product.prodname_ghe_server %} and {% data variables.product.prodname_ghe_cloud %} on a weekly basis. This is achieved through the use of {% data variables.product.prodname_github_connect %}, which enables communication between the two platforms. If multiple {% data variables.product.prodname_ghe_server %} instances are being used, automatic license synchronization can also be enabled between each instance and the same enterprise account on {% data variables.product.prodname_ghe_cloud %}. This ensures that license usage is accurately tracked and managed across all platforms. Manual license synchronization is also possible outside of the weekly automatic sync, as well as the ability to manually upload {% data variables.product.prodname_ghe_server %} user license information to {% data variables.product.prodname_ghe_cloud %}. Verified domains are also required for license synchronization to work.", "6245bc53-f4b7-4a1e-a25b-74fefb8d7904": "To obtain a comma-separated values (CSV) file containing details about the committers for a particular repository on {% data variables.product.prodname_dotcom %}, follow these steps:  1. Go to the repository for which you want to download the CSV file. 2. Click on the \"Settings\" tab in the repository navigation menu. 3. Click on \"Analytics\" in the left-hand menu. 4. Click on \"Commits\" in the \"Analytics\" section. 5. Click on the \"Committers\" tab. 6. Click on the \"Download CSV report\" button in the upper-right corner of the page. 7. Save the CSV file to your local machine.  Note: The \"Download CSV report\" button may be hidden under the horizontal kebab icon (three horizontal dots) in the upper-right corner of the page.    Inclusive language:  - Use \"you\" instead of \"user\" or \"customer.\" - Use \"your\" instead of \"your\" or \"your\" company's.\" - Use \"they\" instead of \"he\" or \"she.\" - Use \"their\" instead of \"his\" or \"her.\" - Use \"they\" instead of \"his\" or \"her\" in plural contexts. - Use \"people\" instead of \"manpower\" or \"personnel.\" - Use \"team\" instead of \"guys\" or \"gals.\" - Use \"everyone\" instead of \"all men\" or \"all women.\" - Use \"human\" instead of \"mankind.\" - Use \"chair\" instead of \"chairman\" or \"chairwoman.\" - Use \"spokesperson\" instead of \"spokesman\" or \"spokeswoman.\" - Use \"host\" instead of \"hostess.\" - Use \"police officer\" instead of \"policeman\" or \"policewoman.\" - Use \"firefighter\" instead of \"fireman\" or \"firewoman.\" - Use \"parent\" instead of \"mother\" or \"father.\" - Use \"chair\" instead of \"chairwoman\" or \"chairman.\" - Use \"spokesperson\" instead of \"spokesman\" or \"spokeswoman", "158e63cf-2486-43b8-a1ac-2db6f58549e3": "The clause regarding independent development in this agreement is significant because it ensures that both parties have the freedom to develop, acquire, license, market, promote, or distribute products, software, or technologies that may compete with the other party's products without any restrictions or obligations. This clause protects the interests of both parties and prevents any potential conflicts or disputes arising from competition in the same market.", "fcfab139-45f7-4531-a12f-1111bff10361": "To set environment variables for the GitHub CLI to authenticate to a Bitbucket Server instance and a Windows machine, replace TOKEN with your {% data variables.product.pat_generic %}, USERNAME with the username of a Bitbucket Server account with admin or super admin permissions, PASSWORD with the password for the Bitbucket Server account, and SMB_PASSWORD with the password for the Windows machine in the following commands:  - If using Terminal:    ```shell copy   export GH_PAT=\"TOKEN\"   export BBS_USERNAME=\"USERNAME\"   export BBS_PASSWORD=\"PASSWORD\"   export SMB_PASSWORD=\"PASSWORD\"   ```  - If using PowerShell:    ```shell copy   $env:GH_PAT=\"TOKEN\"   $env:BBS_USERNAME=\"USERNAME\"   $env:BBS_PASSWORD=\"PASSWORD\"   $env:SMB_PASSWORD=\"PASSWORD\"   ```  Note that these commands set the environment variables for the current shell session only. To make these changes permanent, you can add these commands to your shell configuration file (e.g., `.bash_profile` or `.bashrc` for Bash, or `.bash_login` for Bourne shell).", "d6420cdd-ab30-49d6-b8f7-461324de3c06": "To delete an entire repository-scoped package on GitHub.com, you must have admin permissions to the repository that owns the package. This action cannot be performed by regular contributors or collaborators.  To delete the package, follow these steps:  1. Go to the repository that owns the package. 2. Click on the package's name in the left-hand menu to open the package's settings page. 3. Click on the \"Settings\" tab. 4. Click on the \"Delete this package\" button. 5. In the confirmation box, type the name of the package to confirm you want to delete it. 6. Click \"I understand the consequences, delete this package\".  Note that deleting a repository-scoped package will also delete all its versions and tags. This action cannot be undone.    Deleting an entire organization-scoped package on {% data variables.product.prodname_dotcom %}  To delete an entire organization-scoped package on {% data variables.product.prodname_dotcom %}, you must have admin permissions to the organization that owns the package. For more information, see \"Required permissions.\"  {% data reusables.organizations.navigate-to-org %} {% data reusables.packag  Given the context information and not prior knowledge, answer the following question.   How can I delete an entire organization-scoped package on GitHub.com, and what permissions are required for this action  If the context doesn't give enough information to give an answer,  then say that you weren't given enough information to give an answer.  Answer:  To delete an entire organization-scoped package on GitHub.com, you must have admin permissions to the organization that owns the package. This action cannot be performed by regular members or collaborators.  To delete the package, follow these steps:  1. Go to the organization that owns the package. 2. Click on the package's name in the left-hand menu to open the package's settings page. 3. Click on the \"Settings\" tab. 4. Click on the \"Delete this package\" button. 5. In the confirmation box, type the name of the package to confirm you", "1a6a008d-6c0c-42e0-9c79-3681b2b21645": "To search for a specific repository by name using GitHub's search functionality, you can use the `repo:OWNER/REPOSITORY_NAME` qualifier. For example, to search for a repository named \"hello-world\" owned by \"octocat\", you can use the following search query: `repo:octocat/hello-world`.", "0ea2856e-a7da-42d8-a804-7ce64c0f75d9": "GitHub Projects supports feature tracking from start to finish by allowing users to label issues as features, view all labeled issues in a separate table layout, and create additional project boards to track specific tasks and items related to the feature. The use of labels in the \"Area\" field also allows for grouping tasks by specific areas of focus. This feature planning process is demonstrated in the provided transcript and visual aids.", "0f058788-1167-4bfd-b0d2-663082b3e230": "To confirm whether the current \"Actions\" service status is causing the server error during the execution of a workflow for {% data variables.product.prodname_code_scanning %}, you can check the current \"Actions\" service status on the Status Dashboard. This will provide information on any known issues or outages affecting the \"Actions\" service, which could be causing the server error. If there are no reported issues, it may be a transient communication issue, in which case you can try running the workflow again to see if the error persists. If the problem persists, you should contact {% data variables.contact.contact_support %} for further assistance.", "14991967-0fc1-41d6-92aa-de0a750ba76d": "In the Docker ecosystem, {% data variables.product.prodname_dependabot %} can be configured to only access private registries using either of the following configuration methods:  1. Define the private registry configuration in a `dependabot.yml` file without `replaces-base`. This method requires defining the private registry URL and credentials in the `dependabot.yml` file. In the `Dockerfile` file, add the image name in the format of `IMAGENAME:TAG`.  2. Set `replaces-base` as `true` in the `dependabot.yml` file. This method requires defining the private registry URL in the `Gemfile` file and checking it into the repository. In the `dependabot.yml` file, set `replaces-base: true`.  If the private registry URL is not included in the `Gemfile` file, then the first configuration method should be used.", "73fdd21e-e3be-4bcd-ba84-b7d469e113c6": "According to the context information provided, a repository or organization administrator can create and remove pre-receive hooks for their organization or repository. The context also mentions that the site administrator can allow organization or repository administrators to enable or disable pre-receive hooks. Therefore, it's possible that organization or repository administrators may have permission to enable or disable pre-receive hooks, but this would depend on the specific permissions granted by the site administrator. If the context didn't provide this information, it's unclear whether organization or repository administrators have the ability to enable or disable pre-receive hooks. In that case, you would need additional context to determine whether this is possible.", "8a005d7e-f84a-4a76-baaf-0c87f250a608": "To check the status of your migration, you can use the `getRepositoryMigrationStatus` query in the {% data variables.product.prodname_ghe_cloud %} GraphQL API. This query returns the status of a specific repository migration, including information about the source repository, the migration progress, and any errors that occurred during the migration.  Here's an example query:  ```graphql query getRepositoryMigrationStatus($migrationId: ID!) {   repositoryMigration(id: $migrationId) {     id     migrationSource {       id       name       type     }     sourceUrl     status     error {       message     }     progress {       total       completed     }   } } ```  Replace `$migrationId` with the ID of the repository migration you want to check. The `status` field will return one of the following values: `PENDING`, `IN_PROGRESS`, `COMPLETED`, or `FAILED`. If the migration failed, the `error` field will contain a message describing the error. The `progress` field shows the current progress of the migration, including the total number of items being migrated and the number that have been migrated so far.  To validate your migration and check the error log, you can use the {% data variables.product.prodname_gei_cli %} to view the migration logs. Here's an example command:  ```sh gei logs --migration-id <migration-id> ```  Replace `<migration-id>` with the ID of the repository migration you want to view. This will display the logs for the migration, including any errors that occurred during the migration process.  By using these tools, you can monitor the status of your migration and troubleshoot any issues that arise.", "a2f2b99a-7507-4c0d-854f-ee08c85e20f1": "With {% data variables.product.prodname_emus %}, your enterprise's Identity Provider (IdP) creates and updates user accounts on {% data variables.location.product_location %}. Users must authenticate on your IdP to access your enterprise's resources on {% data variables.location.product_location %}. {% data variables.product.product_name %} maintains a record of the external identity on your IdP that corresponds with the user account. This is achieved through the use of SAML or OIDC for authentication and System for Cross-domain Identity Management (SCIM) for user provisioning. The specific IdPs supported by {% data variables.product.prodname_emus %} are listed in the provided table, which shows that Azure Active Directory, Okta, and PingFederate are all supported.", "39b9cfb6-f9f7-43b2-9d60-56c9584b3702": "Yes, a mutation operation in GraphQL is an operation that modifies data on the server, while a query operation retrieves data from the server. For example, a mutation operation to create a new user might look like this:  ```graphql mutation {   createUser(input: { username: \"johndoe\", email: \"johndoe@example.com\", password: \"password123\" }) {     user {       id       username       email     }   } } ```  In this example, the `createUser` mutation takes an input object with the user's details, and returns the newly created user object. This differs from a query operation, which would only retrieve data without modifying it. For example, a query operation to retrieve a list of all users might look like this:  ```graphql query {   users {     id     username     email   } } ```  In this example, the `users` query returns a list of all users without modifying any data on the server.", "e83a78fe-a446-482b-894d-de642f5e2f96": "To configure a deployment environment using Kubernetes tools in Google Cloud Platform and build a container image, follow these steps:  1. Create a Google Cloud Platform project and enable the Container Registry and Container Engine API.  2. Create a service account and grant it the necessary roles to access the project and perform container-related tasks.  3. Download the JSON keyfile for the service account and store it as a secret named `GKE_SA_KEY`.  4. Store the name of your project as a secret named `GKE_PROJECT`.  5. Create a workflow that builds the container image and pushes it to GCR using the Google Cloud Platform Container Builder.  6. Configure the deployment environment by creating a `kustomization` file and using kustomize to dynamically set fields of the image and pull it into the cluster deployment.  7. Use `kubectl` to create the deployment and expose it as a service.  8. Monitor the deployment and scale it as needed.  Note: The provided context information includes instructions for creating the service account and granting it the necessary roles, as well as an example workflow that demonstrates how to build the container image and push it to GCR. The context also suggests using kustomize to configure the deployment environment, but does not provide detailed instructions for doing so.  If you need more information about how to use kustomize, you can refer to the kustomize documentation.  If you need more information about how to configure the deployment environment using Kubernetes tools in Google Cloud Platform, you can refer to the Google Cloud Platform documentation on Kubernetes Engine.  If you need more information about how to store secrets in GitHub Actions, you can refer to the GitHub documentation on using secrets in workflows.  If you need more information about how to use `kubectl` to create the deployment and expose it as a service, you can refer to the Kubernetes documentation on creating a deployment and exposing a service.  If you need more information about how to monitor the deployment and scale it as needed, you can refer to the Kubernetes documentation on monitoring and scaling deployments.  If you need more information about how to use Google Cloud Platform Container Builder to build the container image, you can refer to the Google Cloud Platform documentation on using", "37c664b4-99e1-42d6-be5c-36963a069168": "To access environment variables within a step in a GitHub Actions workflow, you can use the `env` context provided by GitHub. The syntax for retrieving their values is `{{ env.VARIABLE-NAME }}` within a step's script. The `env` context contains a mapping of variable names to their values, which can be set at the workflow, job, and step levels. The contents of the `env` context can change depending on where it is used in the workflow run. Configuration variables defined at the organization, repository, and environment levels can also be accessed using the `vars` context, which is available at the workflow level. The syntax for retrieving configuration variable values is `{{ steps.my-step.vars.VARIABLE-NAME }}` within a step's script, where `my-step` is the name of the step containing the configuration variable.", "a310c7c2-5182-46b9-b442-1973f332f56b": "LDAP authentication with {% data variables.product.product_name %} allows users to sign in using their credentials from an LDAP directory instead of creating a separate account within {% data variables.product.product_name %}. This integration is supported by several popular LDAP services, including Active Directory, FreeIPA, and OpenLDAP. The benefits of using LDAP authentication instead of built-in authentication include centralized management of user accounts and credentials, as well as the ability to leverage existing directory infrastructure. Additionally, when users sign in for the first time, their profile information is automatically populated from the LDAP directory, simplifying the onboarding process. However, it's important to ensure that the LDAP service supports paged results before configuring LDAP access, as this can improve performance and prevent timeouts.", "9f55d12f-f0ff-49b9-9b48-425111f2c29a": "In the context of Git version control, GitHub associates actions such as commit messages, pull requests, and comments with a user's email address. This is a design feature of the Git version control system, which allows for better tracking and attribution of changes made to a repository. For example, when a user makes a commit, they can include a commit message that is associated with their email address. This allows other users to easily see who made the change and why. Similarly, when a user creates a pull request, their email address is associated with the request, making it easy to track and manage. Overall, this feature helps to improve the transparency and traceability of changes made to a repository, making it easier for collaborators to understand and manage the codebase.", "48064996-9082-4b79-8a8d-a200fe5dcc32": "The `permissions` key in a workflow file can be used to modify permissions for the `GITHUB_TOKEN` in an entire workflow or for individual jobs by configuring the minimum required permissions. This allows you to limit the scope of permissions and prevent unnecessary access. When the `permissions` key is used, all unspecified permissions are set to no access, with the exception of the `metadata` scope, which always gets read access. This feature helps to enhance security by ensuring that tokens have only the permissions they need to perform their intended tasks. For more information, refer to the documentation provided by GitHub.", "824ae85b-1f43-4fca-afcd-422a6c227589": "To search for specific functions or methods in a codebase using the provided context information, you can use the `symbol:` qualifier. For example, to search for the `deleteRows` function in the `Maint` struct in Go, you can use the following query:  ```text symbol:Maint.deleteRows ```  To search for the `deleteRows` method in Rust's `String` type, you can use the following query:  ```text language:rust symbol:/^String::to_.*/ ```  This query uses a regular expression to match any method starting with `to_` in the `String` type.  To filter search results based on repository properties using the `is:` qualifier, you can exclude forked repositories from the search results using the following query:  ```text log4j NOT is:fork ```  This query searches for the `log4j` term and excludes results from forked repositories.", "cb5f17bb-aa66-4a98-8c56-ab89defe66b9": "You should use the tool switcher in your articles when the tasks or concepts change based on what tool someone uses. Before implementing the tool switcher, consider whether the tasks or concepts truly change based on the tool, or if the same conceptual or procedural information will be accurate for multiple tools. If the only way to make information clear and accurate is by distinguishing content by tool, then use the tool switcher. However, if the tasks or concepts are the same regardless of the tool, then do not use the tool switcher, as it can add unnecessary complexity and confusion for readers.", "b029d3cc-fa13-45ac-8687-7b18c8d8126d": "To add a single select field to a project in GitHub, follow these steps:  1. Go to your project in GitHub. 2. Click on the project settings. 3. Click on the \"Custom fields\" tab. 4. Click on the \"Add field\" button. 5. Select \"Single select\" from the options. 6. Type the first option in the \"Options\" field. 7. To add additional options, click on the \"Add option\" button. 8. Click on the \"Save\" button.  Alternatively, you can open the project command palette by pressing the shortcut key and start typing \"Create new field.\"  I hope this helps! Let me know if you have any further questions.", "a763ea40-0ad5-4226-a4fa-489054b88e9c": "The OpenAI GPT-4 large language model plays a crucial role in generating potential fixes for security alerts using {% data variables.product.prodname_codeql %} analysis. It uses data from the codebase, the pull request, and {% data variables.product.prodname_codeql %} analysis to produce suggested fixes in code and explanatory text for those fixes. This contributes significantly to the developer experience by lowering the barrier of entry to fixing security alerts. Instead of starting with a search for information about the vulnerability, the developer starts with a code suggestion that demonstrates a potential solution for their codebase. This saves developers time and effort, as they no longer have to read and understand the alert location and description before fixing the vulnerability. The autofix system using the OpenAI GPT-4 large language model provides developers with targeted recommendations to help them fix security alerts in pull requests, avoiding the introduction of new security vulnerabilities.", "d3056189-faae-46a6-8f00-6f6f05d7dd72": "Based on the context provided, it is unclear what \"AUTOTITLE\" is or what it does. It appears four times, but without further information, it is impossible to summarize or explain its purpose. More context or documentation is needed to provide an accurate answer.", "f0449377-7b5f-427b-b108-a9bb5355187d": "To ensure that your {% data variables.product.prodname_pages %} site is only accessible via HTTPS, you need to follow these steps:  1. Go to your {% data variables.product.prodname_pages %} site's repository on GitHub. 2. Click on the settings icon in the top right corner of the page. 3. Select \"Pages\" from the left-hand menu. 4. Under \"{% data variables.product.prodname_pages %}\", select \"Enforce HTTPS\".  This will ensure that all traffic to your {% data variables.product.prodname_pages %} site is encrypted and secure. Note that this feature is available for all {% data variables.product.prodname_pages %} sites, including those with custom domains.", "12639512-c1a6-4f7c-848c-5140299d5f37": "In GitHub repositories, the custom media types supported for READMEs, files, and symlinks are \"application/vnd.github.raw\" and \"application/vnd.github.html\". The \"application/vnd.github.raw\" media type is used to retrieve the raw format of the file, which is the unrendered and unprocessed content. This is useful for viewing the actual code or text of a file without any formatting or styling applied. The \"application/vnd.github.html\" media type, on the other hand, is used to retrieve the rendered HTML version of a file that has been processed using GitHub's open-source Markup library. This is useful for viewing the formatted and styled content of a file, such as a Markdown or AsciiDoc file.", "d00d36bf-84e6-4004-9e92-0587b6086f8f": "To authenticate to {% data variables.product.prodname_registry %} using Gradle Groovy or Kotlin DSL, you can replace the placeholders in the provided examples with your specific credentials and repository information. For Gradle Groovy, you can add the necessary configuration to your _build.gradle_ file, and for Kotlin DSL, you can add the configuration to your _build.gradle.kts_ file. The examples provided in the context information include replacing REGISTRY_URL with the URL for your instance's Maven registry, USERNAME with your {% data variables.product.prodname_dotcom %} username, TOKEN with your {% data variables.product.pat_v1 %}, REPOSITORY with the name of the repository containing the package you want to publish, and OWNER with the name of the personal account or organization on {% data variables.product.prodname_dotcom %} that owns the repository. Note that uppercase letters aren't supported for the repository owner, so you should use lowercase letters even if the {% data variables.product.prodname_dotcom %} user or organization name contains uppercase letters.", "9ba5cf0a-fc97-49d0-bd3c-3d226ff2e439": "To access the settings tab and manage seats for your enterprise account on GitHub, follow these steps:  1. Log in to your GitHub account and go to your enterprise organization. 2. Click on the organization's name to open the organization settings. 3. In the left-hand menu, click on \"Billing\" to access the billing tab. 4. Click on \"Manage seats\" to view and manage the number of seats available for your organization.  If you're having trouble accessing the settings tab or managing seats, you may encounter a 500 error. In this case, refer to the troubleshooting section provided in the context information.  Alternatively, if your enterprise account is invoiced, you may need to contact GitHub Sales to add seats. This information is provided in a note in the context information.  I hope this helps! Let me know if you have any further questions.", "9dd9dbb9-7bbf-4910-978b-cc07799e3448": "The \"--logdir\" option can be used to write detailed logs to a specific directory during the execution of the CLI by specifying the desired directory as the value of the option. This will cause the CLI to write detailed logs to one or more files in the given directory, with generated names that include timestamps and the name of the running subcommand. This option allows for more fine-grained control over log file names and locations, as opposed to writing logs to standard error or a default directory."}