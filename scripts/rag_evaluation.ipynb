{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import yaml\n",
    "\n",
    "\n",
    "from extract import DocumentProcessor\n",
    "from qa_dataset_manager import QADatasetManager\n",
    "from rag import rag_manager\n",
    "\n",
    "# Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "#LLM\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# search\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"IPython magic command\" to automatically reload any module whose\n",
    "# implementation has been modified during the execution of the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hugging Face token\n",
    "with open('../config.yaml', 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "hugging_face_api_key = config['huggingface']['token_api']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract documents from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files from folder: account-and-profile\n",
      "Extracting files from folder: actions\n",
      "Extracting files from folder: admin\n",
      "Extracting files from folder: apps\n",
      "Extracting files from folder: authentication\n",
      "Extracting files from folder: billing\n",
      "Extracting files from folder: code-security\n",
      "Extracting files from folder: codespaces\n",
      "Extracting files from folder: communities\n",
      "Extracting files from folder: contributing\n",
      "Extracting files from folder: copilot\n",
      "Extracting files from folder: desktop\n",
      "Extracting files from folder: discussions\n",
      "Extracting files from folder: education\n",
      "Extracting files from folder: get-started\n",
      "Extracting files from folder: github-cli\n",
      "Extracting files from folder: graphql\n",
      "Extracting files from folder: index.md\n",
      "Extracting files from folder: issues\n",
      "Extracting files from folder: migrations\n",
      "Extracting files from folder: organizations\n",
      "Extracting files from folder: packages\n",
      "Extracting files from folder: pages\n",
      "Extracting files from folder: pull-requests\n",
      "Extracting files from folder: README.md\n",
      "Extracting files from folder: repositories\n",
      "Extracting files from folder: rest\n",
      "Extracting files from folder: search\n",
      "Extracting files from folder: search-github\n",
      "Extracting files from folder: site-policy\n",
      "Extracting files from folder: sponsors\n",
      "Extracting files from folder: support\n",
      "Extracting files from folder: video-transcripts\n",
      "Extracting files from folder: webhooks\n"
     ]
    }
   ],
   "source": [
    "# Extract markdown documents and store them in \"data/documents.csv\"\n",
    "processor = DocumentProcessor( root_dir='../content', output_path='../data/documents.csv')\n",
    "processor.process_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA Dataset from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = InferenceClient(token=hugging_face_api_key)\n",
    "model_zephyr =\"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "model_open = \"openchat/openchat_3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nChoosing how to unsubscribe\\n\\nTo unwatch ...</td>\n",
       "      <td>managing-your-subscriptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nDiagnosing why you receive too many notifi...</td>\n",
       "      <td>viewing-your-subscriptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\nNotifications and subscriptions\\n\\nYou can...</td>\n",
       "      <td>about-notifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nNotification delivery options\\n\\nYou can r...</td>\n",
       "      <td>configuring-notifications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\nStarting your inbox triage\\n\\nBefore you s...</td>\n",
       "      <td>customizing-a-workflow-for-triaging-your-notif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  \\n\\nChoosing how to unsubscribe\\n\\nTo unwatch ...   \n",
       "1  \\n\\nDiagnosing why you receive too many notifi...   \n",
       "2  \\n\\nNotifications and subscriptions\\n\\nYou can...   \n",
       "3  \\n\\nNotification delivery options\\n\\nYou can r...   \n",
       "4  \\n\\nStarting your inbox triage\\n\\nBefore you s...   \n",
       "\n",
       "                                               title  \n",
       "0                        managing-your-subscriptions  \n",
       "1                         viewing-your-subscriptions  \n",
       "2                                about-notifications  \n",
       "3                          configuring-notifications  \n",
       "4  customizing-a-workflow-for-triaging-your-notif...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/documents.csv\")\n",
    "data = df['content'].to_list()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_manager = QADatasetManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_index': 755,\n",
       " 'last_updated': '2023-12-23 21:47',\n",
       " 'creation_date': '2023-12-21 15:40'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641 - Number of chunks in text 259: 2\n",
      "642 - Number of chunks in text 1340: 2\n",
      "643 - Number of chunks in text 281: 1\n",
      "644 - Number of chunks in text 1126: 4\n",
      "645 - Number of chunks in text 178: 7\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "646 - Number of chunks in text 711: 2\n",
      "647 - Number of chunks in text 554: 1\n",
      "648 - Number of chunks in text 1257: 3\n",
      "649 - Number of chunks in text 1111: 3\n",
      "650 - Number of chunks in text 2113: 5\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "651 - Number of chunks in text 946: 1\n",
      "652 - Number of chunks in text 519: 2\n",
      "653 - Number of chunks in text 2069: 27\n",
      "654 - Number of chunks in text 1529: 1\n",
      "655 - Number of chunks in text 382: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "656 - Number of chunks in text 1539: 1\n",
      "657 - Number of chunks in text 991: 3\n",
      "658 - Number of chunks in text 862: 8\n",
      "659 - Number of chunks in text 148: 1\n",
      "660 - Number of chunks in text 480: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "661 - Number of chunks in text 564: 4\n",
      "Error encountered: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta (Request ID: nN8Q-7dZ42qzfUCiI8duv)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n",
      "Too Many Requests. Retrying after 30 minutes...\n",
      "Error encountered: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta (Request ID: 59pKTZckGhafWNyyFCktH)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n",
      "Too Many Requests. Retrying after 30 minutes...\n",
      "662 - Number of chunks in text 2087: 1\n",
      "663 - Number of chunks in text 219: 2\n",
      "664 - Number of chunks in text 1298: 1\n",
      "665 - Number of chunks in text 1535: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "666 - Number of chunks in text 355: 2\n",
      "667 - Number of chunks in text 1127: 2\n",
      "668 - Number of chunks in text 2011: 4\n",
      "669 - Number of chunks in text 668: 1\n",
      "670 - Number of chunks in text 601: 2\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "671 - Number of chunks in text 905: 3\n",
      "672 - Number of chunks in text 1366: 1\n",
      "673 - Number of chunks in text 1350: 6\n",
      "674 - Number of chunks in text 1368: 1\n",
      "675 - Number of chunks in text 118: 8\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "676 - Number of chunks in text 572: 2\n",
      "677 - Number of chunks in text 1692: 2\n",
      "678 - Number of chunks in text 145: 1\n",
      "679 - Number of chunks in text 1364: 5\n",
      "680 - Number of chunks in text 132: 7\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "681 - Number of chunks in text 1095: 5\n",
      "682 - Number of chunks in text 1177: 3\n",
      "683 - Number of chunks in text 1878: 1\n",
      "684 - Number of chunks in text 1235: 3\n",
      "685 - Number of chunks in text 158: 10\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "686 - Number of chunks in text 1676: 1\n",
      "687 - Number of chunks in text 114: 4\n",
      "688 - Number of chunks in text 583: 4\n",
      "689 - Number of chunks in text 1583: 3\n",
      "690 - Number of chunks in text 69: 3\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "691 - Number of chunks in text 1174: 2\n",
      "692 - Number of chunks in text 418: 3\n",
      "693 - Number of chunks in text 1588: 2\n",
      "694 - Number of chunks in text 1626: 1\n",
      "695 - Number of chunks in text 980: 5\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "696 - Number of chunks in text 683: 3\n",
      "697 - Number of chunks in text 1850: 1\n",
      "698 - Number of chunks in text 557: 2\n",
      "699 - Number of chunks in text 1415: 1\n",
      "700 - Number of chunks in text 624: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "701 - Number of chunks in text 1911: 9\n",
      "702 - Number of chunks in text 1056: 1\n",
      "703 - Number of chunks in text 234: 8\n",
      "704 - Number of chunks in text 1207: 2\n",
      "705 - Number of chunks in text 2066: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "706 - Number of chunks in text 503: 1\n",
      "707 - Number of chunks in text 1877: 1\n",
      "708 - Number of chunks in text 1996: 5\n",
      "709 - Number of chunks in text 320: 4\n",
      "710 - Number of chunks in text 1821: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "711 - Number of chunks in text 1335: 1\n",
      "712 - Number of chunks in text 44: 1\n",
      "713 - Number of chunks in text 804: 2\n",
      "714 - Number of chunks in text 425: 1\n",
      "715 - Number of chunks in text 1222: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "716 - Number of chunks in text 1866: 1\n",
      "717 - Number of chunks in text 1515: 2\n",
      "718 - Number of chunks in text 8: 1\n",
      "719 - Number of chunks in text 1155: 1\n",
      "720 - Number of chunks in text 287: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "721 - Number of chunks in text 636: 1\n",
      "722 - Number of chunks in text 257: 1\n",
      "723 - Number of chunks in text 712: 1\n",
      "724 - Number of chunks in text 1194: 3\n",
      "725 - Number of chunks in text 1240: 2\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "726 - Number of chunks in text 852: 1\n",
      "727 - Number of chunks in text 854: 10\n",
      "728 - Number of chunks in text 2028: 1\n",
      "729 - Number of chunks in text 394: 1\n",
      "730 - Number of chunks in text 1096: 1\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "731 - Number of chunks in text 1533: 7\n",
      "732 - Number of chunks in text 782: 1\n",
      "733 - Number of chunks in text 925: 2\n",
      "734 - Number of chunks in text 1081: 3\n",
      "735 - Number of chunks in text 161: 9\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "736 - Number of chunks in text 485: 1\n",
      "737 - Number of chunks in text 261: 1\n",
      "738 - Number of chunks in text 1660: 1\n",
      "739 - Number of chunks in text 473: 3\n",
      "740 - Number of chunks in text 1698: 3\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "741 - Number of chunks in text 451: 3\n",
      "742 - Number of chunks in text 2022: 2\n",
      "743 - Number of chunks in text 1076: 9\n",
      "744 - Number of chunks in text 302: 2\n",
      "745 - Number of chunks in text 718: 2\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "746 - Number of chunks in text 1590: 1\n",
      "747 - Number of chunks in text 17: 2\n",
      "748 - Number of chunks in text 84: 1\n",
      "749 - Number of chunks in text 330: 9\n",
      "750 - Number of chunks in text 612: 4\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "751 - Number of chunks in text 1826: 1\n",
      "752 - Number of chunks in text 351: 1\n",
      "753 - Number of chunks in text 2090: 1\n",
      "754 - Number of chunks in text 1762: 7\n",
      "755 - Number of chunks in text 1482: 3\n",
      "Updating QA dataset\n",
      "Saved questions to --> ../data/qa_dataset_intermed.json\n",
      "Updated metadata\n",
      "756 - Number of chunks in text 902: 7\n",
      "757 - Number of chunks in text 1982: 1\n",
      "758 - Number of chunks in text 577: 3\n",
      "759 - Number of chunks in text 183: 17\n",
      "Error encountered: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta (Request ID: jlpxUd1SNTiZcBApzaX7O)\n",
      "\n",
      "Rate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate\n",
      "Too Many Requests. Retrying after 30 minutes...\n"
     ]
    }
   ],
   "source": [
    "dataset_manager.create_qa_pairs(\n",
    "    texts=data,\n",
    "    client=InferenceClient(token=hugging_face_api_key),\n",
    "    model= model_zephyr,\n",
    "    max_new_tokens=200,\n",
    "    num_questions_per_chunk=2,\n",
    "    chunk_size = 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added questions from ../data/qa_dataset_intermed.json to --> ../data/qa_dataset.json\n"
     ]
    }
   ],
   "source": [
    "dataset_manager.add_qa_to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = dataset_manager.get_qa_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7160, 7160)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_dataset['queries']), len(qa_dataset['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 to be answered\n",
      "Answering qery b6432350-03b2-4f4f-8664-098d526ba9ef\n",
      "query:  How can I generate a\n",
      "Answering qery 8c788494-6cb4-41e6-90ef-facca8249af6\n",
      "query:  How can a status report be posted without taking any other action on {% data variables.product.prodname_dotcom_the_website %} during a deployment\n",
      "Updating answers dict\n",
      "Saved questions to --> ../data/answers_intermed.json\n"
     ]
    }
   ],
   "source": [
    "dataset_manager.create_answers(qa_dataset=qa_dataset,\n",
    "                               client=InferenceClient(token=hugging_face_api_key),\n",
    "                               model= model_zephyr,\n",
    "                               max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added answers from ../data/answers_intermed.json to --> ../data/qa_dataset.json\n"
     ]
    }
   ],
   "source": [
    "dataset_manager.add_answers_to_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FAISS indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = InferenceClient(token=hugging_face_api_key)\n",
    "model_zephyr =\"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "model_open = \"openchat/openchat_3.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents have been divided into 6014 chunks\n"
     ]
    }
   ],
   "source": [
    "nodes = dataset_manager.parse_documents(texts=data,chunk_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = rag_manager(nodes=nodes, client=InferenceClient(token=hugging_face_api_key), llm_model=model_zephyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_L6_v2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created\n"
     ]
    }
   ],
   "source": [
    "index_flatIP, embeddings = rag_chain.create_index(model=model_L6_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly saved index to ../data/indexes/index_flatIP\n"
     ]
    }
   ],
   "source": [
    "rag_chain.save_index(index_flatIP, '../data/indexes/index_flatIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents have been divided into 6014 chunks\n"
     ]
    }
   ],
   "source": [
    "inference = InferenceClient(token=hugging_face_api_key)\n",
    "model_zephyr =\"HuggingFaceH4/zephyr-7b-beta\"\n",
    "model_mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_falcon = \"tiiuae/falcon-7b-instruct\"\n",
    "model_open = \"openchat/openchat_3.5\"\n",
    "model_L6_v2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512)\n",
    "\n",
    "dataset_manager = QADatasetManager()\n",
    "df = pd.read_csv(\"../data/documents.csv\")\n",
    "data = df['content'].to_list()\n",
    "nodes = dataset_manager.parse_documents(texts=data,chunk_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = rag_manager(nodes=nodes, client=InferenceClient(token=hugging_face_api_key), llm_model=model_zephyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly loaded index from ../data/indexes/index_flatIP_L6_V2\n"
     ]
    }
   ],
   "source": [
    "index = rag_chain.load_index('../data/indexes/index_flatIP_L6_V2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_test = [\"How do you filter notifications?\", \"What are codespaces ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totaltime: 0.16382503509521484\n",
      "\n",
      "Answering query: How do you filter notifications?\n",
      "Answering query: What are codespaces ?\n"
     ]
    }
   ],
   "source": [
    "answers, contexts = rag_chain.augmented_retrieval_generation(queries=queries_test, index=index,embedding_model=model_L6_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = dataset_manager.get_qa_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4837"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts = ['how', 'when', 'who', 'what']\n",
    "filtered_questions = [\n",
    "    (key, question) for key, question in qa_dataset['queries'].items()\n",
    "    if 'example' not in question.lower() and 'context' not in question.lower() and len(question) > 20\n",
    "    and any(question.lower().startswith(start) for start in starts)\n",
    "]\n",
    "\n",
    "# Extracting keys and questions separately\n",
    "selected_keys = [key for key, _ in filtered_questions]\n",
    "questions = [question for _, question in filtered_questions]\n",
    "answers = [qa_dataset['answers'].get(key) for key in selected_keys]\n",
    "relevant_docs = [qa_dataset['relevant_docs'].get(key)[0] for key in selected_keys]\n",
    "\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total search time: 0.1949141025543213\n",
      "\n",
      "Answering query: How can code review assignments be configured to automatically request code owners for review, while still allowing for individual review requests to be made in addition to the team request\n",
      "Answering query: What is the process for privately collaborating to fix a vulnerability in a temporary private fork, and why might this be necessary\n",
      "Answering query: What is the process for publishing a paid plan for an app on {% data variables.product.prodname_marketplace %}\n",
      "Answering query: How can I recover access to a GitHub account that is locked due to two-factor authentication (2FA) if I have lost my 2FA credentials and am unable to recover them? What alternative options are available to me in this situation?\n",
      "Answering query: How can I configure the `publishConfig` fields in my `package.json` file to limit publishing to a specific registry\n",
      "Answering query: How can I write detailed logs to a specific directory during the execution of the CLI\n",
      "Answering query: How can configuration variables be defined at different levels in GitHub Actions, and what is the precedence order for variables with the same name at multiple levels?\n",
      "Answering query: What is the name of the feature that allows for fine-grained permissions and short-lived tokens when using a\n",
      "Answering query: What is the purpose of the {% data variables.code-scanning.tool_status_page %} and how can it help in debugging code scanning issues?\n",
      "Answering query: How can I authenticate to GitHub Enterprise Server Container registries using the CLI tool provided\n"
     ]
    }
   ],
   "source": [
    "rd.seed(12345)\n",
    "q_ids = list(range(len(questions)))\n",
    "rd.shuffle(q_ids)\n",
    "\n",
    "\n",
    "questions = np.array(questions)[q_ids][:10].tolist()\n",
    "ground_truths = np.array(answers)[q_ids][:10].tolist()\n",
    "relevant_docs = np.array(relevant_docs)[q_ids][:10].tolist()\n",
    "\n",
    "answers, contexts, contexts_ids = rag_chain.augmented_retrieval_generation(queries=questions, index=index,embedding_model=model_L6_v2, k=10)\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"questions\": questions,\n",
    "    \"answers\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"contexts_ids\": contexts_ids,\n",
    "    \"ground_truths\": ground_truths,\n",
    "    \"relevant_docs\": relevant_docs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total search time: 82.64047455787659\n",
      "\n",
      "average search time per query: 0.017087084709279467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contexts, contexts_ids, search_time = rag_chain.retrieve_context(index=index, embedding_model=model_L6_v2, queries=questions, k=10)\n",
    "\n",
    "data = {\n",
    "    \"questions\": questions,\n",
    "    #\"answers\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"contexts_ids\": contexts_ids,\n",
    "   # \"ground_truths\": ground_truths,\n",
    "    \"relevant_docs\": relevant_docs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = rag_chain.evaluate_rag(metrics=['mrr', 'hit'], data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m scores\u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m k_range :\n\u001b[1;32m----> 4\u001b[0m     contexts, contexts_ids, search_time \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_L6_v2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m\"\u001b[39m: questions,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts\u001b[39m\u001b[38;5;124m\"\u001b[39m: contexts,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontexts_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: contexts_ids,\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevant_docs\u001b[39m\u001b[38;5;124m\"\u001b[39m: relevant_docs\n\u001b[0;32m     10\u001b[0m     }\n\u001b[0;32m     11\u001b[0m     scores_k \u001b[38;5;241m=\u001b[39m rag_chain\u001b[38;5;241m.\u001b[39mevaluate_rag(metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmrr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhit\u001b[39m\u001b[38;5;124m'\u001b[39m], data \u001b[38;5;241m=\u001b[39m data)\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\scripts\\rag.py:111\u001b[0m, in \u001b[0;36mrag_manager.retrieve_context\u001b[1;34m(self, index, embedding_model, queries, k)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m    107\u001b[0m                      index : faiss\u001b[38;5;241m.\u001b[39mIndex, \n\u001b[0;32m    108\u001b[0m                      embedding_model : SentenceTransformer, \n\u001b[0;32m    109\u001b[0m                      queries : List[\u001b[38;5;28mstr\u001b[39m], \n\u001b[0;32m    110\u001b[0m                      k : \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], List[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m--> 111\u001b[0m     search_results, search_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     contexts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[search_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindexes\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m    113\u001b[0m     contexts_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids[search_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindexes\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\scripts\\rag.py:80\u001b[0m, in \u001b[0;36mrag_manager.search\u001b[1;34m(self, index, model, queries, k)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     74\u001b[0m            index : faiss\u001b[38;5;241m.\u001b[39mIndex, \n\u001b[0;32m     75\u001b[0m            model : SentenceTransformer, \n\u001b[0;32m     76\u001b[0m            queries : List[\u001b[38;5;28mstr\u001b[39m], \n\u001b[0;32m     77\u001b[0m            k : \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m], \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m     79\u001b[0m     t\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 80\u001b[0m     query_vector \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     81\u001b[0m     faiss\u001b[38;5;241m.\u001b[39mnormalize_L2(query_vector)\n\u001b[0;32m     83\u001b[0m     similarities, similarities_ids \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39msearch(query_vector, k)\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    417\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 425\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    434\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    435\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:284\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    276\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    283\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 284\u001b[0m     mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jjdes\\Desktop\\RAG_chatbot\\rag_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k_range = [3,5,10,20]\n",
    "scores= {}\n",
    "for k in k_range :\n",
    "    contexts, contexts_ids, search_time = rag_chain.retrieve_context(index=index, embedding_model=model_L6_v2, queries=questions, k=k)\n",
    "    data = {\n",
    "        \"questions\": questions,\n",
    "        \"contexts\": contexts,\n",
    "        \"contexts_ids\": contexts_ids,\n",
    "        \"relevant_docs\": relevant_docs\n",
    "    }\n",
    "    scores_k = rag_chain.evaluate_rag(metrics=['mrr', 'hit'], data = data)\n",
    "    scores_k['search_time'] = search_time\n",
    "    scores[k] = scores_k\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
